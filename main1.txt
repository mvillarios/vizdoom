/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.8     |
|    ep_rew_mean      | 74.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3109     |
|    time_elapsed     | 0        |
|    total_timesteps  | 367      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.4     |
|    ep_rew_mean      | 54.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 3138     |
|    time_elapsed     | 0        |
|    total_timesteps  | 491      |
----------------------------------
Eval num_timesteps=500, episode_reward=-11.60 +/- 0.00
Episode length: 53.04 +/- 19.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53       |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-11.60 +/- 0.00
Episode length: 54.74 +/- 21.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
Eval num_timesteps=1500, episode_reward=-11.59 +/- 0.00
Episode length: 49.76 +/- 19.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 79       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 12       |
|    fps              | 305      |
|    time_elapsed     | 5        |
|    total_timesteps  | 1623     |
----------------------------------
Eval num_timesteps=2000, episode_reward=-11.59 +/- 0.00
Episode length: 47.32 +/- 16.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
Eval num_timesteps=2500, episode_reward=-11.59 +/- 0.00
Episode length: 54.10 +/- 17.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.1     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 177      |
|    ep_rew_mean      | 90.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 326      |
|    time_elapsed     | 8        |
|    total_timesteps  | 2828     |
----------------------------------
Eval num_timesteps=3000, episode_reward=-11.60 +/- 0.00
Episode length: 47.14 +/- 14.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 157      |
|    ep_rew_mean      | 87.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 20       |
|    fps              | 311      |
|    time_elapsed     | 10       |
|    total_timesteps  | 3139     |
----------------------------------
Eval num_timesteps=3500, episode_reward=-11.59 +/- 0.01
Episode length: 52.44 +/- 22.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.4     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 163      |
|    ep_rew_mean      | 81.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 24       |
|    fps              | 331      |
|    time_elapsed     | 11       |
|    total_timesteps  | 3914     |
----------------------------------
Eval num_timesteps=4000, episode_reward=-11.59 +/- 0.01
Episode length: 51.16 +/- 14.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
Eval num_timesteps=4500, episode_reward=-11.60 +/- 0.00
Episode length: 51.74 +/- 18.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-11.60 +/- 0.00
Episode length: 51.12 +/- 12.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.1     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 87.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 28       |
|    fps              | 307      |
|    time_elapsed     | 16       |
|    total_timesteps  | 5065     |
----------------------------------
Eval num_timesteps=5500, episode_reward=-11.59 +/- 0.01
Episode length: 48.68 +/- 19.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 89.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 32       |
|    fps              | 319      |
|    time_elapsed     | 18       |
|    total_timesteps  | 5776     |
----------------------------------
Eval num_timesteps=6000, episode_reward=-11.60 +/- 0.00
Episode length: 54.74 +/- 15.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
Eval num_timesteps=6500, episode_reward=-11.59 +/- 0.00
Episode length: 51.44 +/- 18.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 93.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 36       |
|    fps              | 325      |
|    time_elapsed     | 21       |
|    total_timesteps  | 6964     |
----------------------------------
Eval num_timesteps=7000, episode_reward=-11.59 +/- 0.00
Episode length: 52.30 +/- 21.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.3     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 89.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 40       |
|    fps              | 321      |
|    time_elapsed     | 23       |
|    total_timesteps  | 7395     |
----------------------------------
Eval num_timesteps=7500, episode_reward=-11.59 +/- 0.01
Episode length: 52.12 +/- 18.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
Eval num_timesteps=8000, episode_reward=-11.59 +/- 0.01
Episode length: 52.18 +/- 14.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
Eval num_timesteps=8500, episode_reward=-11.59 +/- 0.00
Episode length: 48.72 +/- 17.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | 95.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 44       |
|    fps              | 323      |
|    time_elapsed     | 27       |
|    total_timesteps  | 8997     |
----------------------------------
Eval num_timesteps=9000, episode_reward=-11.59 +/- 0.01
Episode length: 49.86 +/- 17.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 91.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 48       |
|    fps              | 315      |
|    time_elapsed     | 29       |
|    total_timesteps  | 9262     |
----------------------------------
Eval num_timesteps=9500, episode_reward=-11.59 +/- 0.01
Episode length: 50.84 +/- 18.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-11.59 +/- 0.00
Episode length: 49.82 +/- 15.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 90.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 52       |
|    fps              | 309      |
|    time_elapsed     | 32       |
|    total_timesteps  | 10053    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.889    |
|    n_updates        | 13       |
----------------------------------
Eval num_timesteps=10500, episode_reward=-11.59 +/- 0.00
Episode length: 46.48 +/- 13.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.114    |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 89.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 56       |
|    fps              | 313      |
|    time_elapsed     | 34       |
|    total_timesteps  | 10745    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.4      |
|    n_updates        | 186      |
----------------------------------
Eval num_timesteps=11000, episode_reward=-17.77 +/- 2.62
Episode length: 46.58 +/- 15.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | -17.8    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0236   |
|    n_updates        | 249      |
----------------------------------
Eval num_timesteps=11500, episode_reward=-7.75 +/- 19.32
Episode length: 53.22 +/- 27.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.2     |
|    mean_reward      | -7.75    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.868    |
|    n_updates        | 374      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 90.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 60       |
|    fps              | 311      |
|    time_elapsed     | 38       |
|    total_timesteps  | 11879    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 469      |
----------------------------------
Eval num_timesteps=12000, episode_reward=11.47 +/- 63.54
Episode length: 97.66 +/- 143.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.7     |
|    mean_reward      | 11.5     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0706   |
|    n_updates        | 499      |
----------------------------------
New best mean reward!
Eval num_timesteps=12500, episode_reward=-11.59 +/- 0.01
Episode length: 48.52 +/- 15.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.792    |
|    n_updates        | 624      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 89.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 64       |
|    fps              | 293      |
|    time_elapsed     | 43       |
|    total_timesteps  | 12713    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.786    |
|    n_updates        | 678      |
----------------------------------
Eval num_timesteps=13000, episode_reward=-11.59 +/- 0.01
Episode length: 50.80 +/- 15.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 85.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 68       |
|    fps              | 288      |
|    time_elapsed     | 45       |
|    total_timesteps  | 13056    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.902    |
|    n_updates        | 763      |
----------------------------------
Eval num_timesteps=13500, episode_reward=-11.59 +/- 0.01
Episode length: 50.58 +/- 16.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.6     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.793    |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 85.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 72       |
|    fps              | 292      |
|    time_elapsed     | 47       |
|    total_timesteps  | 13854    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.02     |
|    n_updates        | 963      |
----------------------------------
Eval num_timesteps=14000, episode_reward=50.80 +/- 46.98
Episode length: 104.36 +/- 71.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 50.8     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.91     |
|    n_updates        | 999      |
----------------------------------
New best mean reward!
Eval num_timesteps=14500, episode_reward=21.23 +/- 45.75
Episode length: 47.80 +/- 15.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.8     |
|    mean_reward      | 21.2     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0204   |
|    n_updates        | 1124     |
----------------------------------
Eval num_timesteps=15000, episode_reward=76.35 +/- 4.60
Episode length: 62.96 +/- 23.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63       |
|    mean_reward      | 76.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0318   |
|    n_updates        | 1249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 88.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 76       |
|    fps              | 277      |
|    time_elapsed     | 54       |
|    total_timesteps  | 15102    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.32     |
|    n_updates        | 1275     |
----------------------------------
Eval num_timesteps=15500, episode_reward=21.73 +/- 46.43
Episode length: 52.36 +/- 20.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.4     |
|    mean_reward      | 21.7     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0235   |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | 87.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 80       |
|    fps              | 281      |
|    time_elapsed     | 56       |
|    total_timesteps  | 15964    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.873    |
|    n_updates        | 1490     |
----------------------------------
Eval num_timesteps=16000, episode_reward=76.13 +/- 4.80
Episode length: 59.68 +/- 22.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.7     |
|    mean_reward      | 76.1     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0232   |
|    n_updates        | 1499     |
----------------------------------
Eval num_timesteps=16500, episode_reward=152.91 +/- 50.35
Episode length: 353.76 +/- 228.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 354      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.868    |
|    n_updates        | 1624     |
----------------------------------
New best mean reward!
Eval num_timesteps=17000, episode_reward=96.27 +/- 36.02
Episode length: 102.94 +/- 142.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 96.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0482   |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 89.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 84       |
|    fps              | 238      |
|    time_elapsed     | 72       |
|    total_timesteps  | 17245    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.806    |
|    n_updates        | 1811     |
----------------------------------
Eval num_timesteps=17500, episode_reward=87.41 +/- 0.00
Episode length: 66.06 +/- 22.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.1     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.108    |
|    n_updates        | 1874     |
----------------------------------
Eval num_timesteps=18000, episode_reward=113.97 +/- 70.64
Episode length: 229.22 +/- 231.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 114      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0398   |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 89.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 220      |
|    time_elapsed     | 82       |
|    total_timesteps  | 18126    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.81     |
|    n_updates        | 2031     |
----------------------------------
Eval num_timesteps=18500, episode_reward=180.24 +/- 8.28
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.804    |
|    n_updates        | 2124     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 89.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 92       |
|    fps              | 192      |
|    time_elapsed     | 98       |
|    total_timesteps  | 18987    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0253   |
|    n_updates        | 2246     |
----------------------------------
Eval num_timesteps=19000, episode_reward=161.51 +/- 75.62
Episode length: 449.76 +/- 172.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 450      |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.845    |
|    n_updates        | 2249     |
----------------------------------
Eval num_timesteps=19500, episode_reward=-1.49 +/- 36.76
Episode length: 58.00 +/- 69.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58       |
|    mean_reward      | -1.49    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.795    |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 89.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 171      |
|    time_elapsed     | 114      |
|    total_timesteps  | 19642    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.121    |
|    n_updates        | 2410     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-9.61 +/- 13.86
Episode length: 47.02 +/- 18.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.864    |
|    n_updates        | 2499     |
----------------------------------
Eval num_timesteps=20500, episode_reward=195.69 +/- 1.32
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 2624     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 90       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 156      |
|    time_elapsed     | 131      |
|    total_timesteps  | 20596    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0381   |
|    n_updates        | 2648     |
----------------------------------
Eval num_timesteps=21000, episode_reward=70.25 +/- 87.79
Episode length: 181.48 +/- 214.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 70.2     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0423   |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 90.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 104      |
|    fps              | 155      |
|    time_elapsed     | 137      |
|    total_timesteps  | 21268    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.138    |
|    n_updates        | 2816     |
----------------------------------
Eval num_timesteps=21500, episode_reward=-11.58 +/- 0.13
Episode length: 45.70 +/- 15.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 2874     |
----------------------------------
Eval num_timesteps=22000, episode_reward=51.61 +/- 82.13
Episode length: 145.66 +/- 190.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 51.6     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0582   |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 93.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 108      |
|    fps              | 156      |
|    time_elapsed     | 143      |
|    total_timesteps  | 22468    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 3116     |
----------------------------------
Eval num_timesteps=22500, episode_reward=188.98 +/- 8.54
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.81     |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 91       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 112      |
|    fps              | 143      |
|    time_elapsed     | 158      |
|    total_timesteps  | 22680    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 3169     |
----------------------------------
Eval num_timesteps=23000, episode_reward=-9.62 +/- 13.86
Episode length: 54.04 +/- 15.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54       |
|    mean_reward      | -9.62    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.794    |
|    n_updates        | 3249     |
----------------------------------
Eval num_timesteps=23500, episode_reward=-11.59 +/- 0.00
Episode length: 49.86 +/- 15.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0791   |
|    n_updates        | 3374     |
----------------------------------
Eval num_timesteps=24000, episode_reward=43.65 +/- 80.60
Episode length: 136.68 +/- 182.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 43.6     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.6      |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 91       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 116      |
|    fps              | 146      |
|    time_elapsed     | 166      |
|    total_timesteps  | 24305    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.296    |
|    n_updates        | 3576     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 89.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 120      |
|    fps              | 146      |
|    time_elapsed     | 166      |
|    total_timesteps  | 24477    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.894    |
|    n_updates        | 3619     |
----------------------------------
Eval num_timesteps=24500, episode_reward=194.41 +/- 5.54
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.19     |
|    n_updates        | 3624     |
----------------------------------
Eval num_timesteps=25000, episode_reward=197.21 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.114    |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 90.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 124      |
|    fps              | 128      |
|    time_elapsed     | 196      |
|    total_timesteps  | 25167    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.846    |
|    n_updates        | 3791     |
----------------------------------
Eval num_timesteps=25500, episode_reward=192.38 +/- 21.56
Episode length: 505.86 +/- 93.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.278    |
|    n_updates        | 3874     |
----------------------------------
Eval num_timesteps=26000, episode_reward=197.19 +/- 0.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.45     |
|    n_updates        | 3999     |
----------------------------------
Eval num_timesteps=26500, episode_reward=185.99 +/- 32.87
Episode length: 476.72 +/- 144.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.173    |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 89.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 128      |
|    fps              | 110      |
|    time_elapsed     | 240      |
|    total_timesteps  | 26678    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 4169     |
----------------------------------
Eval num_timesteps=27000, episode_reward=192.59 +/- 6.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 4249     |
----------------------------------
Eval num_timesteps=27500, episode_reward=194.47 +/- 15.33
Episode length: 515.12 +/- 69.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.949    |
|    n_updates        | 4374     |
----------------------------------
Eval num_timesteps=28000, episode_reward=4.24 +/- 36.29
Episode length: 52.98 +/- 18.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53       |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 91.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 132      |
|    fps              | 103      |
|    time_elapsed     | 273      |
|    total_timesteps  | 28286    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.298    |
|    n_updates        | 4571     |
----------------------------------
Eval num_timesteps=28500, episode_reward=196.49 +/- 1.04
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.177    |
|    n_updates        | 4624     |
----------------------------------
Eval num_timesteps=29000, episode_reward=191.74 +/- 21.32
Episode length: 505.40 +/- 96.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.302    |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 90.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 136      |
|    fps              | 96       |
|    time_elapsed     | 304      |
|    total_timesteps  | 29289    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.161    |
|    n_updates        | 4822     |
----------------------------------
Eval num_timesteps=29500, episode_reward=196.29 +/- 1.70
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.164    |
|    n_updates        | 4874     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-11.59 +/- 0.01
Episode length: 43.52 +/- 13.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.923    |
|    n_updates        | 4999     |
----------------------------------
Eval num_timesteps=30500, episode_reward=185.88 +/- 6.57
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.295    |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 235      |
|    ep_rew_mean      | 93.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 140      |
|    fps              | 91       |
|    time_elapsed     | 336      |
|    total_timesteps  | 30920    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.874    |
|    n_updates        | 5229     |
----------------------------------
Eval num_timesteps=31000, episode_reward=196.59 +/- 1.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.876    |
|    n_updates        | 5249     |
----------------------------------
Eval num_timesteps=31500, episode_reward=196.56 +/- 1.06
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.856    |
|    n_updates        | 5374     |
----------------------------------
Eval num_timesteps=32000, episode_reward=196.75 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.868    |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 230      |
|    ep_rew_mean      | 91.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 144      |
|    fps              | 83       |
|    time_elapsed     | 382      |
|    total_timesteps  | 32039    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 5509     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 231      |
|    ep_rew_mean      | 91.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 148      |
|    fps              | 84       |
|    time_elapsed     | 382      |
|    total_timesteps  | 32321    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.173    |
|    n_updates        | 5580     |
----------------------------------
Eval num_timesteps=32500, episode_reward=196.80 +/- 1.15
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.61     |
|    n_updates        | 5624     |
----------------------------------
Eval num_timesteps=33000, episode_reward=195.01 +/- 4.45
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.982    |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 234      |
|    ep_rew_mean      | 92.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 81       |
|    time_elapsed     | 412      |
|    total_timesteps  | 33490    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.84     |
|    n_updates        | 5872     |
----------------------------------
Eval num_timesteps=33500, episode_reward=196.06 +/- 1.12
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.537    |
|    n_updates        | 5874     |
----------------------------------
Eval num_timesteps=34000, episode_reward=192.29 +/- 21.43
Episode length: 505.56 +/- 95.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.187    |
|    n_updates        | 5999     |
----------------------------------
Eval num_timesteps=34500, episode_reward=195.75 +/- 1.33
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.345    |
|    n_updates        | 6124     |
----------------------------------
Eval num_timesteps=35000, episode_reward=186.04 +/- 8.12
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 6249     |
----------------------------------
Eval num_timesteps=35500, episode_reward=193.55 +/- 15.61
Episode length: 514.92 +/- 70.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.309    |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 248      |
|    ep_rew_mean      | 96.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 156      |
|    fps              | 73       |
|    time_elapsed     | 487      |
|    total_timesteps  | 35590    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.546    |
|    n_updates        | 6397     |
----------------------------------
Eval num_timesteps=36000, episode_reward=146.43 +/- 54.48
Episode length: 310.90 +/- 232.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 311      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.802    |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 246      |
|    ep_rew_mean      | 95.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 160      |
|    fps              | 73       |
|    time_elapsed     | 496      |
|    total_timesteps  | 36476    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.445    |
|    n_updates        | 6618     |
----------------------------------
Eval num_timesteps=36500, episode_reward=196.83 +/- 1.21
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.23     |
|    n_updates        | 6624     |
----------------------------------
Eval num_timesteps=37000, episode_reward=197.20 +/- 0.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.363    |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 246      |
|    ep_rew_mean      | 96.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 164      |
|    fps              | 70       |
|    time_elapsed     | 526      |
|    total_timesteps  | 37304    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.997    |
|    n_updates        | 6825     |
----------------------------------
Eval num_timesteps=37500, episode_reward=196.61 +/- 1.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.63     |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 247      |
|    ep_rew_mean      | 97.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 168      |
|    fps              | 69       |
|    time_elapsed     | 542      |
|    total_timesteps  | 37768    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.12     |
|    n_updates        | 6941     |
----------------------------------
Eval num_timesteps=38000, episode_reward=195.58 +/- 4.23
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.12     |
|    n_updates        | 6999     |
----------------------------------
Eval num_timesteps=38500, episode_reward=18.31 +/- 50.19
Episode length: 61.36 +/- 69.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.4     |
|    mean_reward      | 18.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.298    |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 247      |
|    ep_rew_mean      | 97.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 172      |
|    fps              | 69       |
|    time_elapsed     | 559      |
|    total_timesteps  | 38595    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.272    |
|    n_updates        | 7148     |
----------------------------------
Eval num_timesteps=39000, episode_reward=197.15 +/- 1.30
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.288    |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 240      |
|    ep_rew_mean      | 94.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 176      |
|    fps              | 68       |
|    time_elapsed     | 574      |
|    total_timesteps  | 39107    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.26     |
|    n_updates        | 7276     |
----------------------------------
Eval num_timesteps=39500, episode_reward=197.09 +/- 1.08
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 240      |
|    ep_rew_mean      | 94.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 180      |
|    fps              | 67       |
|    time_elapsed     | 589      |
|    total_timesteps  | 39984    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 7495     |
----------------------------------
Eval num_timesteps=40000, episode_reward=197.10 +/- 1.01
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.692    |
|    n_updates        | 7499     |
----------------------------------
Eval num_timesteps=40500, episode_reward=197.09 +/- 1.17
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.972    |
|    n_updates        | 7624     |
----------------------------------
Eval num_timesteps=41000, episode_reward=197.38 +/- 0.85
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.716    |
|    n_updates        | 7749     |
----------------------------------
New best mean reward!
Eval num_timesteps=41500, episode_reward=189.97 +/- 25.94
Episode length: 495.64 +/- 116.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.41     |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 243      |
|    ep_rew_mean      | 95.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 184      |
|    fps              | 64       |
|    time_elapsed     | 649      |
|    total_timesteps  | 41594    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.953    |
|    n_updates        | 7898     |
----------------------------------
Eval num_timesteps=42000, episode_reward=190.30 +/- 26.01
Episode length: 497.44 +/- 109.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.46     |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 243      |
|    ep_rew_mean      | 94.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 188      |
|    fps              | 63       |
|    time_elapsed     | 664      |
|    total_timesteps  | 42407    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.387    |
|    n_updates        | 8101     |
----------------------------------
Eval num_timesteps=42500, episode_reward=185.70 +/- 32.85
Episode length: 478.00 +/- 141.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.28     |
|    n_updates        | 8124     |
----------------------------------
Eval num_timesteps=43000, episode_reward=196.63 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.749    |
|    n_updates        | 8249     |
----------------------------------
Eval num_timesteps=43500, episode_reward=196.04 +/- 3.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.91     |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 246      |
|    ep_rew_mean      | 95.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 192      |
|    fps              | 61       |
|    time_elapsed     | 708      |
|    total_timesteps  | 43601    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.694    |
|    n_updates        | 8400     |
----------------------------------
Eval num_timesteps=44000, episode_reward=196.03 +/- 3.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3        |
|    n_updates        | 8499     |
----------------------------------
Eval num_timesteps=44500, episode_reward=2.27 +/- 34.35
Episode length: 48.62 +/- 18.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.6     |
|    mean_reward      | 2.27     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.454    |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 252      |
|    ep_rew_mean      | 96.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 196      |
|    fps              | 61       |
|    time_elapsed     | 725      |
|    total_timesteps  | 44803    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.343    |
|    n_updates        | 8700     |
----------------------------------
Eval num_timesteps=45000, episode_reward=197.48 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.679    |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 249      |
|    ep_rew_mean      | 95.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 200      |
|    fps              | 61       |
|    time_elapsed     | 740      |
|    total_timesteps  | 45456    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.624    |
|    n_updates        | 8863     |
----------------------------------
Eval num_timesteps=45500, episode_reward=18.11 +/- 45.37
Episode length: 56.72 +/- 20.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.7     |
|    mean_reward      | 18.1     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.536    |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 245      |
|    ep_rew_mean      | 95.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 204      |
|    fps              | 61       |
|    time_elapsed     | 742      |
|    total_timesteps  | 45771    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.87     |
|    n_updates        | 8942     |
----------------------------------
Eval num_timesteps=46000, episode_reward=195.12 +/- 15.56
Episode length: 514.92 +/- 70.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.812    |
|    n_updates        | 8999     |
----------------------------------
Eval num_timesteps=46500, episode_reward=197.26 +/- 1.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.475    |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 245      |
|    ep_rew_mean      | 94.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 208      |
|    fps              | 60       |
|    time_elapsed     | 772      |
|    total_timesteps  | 46981    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.7      |
|    n_updates        | 9245     |
----------------------------------
Eval num_timesteps=47000, episode_reward=83.83 +/- 35.31
Episode length: 77.66 +/- 94.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 83.8     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.73     |
|    n_updates        | 9249     |
----------------------------------
Eval num_timesteps=47500, episode_reward=197.35 +/- 0.85
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.787    |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 251      |
|    ep_rew_mean      | 96.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 60       |
|    time_elapsed     | 790      |
|    total_timesteps  | 47732    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.16     |
|    n_updates        | 9432     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 236      |
|    ep_rew_mean      | 92.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 216      |
|    fps              | 60       |
|    time_elapsed     | 790      |
|    total_timesteps  | 47930    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.979    |
|    n_updates        | 9482     |
----------------------------------
Eval num_timesteps=48000, episode_reward=197.38 +/- 0.97
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.22     |
|    n_updates        | 9499     |
----------------------------------
Eval num_timesteps=48500, episode_reward=197.14 +/- 0.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.439    |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 245      |
|    ep_rew_mean      | 95.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 59       |
|    time_elapsed     | 820      |
|    total_timesteps  | 48953    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 9738     |
----------------------------------
Eval num_timesteps=49000, episode_reward=120.55 +/- 57.85
Episode length: 212.70 +/- 224.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 121      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.529    |
|    n_updates        | 9749     |
----------------------------------
Eval num_timesteps=49500, episode_reward=196.41 +/- 1.38
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.16     |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 245      |
|    ep_rew_mean      | 94.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 58       |
|    time_elapsed     | 842      |
|    total_timesteps  | 49678    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.355    |
|    n_updates        | 9919     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 233      |
|    ep_rew_mean      | 90.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 59       |
|    time_elapsed     | 842      |
|    total_timesteps  | 49951    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.84     |
|    n_updates        | 9987     |
----------------------------------
Eval num_timesteps=50000, episode_reward=49.97 +/- 52.50
Episode length: 66.30 +/- 68.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.3     |
|    mean_reward      | 50       |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 9999     |
----------------------------------
Eval num_timesteps=50500, episode_reward=188.23 +/- 29.81
Episode length: 486.60 +/- 130.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.32     |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 87.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 232      |
|    fps              | 59       |
|    time_elapsed     | 858      |
|    total_timesteps  | 50704    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.75     |
|    n_updates        | 10175    |
----------------------------------
Eval num_timesteps=51000, episode_reward=197.31 +/- 0.98
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.716    |
|    n_updates        | 10249    |
----------------------------------
Eval num_timesteps=51500, episode_reward=196.77 +/- 1.19
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.884    |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 88.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 236      |
|    fps              | 58       |
|    time_elapsed     | 889      |
|    total_timesteps  | 51974    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.376    |
|    n_updates        | 10493    |
----------------------------------
Eval num_timesteps=52000, episode_reward=195.20 +/- 15.56
Episode length: 515.12 +/- 69.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.2      |
|    n_updates        | 10499    |
----------------------------------
Eval num_timesteps=52500, episode_reward=32.17 +/- 53.58
Episode length: 60.36 +/- 68.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.4     |
|    mean_reward      | 32.2     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.31     |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 86.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 240      |
|    fps              | 58       |
|    time_elapsed     | 906      |
|    total_timesteps  | 52645    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.77     |
|    n_updates        | 10661    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 85.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 244      |
|    fps              | 58       |
|    time_elapsed     | 906      |
|    total_timesteps  | 52947    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.23     |
|    n_updates        | 10736    |
----------------------------------
Eval num_timesteps=53000, episode_reward=197.07 +/- 0.92
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 10749    |
----------------------------------
Eval num_timesteps=53500, episode_reward=197.64 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 10874    |
----------------------------------
New best mean reward!
Eval num_timesteps=54000, episode_reward=196.25 +/- 4.48
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.12     |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 88.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 248      |
|    fps              | 56       |
|    time_elapsed     | 951      |
|    total_timesteps  | 54109    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.615    |
|    n_updates        | 11027    |
----------------------------------
Eval num_timesteps=54500, episode_reward=197.18 +/- 1.12
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.883    |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 87.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 252      |
|    fps              | 56       |
|    time_elapsed     | 966      |
|    total_timesteps  | 54847    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 11211    |
----------------------------------
Eval num_timesteps=55000, episode_reward=196.95 +/- 0.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.292    |
|    n_updates        | 11249    |
----------------------------------
Eval num_timesteps=55500, episode_reward=177.36 +/- 42.26
Episode length: 437.98 +/- 185.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 438      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.34     |
|    n_updates        | 11374    |
----------------------------------
Eval num_timesteps=56000, episode_reward=194.77 +/- 15.49
Episode length: 515.06 +/- 69.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.537    |
|    n_updates        | 11499    |
----------------------------------
Eval num_timesteps=56500, episode_reward=197.35 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.57     |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 86.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 256      |
|    fps              | 55       |
|    time_elapsed     | 1024     |
|    total_timesteps  | 56671    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.653    |
|    n_updates        | 11667    |
----------------------------------
Eval num_timesteps=57000, episode_reward=197.19 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.99     |
|    n_updates        | 11749    |
----------------------------------
Eval num_timesteps=57500, episode_reward=197.48 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.993    |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 88.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 260      |
|    fps              | 54       |
|    time_elapsed     | 1055     |
|    total_timesteps  | 57897    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.09     |
|    n_updates        | 11974    |
----------------------------------
Eval num_timesteps=58000, episode_reward=197.24 +/- 0.95
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.09     |
|    n_updates        | 11999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 85.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 264      |
|    fps              | 54       |
|    time_elapsed     | 1070     |
|    total_timesteps  | 58115    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.586    |
|    n_updates        | 12028    |
----------------------------------
Eval num_timesteps=58500, episode_reward=10.39 +/- 46.33
Episode length: 61.90 +/- 67.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.9     |
|    mean_reward      | 10.4     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.475    |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 86.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 268      |
|    fps              | 54       |
|    time_elapsed     | 1072     |
|    total_timesteps  | 58824    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 12205    |
----------------------------------
Eval num_timesteps=59000, episode_reward=197.43 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.447    |
|    n_updates        | 12249    |
----------------------------------
Eval num_timesteps=59500, episode_reward=66.43 +/- 58.74
Episode length: 92.26 +/- 129.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 66.4     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.11     |
|    n_updates        | 12374    |
----------------------------------
Eval num_timesteps=60000, episode_reward=87.61 +/- 70.44
Episode length: 155.82 +/- 196.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 87.6     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.68     |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 88.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 54       |
|    time_elapsed     | 1094     |
|    total_timesteps  | 60025    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.39     |
|    n_updates        | 12506    |
----------------------------------
Eval num_timesteps=60500, episode_reward=193.27 +/- 6.52
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.25     |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 88.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 276      |
|    fps              | 54       |
|    time_elapsed     | 1110     |
|    total_timesteps  | 60679    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.94     |
|    n_updates        | 12669    |
----------------------------------
Eval num_timesteps=61000, episode_reward=137.69 +/- 54.48
Episode length: 267.08 +/- 238.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 267      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.515    |
|    n_updates        | 12749    |
----------------------------------
Eval num_timesteps=61500, episode_reward=173.49 +/- 45.86
Episode length: 421.92 +/- 194.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 422      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.876    |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 88.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 280      |
|    fps              | 54       |
|    time_elapsed     | 1130     |
|    total_timesteps  | 61801    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 12950    |
----------------------------------
Eval num_timesteps=62000, episode_reward=197.48 +/- 0.91
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.593    |
|    n_updates        | 12999    |
----------------------------------
Eval num_timesteps=62500, episode_reward=41.87 +/- 49.34
Episode length: 55.70 +/- 20.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.7     |
|    mean_reward      | 41.9     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.524    |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 85.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 54       |
|    time_elapsed     | 1147     |
|    total_timesteps  | 62612    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.603    |
|    n_updates        | 13152    |
----------------------------------
Eval num_timesteps=63000, episode_reward=197.47 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.68     |
|    n_updates        | 13249    |
----------------------------------
Eval num_timesteps=63500, episode_reward=197.52 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.59     |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 87.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 288      |
|    fps              | 54       |
|    time_elapsed     | 1178     |
|    total_timesteps  | 63843    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 13460    |
----------------------------------
Eval num_timesteps=64000, episode_reward=197.41 +/- 0.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 84.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 292      |
|    fps              | 53       |
|    time_elapsed     | 1193     |
|    total_timesteps  | 64132    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.09     |
|    n_updates        | 13532    |
----------------------------------
Eval num_timesteps=64500, episode_reward=197.62 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.847    |
|    n_updates        | 13624    |
----------------------------------
Eval num_timesteps=65000, episode_reward=196.64 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.24     |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | 83.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 296      |
|    fps              | 53       |
|    time_elapsed     | 1224     |
|    total_timesteps  | 65214    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 13803    |
----------------------------------
Eval num_timesteps=65500, episode_reward=197.21 +/- 0.75
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.76     |
|    n_updates        | 13874    |
----------------------------------
Eval num_timesteps=66000, episode_reward=197.37 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.37     |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 84.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 300      |
|    fps              | 52       |
|    time_elapsed     | 1254     |
|    total_timesteps  | 66121    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.42     |
|    n_updates        | 14030    |
----------------------------------
Eval num_timesteps=66500, episode_reward=197.64 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.03     |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 84.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 304      |
|    fps              | 52       |
|    time_elapsed     | 1269     |
|    total_timesteps  | 66892    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.48     |
|    n_updates        | 14222    |
----------------------------------
Eval num_timesteps=67000, episode_reward=197.59 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 14249    |
----------------------------------
Eval num_timesteps=67500, episode_reward=197.16 +/- 0.90
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.98     |
|    n_updates        | 14374    |
----------------------------------
Eval num_timesteps=68000, episode_reward=197.34 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.15     |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 84.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 308      |
|    fps              | 51       |
|    time_elapsed     | 1315     |
|    total_timesteps  | 68004    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.29     |
|    n_updates        | 14500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 83.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 312      |
|    fps              | 52       |
|    time_elapsed     | 1315     |
|    total_timesteps  | 68406    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 14601    |
----------------------------------
Eval num_timesteps=68500, episode_reward=197.59 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.977    |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 85.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 316      |
|    fps              | 51       |
|    time_elapsed     | 1330     |
|    total_timesteps  | 68836    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.03     |
|    n_updates        | 14708    |
----------------------------------
Eval num_timesteps=69000, episode_reward=196.97 +/- 2.05
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.95     |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 82.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 320      |
|    fps              | 51       |
|    time_elapsed     | 1345     |
|    total_timesteps  | 69129    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.71     |
|    n_updates        | 14782    |
----------------------------------
Eval num_timesteps=69500, episode_reward=182.92 +/- 5.24
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.693    |
|    n_updates        | 14874    |
----------------------------------
Eval num_timesteps=70000, episode_reward=195.61 +/- 4.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.28     |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 85.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 324      |
|    fps              | 51       |
|    time_elapsed     | 1375     |
|    total_timesteps  | 70270    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.11     |
|    n_updates        | 15067    |
----------------------------------
Eval num_timesteps=70500, episode_reward=24.64 +/- 60.29
Episode length: 76.62 +/- 114.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.6     |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.12     |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 86.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 328      |
|    fps              | 51       |
|    time_elapsed     | 1378     |
|    total_timesteps  | 70994    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.75     |
|    n_updates        | 15248    |
----------------------------------
Eval num_timesteps=71000, episode_reward=197.22 +/- 0.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.05     |
|    n_updates        | 15249    |
----------------------------------
Eval num_timesteps=71500, episode_reward=196.99 +/- 1.12
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 15374    |
----------------------------------
Eval num_timesteps=72000, episode_reward=197.36 +/- 0.86
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.78     |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 88.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 332      |
|    fps              | 50       |
|    time_elapsed     | 1423     |
|    total_timesteps  | 72309    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.6      |
|    n_updates        | 15577    |
----------------------------------
Eval num_timesteps=72500, episode_reward=170.94 +/- 54.70
Episode length: 428.52 +/- 193.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.25     |
|    n_updates        | 15624    |
----------------------------------
Eval num_timesteps=73000, episode_reward=159.55 +/- 55.42
Episode length: 374.80 +/- 219.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 375      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.71     |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 87.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 336      |
|    fps              | 50       |
|    time_elapsed     | 1446     |
|    total_timesteps  | 73283    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.07     |
|    n_updates        | 15820    |
----------------------------------
Eval num_timesteps=73500, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.71     |
|    n_updates        | 15874    |
----------------------------------
New best mean reward!
Eval num_timesteps=74000, episode_reward=197.98 +/- 0.14
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.96     |
|    n_updates        | 15999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 87.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 340      |
|    fps              | 50       |
|    time_elapsed     | 1477     |
|    total_timesteps  | 74032    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 16007    |
----------------------------------
Eval num_timesteps=74500, episode_reward=194.03 +/- 5.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.97     |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 87.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 344      |
|    fps              | 50       |
|    time_elapsed     | 1492     |
|    total_timesteps  | 74952    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.64     |
|    n_updates        | 16237    |
----------------------------------
Eval num_timesteps=75000, episode_reward=197.59 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.06     |
|    n_updates        | 16249    |
----------------------------------
Eval num_timesteps=75500, episode_reward=69.98 +/- 82.85
Episode length: 179.12 +/- 197.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 70       |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 16374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 88.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 348      |
|    fps              | 50       |
|    time_elapsed     | 1512     |
|    total_timesteps  | 75756    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.944    |
|    n_updates        | 16438    |
----------------------------------
Eval num_timesteps=76000, episode_reward=197.50 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.62     |
|    n_updates        | 16499    |
----------------------------------
Eval num_timesteps=76500, episode_reward=197.60 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.87     |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 90.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 49       |
|    time_elapsed     | 1543     |
|    total_timesteps  | 76917    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 16729    |
----------------------------------
Eval num_timesteps=77000, episode_reward=197.01 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.26     |
|    n_updates        | 16749    |
----------------------------------
Eval num_timesteps=77500, episode_reward=197.02 +/- 1.20
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.38     |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 88.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 356      |
|    fps              | 49       |
|    time_elapsed     | 1573     |
|    total_timesteps  | 77567    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.65     |
|    n_updates        | 16891    |
----------------------------------
Eval num_timesteps=78000, episode_reward=197.12 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.891    |
|    n_updates        | 16999    |
----------------------------------
Eval num_timesteps=78500, episode_reward=197.20 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.818    |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 88.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 49       |
|    time_elapsed     | 1604     |
|    total_timesteps  | 78756    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.07     |
|    n_updates        | 17188    |
----------------------------------
Eval num_timesteps=79000, episode_reward=188.01 +/- 29.67
Episode length: 486.42 +/- 130.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.94     |
|    n_updates        | 17249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 89.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 364      |
|    fps              | 48       |
|    time_elapsed     | 1618     |
|    total_timesteps  | 79022    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1        |
|    n_updates        | 17255    |
----------------------------------
Eval num_timesteps=79500, episode_reward=197.56 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.25     |
|    n_updates        | 17374    |
----------------------------------
Eval num_timesteps=80000, episode_reward=197.26 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.77     |
|    n_updates        | 17499    |
----------------------------------
Eval num_timesteps=80500, episode_reward=197.13 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.65     |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 92.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 368      |
|    fps              | 48       |
|    time_elapsed     | 1663     |
|    total_timesteps  | 80650    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 17662    |
----------------------------------
Eval num_timesteps=81000, episode_reward=-11.59 +/- 0.01
Episode length: 47.20 +/- 19.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 90.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 372      |
|    fps              | 48       |
|    time_elapsed     | 1665     |
|    total_timesteps  | 81492    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.67     |
|    n_updates        | 17872    |
----------------------------------
Eval num_timesteps=81500, episode_reward=197.51 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.99     |
|    n_updates        | 17874    |
----------------------------------
Eval num_timesteps=82000, episode_reward=183.57 +/- 35.52
Episode length: 467.70 +/- 155.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 468      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 92.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 376      |
|    fps              | 48       |
|    time_elapsed     | 1694     |
|    total_timesteps  | 82295    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.23     |
|    n_updates        | 18073    |
----------------------------------
Eval num_timesteps=82500, episode_reward=197.04 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.62     |
|    n_updates        | 18124    |
----------------------------------
Eval num_timesteps=83000, episode_reward=197.54 +/- 0.71
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.27     |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 90.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 380      |
|    fps              | 48       |
|    time_elapsed     | 1724     |
|    total_timesteps  | 83306    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.5      |
|    n_updates        | 18326    |
----------------------------------
Eval num_timesteps=83500, episode_reward=188.36 +/- 29.78
Episode length: 487.58 +/- 127.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 488      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 18374    |
----------------------------------
Eval num_timesteps=84000, episode_reward=197.75 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.35     |
|    n_updates        | 18499    |
----------------------------------
Eval num_timesteps=84500, episode_reward=44.03 +/- 53.56
Episode length: 65.78 +/- 70.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.8     |
|    mean_reward      | 44       |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.93     |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 92.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 384      |
|    fps              | 48       |
|    time_elapsed     | 1755     |
|    total_timesteps  | 84643    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.31     |
|    n_updates        | 18660    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 89.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 388      |
|    fps              | 48       |
|    time_elapsed     | 1755     |
|    total_timesteps  | 84921    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.83     |
|    n_updates        | 18730    |
----------------------------------
Eval num_timesteps=85000, episode_reward=197.37 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.92     |
|    n_updates        | 18749    |
----------------------------------
Eval num_timesteps=85500, episode_reward=197.31 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.9      |
|    n_updates        | 18874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 90.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 392      |
|    fps              | 48       |
|    time_elapsed     | 1786     |
|    total_timesteps  | 85974    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.91     |
|    n_updates        | 18993    |
----------------------------------
Eval num_timesteps=86000, episode_reward=197.24 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.29     |
|    n_updates        | 18999    |
----------------------------------
Eval num_timesteps=86500, episode_reward=91.47 +/- 93.12
Episode length: 241.12 +/- 232.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 241      |
|    mean_reward      | 91.5     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 91.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 396      |
|    fps              | 47       |
|    time_elapsed     | 1808     |
|    total_timesteps  | 86695    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.02     |
|    n_updates        | 19173    |
----------------------------------
Eval num_timesteps=87000, episode_reward=196.77 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.65     |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 91.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 400      |
|    fps              | 47       |
|    time_elapsed     | 1823     |
|    total_timesteps  | 87435    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.42     |
|    n_updates        | 19358    |
----------------------------------
Eval num_timesteps=87500, episode_reward=-11.59 +/- 0.00
Episode length: 50.90 +/- 18.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.56     |
|    n_updates        | 19374    |
----------------------------------
Eval num_timesteps=88000, episode_reward=-5.66 +/- 23.51
Episode length: 55.94 +/- 18.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.9     |
|    mean_reward      | -5.66    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.47     |
|    n_updates        | 19499    |
----------------------------------
Eval num_timesteps=88500, episode_reward=60.29 +/- 57.53
Episode length: 81.46 +/- 113.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.5     |
|    mean_reward      | 60.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.64     |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 92.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 404      |
|    fps              | 48       |
|    time_elapsed     | 1829     |
|    total_timesteps  | 88641    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.65     |
|    n_updates        | 19660    |
----------------------------------
Eval num_timesteps=89000, episode_reward=-11.59 +/- 0.00
Episode length: 48.26 +/- 14.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.67     |
|    n_updates        | 19749    |
----------------------------------
Eval num_timesteps=89500, episode_reward=190.39 +/- 26.03
Episode length: 497.50 +/- 108.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 94.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 408      |
|    fps              | 48       |
|    time_elapsed     | 1846     |
|    total_timesteps  | 89828    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.75     |
|    n_updates        | 19956    |
----------------------------------
Eval num_timesteps=90000, episode_reward=187.99 +/- 29.67
Episode length: 487.16 +/- 128.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.91     |
|    n_updates        | 19999    |
----------------------------------
Eval num_timesteps=90500, episode_reward=174.79 +/- 43.70
Episode length: 429.72 +/- 190.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 430      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.51     |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 95.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 412      |
|    fps              | 48       |
|    time_elapsed     | 1872     |
|    total_timesteps  | 90988    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.39     |
|    n_updates        | 20246    |
----------------------------------
Eval num_timesteps=91000, episode_reward=195.56 +/- 15.46
Episode length: 515.76 +/- 64.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.94     |
|    n_updates        | 20249    |
----------------------------------
Eval num_timesteps=91500, episode_reward=196.95 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.54     |
|    n_updates        | 20374    |
----------------------------------
Eval num_timesteps=92000, episode_reward=164.10 +/- 50.21
Episode length: 378.96 +/- 223.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 379      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.38     |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 234      |
|    ep_rew_mean      | 96       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 416      |
|    fps              | 48       |
|    time_elapsed     | 1913     |
|    total_timesteps  | 92273    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.949    |
|    n_updates        | 20568    |
----------------------------------
Eval num_timesteps=92500, episode_reward=97.13 +/- 54.99
Episode length: 145.14 +/- 179.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 97.1     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.19     |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 234      |
|    ep_rew_mean      | 98       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 420      |
|    fps              | 48       |
|    time_elapsed     | 1918     |
|    total_timesteps  | 92575    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.79     |
|    n_updates        | 20643    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 94.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 424      |
|    fps              | 48       |
|    time_elapsed     | 1918     |
|    total_timesteps  | 92943    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.8      |
|    n_updates        | 20735    |
----------------------------------
Eval num_timesteps=93000, episode_reward=166.29 +/- 49.20
Episode length: 390.22 +/- 216.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 390      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.13     |
|    n_updates        | 20749    |
----------------------------------
Eval num_timesteps=93500, episode_reward=190.48 +/- 26.05
Episode length: 497.16 +/- 110.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.63     |
|    n_updates        | 20874    |
----------------------------------
Eval num_timesteps=94000, episode_reward=192.99 +/- 21.67
Episode length: 505.98 +/- 93.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.68     |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 231      |
|    ep_rew_mean      | 96.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 428      |
|    fps              | 48       |
|    time_elapsed     | 1958     |
|    total_timesteps  | 94088    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.4      |
|    n_updates        | 21021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 95       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 432      |
|    fps              | 48       |
|    time_elapsed     | 1958     |
|    total_timesteps  | 94422    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.54     |
|    n_updates        | 21105    |
----------------------------------
Eval num_timesteps=94500, episode_reward=196.74 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.2      |
|    n_updates        | 21124    |
----------------------------------
Eval num_timesteps=95000, episode_reward=192.49 +/- 21.46
Episode length: 505.36 +/- 96.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.41     |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 95.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 436      |
|    fps              | 47       |
|    time_elapsed     | 1988     |
|    total_timesteps  | 95124    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 21280    |
----------------------------------
Eval num_timesteps=95500, episode_reward=196.44 +/- 1.85
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.01     |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 95.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 440      |
|    fps              | 47       |
|    time_elapsed     | 2003     |
|    total_timesteps  | 95967    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.46     |
|    n_updates        | 21491    |
----------------------------------
Eval num_timesteps=96000, episode_reward=4.25 +/- 36.29
Episode length: 51.46 +/- 20.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.5     |
|    mean_reward      | 4.25     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.6      |
|    n_updates        | 21499    |
----------------------------------
Eval num_timesteps=96500, episode_reward=181.64 +/- 38.03
Episode length: 459.32 +/- 162.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.75     |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 96.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 444      |
|    fps              | 47       |
|    time_elapsed     | 2018     |
|    total_timesteps  | 96888    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 21721    |
----------------------------------
Eval num_timesteps=97000, episode_reward=186.00 +/- 32.87
Episode length: 478.18 +/- 140.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.98     |
|    n_updates        | 21749    |
----------------------------------
Eval num_timesteps=97500, episode_reward=179.44 +/- 40.17
Episode length: 450.08 +/- 171.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 450      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.6      |
|    n_updates        | 21874    |
----------------------------------
Eval num_timesteps=98000, episode_reward=175.14 +/- 43.87
Episode length: 432.36 +/- 185.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 432      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.45     |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 94.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 448      |
|    fps              | 47       |
|    time_elapsed     | 2057     |
|    total_timesteps  | 98044    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.28     |
|    n_updates        | 22010    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 91.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 452      |
|    fps              | 47       |
|    time_elapsed     | 2057     |
|    total_timesteps  | 98433    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.93     |
|    n_updates        | 22108    |
----------------------------------
Eval num_timesteps=98500, episode_reward=193.57 +/- 15.45
Episode length: 514.88 +/- 70.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.04     |
|    n_updates        | 22124    |
----------------------------------
Eval num_timesteps=99000, episode_reward=190.38 +/- 26.02
Episode length: 496.06 +/- 114.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.6      |
|    n_updates        | 22249    |
----------------------------------
Eval num_timesteps=99500, episode_reward=192.93 +/- 21.56
Episode length: 505.84 +/- 93.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.52     |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 92       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 456      |
|    fps              | 47       |
|    time_elapsed     | 2101     |
|    total_timesteps  | 99658    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.16     |
|    n_updates        | 22414    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 212      |
|    ep_rew_mean      | 89       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 460      |
|    fps              | 47       |
|    time_elapsed     | 2101     |
|    total_timesteps  | 99940    |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.82     |
|    n_updates        | 22484    |
----------------------------------
Eval num_timesteps=100000, episode_reward=96.93 +/- 50.93
Episode length: 131.26 +/- 173.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 96.9     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4        |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 89.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 464      |
|    fps              | 47       |
|    time_elapsed     | 2106     |
|    total_timesteps  | 100437   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 22609    |
----------------------------------
Eval num_timesteps=100500, episode_reward=177.02 +/- 41.99
Episode length: 438.80 +/- 184.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 439      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.41     |
|    n_updates        | 22624    |
----------------------------------
Eval num_timesteps=101000, episode_reward=-11.60 +/- 0.00
Episode length: 48.90 +/- 20.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.94     |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 87       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 468      |
|    fps              | 47       |
|    time_elapsed     | 2120     |
|    total_timesteps  | 101128   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3        |
|    n_updates        | 22781    |
----------------------------------
Eval num_timesteps=101500, episode_reward=62.48 +/- 60.54
Episode length: 95.68 +/- 128.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.7     |
|    mean_reward      | 62.5     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.98     |
|    n_updates        | 22874    |
----------------------------------
Eval num_timesteps=102000, episode_reward=190.54 +/- 26.07
Episode length: 495.92 +/- 115.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.71     |
|    n_updates        | 22999    |
----------------------------------
Eval num_timesteps=102500, episode_reward=195.06 +/- 15.40
Episode length: 515.04 +/- 69.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.62     |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 91       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 472      |
|    fps              | 47       |
|    time_elapsed     | 2152     |
|    total_timesteps  | 102791   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.83     |
|    n_updates        | 23197    |
----------------------------------
Eval num_timesteps=103000, episode_reward=177.31 +/- 42.13
Episode length: 439.82 +/- 181.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.89     |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 88.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 476      |
|    fps              | 47       |
|    time_elapsed     | 2165     |
|    total_timesteps  | 103070   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.29     |
|    n_updates        | 23267    |
----------------------------------
Eval num_timesteps=103500, episode_reward=-9.61 +/- 13.86
Episode length: 51.14 +/- 17.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.1     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.29     |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | 87.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 480      |
|    fps              | 47       |
|    time_elapsed     | 2167     |
|    total_timesteps  | 103717   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.48     |
|    n_updates        | 23429    |
----------------------------------
Eval num_timesteps=104000, episode_reward=137.67 +/- 54.46
Episode length: 268.34 +/- 237.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 268      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.16     |
|    n_updates        | 23499    |
----------------------------------
Eval num_timesteps=104500, episode_reward=57.91 +/- 50.04
Episode length: 64.50 +/- 69.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.5     |
|    mean_reward      | 57.9     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.86     |
|    n_updates        | 23624    |
----------------------------------
Eval num_timesteps=105000, episode_reward=-11.59 +/- 0.00
Episode length: 48.54 +/- 14.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.99     |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 87.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 484      |
|    fps              | 48       |
|    time_elapsed     | 2179     |
|    total_timesteps  | 105096   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.98     |
|    n_updates        | 23773    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 88.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 488      |
|    fps              | 48       |
|    time_elapsed     | 2179     |
|    total_timesteps  | 105379   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.07     |
|    n_updates        | 23844    |
----------------------------------
Eval num_timesteps=105500, episode_reward=54.35 +/- 59.50
Episode length: 77.84 +/- 114.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.8     |
|    mean_reward      | 54.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.07     |
|    n_updates        | 23874    |
----------------------------------
Eval num_timesteps=106000, episode_reward=192.91 +/- 21.55
Episode length: 506.18 +/- 92.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.22     |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | 89.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 492      |
|    fps              | 48       |
|    time_elapsed     | 2196     |
|    total_timesteps  | 106110   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.57     |
|    n_updates        | 24027    |
----------------------------------
Eval num_timesteps=106500, episode_reward=197.37 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.42     |
|    n_updates        | 24124    |
----------------------------------
Eval num_timesteps=107000, episode_reward=191.71 +/- 5.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.66     |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | 89.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 496      |
|    fps              | 48       |
|    time_elapsed     | 2226     |
|    total_timesteps  | 107004   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 24250    |
----------------------------------
Eval num_timesteps=107500, episode_reward=190.63 +/- 26.09
Episode length: 497.06 +/- 110.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.18     |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | 89.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 500      |
|    fps              | 48       |
|    time_elapsed     | 2240     |
|    total_timesteps  | 107751   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.61     |
|    n_updates        | 24437    |
----------------------------------
Eval num_timesteps=108000, episode_reward=194.53 +/- 15.31
Episode length: 515.16 +/- 68.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.62     |
|    n_updates        | 24499    |
----------------------------------
Eval num_timesteps=108500, episode_reward=188.10 +/- 29.70
Episode length: 485.86 +/- 132.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.8      |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 89.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 504      |
|    fps              | 47       |
|    time_elapsed     | 2269     |
|    total_timesteps  | 108526   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 24631    |
----------------------------------
Eval num_timesteps=109000, episode_reward=116.76 +/- 66.32
Episode length: 223.48 +/- 226.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 117      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.23     |
|    n_updates        | 24749    |
----------------------------------
Eval num_timesteps=109500, episode_reward=192.59 +/- 21.48
Episode length: 505.32 +/- 96.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 87.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 508      |
|    fps              | 47       |
|    time_elapsed     | 2290     |
|    total_timesteps  | 109699   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.27     |
|    n_updates        | 24924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 85.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 512      |
|    fps              | 48       |
|    time_elapsed     | 2290     |
|    total_timesteps  | 109995   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.25     |
|    n_updates        | 24998    |
----------------------------------
Eval num_timesteps=110000, episode_reward=196.93 +/- 1.34
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.13     |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 84       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 516      |
|    fps              | 47       |
|    time_elapsed     | 2305     |
|    total_timesteps  | 110179   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.15     |
|    n_updates        | 25044    |
----------------------------------
Eval num_timesteps=110500, episode_reward=194.58 +/- 15.32
Episode length: 515.38 +/- 67.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 25124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 82.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 520      |
|    fps              | 47       |
|    time_elapsed     | 2320     |
|    total_timesteps  | 110537   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.33     |
|    n_updates        | 25134    |
----------------------------------
Eval num_timesteps=111000, episode_reward=194.47 +/- 15.31
Episode length: 516.14 +/- 62.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.36     |
|    n_updates        | 25249    |
----------------------------------
Eval num_timesteps=111500, episode_reward=14.15 +/- 43.42
Episode length: 53.30 +/- 22.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.3     |
|    mean_reward      | 14.1     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.3      |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 85       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 524      |
|    fps              | 47       |
|    time_elapsed     | 2337     |
|    total_timesteps  | 111666   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.04     |
|    n_updates        | 25416    |
----------------------------------
Eval num_timesteps=112000, episode_reward=181.84 +/- 47.24
Episode length: 476.64 +/- 145.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.66     |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 183      |
|    ep_rew_mean      | 85       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 528      |
|    fps              | 47       |
|    time_elapsed     | 2351     |
|    total_timesteps  | 112385   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.25     |
|    n_updates        | 25596    |
----------------------------------
Eval num_timesteps=112500, episode_reward=186.44 +/- 38.47
Episode length: 485.74 +/- 133.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.98     |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 84.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 532      |
|    fps              | 47       |
|    time_elapsed     | 2365     |
|    total_timesteps  | 112655   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.03     |
|    n_updates        | 25663    |
----------------------------------
Eval num_timesteps=113000, episode_reward=197.47 +/- 0.75
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.56     |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 83.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 536      |
|    fps              | 47       |
|    time_elapsed     | 2380     |
|    total_timesteps  | 113352   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.11     |
|    n_updates        | 25837    |
----------------------------------
Eval num_timesteps=113500, episode_reward=179.96 +/- 40.39
Episode length: 449.48 +/- 173.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.61     |
|    n_updates        | 25874    |
----------------------------------
Eval num_timesteps=114000, episode_reward=195.23 +/- 15.42
Episode length: 515.12 +/- 69.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.71     |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 84.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 540      |
|    fps              | 47       |
|    time_elapsed     | 2408     |
|    total_timesteps  | 114159   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.04     |
|    n_updates        | 26039    |
----------------------------------
Eval num_timesteps=114500, episode_reward=194.88 +/- 15.37
Episode length: 515.22 +/- 68.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.15     |
|    n_updates        | 26124    |
----------------------------------
Eval num_timesteps=115000, episode_reward=177.18 +/- 42.07
Episode length: 440.42 +/- 180.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 26249    |
----------------------------------
Eval num_timesteps=115500, episode_reward=186.23 +/- 32.95
Episode length: 480.84 +/- 132.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 481      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.95     |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 86.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 544      |
|    fps              | 47       |
|    time_elapsed     | 2450     |
|    total_timesteps  | 115774   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.12     |
|    n_updates        | 26443    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 82.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 548      |
|    fps              | 47       |
|    time_elapsed     | 2450     |
|    total_timesteps  | 115994   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.2      |
|    n_updates        | 26498    |
----------------------------------
Eval num_timesteps=116000, episode_reward=196.64 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 82.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 552      |
|    fps              | 47       |
|    time_elapsed     | 2465     |
|    total_timesteps  | 116232   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.19     |
|    n_updates        | 26557    |
----------------------------------
Eval num_timesteps=116500, episode_reward=196.64 +/- 0.62
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.24     |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 173      |
|    ep_rew_mean      | 81.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 556      |
|    fps              | 47       |
|    time_elapsed     | 2481     |
|    total_timesteps  | 116965   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.89     |
|    n_updates        | 26741    |
----------------------------------
Eval num_timesteps=117000, episode_reward=196.86 +/- 0.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 26749    |
----------------------------------
Eval num_timesteps=117500, episode_reward=-11.59 +/- 0.00
Episode length: 47.58 +/- 17.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.77     |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 82.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 560      |
|    fps              | 47       |
|    time_elapsed     | 2497     |
|    total_timesteps  | 117725   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.62     |
|    n_updates        | 26931    |
----------------------------------
Eval num_timesteps=118000, episode_reward=164.19 +/- 54.01
Episode length: 391.42 +/- 214.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 391      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.66     |
|    n_updates        | 26999    |
----------------------------------
Eval num_timesteps=118500, episode_reward=186.11 +/- 32.91
Episode length: 477.92 +/- 141.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.55     |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 82.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 564      |
|    fps              | 46       |
|    time_elapsed     | 2522     |
|    total_timesteps  | 118534   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.86     |
|    n_updates        | 27133    |
----------------------------------
Eval num_timesteps=119000, episode_reward=195.31 +/- 15.43
Episode length: 515.14 +/- 69.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.39     |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 82.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 568      |
|    fps              | 46       |
|    time_elapsed     | 2537     |
|    total_timesteps  | 119211   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.39     |
|    n_updates        | 27302    |
----------------------------------
Eval num_timesteps=119500, episode_reward=188.20 +/- 29.73
Episode length: 486.98 +/- 128.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.28     |
|    n_updates        | 27374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 79.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 572      |
|    fps              | 46       |
|    time_elapsed     | 2551     |
|    total_timesteps  | 119871   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.99     |
|    n_updates        | 27467    |
----------------------------------
Eval num_timesteps=120000, episode_reward=0.49 +/- 38.78
Episode length: 61.68 +/- 68.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.7     |
|    mean_reward      | 0.485    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.42     |
|    n_updates        | 27499    |
----------------------------------
Eval num_timesteps=120500, episode_reward=-9.58 +/- 13.86
Episode length: 51.72 +/- 21.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.7     |
|    mean_reward      | -9.58    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.84     |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 83       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 576      |
|    fps              | 47       |
|    time_elapsed     | 2555     |
|    total_timesteps  | 120972   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.28     |
|    n_updates        | 27742    |
----------------------------------
Eval num_timesteps=121000, episode_reward=195.82 +/- 2.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 27749    |
----------------------------------
Eval num_timesteps=121500, episode_reward=45.83 +/- 48.86
Episode length: 50.98 +/- 18.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51       |
|    mean_reward      | 45.8     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.03     |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 83.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 580      |
|    fps              | 47       |
|    time_elapsed     | 2572     |
|    total_timesteps  | 121612   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.99     |
|    n_updates        | 27902    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 169      |
|    ep_rew_mean      | 82.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 584      |
|    fps              | 47       |
|    time_elapsed     | 2572     |
|    total_timesteps  | 121991   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.67     |
|    n_updates        | 27997    |
----------------------------------
Eval num_timesteps=122000, episode_reward=105.25 +/- 49.50
Episode length: 149.86 +/- 188.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.82     |
|    n_updates        | 27999    |
----------------------------------
Eval num_timesteps=122500, episode_reward=179.40 +/- 40.16
Episode length: 449.32 +/- 173.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.93     |
|    n_updates        | 28124    |
----------------------------------
Eval num_timesteps=123000, episode_reward=176.93 +/- 41.95
Episode length: 438.66 +/- 184.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 439      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.81     |
|    n_updates        | 28249    |
----------------------------------
Eval num_timesteps=123500, episode_reward=176.98 +/- 41.97
Episode length: 439.22 +/- 183.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 439      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 85.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 588      |
|    fps              | 47       |
|    time_elapsed     | 2615     |
|    total_timesteps  | 123751   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.53     |
|    n_updates        | 28437    |
----------------------------------
Eval num_timesteps=124000, episode_reward=196.56 +/- 1.74
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.13     |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 84.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 592      |
|    fps              | 47       |
|    time_elapsed     | 2630     |
|    total_timesteps  | 124481   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.41     |
|    n_updates        | 28620    |
----------------------------------
Eval num_timesteps=124500, episode_reward=185.73 +/- 32.78
Episode length: 478.10 +/- 140.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.32     |
|    n_updates        | 28624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 83.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 596      |
|    fps              | 47       |
|    time_elapsed     | 2644     |
|    total_timesteps  | 124848   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.75     |
|    n_updates        | 28711    |
----------------------------------
Eval num_timesteps=125000, episode_reward=161.87 +/- 58.36
Episode length: 390.74 +/- 215.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 391      |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.35     |
|    n_updates        | 28749    |
----------------------------------
Eval num_timesteps=125500, episode_reward=-9.61 +/- 13.86
Episode length: 48.16 +/- 17.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.9      |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 83       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 600      |
|    fps              | 47       |
|    time_elapsed     | 2657     |
|    total_timesteps  | 125669   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.72     |
|    n_updates        | 28917    |
----------------------------------
Eval num_timesteps=126000, episode_reward=190.67 +/- 26.10
Episode length: 496.36 +/- 113.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.6      |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 83       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 604      |
|    fps              | 47       |
|    time_elapsed     | 2672     |
|    total_timesteps  | 126433   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.37     |
|    n_updates        | 29108    |
----------------------------------
Eval num_timesteps=126500, episode_reward=-11.59 +/- 0.00
Episode length: 46.72 +/- 15.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.71     |
|    n_updates        | 29124    |
----------------------------------
Eval num_timesteps=127000, episode_reward=197.31 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.79     |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 174      |
|    ep_rew_mean      | 82       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 608      |
|    fps              | 47       |
|    time_elapsed     | 2689     |
|    total_timesteps  | 127145   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.94     |
|    n_updates        | 29286    |
----------------------------------
Eval num_timesteps=127500, episode_reward=2.26 +/- 34.35
Episode length: 52.86 +/- 19.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 29374    |
----------------------------------
Eval num_timesteps=128000, episode_reward=174.76 +/- 43.68
Episode length: 430.36 +/- 189.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 430      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3        |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 84       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 612      |
|    fps              | 47       |
|    time_elapsed     | 2704     |
|    total_timesteps  | 128092   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.68     |
|    n_updates        | 29522    |
----------------------------------
Eval num_timesteps=128500, episode_reward=134.57 +/- 73.54
Episode length: 304.70 +/- 239.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 305      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.63     |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 82.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 616      |
|    fps              | 47       |
|    time_elapsed     | 2713     |
|    total_timesteps  | 128589   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.47     |
|    n_updates        | 29647    |
----------------------------------
Eval num_timesteps=129000, episode_reward=196.65 +/- 0.52
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.03     |
|    n_updates        | 29749    |
----------------------------------
Eval num_timesteps=129500, episode_reward=52.57 +/- 63.63
Episode length: 92.04 +/- 129.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 52.6     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.12     |
|    n_updates        | 29874    |
----------------------------------
Eval num_timesteps=130000, episode_reward=-11.60 +/- 0.00
Episode length: 53.18 +/- 17.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.2     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.09     |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 86.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 620      |
|    fps              | 47       |
|    time_elapsed     | 2733     |
|    total_timesteps  | 130193   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.44     |
|    n_updates        | 30048    |
----------------------------------
Eval num_timesteps=130500, episode_reward=-11.60 +/- 0.00
Episode length: 55.02 +/- 21.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55       |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.24     |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 85.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 624      |
|    fps              | 47       |
|    time_elapsed     | 2735     |
|    total_timesteps  | 130702   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.02     |
|    n_updates        | 30175    |
----------------------------------
Eval num_timesteps=131000, episode_reward=-9.61 +/- 13.86
Episode length: 48.22 +/- 14.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.01     |
|    n_updates        | 30249    |
----------------------------------
Eval num_timesteps=131500, episode_reward=174.96 +/- 43.78
Episode length: 428.84 +/- 192.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.27     |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 84.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 628      |
|    fps              | 47       |
|    time_elapsed     | 2749     |
|    total_timesteps  | 131561   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.16     |
|    n_updates        | 30390    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 83.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 632      |
|    fps              | 47       |
|    time_elapsed     | 2749     |
|    total_timesteps  | 131780   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.74     |
|    n_updates        | 30444    |
----------------------------------
Eval num_timesteps=132000, episode_reward=89.79 +/- 25.80
Episode length: 81.24 +/- 93.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.2     |
|    mean_reward      | 89.8     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.79     |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 83.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 636      |
|    fps              | 48       |
|    time_elapsed     | 2752     |
|    total_timesteps  | 132446   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.92     |
|    n_updates        | 30611    |
----------------------------------
Eval num_timesteps=132500, episode_reward=196.46 +/- 1.47
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.21     |
|    n_updates        | 30624    |
----------------------------------
Eval num_timesteps=133000, episode_reward=-11.60 +/- 0.00
Episode length: 48.56 +/- 12.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.6     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.53     |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 80.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 640      |
|    fps              | 48       |
|    time_elapsed     | 2769     |
|    total_timesteps  | 133118   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.82     |
|    n_updates        | 30779    |
----------------------------------
Eval num_timesteps=133500, episode_reward=197.21 +/- 0.95
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.18     |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 77.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 644      |
|    fps              | 48       |
|    time_elapsed     | 2784     |
|    total_timesteps  | 133805   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.48     |
|    n_updates        | 30951    |
----------------------------------
Eval num_timesteps=134000, episode_reward=87.83 +/- 73.49
Episode length: 166.96 +/- 201.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 167      |
|    mean_reward      | 87.8     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.4      |
|    n_updates        | 30999    |
----------------------------------
Eval num_timesteps=134500, episode_reward=190.49 +/- 26.06
Episode length: 496.28 +/- 113.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6        |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 81.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 648      |
|    fps              | 47       |
|    time_elapsed     | 2803     |
|    total_timesteps  | 134526   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.78     |
|    n_updates        | 31131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 81.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 652      |
|    fps              | 48       |
|    time_elapsed     | 2803     |
|    total_timesteps  | 134689   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.16     |
|    n_updates        | 31172    |
----------------------------------
Eval num_timesteps=135000, episode_reward=197.38 +/- 0.85
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.51     |
|    n_updates        | 31249    |
----------------------------------
Eval num_timesteps=135500, episode_reward=-9.61 +/- 13.86
Episode length: 48.78 +/- 14.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.8     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.66     |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 81.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 656      |
|    fps              | 48       |
|    time_elapsed     | 2820     |
|    total_timesteps  | 135938   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.52     |
|    n_updates        | 31484    |
----------------------------------
Eval num_timesteps=136000, episode_reward=166.72 +/- 60.24
Episode length: 419.54 +/- 198.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 420      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.26     |
|    n_updates        | 31499    |
----------------------------------
Eval num_timesteps=136500, episode_reward=-9.61 +/- 13.86
Episode length: 49.10 +/- 17.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.61     |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 80.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 660      |
|    fps              | 48       |
|    time_elapsed     | 2834     |
|    total_timesteps  | 136596   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.74     |
|    n_updates        | 31648    |
----------------------------------
Eval num_timesteps=137000, episode_reward=197.41 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.43     |
|    n_updates        | 31749    |
----------------------------------
Eval num_timesteps=137500, episode_reward=-11.59 +/- 0.00
Episode length: 48.02 +/- 17.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48       |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.77     |
|    n_updates        | 31874    |
----------------------------------
Eval num_timesteps=138000, episode_reward=197.54 +/- 0.70
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.77     |
|    n_updates        | 31999    |
----------------------------------
Eval num_timesteps=138500, episode_reward=-11.59 +/- 0.00
Episode length: 47.88 +/- 19.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.2      |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | 83.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 664      |
|    fps              | 48       |
|    time_elapsed     | 2867     |
|    total_timesteps  | 138514   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.34     |
|    n_updates        | 32128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 81.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 668      |
|    fps              | 48       |
|    time_elapsed     | 2868     |
|    total_timesteps  | 138870   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.72     |
|    n_updates        | 32217    |
----------------------------------
Eval num_timesteps=139000, episode_reward=4.65 +/- 47.44
Episode length: 66.58 +/- 95.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.6     |
|    mean_reward      | 4.65     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.41     |
|    n_updates        | 32249    |
----------------------------------
Eval num_timesteps=139500, episode_reward=-11.59 +/- 0.00
Episode length: 49.60 +/- 13.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.08     |
|    n_updates        | 32374    |
----------------------------------
Eval num_timesteps=140000, episode_reward=4.45 +/- 42.24
Episode length: 62.26 +/- 68.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.3     |
|    mean_reward      | 4.45     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.53     |
|    n_updates        | 32499    |
----------------------------------
Eval num_timesteps=140500, episode_reward=173.06 +/- 49.64
Episode length: 429.08 +/- 192.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.33     |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 83.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 672      |
|    fps              | 48       |
|    time_elapsed     | 2886     |
|    total_timesteps  | 140545   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.96     |
|    n_updates        | 32636    |
----------------------------------
Eval num_timesteps=141000, episode_reward=192.32 +/- 21.52
Episode length: 505.44 +/- 95.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 32749    |
----------------------------------
Eval num_timesteps=141500, episode_reward=-11.59 +/- 0.00
Episode length: 51.84 +/- 15.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.46     |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 84.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 676      |
|    fps              | 48       |
|    time_elapsed     | 2903     |
|    total_timesteps  | 141769   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.49     |
|    n_updates        | 32942    |
----------------------------------
Eval num_timesteps=142000, episode_reward=192.79 +/- 21.52
Episode length: 505.26 +/- 96.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.42     |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 85.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 680      |
|    fps              | 48       |
|    time_elapsed     | 2917     |
|    total_timesteps  | 142198   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.85     |
|    n_updates        | 33049    |
----------------------------------
Eval num_timesteps=142500, episode_reward=179.39 +/- 44.74
Episode length: 456.52 +/- 169.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 457      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.52     |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 85       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 684      |
|    fps              | 48       |
|    time_elapsed     | 2930     |
|    total_timesteps  | 142849   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.2      |
|    n_updates        | 33212    |
----------------------------------
Eval num_timesteps=143000, episode_reward=135.84 +/- 61.36
Episode length: 280.98 +/- 234.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 281      |
|    mean_reward      | 136      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.33     |
|    n_updates        | 33249    |
----------------------------------
Eval num_timesteps=143500, episode_reward=192.13 +/- 21.38
Episode length: 506.64 +/- 90.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.44     |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 81.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 688      |
|    fps              | 48       |
|    time_elapsed     | 2953     |
|    total_timesteps  | 143653   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.53     |
|    n_updates        | 33413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 79.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 692      |
|    fps              | 48       |
|    time_elapsed     | 2953     |
|    total_timesteps  | 143860   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.36     |
|    n_updates        | 33464    |
----------------------------------
Eval num_timesteps=144000, episode_reward=-11.59 +/- 0.01
Episode length: 49.04 +/- 15.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49       |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.63     |
|    n_updates        | 33499    |
----------------------------------
Eval num_timesteps=144500, episode_reward=197.73 +/- 0.59
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.48     |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 81.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 696      |
|    fps              | 48       |
|    time_elapsed     | 2970     |
|    total_timesteps  | 144550   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.15     |
|    n_updates        | 33637    |
----------------------------------
Eval num_timesteps=145000, episode_reward=144.13 +/- 54.50
Episode length: 299.44 +/- 235.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 299      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.13     |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 80.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 700      |
|    fps              | 48       |
|    time_elapsed     | 2979     |
|    total_timesteps  | 145240   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.97     |
|    n_updates        | 33809    |
----------------------------------
Eval num_timesteps=145500, episode_reward=188.73 +/- 29.89
Episode length: 488.54 +/- 124.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 489      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.66     |
|    n_updates        | 33874    |
----------------------------------
Eval num_timesteps=146000, episode_reward=197.66 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.2      |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 80.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 704      |
|    fps              | 48       |
|    time_elapsed     | 3008     |
|    total_timesteps  | 146057   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.25     |
|    n_updates        | 34014    |
----------------------------------
Eval num_timesteps=146500, episode_reward=197.45 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.89     |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 80.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 708      |
|    fps              | 48       |
|    time_elapsed     | 3023     |
|    total_timesteps  | 146748   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.82     |
|    n_updates        | 34186    |
----------------------------------
Eval num_timesteps=147000, episode_reward=187.91 +/- 29.64
Episode length: 485.28 +/- 134.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 485      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.35     |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 79.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 712      |
|    fps              | 48       |
|    time_elapsed     | 3037     |
|    total_timesteps  | 147481   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.14     |
|    n_updates        | 34370    |
----------------------------------
Eval num_timesteps=147500, episode_reward=-11.59 +/- 0.01
Episode length: 52.68 +/- 17.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.67     |
|    n_updates        | 34374    |
----------------------------------
Eval num_timesteps=148000, episode_reward=188.17 +/- 29.73
Episode length: 487.30 +/- 127.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.32     |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 81.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 716      |
|    fps              | 48       |
|    time_elapsed     | 3053     |
|    total_timesteps  | 148268   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.56     |
|    n_updates        | 34566    |
----------------------------------
Eval num_timesteps=148500, episode_reward=172.93 +/- 45.42
Episode length: 423.08 +/- 192.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 423      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.89     |
|    n_updates        | 34624    |
----------------------------------
Eval num_timesteps=149000, episode_reward=-11.59 +/- 0.00
Episode length: 49.36 +/- 15.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.4     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.36     |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 80.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 720      |
|    fps              | 48       |
|    time_elapsed     | 3068     |
|    total_timesteps  | 149495   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.78     |
|    n_updates        | 34873    |
----------------------------------
Eval num_timesteps=149500, episode_reward=-9.61 +/- 13.86
Episode length: 49.26 +/- 13.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.74     |
|    n_updates        | 34874    |
----------------------------------
Eval num_timesteps=150000, episode_reward=-11.59 +/- 0.00
Episode length: 47.72 +/- 17.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.85     |
|    n_updates        | 34999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 81.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 724      |
|    fps              | 48       |
|    time_elapsed     | 3071     |
|    total_timesteps  | 150278   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.52     |
|    n_updates        | 35069    |
----------------------------------
Eval num_timesteps=150500, episode_reward=179.23 +/- 44.62
Episode length: 458.42 +/- 165.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 458      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.35     |
|    n_updates        | 35124    |
----------------------------------
Eval num_timesteps=151000, episode_reward=172.50 +/- 45.19
Episode length: 418.44 +/- 200.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 418      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.9      |
|    n_updates        | 35249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 82.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 728      |
|    fps              | 48       |
|    time_elapsed     | 3097     |
|    total_timesteps  | 151411   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.18     |
|    n_updates        | 35352    |
----------------------------------
Eval num_timesteps=151500, episode_reward=197.19 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.85     |
|    n_updates        | 35374    |
----------------------------------
Eval num_timesteps=152000, episode_reward=83.16 +/- 92.67
Episode length: 221.16 +/- 228.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 83.2     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.91     |
|    n_updates        | 35499    |
----------------------------------
Eval num_timesteps=152500, episode_reward=197.27 +/- 0.85
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.1      |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 86.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 732      |
|    fps              | 48       |
|    time_elapsed     | 3133     |
|    total_timesteps  | 152748   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.85     |
|    n_updates        | 35686    |
----------------------------------
Eval num_timesteps=153000, episode_reward=35.13 +/- 71.81
Episode length: 104.82 +/- 155.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 35.1     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.61     |
|    n_updates        | 35749    |
----------------------------------
Eval num_timesteps=153500, episode_reward=194.97 +/- 15.39
Episode length: 515.80 +/- 64.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.81     |
|    n_updates        | 35874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 85.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 736      |
|    fps              | 48       |
|    time_elapsed     | 3151     |
|    total_timesteps  | 153591   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.37     |
|    n_updates        | 35897    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 85.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 740      |
|    fps              | 48       |
|    time_elapsed     | 3152     |
|    total_timesteps  | 153895   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.64     |
|    n_updates        | 35973    |
----------------------------------
Eval num_timesteps=154000, episode_reward=-11.60 +/- 0.00
Episode length: 49.94 +/- 18.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.16     |
|    n_updates        | 35999    |
----------------------------------
Eval num_timesteps=154500, episode_reward=-11.59 +/- 0.00
Episode length: 50.88 +/- 13.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.18     |
|    n_updates        | 36124    |
----------------------------------
Eval num_timesteps=155000, episode_reward=186.10 +/- 32.91
Episode length: 476.82 +/- 144.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.75     |
|    n_updates        | 36249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 212      |
|    ep_rew_mean      | 86       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 744      |
|    fps              | 48       |
|    time_elapsed     | 3169     |
|    total_timesteps  | 155034   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.94     |
|    n_updates        | 36258    |
----------------------------------
Eval num_timesteps=155500, episode_reward=184.13 +/- 40.82
Episode length: 476.62 +/- 145.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.66     |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 212      |
|    ep_rew_mean      | 85       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 748      |
|    fps              | 48       |
|    time_elapsed     | 3183     |
|    total_timesteps  | 155698   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.41     |
|    n_updates        | 36424    |
----------------------------------
Eval num_timesteps=156000, episode_reward=20.68 +/- 70.08
Episode length: 103.92 +/- 156.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 20.7     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.37     |
|    n_updates        | 36499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 85.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 752      |
|    fps              | 49       |
|    time_elapsed     | 3186     |
|    total_timesteps  | 156405   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.81     |
|    n_updates        | 36601    |
----------------------------------
Eval num_timesteps=156500, episode_reward=2.26 +/- 34.35
Episode length: 58.08 +/- 25.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.36     |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 85       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 756      |
|    fps              | 49       |
|    time_elapsed     | 3188     |
|    total_timesteps  | 156665   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 10.4     |
|    n_updates        | 36666    |
----------------------------------
Eval num_timesteps=157000, episode_reward=194.58 +/- 15.33
Episode length: 515.36 +/- 67.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.66     |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 85.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 760      |
|    fps              | 49       |
|    time_elapsed     | 3203     |
|    total_timesteps  | 157468   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.29     |
|    n_updates        | 36866    |
----------------------------------
Eval num_timesteps=157500, episode_reward=-9.61 +/- 13.86
Episode length: 51.22 +/- 17.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.92     |
|    n_updates        | 36874    |
----------------------------------
Eval num_timesteps=158000, episode_reward=47.46 +/- 77.44
Episode length: 127.38 +/- 174.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 47.5     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.79     |
|    n_updates        | 36999    |
----------------------------------
Eval num_timesteps=158500, episode_reward=197.60 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.73     |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 84.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 764      |
|    fps              | 49       |
|    time_elapsed     | 3224     |
|    total_timesteps  | 158739   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.45     |
|    n_updates        | 37184    |
----------------------------------
Eval num_timesteps=159000, episode_reward=197.18 +/- 0.98
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.4      |
|    n_updates        | 37249    |
----------------------------------
Eval num_timesteps=159500, episode_reward=-11.59 +/- 0.01
Episode length: 52.08 +/- 16.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.96     |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 87.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 768      |
|    fps              | 49       |
|    time_elapsed     | 3241     |
|    total_timesteps  | 159585   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.7      |
|    n_updates        | 37396    |
----------------------------------
Eval num_timesteps=160000, episode_reward=-11.59 +/- 0.01
Episode length: 46.66 +/- 18.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.86     |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 85.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 772      |
|    fps              | 49       |
|    time_elapsed     | 3243     |
|    total_timesteps  | 160373   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.79     |
|    n_updates        | 37593    |
----------------------------------
Eval num_timesteps=160500, episode_reward=179.71 +/- 40.29
Episode length: 448.12 +/- 176.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.29     |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 82.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 776      |
|    fps              | 49       |
|    time_elapsed     | 3256     |
|    total_timesteps  | 160696   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.42     |
|    n_updates        | 37673    |
----------------------------------
Eval num_timesteps=161000, episode_reward=-11.60 +/- 0.00
Episode length: 52.84 +/- 17.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.45     |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 81.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 780      |
|    fps              | 49       |
|    time_elapsed     | 3258     |
|    total_timesteps  | 161422   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.17     |
|    n_updates        | 37855    |
----------------------------------
Eval num_timesteps=161500, episode_reward=196.84 +/- 1.20
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.14     |
|    n_updates        | 37874    |
----------------------------------
Eval num_timesteps=162000, episode_reward=197.08 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.09     |
|    n_updates        | 37999    |
----------------------------------
Eval num_timesteps=162500, episode_reward=190.28 +/- 26.00
Episode length: 495.22 +/- 117.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.89     |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 84.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 784      |
|    fps              | 49       |
|    time_elapsed     | 3302     |
|    total_timesteps  | 162604   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.99     |
|    n_updates        | 38150    |
----------------------------------
Eval num_timesteps=163000, episode_reward=144.69 +/- 64.80
Episode length: 329.68 +/- 230.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 330      |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.1      |
|    n_updates        | 38249    |
----------------------------------
Eval num_timesteps=163500, episode_reward=130.99 +/- 85.89
Episode length: 330.88 +/- 237.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 331      |
|    mean_reward      | 131      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.09     |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 86       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 788      |
|    fps              | 49       |
|    time_elapsed     | 3322     |
|    total_timesteps  | 163886   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.55     |
|    n_updates        | 38471    |
----------------------------------
Eval num_timesteps=164000, episode_reward=-11.59 +/- 0.03
Episode length: 49.64 +/- 16.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.57     |
|    n_updates        | 38499    |
----------------------------------
Eval num_timesteps=164500, episode_reward=83.86 +/- 101.52
Episode length: 256.10 +/- 238.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 256      |
|    mean_reward      | 83.9     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.8      |
|    n_updates        | 38624    |
----------------------------------
Eval num_timesteps=165000, episode_reward=196.47 +/- 1.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 89.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 792      |
|    fps              | 49       |
|    time_elapsed     | 3346     |
|    total_timesteps  | 165186   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.07     |
|    n_updates        | 38796    |
----------------------------------
Eval num_timesteps=165500, episode_reward=170.49 +/- 50.69
Episode length: 419.84 +/- 198.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 420      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.33     |
|    n_updates        | 38874    |
----------------------------------
Eval num_timesteps=166000, episode_reward=197.84 +/- 0.46
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.86     |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 91.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 796      |
|    fps              | 49       |
|    time_elapsed     | 3373     |
|    total_timesteps  | 166352   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.73     |
|    n_updates        | 39087    |
----------------------------------
Eval num_timesteps=166500, episode_reward=197.44 +/- 0.74
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.34     |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 91.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 800      |
|    fps              | 49       |
|    time_elapsed     | 3389     |
|    total_timesteps  | 166672   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.49     |
|    n_updates        | 39167    |
----------------------------------
Eval num_timesteps=167000, episode_reward=192.93 +/- 21.55
Episode length: 505.00 +/- 97.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.3      |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 89.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 804      |
|    fps              | 49       |
|    time_elapsed     | 3404     |
|    total_timesteps  | 167376   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.33     |
|    n_updates        | 39343    |
----------------------------------
Eval num_timesteps=167500, episode_reward=20.29 +/- 50.92
Episode length: 56.08 +/- 69.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.1     |
|    mean_reward      | 20.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.49     |
|    n_updates        | 39374    |
----------------------------------
Eval num_timesteps=168000, episode_reward=2.26 +/- 34.35
Episode length: 53.06 +/- 19.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.1     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.62     |
|    n_updates        | 39499    |
----------------------------------
Eval num_timesteps=168500, episode_reward=-11.59 +/- 0.00
Episode length: 48.66 +/- 17.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.22     |
|    n_updates        | 39624    |
----------------------------------
Eval num_timesteps=169000, episode_reward=173.35 +/- 60.35
Episode length: 457.32 +/- 167.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 457      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.45     |
|    n_updates        | 39749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 91.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 808      |
|    fps              | 49       |
|    time_elapsed     | 3422     |
|    total_timesteps  | 169003   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.25     |
|    n_updates        | 39750    |
----------------------------------
Eval num_timesteps=169500, episode_reward=98.03 +/- 96.39
Episode length: 266.30 +/- 238.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 266      |
|    mean_reward      | 98       |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 39874    |
----------------------------------
Eval num_timesteps=170000, episode_reward=197.41 +/- 0.75
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 91.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 812      |
|    fps              | 49       |
|    time_elapsed     | 3445     |
|    total_timesteps  | 170065   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.27     |
|    n_updates        | 40016    |
----------------------------------
Eval num_timesteps=170500, episode_reward=197.58 +/- 0.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.29     |
|    n_updates        | 40124    |
----------------------------------
Eval num_timesteps=171000, episode_reward=181.37 +/- 37.92
Episode length: 470.86 +/- 135.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 471      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.4      |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 230      |
|    ep_rew_mean      | 92.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 816      |
|    fps              | 49       |
|    time_elapsed     | 3474     |
|    total_timesteps  | 171274   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.34     |
|    n_updates        | 40318    |
----------------------------------
Eval num_timesteps=171500, episode_reward=196.73 +/- 1.29
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.81     |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 89.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 820      |
|    fps              | 49       |
|    time_elapsed     | 3489     |
|    total_timesteps  | 171727   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.16     |
|    n_updates        | 40431    |
----------------------------------
Eval num_timesteps=172000, episode_reward=65.05 +/- 69.92
Episode length: 156.16 +/- 155.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 65       |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.97     |
|    n_updates        | 40499    |
----------------------------------
Eval num_timesteps=172500, episode_reward=197.64 +/- 0.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.16     |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 89.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 824      |
|    fps              | 49       |
|    time_elapsed     | 3509     |
|    total_timesteps  | 172594   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.37     |
|    n_updates        | 40648    |
----------------------------------
Eval num_timesteps=173000, episode_reward=-11.59 +/- 0.00
Episode length: 52.10 +/- 17.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.54     |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 88.3     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 828      |
|    fps              | 49       |
|    time_elapsed     | 3511     |
|    total_timesteps  | 173282   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.52     |
|    n_updates        | 40820    |
----------------------------------
Eval num_timesteps=173500, episode_reward=132.10 +/- 70.31
Episode length: 308.38 +/- 224.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 308      |
|    mean_reward      | 132      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.7      |
|    n_updates        | 40874    |
----------------------------------
Eval num_timesteps=174000, episode_reward=197.61 +/- 0.74
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.5      |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 87.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 832      |
|    fps              | 49       |
|    time_elapsed     | 3535     |
|    total_timesteps  | 174016   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.86     |
|    n_updates        | 41003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 87.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 836      |
|    fps              | 49       |
|    time_elapsed     | 3535     |
|    total_timesteps  | 174208   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.27     |
|    n_updates        | 41051    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 88       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 840      |
|    fps              | 49       |
|    time_elapsed     | 3536     |
|    total_timesteps  | 174403   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.99     |
|    n_updates        | 41100    |
----------------------------------
Eval num_timesteps=174500, episode_reward=172.77 +/- 45.34
Episode length: 421.30 +/- 195.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 421      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.61     |
|    n_updates        | 41124    |
----------------------------------
Eval num_timesteps=175000, episode_reward=193.16 +/- 21.60
Episode length: 506.82 +/- 89.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.59     |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | 88.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 844      |
|    fps              | 49       |
|    time_elapsed     | 3562     |
|    total_timesteps  | 175067   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.12     |
|    n_updates        | 41266    |
----------------------------------
Eval num_timesteps=175500, episode_reward=100.95 +/- 81.56
Episode length: 218.54 +/- 230.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 101      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.61     |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | 86.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 848      |
|    fps              | 49       |
|    time_elapsed     | 3569     |
|    total_timesteps  | 175670   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.03     |
|    n_updates        | 41417    |
----------------------------------
Eval num_timesteps=176000, episode_reward=195.96 +/- 1.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.89     |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 84.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 852      |
|    fps              | 49       |
|    time_elapsed     | 3584     |
|    total_timesteps  | 176033   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.94     |
|    n_updates        | 41508    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 84.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 856      |
|    fps              | 49       |
|    time_elapsed     | 3584     |
|    total_timesteps  | 176348   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.51     |
|    n_updates        | 41586    |
----------------------------------
Eval num_timesteps=176500, episode_reward=-11.59 +/- 0.01
Episode length: 48.60 +/- 16.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.6     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.25     |
|    n_updates        | 41624    |
----------------------------------
Eval num_timesteps=177000, episode_reward=-11.59 +/- 0.00
Episode length: 54.96 +/- 18.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55       |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.38     |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 84.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 860      |
|    fps              | 49       |
|    time_elapsed     | 3588     |
|    total_timesteps  | 177194   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.46     |
|    n_updates        | 41798    |
----------------------------------
Eval num_timesteps=177500, episode_reward=-11.60 +/- 0.00
Episode length: 53.06 +/- 18.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.1     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.35     |
|    n_updates        | 41874    |
----------------------------------
Eval num_timesteps=178000, episode_reward=195.36 +/- 15.58
Episode length: 515.34 +/- 67.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.1      |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 83.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 864      |
|    fps              | 49       |
|    time_elapsed     | 3605     |
|    total_timesteps  | 178484   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 42120    |
----------------------------------
Eval num_timesteps=178500, episode_reward=-11.60 +/- 0.00
Episode length: 52.22 +/- 18.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.8      |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 81.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 868      |
|    fps              | 49       |
|    time_elapsed     | 3606     |
|    total_timesteps  | 178750   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.11     |
|    n_updates        | 42187    |
----------------------------------
Eval num_timesteps=179000, episode_reward=-11.59 +/- 0.01
Episode length: 45.94 +/- 14.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.39     |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 79.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 872      |
|    fps              | 49       |
|    time_elapsed     | 3608     |
|    total_timesteps  | 179192   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.51     |
|    n_updates        | 42297    |
----------------------------------
Eval num_timesteps=179500, episode_reward=-11.59 +/- 0.00
Episode length: 46.70 +/- 13.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.01     |
|    n_updates        | 42374    |
----------------------------------
Eval num_timesteps=180000, episode_reward=103.58 +/- 56.22
Episode length: 158.12 +/- 195.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.51     |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 81.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 876      |
|    fps              | 49       |
|    time_elapsed     | 3615     |
|    total_timesteps  | 180357   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.17     |
|    n_updates        | 42589    |
----------------------------------
Eval num_timesteps=180500, episode_reward=194.71 +/- 15.35
Episode length: 515.26 +/- 68.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.93     |
|    n_updates        | 42624    |
----------------------------------
Eval num_timesteps=181000, episode_reward=195.02 +/- 15.39
Episode length: 515.14 +/- 69.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6        |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 83.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 880      |
|    fps              | 49       |
|    time_elapsed     | 3644     |
|    total_timesteps  | 181239   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.13     |
|    n_updates        | 42809    |
----------------------------------
Eval num_timesteps=181500, episode_reward=197.65 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.19     |
|    n_updates        | 42874    |
----------------------------------
Eval num_timesteps=182000, episode_reward=6.23 +/- 38.03
Episode length: 52.78 +/- 18.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.8     |
|    mean_reward      | 6.23     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.5      |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 84.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 884      |
|    fps              | 49       |
|    time_elapsed     | 3662     |
|    total_timesteps  | 182422   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.97     |
|    n_updates        | 43105    |
----------------------------------
Eval num_timesteps=182500, episode_reward=197.76 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.66     |
|    n_updates        | 43124    |
----------------------------------
Eval num_timesteps=183000, episode_reward=197.81 +/- 0.50
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.67     |
|    n_updates        | 43249    |
----------------------------------
Eval num_timesteps=183500, episode_reward=-11.60 +/- 0.00
Episode length: 51.44 +/- 17.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.37     |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 84.6     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 888      |
|    fps              | 49       |
|    time_elapsed     | 3694     |
|    total_timesteps  | 183671   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.43     |
|    n_updates        | 43417    |
----------------------------------
Eval num_timesteps=184000, episode_reward=20.08 +/- 46.18
Episode length: 56.68 +/- 17.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.7     |
|    mean_reward      | 20.1     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.39     |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 82.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 892      |
|    fps              | 49       |
|    time_elapsed     | 3695     |
|    total_timesteps  | 184014   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.13     |
|    n_updates        | 43503    |
----------------------------------
Eval num_timesteps=184500, episode_reward=197.89 +/- 0.39
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.57     |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 183      |
|    ep_rew_mean      | 78.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 896      |
|    fps              | 49       |
|    time_elapsed     | 3711     |
|    total_timesteps  | 184650   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.78     |
|    n_updates        | 43662    |
----------------------------------
Eval num_timesteps=185000, episode_reward=-9.61 +/- 13.86
Episode length: 51.14 +/- 17.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.1     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3        |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 79.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 900      |
|    fps              | 49       |
|    time_elapsed     | 3713     |
|    total_timesteps  | 185325   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.67     |
|    n_updates        | 43831    |
----------------------------------
Eval num_timesteps=185500, episode_reward=-11.59 +/- 0.00
Episode length: 46.80 +/- 11.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.25     |
|    n_updates        | 43874    |
----------------------------------
Eval num_timesteps=186000, episode_reward=197.51 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.88     |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 81.8     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 904      |
|    fps              | 49       |
|    time_elapsed     | 3730     |
|    total_timesteps  | 186197   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.53     |
|    n_updates        | 44049    |
----------------------------------
Eval num_timesteps=186500, episode_reward=197.68 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.09     |
|    n_updates        | 44124    |
----------------------------------
Eval num_timesteps=187000, episode_reward=194.86 +/- 15.37
Episode length: 515.24 +/- 68.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.56     |
|    n_updates        | 44249    |
----------------------------------
Eval num_timesteps=187500, episode_reward=186.29 +/- 32.97
Episode length: 478.46 +/- 139.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 81.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 908      |
|    fps              | 49       |
|    time_elapsed     | 3773     |
|    total_timesteps  | 187815   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.65     |
|    n_updates        | 44453    |
----------------------------------
Eval num_timesteps=188000, episode_reward=197.47 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.61     |
|    n_updates        | 44499    |
----------------------------------
Eval num_timesteps=188500, episode_reward=197.69 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.68     |
|    n_updates        | 44624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 82.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 912      |
|    fps              | 49       |
|    time_elapsed     | 3803     |
|    total_timesteps  | 188596   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 44648    |
----------------------------------
Eval num_timesteps=189000, episode_reward=-7.64 +/- 19.40
Episode length: 53.28 +/- 19.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.3     |
|    mean_reward      | -7.64    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.56     |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 83.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 916      |
|    fps              | 49       |
|    time_elapsed     | 3806     |
|    total_timesteps  | 189283   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.51     |
|    n_updates        | 44820    |
----------------------------------
Eval num_timesteps=189500, episode_reward=-11.60 +/- 0.00
Episode length: 47.52 +/- 14.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.5      |
|    n_updates        | 44874    |
----------------------------------
Eval num_timesteps=190000, episode_reward=167.23 +/- 66.53
Episode length: 437.36 +/- 187.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 437      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.22     |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 85       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 920      |
|    fps              | 49       |
|    time_elapsed     | 3820     |
|    total_timesteps  | 190264   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.82     |
|    n_updates        | 45065    |
----------------------------------
Eval num_timesteps=190500, episode_reward=197.71 +/- 0.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.65     |
|    n_updates        | 45124    |
----------------------------------
Eval num_timesteps=191000, episode_reward=197.78 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.02     |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 84       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 924      |
|    fps              | 49       |
|    time_elapsed     | 3850     |
|    total_timesteps  | 191269   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.74     |
|    n_updates        | 45317    |
----------------------------------
Eval num_timesteps=191500, episode_reward=88.23 +/- 79.16
Episode length: 180.54 +/- 215.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 88.2     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.21     |
|    n_updates        | 45374    |
----------------------------------
Eval num_timesteps=192000, episode_reward=197.75 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.13     |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 83.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 928      |
|    fps              | 49       |
|    time_elapsed     | 3871     |
|    total_timesteps  | 192419   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.81     |
|    n_updates        | 45604    |
----------------------------------
Eval num_timesteps=192500, episode_reward=177.14 +/- 46.45
Episode length: 450.26 +/- 171.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 450      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.4      |
|    n_updates        | 45624    |
----------------------------------
Eval num_timesteps=193000, episode_reward=197.68 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.86     |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 84       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 932      |
|    fps              | 49       |
|    time_elapsed     | 3899     |
|    total_timesteps  | 193132   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.94     |
|    n_updates        | 45782    |
----------------------------------
Eval num_timesteps=193500, episode_reward=104.87 +/- 78.28
Episode length: 222.38 +/- 227.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.42     |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 83.7     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 936      |
|    fps              | 49       |
|    time_elapsed     | 3906     |
|    total_timesteps  | 193854   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.25     |
|    n_updates        | 45963    |
----------------------------------
Eval num_timesteps=194000, episode_reward=197.73 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.19     |
|    n_updates        | 45999    |
----------------------------------
Eval num_timesteps=194500, episode_reward=197.49 +/- 0.74
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.84     |
|    n_updates        | 46124    |
----------------------------------
Eval num_timesteps=195000, episode_reward=197.60 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.57     |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 86.5     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 940      |
|    fps              | 49       |
|    time_elapsed     | 3951     |
|    total_timesteps  | 195051   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.45     |
|    n_updates        | 46262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | 84.4     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 944      |
|    fps              | 49       |
|    time_elapsed     | 3951     |
|    total_timesteps  | 195368   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.87     |
|    n_updates        | 46341    |
----------------------------------
Eval num_timesteps=195500, episode_reward=196.91 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.95     |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 84.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 948      |
|    fps              | 49       |
|    time_elapsed     | 3966     |
|    total_timesteps  | 195550   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.33     |
|    n_updates        | 46387    |
----------------------------------
Eval num_timesteps=196000, episode_reward=195.26 +/- 15.42
Episode length: 515.42 +/- 67.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.49     |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 85.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 952      |
|    fps              | 49       |
|    time_elapsed     | 3981     |
|    total_timesteps  | 196247   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.5      |
|    n_updates        | 46561    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | 85       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 956      |
|    fps              | 49       |
|    time_elapsed     | 3981     |
|    total_timesteps  | 196488   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.15     |
|    n_updates        | 46621    |
----------------------------------
Eval num_timesteps=196500, episode_reward=195.25 +/- 15.42
Episode length: 515.80 +/- 64.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.71     |
|    n_updates        | 46624    |
----------------------------------
Eval num_timesteps=197000, episode_reward=197.62 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.1      |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 86       |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 960      |
|    fps              | 49       |
|    time_elapsed     | 4011     |
|    total_timesteps  | 197375   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.9      |
|    n_updates        | 46843    |
----------------------------------
Eval num_timesteps=197500, episode_reward=114.57 +/- 65.44
Episode length: 212.58 +/- 224.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.25     |
|    n_updates        | 46874    |
----------------------------------
Eval num_timesteps=198000, episode_reward=197.55 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.18     |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 84.2     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 964      |
|    fps              | 49       |
|    time_elapsed     | 4032     |
|    total_timesteps  | 198057   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.96     |
|    n_updates        | 47014    |
----------------------------------
Eval num_timesteps=198500, episode_reward=192.35 +/- 21.65
Episode length: 505.92 +/- 93.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.6      |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 86.1     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 968      |
|    fps              | 49       |
|    time_elapsed     | 4047     |
|    total_timesteps  | 198920   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.83     |
|    n_updates        | 47229    |
----------------------------------
Eval num_timesteps=199000, episode_reward=108.41 +/- 68.76
Episode length: 204.38 +/- 221.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 108      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.57     |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 85.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 972      |
|    fps              | 49       |
|    time_elapsed     | 4053     |
|    total_timesteps  | 199420   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.55     |
|    n_updates        | 47354    |
----------------------------------
Eval num_timesteps=199500, episode_reward=197.64 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.63     |
|    n_updates        | 47374    |
----------------------------------
Eval num_timesteps=200000, episode_reward=180.14 +/- 52.98
Episode length: 477.54 +/- 142.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.73     |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 85.9     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 976      |
|    fps              | 49       |
|    time_elapsed     | 4083     |
|    total_timesteps  | 200133   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.96     |
|    n_updates        | 47533    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 83.3     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 980      |
|    fps              | 49       |
|    time_elapsed     | 4083     |
|    total_timesteps  | 200307   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.02     |
|    n_updates        | 47576    |
----------------------------------
Eval num_timesteps=200500, episode_reward=195.29 +/- 15.43
Episode length: 515.92 +/- 63.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.13     |
|    n_updates        | 47624    |
----------------------------------
Eval num_timesteps=201000, episode_reward=197.50 +/- 0.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.33     |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 80.3     |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 984      |
|    fps              | 48       |
|    time_elapsed     | 4114     |
|    total_timesteps  | 201496   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.54     |
|    n_updates        | 47873    |
----------------------------------
Eval num_timesteps=201500, episode_reward=197.81 +/- 0.49
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.09     |
|    n_updates        | 47874    |
----------------------------------
Eval num_timesteps=202000, episode_reward=133.82 +/- 64.37
Episode length: 280.14 +/- 235.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 280      |
|    mean_reward      | 134      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.14     |
|    n_updates        | 47999    |
----------------------------------
Eval num_timesteps=202500, episode_reward=197.79 +/- 0.53
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.44     |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 80.3     |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 988      |
|    fps              | 48       |
|    time_elapsed     | 4152     |
|    total_timesteps  | 202814   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.08     |
|    n_updates        | 48203    |
----------------------------------
Eval num_timesteps=203000, episode_reward=195.29 +/- 15.43
Episode length: 515.38 +/- 67.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.81     |
|    n_updates        | 48249    |
----------------------------------
Eval num_timesteps=203500, episode_reward=193.27 +/- 21.62
Episode length: 505.82 +/- 93.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.23     |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | 82.3     |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 992      |
|    fps              | 48       |
|    time_elapsed     | 4182     |
|    total_timesteps  | 203987   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.33     |
|    n_updates        | 48496    |
----------------------------------
Eval num_timesteps=204000, episode_reward=193.02 +/- 21.57
Episode length: 505.28 +/- 96.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.01     |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 82.2     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 996      |
|    fps              | 48       |
|    time_elapsed     | 4196     |
|    total_timesteps  | 204406   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.44     |
|    n_updates        | 48601    |
----------------------------------
Eval num_timesteps=204500, episode_reward=2.27 +/- 34.35
Episode length: 54.24 +/- 16.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.2     |
|    mean_reward      | 2.27     |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.19     |
|    n_updates        | 48624    |
----------------------------------
Eval num_timesteps=205000, episode_reward=197.78 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.95     |
|    n_updates        | 48749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 200      |
|    ep_rew_mean      | 80.9     |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 48       |
|    time_elapsed     | 4213     |
|    total_timesteps  | 205358   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.63     |
|    n_updates        | 48839    |
----------------------------------
Eval num_timesteps=205500, episode_reward=197.63 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.88     |
|    n_updates        | 48874    |
----------------------------------
Eval num_timesteps=206000, episode_reward=197.81 +/- 0.51
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.9      |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 80       |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 48       |
|    time_elapsed     | 4243     |
|    total_timesteps  | 206052   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.11     |
|    n_updates        | 49012    |
----------------------------------
Eval num_timesteps=206500, episode_reward=197.73 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.67     |
|    n_updates        | 49124    |
----------------------------------
Eval num_timesteps=207000, episode_reward=195.06 +/- 15.40
Episode length: 515.70 +/- 65.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.25     |
|    n_updates        | 49249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | 79.9     |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 48       |
|    time_elapsed     | 4274     |
|    total_timesteps  | 207322   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.18     |
|    n_updates        | 49330    |
----------------------------------
Eval num_timesteps=207500, episode_reward=144.56 +/- 58.36
Episode length: 307.92 +/- 235.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 308      |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.85     |
|    n_updates        | 49374    |
----------------------------------
Eval num_timesteps=208000, episode_reward=175.40 +/- 44.00
Episode length: 430.60 +/- 189.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.17     |
|    n_updates        | 49499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 80.1     |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 48       |
|    time_elapsed     | 4295     |
|    total_timesteps  | 208041   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.7      |
|    n_updates        | 49510    |
----------------------------------
Eval num_timesteps=208500, episode_reward=197.64 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.18     |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | 79       |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 48       |
|    time_elapsed     | 4311     |
|    total_timesteps  | 208761   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.29     |
|    n_updates        | 49690    |
----------------------------------
Eval num_timesteps=209000, episode_reward=192.72 +/- 21.51
Episode length: 505.88 +/- 93.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.17     |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 79.4     |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 48       |
|    time_elapsed     | 4326     |
|    total_timesteps  | 209484   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.55     |
|    n_updates        | 49870    |
----------------------------------
Eval num_timesteps=209500, episode_reward=186.66 +/- 33.09
Episode length: 477.12 +/- 143.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.18     |
|    n_updates        | 49874    |
----------------------------------
Eval num_timesteps=210000, episode_reward=179.94 +/- 40.39
Episode length: 448.74 +/- 174.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.45     |
|    n_updates        | 49999    |
----------------------------------
Eval num_timesteps=210500, episode_reward=175.20 +/- 43.90
Episode length: 430.70 +/- 188.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.12     |
|    n_updates        | 50124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 81.6     |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 48       |
|    time_elapsed     | 4365     |
|    total_timesteps  | 210640   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.53     |
|    n_updates        | 50159    |
----------------------------------
Eval num_timesteps=211000, episode_reward=197.65 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.12     |
|    n_updates        | 50249    |
----------------------------------
Eval num_timesteps=211500, episode_reward=195.31 +/- 15.43
Episode length: 515.50 +/- 66.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.18     |
|    n_updates        | 50374    |
----------------------------------
Eval num_timesteps=212000, episode_reward=197.12 +/- 0.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.44     |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 84.6     |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 48       |
|    time_elapsed     | 4410     |
|    total_timesteps  | 212257   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 50564    |
----------------------------------
Eval num_timesteps=212500, episode_reward=195.03 +/- 15.39
Episode length: 515.18 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.88     |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 83.6     |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 48       |
|    time_elapsed     | 4425     |
|    total_timesteps  | 212905   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.91     |
|    n_updates        | 50726    |
----------------------------------
Eval num_timesteps=213000, episode_reward=197.87 +/- 0.37
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.42     |
|    n_updates        | 50749    |
----------------------------------
Eval num_timesteps=213500, episode_reward=18.11 +/- 45.37
Episode length: 49.24 +/- 19.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.2     |
|    mean_reward      | 18.1     |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.71     |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 83.7     |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 48       |
|    time_elapsed     | 4442     |
|    total_timesteps  | 213592   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.75     |
|    n_updates        | 50897    |
----------------------------------
Eval num_timesteps=214000, episode_reward=188.13 +/- 35.68
Episode length: 494.88 +/- 119.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.08     |
|    n_updates        | 50999    |
----------------------------------
Eval num_timesteps=214500, episode_reward=69.01 +/- 68.24
Episode length: 118.60 +/- 164.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 69       |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.52     |
|    n_updates        | 51124    |
----------------------------------
Eval num_timesteps=215000, episode_reward=160.01 +/- 62.33
Episode length: 391.52 +/- 214.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 392      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 86.7     |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 48       |
|    time_elapsed     | 4471     |
|    total_timesteps  | 215235   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.46     |
|    n_updates        | 51308    |
----------------------------------
Eval num_timesteps=215500, episode_reward=187.84 +/- 29.62
Episode length: 486.48 +/- 130.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.58     |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 88.7     |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 48       |
|    time_elapsed     | 4485     |
|    total_timesteps  | 215948   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.89     |
|    n_updates        | 51486    |
----------------------------------
Eval num_timesteps=216000, episode_reward=197.28 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.968    |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.46     |
|    n_updates        | 51499    |
----------------------------------
Eval num_timesteps=216500, episode_reward=197.24 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.23     |
|    n_updates        | 51624    |
----------------------------------
Eval num_timesteps=217000, episode_reward=162.28 +/- 58.46
Episode length: 391.14 +/- 215.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 391      |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.02     |
|    n_updates        | 51749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 91.4     |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 47       |
|    time_elapsed     | 4527     |
|    total_timesteps  | 217075   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.38     |
|    n_updates        | 51768    |
----------------------------------
Eval num_timesteps=217500, episode_reward=193.00 +/- 29.24
Episode length: 515.18 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.74     |
|    n_updates        | 51874    |
----------------------------------
Eval num_timesteps=218000, episode_reward=164.51 +/- 60.97
Episode length: 410.62 +/- 203.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 411      |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.68     |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 95.3     |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 47       |
|    time_elapsed     | 4554     |
|    total_timesteps  | 218317   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.36     |
|    n_updates        | 52079    |
----------------------------------
Eval num_timesteps=218500, episode_reward=149.44 +/- 67.69
Episode length: 352.84 +/- 230.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 353      |
|    mean_reward      | 149      |
| rollout/            |          |
|    exploration_rate | 0.963    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 52124    |
----------------------------------
Eval num_timesteps=219000, episode_reward=197.38 +/- 0.75
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.69     |
|    n_updates        | 52249    |
----------------------------------
Eval num_timesteps=219500, episode_reward=197.78 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.59     |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 234      |
|    ep_rew_mean      | 99.1     |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 47       |
|    time_elapsed     | 4595     |
|    total_timesteps  | 219937   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.92     |
|    n_updates        | 52484    |
----------------------------------
Eval num_timesteps=220000, episode_reward=192.70 +/- 21.51
Episode length: 506.02 +/- 93.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.96     |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 52499    |
----------------------------------
Eval num_timesteps=220500, episode_reward=197.23 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.6      |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 234      |
|    ep_rew_mean      | 99.1     |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 47       |
|    time_elapsed     | 4625     |
|    total_timesteps  | 220744   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.71     |
|    n_updates        | 52685    |
----------------------------------
Eval num_timesteps=221000, episode_reward=190.34 +/- 26.01
Episode length: 496.06 +/- 114.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.958    |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.71     |
|    n_updates        | 52749    |
----------------------------------
Eval num_timesteps=221500, episode_reward=186.42 +/- 33.01
Episode length: 478.22 +/- 140.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.4      |
|    n_updates        | 52874    |
----------------------------------
Eval num_timesteps=222000, episode_reward=81.47 +/- 71.31
Episode length: 146.12 +/- 190.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 81.5     |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.64     |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 243      |
|    ep_rew_mean      | 103      |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 47       |
|    time_elapsed     | 4658     |
|    total_timesteps  | 222377   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.46     |
|    n_updates        | 53094    |
----------------------------------
Eval num_timesteps=222500, episode_reward=197.49 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 53124    |
----------------------------------
Eval num_timesteps=223000, episode_reward=197.57 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.07     |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 243      |
|    ep_rew_mean      | 102      |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 47       |
|    time_elapsed     | 4688     |
|    total_timesteps  | 223215   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5        |
|    n_updates        | 53303    |
----------------------------------
Eval num_timesteps=223500, episode_reward=164.21 +/- 57.51
Episode length: 399.48 +/- 211.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 399      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.01     |
|    n_updates        | 53374    |
----------------------------------
Eval num_timesteps=224000, episode_reward=2.27 +/- 34.35
Episode length: 52.56 +/- 19.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.6     |
|    mean_reward      | 2.27     |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.29     |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 249      |
|    ep_rew_mean      | 105      |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 47       |
|    time_elapsed     | 4702     |
|    total_timesteps  | 224363   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.3      |
|    n_updates        | 53590    |
----------------------------------
Eval num_timesteps=224500, episode_reward=18.11 +/- 45.37
Episode length: 49.70 +/- 16.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | 18.1     |
| rollout/            |          |
|    exploration_rate | 0.951    |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.51     |
|    n_updates        | 53624    |
----------------------------------
Eval num_timesteps=225000, episode_reward=195.12 +/- 15.41
Episode length: 515.12 +/- 69.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.951    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 53749    |
----------------------------------
Eval num_timesteps=225500, episode_reward=196.92 +/- 0.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.55     |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 256      |
|    ep_rew_mean      | 106      |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 47       |
|    time_elapsed     | 4734     |
|    total_timesteps  | 225723   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.05     |
|    n_updates        | 53930    |
----------------------------------
Eval num_timesteps=226000, episode_reward=195.52 +/- 15.45
Episode length: 514.88 +/- 70.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.949    |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.47     |
|    n_updates        | 53999    |
----------------------------------
Eval num_timesteps=226500, episode_reward=194.61 +/- 15.33
Episode length: 515.74 +/- 64.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.64     |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 265      |
|    ep_rew_mean      | 109      |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 47       |
|    time_elapsed     | 4764     |
|    total_timesteps  | 226853   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.19     |
|    n_updates        | 54213    |
----------------------------------
Eval num_timesteps=227000, episode_reward=-9.61 +/- 13.86
Episode length: 50.36 +/- 19.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.4     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 0.947    |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.48     |
|    n_updates        | 54249    |
----------------------------------
Eval num_timesteps=227500, episode_reward=47.99 +/- 52.95
Episode length: 60.24 +/- 68.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.2     |
|    mean_reward      | 48       |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.79     |
|    n_updates        | 54374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 261      |
|    ep_rew_mean      | 110      |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 47       |
|    time_elapsed     | 4767     |
|    total_timesteps  | 227620   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.86     |
|    n_updates        | 54404    |
----------------------------------
Eval num_timesteps=228000, episode_reward=156.59 +/- 77.78
Episode length: 419.10 +/- 199.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 419      |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.945    |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.54     |
|    n_updates        | 54499    |
----------------------------------
Eval num_timesteps=228500, episode_reward=197.59 +/- 0.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.04     |
|    n_updates        | 54624    |
----------------------------------
Eval num_timesteps=229000, episode_reward=81.87 +/- 37.74
Episode length: 75.54 +/- 93.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.5     |
|    mean_reward      | 81.9     |
| rollout/            |          |
|    exploration_rate | 0.943    |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 264      |
|    ep_rew_mean      | 111      |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 47       |
|    time_elapsed     | 4797     |
|    total_timesteps  | 229254   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.99     |
|    n_updates        | 54813    |
----------------------------------
Eval num_timesteps=229500, episode_reward=179.81 +/- 40.34
Episode length: 451.56 +/- 168.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 452      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.942    |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.82     |
|    n_updates        | 54874    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-11.59 +/- 0.01
Episode length: 47.82 +/- 17.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.91     |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 264      |
|    ep_rew_mean      | 111      |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 47       |
|    time_elapsed     | 4812     |
|    total_timesteps  | 230422   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.03     |
|    n_updates        | 55105    |
----------------------------------
Eval num_timesteps=230500, episode_reward=191.09 +/- 26.20
Episode length: 495.96 +/- 114.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.94     |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.78     |
|    n_updates        | 55124    |
----------------------------------
Eval num_timesteps=231000, episode_reward=190.89 +/- 26.15
Episode length: 496.72 +/- 112.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.939    |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 266      |
|    ep_rew_mean      | 112      |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 47       |
|    time_elapsed     | 4842     |
|    total_timesteps  | 231053   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.85     |
|    n_updates        | 55263    |
----------------------------------
Eval num_timesteps=231500, episode_reward=61.65 +/- 43.46
Episode length: 55.56 +/- 21.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.6     |
|    mean_reward      | 61.6     |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.16     |
|    n_updates        | 55374    |
----------------------------------
Eval num_timesteps=232000, episode_reward=49.79 +/- 48.05
Episode length: 55.92 +/- 20.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.9     |
|    mean_reward      | 49.8     |
| rollout/            |          |
|    exploration_rate | 0.937    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.16     |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 268      |
|    ep_rew_mean      | 115      |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 47       |
|    time_elapsed     | 4846     |
|    total_timesteps  | 232179   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.08     |
|    n_updates        | 55544    |
----------------------------------
Eval num_timesteps=232500, episode_reward=71.55 +/- 36.34
Episode length: 53.36 +/- 21.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.4     |
|    mean_reward      | 71.5     |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 268      |
|    ep_rew_mean      | 114      |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 48       |
|    time_elapsed     | 4848     |
|    total_timesteps  | 232847   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.89     |
|    n_updates        | 55711    |
----------------------------------
Eval num_timesteps=233000, episode_reward=194.19 +/- 15.40
Episode length: 516.04 +/- 62.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.49     |
|    n_updates        | 55749    |
----------------------------------
Eval num_timesteps=233500, episode_reward=192.32 +/- 29.13
Episode length: 514.96 +/- 70.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.934    |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.5      |
|    n_updates        | 55874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 263      |
|    ep_rew_mean      | 112      |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 47       |
|    time_elapsed     | 4877     |
|    total_timesteps  | 233620   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.04     |
|    n_updates        | 55904    |
----------------------------------
Eval num_timesteps=234000, episode_reward=197.10 +/- 0.92
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.51     |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 263      |
|    ep_rew_mean      | 111      |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 47       |
|    time_elapsed     | 4892     |
|    total_timesteps  | 234322   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.39     |
|    n_updates        | 56080    |
----------------------------------
Eval num_timesteps=234500, episode_reward=47.81 +/- 48.50
Episode length: 54.20 +/- 23.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.2     |
|    mean_reward      | 47.8     |
| rollout/            |          |
|    exploration_rate | 0.932    |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.36     |
|    n_updates        | 56124    |
----------------------------------
Eval num_timesteps=235000, episode_reward=120.33 +/- 50.30
Episode length: 200.02 +/- 213.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 120      |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.77     |
|    n_updates        | 56249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 267      |
|    ep_rew_mean      | 113      |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 48       |
|    time_elapsed     | 4901     |
|    total_timesteps  | 235479   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.89     |
|    n_updates        | 56369    |
----------------------------------
Eval num_timesteps=235500, episode_reward=197.32 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.93     |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.93     |
|    n_updates        | 56374    |
----------------------------------
Eval num_timesteps=236000, episode_reward=73.75 +/- 40.24
Episode length: 73.64 +/- 68.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | 73.7     |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.22     |
|    n_updates        | 56499    |
----------------------------------
Eval num_timesteps=236500, episode_reward=146.49 +/- 54.53
Episode length: 309.58 +/- 233.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 310      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.928    |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 272      |
|    ep_rew_mean      | 113      |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 48       |
|    time_elapsed     | 4927     |
|    total_timesteps  | 236675   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.46     |
|    n_updates        | 56668    |
----------------------------------
Eval num_timesteps=237000, episode_reward=197.36 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.927    |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.5      |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 267      |
|    ep_rew_mean      | 113      |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 48       |
|    time_elapsed     | 4942     |
|    total_timesteps  | 237326   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.29     |
|    n_updates        | 56831    |
----------------------------------
Eval num_timesteps=237500, episode_reward=126.85 +/- 56.18
Episode length: 234.94 +/- 227.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.35     |
|    n_updates        | 56874    |
----------------------------------
Eval num_timesteps=238000, episode_reward=33.95 +/- 49.34
Episode length: 54.98 +/- 21.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55       |
|    mean_reward      | 33.9     |
| rollout/            |          |
|    exploration_rate | 0.925    |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.4      |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 258      |
|    ep_rew_mean      | 110      |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 48       |
|    time_elapsed     | 4951     |
|    total_timesteps  | 238016   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.09     |
|    n_updates        | 57003    |
----------------------------------
Eval num_timesteps=238500, episode_reward=190.94 +/- 29.85
Episode length: 515.06 +/- 69.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.924    |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.17     |
|    n_updates        | 57124    |
----------------------------------
Eval num_timesteps=239000, episode_reward=41.86 +/- 49.34
Episode length: 60.58 +/- 26.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.6     |
|    mean_reward      | 41.9     |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.2      |
|    n_updates        | 57249    |
----------------------------------
Eval num_timesteps=239500, episode_reward=81.87 +/- 37.74
Episode length: 72.60 +/- 94.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.6     |
|    mean_reward      | 81.9     |
| rollout/            |          |
|    exploration_rate | 0.922    |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.3      |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 267      |
|    ep_rew_mean      | 111      |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 48       |
|    time_elapsed     | 4971     |
|    total_timesteps  | 239638   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.64     |
|    n_updates        | 57409    |
----------------------------------
Eval num_timesteps=240000, episode_reward=78.90 +/- 62.80
Episode length: 120.00 +/- 164.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 78.9     |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 57499    |
----------------------------------
Eval num_timesteps=240500, episode_reward=157.61 +/- 52.65
Episode length: 355.66 +/- 226.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 356      |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5        |
|    n_updates        | 57624    |
----------------------------------
Eval num_timesteps=241000, episode_reward=-11.59 +/- 0.00
Episode length: 51.44 +/- 20.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.919    |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.48     |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 277      |
|    ep_rew_mean      | 115      |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 48       |
|    time_elapsed     | 4987     |
|    total_timesteps  | 241262   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.25     |
|    n_updates        | 57815    |
----------------------------------
Eval num_timesteps=241500, episode_reward=131.63 +/- 64.05
Episode length: 270.02 +/- 235.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 270      |
|    mean_reward      | 132      |
| rollout/            |          |
|    exploration_rate | 0.918    |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 57874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 267      |
|    ep_rew_mean      | 111      |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 48       |
|    time_elapsed     | 4995     |
|    total_timesteps  | 241929   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.33     |
|    n_updates        | 57982    |
----------------------------------
Eval num_timesteps=242000, episode_reward=-11.59 +/- 0.01
Episode length: 48.94 +/- 21.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.917    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.08     |
|    n_updates        | 57999    |
----------------------------------
Eval num_timesteps=242500, episode_reward=194.94 +/- 15.38
Episode length: 514.96 +/- 70.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.916    |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.99     |
|    n_updates        | 58124    |
----------------------------------
Eval num_timesteps=243000, episode_reward=197.24 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.57     |
|    n_updates        | 58249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 271      |
|    ep_rew_mean      | 112      |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 48       |
|    time_elapsed     | 5027     |
|    total_timesteps  | 243069   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.72     |
|    n_updates        | 58267    |
----------------------------------
Eval num_timesteps=243500, episode_reward=184.37 +/- 35.81
Episode length: 468.72 +/- 152.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 469      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.914    |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.45     |
|    n_updates        | 58374    |
----------------------------------
Eval num_timesteps=244000, episode_reward=194.57 +/- 15.32
Episode length: 515.02 +/- 69.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.58     |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 271      |
|    ep_rew_mean      | 112      |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 48       |
|    time_elapsed     | 5055     |
|    total_timesteps  | 244213   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.72     |
|    n_updates        | 58553    |
----------------------------------
Eval num_timesteps=244500, episode_reward=182.00 +/- 38.17
Episode length: 458.74 +/- 164.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.912    |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.07     |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 266      |
|    ep_rew_mean      | 110      |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 48       |
|    time_elapsed     | 5069     |
|    total_timesteps  | 244873   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.91     |
|    n_updates        | 58718    |
----------------------------------
Eval num_timesteps=245000, episode_reward=162.29 +/- 51.37
Episode length: 377.24 +/- 215.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 377      |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.14     |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 252      |
|    ep_rew_mean      | 105      |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 48       |
|    time_elapsed     | 5079     |
|    total_timesteps  | 245096   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.18     |
|    n_updates        | 58773    |
----------------------------------
Eval num_timesteps=245500, episode_reward=149.15 +/- 61.44
Episode length: 336.22 +/- 231.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 336      |
|    mean_reward      | 149      |
| rollout/            |          |
|    exploration_rate | 0.91     |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.44     |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 250      |
|    ep_rew_mean      | 105      |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 48       |
|    time_elapsed     | 5090     |
|    total_timesteps  | 245758   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.16     |
|    n_updates        | 58939    |
----------------------------------
Eval num_timesteps=246000, episode_reward=197.18 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.909    |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.76     |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 241      |
|    ep_rew_mean      | 103      |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 48       |
|    time_elapsed     | 5105     |
|    total_timesteps  | 246442   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.03     |
|    n_updates        | 59110    |
----------------------------------
Eval num_timesteps=246500, episode_reward=14.15 +/- 43.42
Episode length: 53.02 +/- 16.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53       |
|    mean_reward      | 14.1     |
| rollout/            |          |
|    exploration_rate | 0.908    |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.14     |
|    n_updates        | 59124    |
----------------------------------
Eval num_timesteps=247000, episode_reward=196.93 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.63     |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 239      |
|    ep_rew_mean      | 104      |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 48       |
|    time_elapsed     | 5122     |
|    total_timesteps  | 247109   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.43     |
|    n_updates        | 59277    |
----------------------------------
Eval num_timesteps=247500, episode_reward=168.83 +/- 48.27
Episode length: 405.74 +/- 201.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 406      |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.906    |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.35     |
|    n_updates        | 59374    |
----------------------------------
Eval num_timesteps=248000, episode_reward=81.87 +/- 77.21
Episode length: 165.08 +/- 203.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 81.9     |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.19     |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 239      |
|    ep_rew_mean      | 104      |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 48       |
|    time_elapsed     | 5139     |
|    total_timesteps  | 248264   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.45     |
|    n_updates        | 59565    |
----------------------------------
Eval num_timesteps=248500, episode_reward=60.29 +/- 57.53
Episode length: 82.34 +/- 113.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.3     |
|    mean_reward      | 60.3     |
| rollout/            |          |
|    exploration_rate | 0.904    |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.71     |
|    n_updates        | 59624    |
----------------------------------
Eval num_timesteps=249000, episode_reward=153.44 +/- 53.92
Episode length: 343.56 +/- 222.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 344      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.903    |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 237      |
|    ep_rew_mean      | 104      |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 48       |
|    time_elapsed     | 5152     |
|    total_timesteps  | 249440   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.58     |
|    n_updates        | 59859    |
----------------------------------
Eval num_timesteps=249500, episode_reward=137.68 +/- 86.57
Episode length: 361.86 +/- 227.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 362      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 0.902    |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 59874    |
----------------------------------
Eval num_timesteps=250000, episode_reward=58.31 +/- 58.26
Episode length: 82.38 +/- 113.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.4     |
|    mean_reward      | 58.3     |
| rollout/            |          |
|    exploration_rate | 0.901    |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.64     |
|    n_updates        | 59999    |
----------------------------------
Eval num_timesteps=250500, episode_reward=-3.67 +/- 26.86
Episode length: 48.46 +/- 16.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | -3.67    |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.43     |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 237      |
|    ep_rew_mean      | 105      |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 48       |
|    time_elapsed     | 5167     |
|    total_timesteps  | 250561   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.77     |
|    n_updates        | 60140    |
----------------------------------
Eval num_timesteps=251000, episode_reward=197.62 +/- 0.63
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.899    |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.34     |
|    n_updates        | 60249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 237      |
|    ep_rew_mean      | 105      |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 48       |
|    time_elapsed     | 5182     |
|    total_timesteps  | 251328   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.54     |
|    n_updates        | 60331    |
----------------------------------
Eval num_timesteps=251500, episode_reward=196.88 +/- 0.94
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.85     |
|    n_updates        | 60374    |
----------------------------------
Eval num_timesteps=252000, episode_reward=197.17 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.897    |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.14     |
|    n_updates        | 60499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 228      |
|    ep_rew_mean      | 103      |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 48       |
|    time_elapsed     | 5213     |
|    total_timesteps  | 252060   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.58     |
|    n_updates        | 60514    |
----------------------------------
Eval num_timesteps=252500, episode_reward=197.64 +/- 0.63
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.74     |
|    n_updates        | 60624    |
----------------------------------
Eval num_timesteps=253000, episode_reward=195.09 +/- 15.40
Episode length: 514.94 +/- 70.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.895    |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.45     |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 228      |
|    ep_rew_mean      | 102      |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 48       |
|    time_elapsed     | 5243     |
|    total_timesteps  | 253196   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.78     |
|    n_updates        | 60798    |
----------------------------------
Eval num_timesteps=253500, episode_reward=-11.59 +/- 0.00
Episode length: 50.76 +/- 15.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.8     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.894    |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.12     |
|    n_updates        | 60874    |
----------------------------------
Eval num_timesteps=254000, episode_reward=192.89 +/- 21.54
Episode length: 506.56 +/- 90.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.97     |
|    n_updates        | 60999    |
----------------------------------
Eval num_timesteps=254500, episode_reward=197.99 +/- 0.10
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.892    |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.11     |
|    n_updates        | 61124    |
----------------------------------
New best mean reward!
Eval num_timesteps=255000, episode_reward=-9.61 +/- 13.86
Episode length: 52.14 +/- 19.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.14     |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 242      |
|    ep_rew_mean      | 106      |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 48       |
|    time_elapsed     | 5277     |
|    total_timesteps  | 255296   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.56     |
|    n_updates        | 61323    |
----------------------------------
Eval num_timesteps=255500, episode_reward=190.87 +/- 26.15
Episode length: 498.44 +/- 105.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.89     |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.77     |
|    n_updates        | 61374    |
----------------------------------
Eval num_timesteps=256000, episode_reward=-9.61 +/- 13.86
Episode length: 43.22 +/- 11.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 0.889    |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 243      |
|    ep_rew_mean      | 106      |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 48       |
|    time_elapsed     | 5293     |
|    total_timesteps  | 256438   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.5      |
|    n_updates        | 61609    |
----------------------------------
Eval num_timesteps=256500, episode_reward=190.38 +/- 26.03
Episode length: 496.08 +/- 114.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.888    |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.11     |
|    n_updates        | 61624    |
----------------------------------
Eval num_timesteps=257000, episode_reward=177.86 +/- 42.38
Episode length: 439.70 +/- 182.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.11     |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 242      |
|    ep_rew_mean      | 106      |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 48       |
|    time_elapsed     | 5320     |
|    total_timesteps  | 257068   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.89     |
|    n_updates        | 61766    |
----------------------------------
Eval num_timesteps=257500, episode_reward=-11.60 +/- 0.00
Episode length: 53.50 +/- 18.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.886    |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.37     |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 241      |
|    ep_rew_mean      | 107      |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 48       |
|    time_elapsed     | 5322     |
|    total_timesteps  | 257760   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.14     |
|    n_updates        | 61939    |
----------------------------------
Eval num_timesteps=258000, episode_reward=195.06 +/- 15.40
Episode length: 514.98 +/- 70.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.885    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.61     |
|    n_updates        | 61999    |
----------------------------------
Eval num_timesteps=258500, episode_reward=188.82 +/- 29.91
Episode length: 486.98 +/- 128.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.18     |
|    n_updates        | 62124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 242      |
|    ep_rew_mean      | 108      |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 48       |
|    time_elapsed     | 5350     |
|    total_timesteps  | 258556   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.86     |
|    n_updates        | 62138    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 232      |
|    ep_rew_mean      | 104      |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 48       |
|    time_elapsed     | 5351     |
|    total_timesteps  | 258726   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.94     |
|    n_updates        | 62181    |
----------------------------------
Eval num_timesteps=259000, episode_reward=191.03 +/- 26.19
Episode length: 497.80 +/- 107.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.883    |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.42     |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 101      |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 48       |
|    time_elapsed     | 5365     |
|    total_timesteps  | 259399   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.03     |
|    n_updates        | 62349    |
----------------------------------
Eval num_timesteps=259500, episode_reward=94.58 +/- 44.38
Episode length: 114.22 +/- 153.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 94.6     |
| rollout/            |          |
|    exploration_rate | 0.882    |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.34     |
|    n_updates        | 62374    |
----------------------------------
Eval num_timesteps=260000, episode_reward=197.34 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.881    |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.26     |
|    n_updates        | 62499    |
----------------------------------
Eval num_timesteps=260500, episode_reward=-9.61 +/- 13.86
Episode length: 49.16 +/- 13.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.2     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 0.88     |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.45     |
|    n_updates        | 62624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 233      |
|    ep_rew_mean      | 102      |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 48       |
|    time_elapsed     | 5386     |
|    total_timesteps  | 260603   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.65     |
|    n_updates        | 62650    |
----------------------------------
Eval num_timesteps=261000, episode_reward=2.27 +/- 34.35
Episode length: 49.06 +/- 16.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | 2.27     |
| rollout/            |          |
|    exploration_rate | 0.879    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.03     |
|    n_updates        | 62749    |
----------------------------------
Eval num_timesteps=261500, episode_reward=73.95 +/- 45.38
Episode length: 76.64 +/- 93.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.6     |
|    mean_reward      | 73.9     |
| rollout/            |          |
|    exploration_rate | 0.878    |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.7      |
|    n_updates        | 62874    |
----------------------------------
Eval num_timesteps=262000, episode_reward=181.80 +/- 47.23
Episode length: 476.94 +/- 144.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.83     |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 242      |
|    ep_rew_mean      | 105      |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 48       |
|    time_elapsed     | 5404     |
|    total_timesteps  | 262204   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.64     |
|    n_updates        | 63050    |
----------------------------------
Eval num_timesteps=262500, episode_reward=192.73 +/- 29.20
Episode length: 515.40 +/- 67.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.876    |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 63124    |
----------------------------------
Eval num_timesteps=263000, episode_reward=197.68 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.1      |
|    n_updates        | 63249    |
----------------------------------
Eval num_timesteps=263500, episode_reward=122.32 +/- 50.89
Episode length: 208.48 +/- 217.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.874    |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.48     |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 242      |
|    ep_rew_mean      | 106      |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 48       |
|    time_elapsed     | 5441     |
|    total_timesteps  | 263860   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.38     |
|    n_updates        | 63464    |
----------------------------------
Eval num_timesteps=264000, episode_reward=0.29 +/- 32.17
Episode length: 49.02 +/- 18.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49       |
|    mean_reward      | 0.286    |
| rollout/            |          |
|    exploration_rate | 0.873    |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.7      |
|    n_updates        | 63499    |
----------------------------------
Eval num_timesteps=264500, episode_reward=85.43 +/- 13.86
Episode length: 62.98 +/- 18.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63       |
|    mean_reward      | 85.4     |
| rollout/            |          |
|    exploration_rate | 0.872    |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.24     |
|    n_updates        | 63624    |
----------------------------------
Eval num_timesteps=265000, episode_reward=197.62 +/- 0.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.871    |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3        |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 242      |
|    ep_rew_mean      | 106      |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 48       |
|    time_elapsed     | 5461     |
|    total_timesteps  | 265475   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.23     |
|    n_updates        | 63868    |
----------------------------------
Eval num_timesteps=265500, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.87     |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.74     |
|    n_updates        | 63874    |
----------------------------------
Eval num_timesteps=266000, episode_reward=182.59 +/- 51.42
Episode length: 487.34 +/- 127.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.869    |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.97     |
|    n_updates        | 63999    |
----------------------------------
Eval num_timesteps=266500, episode_reward=197.65 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.868    |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.94     |
|    n_updates        | 64124    |
----------------------------------
Eval num_timesteps=267000, episode_reward=26.23 +/- 52.60
Episode length: 61.26 +/- 68.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.3     |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.867    |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.76     |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 251      |
|    ep_rew_mean      | 109      |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 48       |
|    time_elapsed     | 5507     |
|    total_timesteps  | 267076   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.11     |
|    n_updates        | 64268    |
----------------------------------
Eval num_timesteps=267500, episode_reward=197.71 +/- 0.59
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.866    |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.47     |
|    n_updates        | 64374    |
----------------------------------
Eval num_timesteps=268000, episode_reward=-11.56 +/- 0.22
Episode length: 49.52 +/- 16.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.865    |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.49     |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 252      |
|    ep_rew_mean      | 109      |
|    exploration_rate | 0.865    |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 48       |
|    time_elapsed     | 5524     |
|    total_timesteps  | 268274   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.26     |
|    n_updates        | 64568    |
----------------------------------
Eval num_timesteps=268500, episode_reward=197.80 +/- 0.48
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.864    |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.01     |
|    n_updates        | 64624    |
----------------------------------
Eval num_timesteps=269000, episode_reward=115.92 +/- 101.00
Episode length: 334.08 +/- 234.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 334      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.863    |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.74     |
|    n_updates        | 64749    |
----------------------------------
Eval num_timesteps=269500, episode_reward=197.62 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.16     |
|    n_updates        | 64874    |
----------------------------------
Eval num_timesteps=270000, episode_reward=197.90 +/- 0.35
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.861    |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.31     |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 262      |
|    ep_rew_mean      | 110      |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 48       |
|    time_elapsed     | 5580     |
|    total_timesteps  | 270374   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.04     |
|    n_updates        | 65093    |
----------------------------------
Eval num_timesteps=270500, episode_reward=-11.60 +/- 0.00
Episode length: 49.92 +/- 16.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.86     |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.76     |
|    n_updates        | 65124    |
----------------------------------
Eval num_timesteps=271000, episode_reward=197.85 +/- 0.46
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.859    |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.45     |
|    n_updates        | 65249    |
----------------------------------
Eval num_timesteps=271500, episode_reward=33.36 +/- 74.86
Episode length: 119.76 +/- 164.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 33.4     |
| rollout/            |          |
|    exploration_rate | 0.858    |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.42     |
|    n_updates        | 65374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 271      |
|    ep_rew_mean      | 111      |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 48       |
|    time_elapsed     | 5601     |
|    total_timesteps  | 271983   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 65495    |
----------------------------------
Eval num_timesteps=272000, episode_reward=-11.59 +/- 0.00
Episode length: 49.34 +/- 16.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.857    |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.66     |
|    n_updates        | 65499    |
----------------------------------
Eval num_timesteps=272500, episode_reward=112.39 +/- 93.23
Episode length: 303.10 +/- 233.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 303      |
|    mean_reward      | 112      |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.1      |
|    n_updates        | 65624    |
----------------------------------
Eval num_timesteps=273000, episode_reward=161.16 +/- 74.35
Episode length: 428.96 +/- 192.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.855    |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.95     |
|    n_updates        | 65749    |
----------------------------------
Eval num_timesteps=273500, episode_reward=190.86 +/- 26.15
Episode length: 495.36 +/- 117.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.854    |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.13     |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 285      |
|    ep_rew_mean      | 115      |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 48       |
|    time_elapsed     | 5638     |
|    total_timesteps  | 273585   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.23     |
|    n_updates        | 65896    |
----------------------------------
Eval num_timesteps=274000, episode_reward=96.13 +/- 29.57
Episode length: 99.36 +/- 127.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 96.1     |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.86     |
|    n_updates        | 65999    |
----------------------------------
Eval num_timesteps=274500, episode_reward=102.64 +/- 37.77
Episode length: 123.00 +/- 163.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.852    |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.45     |
|    n_updates        | 66124    |
----------------------------------
Eval num_timesteps=275000, episode_reward=124.82 +/- 52.13
Episode length: 219.22 +/- 220.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.852    |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.98     |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 295      |
|    ep_rew_mean      | 117      |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 48       |
|    time_elapsed     | 5652     |
|    total_timesteps  | 275234   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.32     |
|    n_updates        | 66308    |
----------------------------------
Eval num_timesteps=275500, episode_reward=197.42 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.851    |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.61     |
|    n_updates        | 66374    |
----------------------------------
Eval num_timesteps=276000, episode_reward=59.69 +/- 44.45
Episode length: 58.74 +/- 19.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.7     |
|    mean_reward      | 59.7     |
| rollout/            |          |
|    exploration_rate | 0.85     |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.81     |
|    n_updates        | 66499    |
----------------------------------
Eval num_timesteps=276500, episode_reward=184.01 +/- 35.68
Episode length: 472.66 +/- 142.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.849    |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.84     |
|    n_updates        | 66624    |
----------------------------------
Eval num_timesteps=277000, episode_reward=8.61 +/- 49.99
Episode length: 74.24 +/- 93.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.2     |
|    mean_reward      | 8.61     |
| rollout/            |          |
|    exploration_rate | 0.848    |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.07     |
|    n_updates        | 66749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 309      |
|    ep_rew_mean      | 120      |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 48       |
|    time_elapsed     | 5685     |
|    total_timesteps  | 277334   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.26     |
|    n_updates        | 66833    |
----------------------------------
Eval num_timesteps=277500, episode_reward=197.08 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.847    |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.64     |
|    n_updates        | 66874    |
----------------------------------
Eval num_timesteps=278000, episode_reward=0.28 +/- 32.17
Episode length: 51.62 +/- 18.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.6     |
|    mean_reward      | 0.285    |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.53     |
|    n_updates        | 66999    |
----------------------------------
Eval num_timesteps=278500, episode_reward=166.05 +/- 49.05
Episode length: 394.42 +/- 209.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 394      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.845    |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2        |
|    n_updates        | 67124    |
----------------------------------
Eval num_timesteps=279000, episode_reward=15.15 +/- 64.80
Episode length: 101.38 +/- 144.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 15.1     |
| rollout/            |          |
|    exploration_rate | 0.844    |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.63     |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 323      |
|    ep_rew_mean      | 123      |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 48       |
|    time_elapsed     | 5717     |
|    total_timesteps  | 279434   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.31     |
|    n_updates        | 67358    |
----------------------------------
Eval num_timesteps=279500, episode_reward=89.40 +/- 65.82
Episode length: 191.08 +/- 179.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 89.4     |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.68     |
|    n_updates        | 67374    |
----------------------------------
Eval num_timesteps=280000, episode_reward=-7.63 +/- 19.40
Episode length: 52.40 +/- 16.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.4     |
|    mean_reward      | -7.63    |
| rollout/            |          |
|    exploration_rate | 0.842    |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.65     |
|    n_updates        | 67499    |
----------------------------------
Eval num_timesteps=280500, episode_reward=-11.59 +/- 0.01
Episode length: 53.48 +/- 19.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.841    |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.24     |
|    n_updates        | 67624    |
----------------------------------
Eval num_timesteps=281000, episode_reward=142.25 +/- 55.01
Episode length: 363.04 +/- 168.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 363      |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.84     |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.92     |
|    n_updates        | 67749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 328      |
|    ep_rew_mean      | 123      |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 48       |
|    time_elapsed     | 5737     |
|    total_timesteps  | 281040   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.74     |
|    n_updates        | 67759    |
----------------------------------
Eval num_timesteps=281500, episode_reward=26.22 +/- 52.60
Episode length: 73.88 +/- 72.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.9     |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.839    |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.6      |
|    n_updates        | 67874    |
----------------------------------
Eval num_timesteps=282000, episode_reward=-11.59 +/- 0.00
Episode length: 49.88 +/- 15.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.9     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.838    |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.13     |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 328      |
|    ep_rew_mean      | 123      |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 49       |
|    time_elapsed     | 5742     |
|    total_timesteps  | 282246   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.93     |
|    n_updates        | 68061    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 318      |
|    ep_rew_mean      | 120      |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 49       |
|    time_elapsed     | 5742     |
|    total_timesteps  | 282409   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.79     |
|    n_updates        | 68102    |
----------------------------------
Eval num_timesteps=282500, episode_reward=196.77 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.837    |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.4      |
|    n_updates        | 68124    |
----------------------------------
Eval num_timesteps=283000, episode_reward=195.60 +/- 15.47
Episode length: 515.46 +/- 66.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.86     |
|    n_updates        | 68249    |
----------------------------------
Eval num_timesteps=283500, episode_reward=114.83 +/- 96.14
Episode length: 311.84 +/- 240.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 312      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 0.835    |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.4      |
|    n_updates        | 68374    |
----------------------------------
Eval num_timesteps=284000, episode_reward=191.01 +/- 26.18
Episode length: 497.10 +/- 110.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.834    |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.72     |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 327      |
|    ep_rew_mean      | 122      |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 49       |
|    time_elapsed     | 5795     |
|    total_timesteps  | 284061   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.67     |
|    n_updates        | 68515    |
----------------------------------
Eval num_timesteps=284500, episode_reward=102.50 +/- 74.42
Episode length: 262.26 +/- 194.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 262      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.63     |
|    n_updates        | 68624    |
----------------------------------
Eval num_timesteps=285000, episode_reward=195.86 +/- 3.91
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.832    |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.77     |
|    n_updates        | 68749    |
----------------------------------
Eval num_timesteps=285500, episode_reward=17.13 +/- 65.46
Episode length: 92.42 +/- 144.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.4     |
|    mean_reward      | 17.1     |
| rollout/            |          |
|    exploration_rate | 0.831    |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.8      |
|    n_updates        | 68874    |
----------------------------------
Eval num_timesteps=286000, episode_reward=194.99 +/- 15.40
Episode length: 518.82 +/- 43.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 519      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.83     |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.64     |
|    n_updates        | 68999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 341      |
|    ep_rew_mean      | 125      |
|    exploration_rate | 0.829    |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 49       |
|    time_elapsed     | 5836     |
|    total_timesteps  | 286161   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.74     |
|    n_updates        | 69040    |
----------------------------------
Eval num_timesteps=286500, episode_reward=133.22 +/- 53.92
Episode length: 333.50 +/- 174.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 334      |
|    mean_reward      | 133      |
| rollout/            |          |
|    exploration_rate | 0.829    |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.79     |
|    n_updates        | 69124    |
----------------------------------
Eval num_timesteps=287000, episode_reward=181.31 +/- 37.89
Episode length: 469.92 +/- 138.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 470      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.46     |
|    n_updates        | 69249    |
----------------------------------
Eval num_timesteps=287500, episode_reward=196.97 +/- 0.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.827    |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.39     |
|    n_updates        | 69374    |
----------------------------------
Eval num_timesteps=288000, episode_reward=195.32 +/- 15.43
Episode length: 516.18 +/- 61.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.826    |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.01     |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 351      |
|    ep_rew_mean      | 128      |
|    exploration_rate | 0.825    |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 48       |
|    time_elapsed     | 5889     |
|    total_timesteps  | 288261   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.05     |
|    n_updates        | 69565    |
----------------------------------
Eval num_timesteps=288500, episode_reward=197.95 +/- 0.24
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.825    |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.34     |
|    n_updates        | 69624    |
----------------------------------
Eval num_timesteps=289000, episode_reward=195.54 +/- 15.46
Episode length: 516.00 +/- 63.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.824    |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.65     |
|    n_updates        | 69749    |
----------------------------------
Eval num_timesteps=289500, episode_reward=17.13 +/- 65.46
Episode length: 95.96 +/- 143.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 17.1     |
| rollout/            |          |
|    exploration_rate | 0.823    |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.5      |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 346      |
|    ep_rew_mean      | 127      |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 48       |
|    time_elapsed     | 5923     |
|    total_timesteps  | 289861   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 69965    |
----------------------------------
Eval num_timesteps=290000, episode_reward=140.93 +/- 73.76
Episode length: 359.04 +/- 209.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 359      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.822    |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.17     |
|    n_updates        | 69999    |
----------------------------------
Eval num_timesteps=290500, episode_reward=24.65 +/- 60.29
Episode length: 89.40 +/- 120.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.821    |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.6      |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 346      |
|    ep_rew_mean      | 127      |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 49       |
|    time_elapsed     | 5936     |
|    total_timesteps  | 290999   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.36     |
|    n_updates        | 70249    |
----------------------------------
Eval num_timesteps=291000, episode_reward=170.97 +/- 46.96
Episode length: 445.58 +/- 143.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 446      |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 291000   |
---------------------------------
Eval num_timesteps=291500, episode_reward=179.13 +/- 40.04
Episode length: 473.14 +/- 120.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.819    |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.93     |
|    n_updates        | 70374    |
----------------------------------
Eval num_timesteps=292000, episode_reward=89.62 +/- 15.48
Episode length: 72.46 +/- 69.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.5     |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.46     |
|    n_updates        | 70499    |
----------------------------------
Eval num_timesteps=292500, episode_reward=87.41 +/- 0.01
Episode length: 61.84 +/- 24.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.8     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.817    |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.59     |
|    n_updates        | 70624    |
----------------------------------
Eval num_timesteps=293000, episode_reward=31.97 +/- 49.14
Episode length: 54.94 +/- 19.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.9     |
|    mean_reward      | 32       |
| rollout/            |          |
|    exploration_rate | 0.816    |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.06     |
|    n_updates        | 70749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 360      |
|    ep_rew_mean      | 132      |
|    exploration_rate | 0.816    |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 49       |
|    time_elapsed     | 5970     |
|    total_timesteps  | 293099   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.96     |
|    n_updates        | 70774    |
----------------------------------
Eval num_timesteps=293500, episode_reward=179.78 +/- 40.32
Episode length: 462.42 +/- 145.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 462      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.79     |
|    n_updates        | 70874    |
----------------------------------
Eval num_timesteps=294000, episode_reward=141.48 +/- 54.56
Episode length: 359.00 +/- 171.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 359      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.814    |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.42     |
|    n_updates        | 70999    |
----------------------------------
Eval num_timesteps=294500, episode_reward=144.02 +/- 54.39
Episode length: 358.76 +/- 178.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 359      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.813    |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 71124    |
----------------------------------
Eval num_timesteps=295000, episode_reward=157.22 +/- 52.36
Episode length: 396.78 +/- 175.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 397      |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.84     |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 374      |
|    ep_rew_mean      | 136      |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 49       |
|    time_elapsed     | 6016     |
|    total_timesteps  | 295199   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.11     |
|    n_updates        | 71299    |
----------------------------------
Eval num_timesteps=295500, episode_reward=193.37 +/- 21.64
Episode length: 505.86 +/- 93.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.811    |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 71374    |
----------------------------------
Eval num_timesteps=296000, episode_reward=197.62 +/- 1.39
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.81     |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.9      |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 378      |
|    ep_rew_mean      | 137      |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 49       |
|    time_elapsed     | 6046     |
|    total_timesteps  | 296333   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.35     |
|    n_updates        | 71583    |
----------------------------------
Eval num_timesteps=296500, episode_reward=186.19 +/- 32.93
Episode length: 476.64 +/- 145.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.809    |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.29     |
|    n_updates        | 71624    |
----------------------------------
Eval num_timesteps=297000, episode_reward=-1.69 +/- 29.70
Episode length: 47.24 +/- 17.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | -1.69    |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.6      |
|    n_updates        | 71749    |
----------------------------------
Eval num_timesteps=297500, episode_reward=113.64 +/- 46.69
Episode length: 194.70 +/- 191.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 114      |
| rollout/            |          |
|    exploration_rate | 0.807    |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.93     |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 392      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 49       |
|    time_elapsed     | 6067     |
|    total_timesteps  | 297931   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.26     |
|    n_updates        | 71982    |
----------------------------------
Eval num_timesteps=298000, episode_reward=-7.63 +/- 19.40
Episode length: 50.32 +/- 17.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.3     |
|    mean_reward      | -7.63    |
| rollout/            |          |
|    exploration_rate | 0.806    |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 71999    |
----------------------------------
Eval num_timesteps=298500, episode_reward=-1.70 +/- 29.70
Episode length: 52.74 +/- 18.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.7     |
|    mean_reward      | -1.7     |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.1      |
|    n_updates        | 72124    |
----------------------------------
Eval num_timesteps=299000, episode_reward=197.48 +/- 0.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.804    |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 72249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 397      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 49       |
|    time_elapsed     | 6086     |
|    total_timesteps  | 299068   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.48     |
|    n_updates        | 72266    |
----------------------------------
Eval num_timesteps=299500, episode_reward=197.85 +/- 0.41
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.803    |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.64     |
|    n_updates        | 72374    |
----------------------------------
Eval num_timesteps=300000, episode_reward=190.82 +/- 26.14
Episode length: 504.76 +/- 80.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.802    |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 72499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 396      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.802    |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 49       |
|    time_elapsed     | 6116     |
|    total_timesteps  | 300164   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.07     |
|    n_updates        | 72540    |
----------------------------------
Eval num_timesteps=300500, episode_reward=20.49 +/- 55.26
Episode length: 84.84 +/- 101.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.5      |
|    n_updates        | 72624    |
----------------------------------
Eval num_timesteps=301000, episode_reward=94.01 +/- 26.16
Episode length: 96.94 +/- 110.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.9     |
|    mean_reward      | 94       |
| rollout/            |          |
|    exploration_rate | 0.8      |
| time/               |          |
|    total_timesteps  | 301000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.821    |
|    n_updates        | 72749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 391      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 49       |
|    time_elapsed     | 6122     |
|    total_timesteps  | 301306   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.62     |
|    n_updates        | 72826    |
----------------------------------
Eval num_timesteps=301500, episode_reward=-1.69 +/- 29.70
Episode length: 46.50 +/- 15.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | -1.69    |
| rollout/            |          |
|    exploration_rate | 0.799    |
| time/               |          |
|    total_timesteps  | 301500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.32     |
|    n_updates        | 72874    |
----------------------------------
Eval num_timesteps=302000, episode_reward=184.67 +/- 49.59
Episode length: 497.16 +/- 110.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 302000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.07     |
|    n_updates        | 72999    |
----------------------------------
Eval num_timesteps=302500, episode_reward=177.67 +/- 42.35
Episode length: 458.54 +/- 145.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.797    |
| time/               |          |
|    total_timesteps  | 302500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.63     |
|    n_updates        | 73124    |
----------------------------------
Eval num_timesteps=303000, episode_reward=181.24 +/- 37.86
Episode length: 459.00 +/- 163.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.796    |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 73249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 395      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.795    |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 49       |
|    time_elapsed     | 6165     |
|    total_timesteps  | 303406   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 73351    |
----------------------------------
Eval num_timesteps=303500, episode_reward=4.25 +/- 36.29
Episode length: 54.38 +/- 22.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.4     |
|    mean_reward      | 4.25     |
| rollout/            |          |
|    exploration_rate | 0.795    |
| time/               |          |
|    total_timesteps  | 303500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.9      |
|    n_updates        | 73374    |
----------------------------------
Eval num_timesteps=304000, episode_reward=181.96 +/- 38.16
Episode length: 471.86 +/- 133.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 472      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.794    |
| time/               |          |
|    total_timesteps  | 304000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.42     |
|    n_updates        | 73499    |
----------------------------------
Eval num_timesteps=304500, episode_reward=94.01 +/- 26.14
Episode length: 93.86 +/- 111.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.9     |
|    mean_reward      | 94       |
| rollout/            |          |
|    exploration_rate | 0.793    |
| time/               |          |
|    total_timesteps  | 304500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.22     |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 392      |
|    ep_rew_mean      | 138      |
|    exploration_rate | 0.793    |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 49       |
|    time_elapsed     | 6183     |
|    total_timesteps  | 304688   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.34     |
|    n_updates        | 73671    |
----------------------------------
Eval num_timesteps=305000, episode_reward=89.59 +/- 15.26
Episode length: 74.98 +/- 70.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.792    |
| time/               |          |
|    total_timesteps  | 305000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 73749    |
----------------------------------
Eval num_timesteps=305500, episode_reward=20.29 +/- 50.92
Episode length: 74.12 +/- 78.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.1     |
|    mean_reward      | 20.3     |
| rollout/            |          |
|    exploration_rate | 0.791    |
| time/               |          |
|    total_timesteps  | 305500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.43     |
|    n_updates        | 73874    |
----------------------------------
Eval num_timesteps=306000, episode_reward=188.50 +/- 29.82
Episode length: 486.76 +/- 129.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.07     |
|    n_updates        | 73999    |
----------------------------------
Eval num_timesteps=306500, episode_reward=186.13 +/- 32.99
Episode length: 490.56 +/- 103.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 491      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.789    |
| time/               |          |
|    total_timesteps  | 306500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.8      |
|    n_updates        | 74124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 397      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 49       |
|    time_elapsed     | 6217     |
|    total_timesteps  | 306788   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.5      |
|    n_updates        | 74196    |
----------------------------------
Eval num_timesteps=307000, episode_reward=31.98 +/- 49.13
Episode length: 53.22 +/- 23.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.2     |
|    mean_reward      | 32       |
| rollout/            |          |
|    exploration_rate | 0.788    |
| time/               |          |
|    total_timesteps  | 307000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.04     |
|    n_updates        | 74249    |
----------------------------------
Eval num_timesteps=307500, episode_reward=144.55 +/- 54.91
Episode length: 304.22 +/- 230.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 304      |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 0.787    |
| time/               |          |
|    total_timesteps  | 307500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.75     |
|    n_updates        | 74374    |
----------------------------------
Eval num_timesteps=308000, episode_reward=60.46 +/- 61.29
Episode length: 124.56 +/- 131.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 60.5     |
| rollout/            |          |
|    exploration_rate | 0.786    |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.75     |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 401      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 49       |
|    time_elapsed     | 6232     |
|    total_timesteps  | 308411   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.24     |
|    n_updates        | 74602    |
----------------------------------
Eval num_timesteps=308500, episode_reward=160.10 +/- 50.87
Episode length: 376.84 +/- 216.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 377      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.785    |
| time/               |          |
|    total_timesteps  | 308500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.13     |
|    n_updates        | 74624    |
----------------------------------
Eval num_timesteps=309000, episode_reward=105.10 +/- 81.00
Episode length: 271.78 +/- 211.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 272      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.784    |
| time/               |          |
|    total_timesteps  | 309000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.34     |
|    n_updates        | 74749    |
----------------------------------
Eval num_timesteps=309500, episode_reward=124.44 +/- 51.61
Episode length: 314.04 +/- 161.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 314      |
|    mean_reward      | 124      |
| rollout/            |          |
|    exploration_rate | 0.783    |
| time/               |          |
|    total_timesteps  | 309500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.923    |
|    n_updates        | 74874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 394      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.783    |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 49       |
|    time_elapsed     | 6260     |
|    total_timesteps  | 309725   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.37     |
|    n_updates        | 74931    |
----------------------------------
Eval num_timesteps=310000, episode_reward=122.35 +/- 50.94
Episode length: 206.52 +/- 219.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.782    |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.92     |
|    n_updates        | 74999    |
----------------------------------
Eval num_timesteps=310500, episode_reward=195.09 +/- 15.56
Episode length: 516.22 +/- 61.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.781    |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.5      |
|    n_updates        | 75124    |
----------------------------------
Eval num_timesteps=311000, episode_reward=135.40 +/- 54.14
Episode length: 334.84 +/- 174.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 335      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.78     |
| time/               |          |
|    total_timesteps  | 311000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 75249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 394      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 49       |
|    time_elapsed     | 6291     |
|    total_timesteps  | 311402   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.08     |
|    n_updates        | 75350    |
----------------------------------
Eval num_timesteps=311500, episode_reward=137.70 +/- 54.58
Episode length: 323.62 +/- 195.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 324      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 311500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.1      |
|    n_updates        | 75374    |
----------------------------------
Eval num_timesteps=312000, episode_reward=-9.61 +/- 13.86
Episode length: 49.30 +/- 16.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 0.778    |
| time/               |          |
|    total_timesteps  | 312000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.66     |
|    n_updates        | 75499    |
----------------------------------
Eval num_timesteps=312500, episode_reward=69.59 +/- 38.03
Episode length: 58.38 +/- 18.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.4     |
|    mean_reward      | 69.6     |
| rollout/            |          |
|    exploration_rate | 0.777    |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.95     |
|    n_updates        | 75624    |
----------------------------------
Eval num_timesteps=313000, episode_reward=8.37 +/- 45.07
Episode length: 59.92 +/- 70.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.9     |
|    mean_reward      | 8.37     |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 313000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 75749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 395      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 49       |
|    time_elapsed     | 6307     |
|    total_timesteps  | 313044   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.43     |
|    n_updates        | 75760    |
----------------------------------
Eval num_timesteps=313500, episode_reward=67.59 +/- 39.59
Episode length: 62.64 +/- 21.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.6     |
|    mean_reward      | 67.6     |
| rollout/            |          |
|    exploration_rate | 0.775    |
| time/               |          |
|    total_timesteps  | 313500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.42     |
|    n_updates        | 75874    |
----------------------------------
Eval num_timesteps=314000, episode_reward=142.38 +/- 54.97
Episode length: 293.56 +/- 231.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 294      |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.774    |
| time/               |          |
|    total_timesteps  | 314000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.69     |
|    n_updates        | 75999    |
----------------------------------
Eval num_timesteps=314500, episode_reward=181.91 +/- 38.14
Episode length: 459.52 +/- 162.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 460      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.773    |
| time/               |          |
|    total_timesteps  | 314500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.02     |
|    n_updates        | 76124    |
----------------------------------
Eval num_timesteps=315000, episode_reward=190.74 +/- 22.16
Episode length: 505.74 +/- 94.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.772    |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 76249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 399      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.772    |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 49       |
|    time_elapsed     | 6346     |
|    total_timesteps  | 315144   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.52     |
|    n_updates        | 76285    |
----------------------------------
Eval num_timesteps=315500, episode_reward=98.70 +/- 43.99
Episode length: 221.92 +/- 147.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 98.7     |
| rollout/            |          |
|    exploration_rate | 0.771    |
| time/               |          |
|    total_timesteps  | 315500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.16     |
|    n_updates        | 76374    |
----------------------------------
Eval num_timesteps=316000, episode_reward=89.62 +/- 15.48
Episode length: 74.42 +/- 67.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.4     |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.77     |
| time/               |          |
|    total_timesteps  | 316000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.22     |
|    n_updates        | 76499    |
----------------------------------
Eval num_timesteps=316500, episode_reward=105.06 +/- 45.01
Episode length: 165.24 +/- 176.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.769    |
| time/               |          |
|    total_timesteps  | 316500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.38     |
|    n_updates        | 76624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 395      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 49       |
|    time_elapsed     | 6360     |
|    total_timesteps  | 316817   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 76704    |
----------------------------------
Eval num_timesteps=317000, episode_reward=85.97 +/- 38.50
Episode length: 91.50 +/- 112.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 86       |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 317000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.31     |
|    n_updates        | 76749    |
----------------------------------
Eval num_timesteps=317500, episode_reward=103.72 +/- 41.53
Episode length: 240.80 +/- 147.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 241      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.767    |
| time/               |          |
|    total_timesteps  | 317500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.19     |
|    n_updates        | 76874    |
----------------------------------
Eval num_timesteps=318000, episode_reward=77.83 +/- 46.71
Episode length: 149.38 +/- 133.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 77.8     |
| rollout/            |          |
|    exploration_rate | 0.766    |
| time/               |          |
|    total_timesteps  | 318000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 76999    |
----------------------------------
Eval num_timesteps=318500, episode_reward=-11.59 +/- 0.01
Episode length: 48.18 +/- 15.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 318500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 77124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 395      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 50       |
|    time_elapsed     | 6377     |
|    total_timesteps  | 318917   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.75     |
|    n_updates        | 77229    |
----------------------------------
Eval num_timesteps=319000, episode_reward=197.82 +/- 0.83
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.764    |
| time/               |          |
|    total_timesteps  | 319000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.78     |
|    n_updates        | 77249    |
----------------------------------
Eval num_timesteps=319500, episode_reward=59.67 +/- 44.44
Episode length: 58.08 +/- 28.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | 59.7     |
| rollout/            |          |
|    exploration_rate | 0.763    |
| time/               |          |
|    total_timesteps  | 319500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 77374    |
----------------------------------
Eval num_timesteps=320000, episode_reward=99.31 +/- 60.12
Episode length: 172.48 +/- 190.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 99.3     |
| rollout/            |          |
|    exploration_rate | 0.762    |
| time/               |          |
|    total_timesteps  | 320000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.68     |
|    n_updates        | 77499    |
----------------------------------
Eval num_timesteps=320500, episode_reward=105.38 +/- 53.61
Episode length: 221.38 +/- 176.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.761    |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.63     |
|    n_updates        | 77624    |
----------------------------------
Eval num_timesteps=321000, episode_reward=96.26 +/- 36.01
Episode length: 230.10 +/- 132.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 96.3     |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.1      |
|    n_updates        | 77749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 400      |
|    ep_rew_mean      | 144      |
|    exploration_rate | 0.76     |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 50       |
|    time_elapsed     | 6413     |
|    total_timesteps  | 321017   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 77754    |
----------------------------------
Eval num_timesteps=321500, episode_reward=0.51 +/- 38.77
Episode length: 62.30 +/- 68.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.3     |
|    mean_reward      | 0.51     |
| rollout/            |          |
|    exploration_rate | 0.759    |
| time/               |          |
|    total_timesteps  | 321500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.75     |
|    n_updates        | 77874    |
----------------------------------
Eval num_timesteps=322000, episode_reward=196.76 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.758    |
| time/               |          |
|    total_timesteps  | 322000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.47     |
|    n_updates        | 77999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 399      |
|    ep_rew_mean      | 144      |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 50       |
|    time_elapsed     | 6431     |
|    total_timesteps  | 322158   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.08     |
|    n_updates        | 78039    |
----------------------------------
Eval num_timesteps=322500, episode_reward=20.25 +/- 50.95
Episode length: 62.44 +/- 69.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.4     |
|    mean_reward      | 20.2     |
| rollout/            |          |
|    exploration_rate | 0.757    |
| time/               |          |
|    total_timesteps  | 322500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 78124    |
----------------------------------
Eval num_timesteps=323000, episode_reward=71.74 +/- 79.30
Episode length: 174.70 +/- 191.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 71.7     |
| rollout/            |          |
|    exploration_rate | 0.756    |
| time/               |          |
|    total_timesteps  | 323000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.1      |
|    n_updates        | 78249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 50       |
|    time_elapsed     | 6438     |
|    total_timesteps  | 323297   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.43     |
|    n_updates        | 78324    |
----------------------------------
Eval num_timesteps=323500, episode_reward=100.51 +/- 35.61
Episode length: 146.08 +/- 157.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 101      |
| rollout/            |          |
|    exploration_rate | 0.755    |
| time/               |          |
|    total_timesteps  | 323500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.29     |
|    n_updates        | 78374    |
----------------------------------
Eval num_timesteps=324000, episode_reward=87.41 +/- 0.01
Episode length: 67.62 +/- 22.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.6     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.754    |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 78499    |
----------------------------------
Eval num_timesteps=324500, episode_reward=89.59 +/- 15.26
Episode length: 71.02 +/- 69.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71       |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 324500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.55     |
|    n_updates        | 78624    |
----------------------------------
Eval num_timesteps=325000, episode_reward=58.04 +/- 58.52
Episode length: 105.50 +/- 116.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 58       |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 325000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 78749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 413      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 50       |
|    time_elapsed     | 6451     |
|    total_timesteps  | 325397   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 78849    |
----------------------------------
Eval num_timesteps=325500, episode_reward=93.89 +/- 25.65
Episode length: 93.20 +/- 111.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.2     |
|    mean_reward      | 93.9     |
| rollout/            |          |
|    exploration_rate | 0.752    |
| time/               |          |
|    total_timesteps  | 325500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.52     |
|    n_updates        | 78874    |
----------------------------------
Eval num_timesteps=326000, episode_reward=91.51 +/- 21.33
Episode length: 109.72 +/- 100.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 91.5     |
| rollout/            |          |
|    exploration_rate | 0.751    |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3        |
|    n_updates        | 78999    |
----------------------------------
Eval num_timesteps=326500, episode_reward=88.69 +/- 65.59
Episode length: 164.60 +/- 184.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 88.7     |
| rollout/            |          |
|    exploration_rate | 0.75     |
| time/               |          |
|    total_timesteps  | 326500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.15     |
|    n_updates        | 79124    |
----------------------------------
Eval num_timesteps=327000, episode_reward=125.94 +/- 55.61
Episode length: 307.44 +/- 176.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 307      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.749    |
| time/               |          |
|    total_timesteps  | 327000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.5      |
|    n_updates        | 79249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 413      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 50       |
|    time_elapsed     | 6472     |
|    total_timesteps  | 327497   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.71     |
|    n_updates        | 79374    |
----------------------------------
Eval num_timesteps=327500, episode_reward=117.06 +/- 80.59
Episode length: 290.82 +/- 222.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 291      |
|    mean_reward      | 117      |
| rollout/            |          |
|    exploration_rate | 0.748    |
| time/               |          |
|    total_timesteps  | 327500   |
----------------------------------
Eval num_timesteps=328000, episode_reward=141.36 +/- 54.83
Episode length: 347.20 +/- 184.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 347      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.747    |
| time/               |          |
|    total_timesteps  | 328000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.06     |
|    n_updates        | 79499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 399      |
|    ep_rew_mean      | 144      |
|    exploration_rate | 0.746    |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 50       |
|    time_elapsed     | 6490     |
|    total_timesteps  | 328113   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.47     |
|    n_updates        | 79528    |
----------------------------------
Eval num_timesteps=328500, episode_reward=87.77 +/- 29.45
Episode length: 92.04 +/- 95.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 87.8     |
| rollout/            |          |
|    exploration_rate | 0.746    |
| time/               |          |
|    total_timesteps  | 328500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.35     |
|    n_updates        | 79624    |
----------------------------------
Eval num_timesteps=329000, episode_reward=192.80 +/- 21.53
Episode length: 506.54 +/- 90.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.745    |
| time/               |          |
|    total_timesteps  | 329000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.26     |
|    n_updates        | 79749    |
----------------------------------
Eval num_timesteps=329500, episode_reward=112.81 +/- 73.80
Episode length: 260.16 +/- 213.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 260      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 329500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 79874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 399      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.743    |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 50       |
|    time_elapsed     | 6516     |
|    total_timesteps  | 329713   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 79928    |
----------------------------------
Eval num_timesteps=330000, episode_reward=8.39 +/- 45.10
Episode length: 57.84 +/- 69.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.8     |
|    mean_reward      | 8.39     |
| rollout/            |          |
|    exploration_rate | 0.743    |
| time/               |          |
|    total_timesteps  | 330000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2        |
|    n_updates        | 79999    |
----------------------------------
Eval num_timesteps=330500, episode_reward=79.28 +/- 69.40
Episode length: 171.74 +/- 178.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 79.3     |
| rollout/            |          |
|    exploration_rate | 0.742    |
| time/               |          |
|    total_timesteps  | 330500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.69     |
|    n_updates        | 80124    |
----------------------------------
Eval num_timesteps=331000, episode_reward=184.34 +/- 35.80
Episode length: 468.40 +/- 153.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 468      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.741    |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 80249    |
----------------------------------
Eval num_timesteps=331500, episode_reward=87.41 +/- 0.01
Episode length: 58.12 +/- 15.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.74     |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.39     |
|    n_updates        | 80374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 408      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.739    |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 50       |
|    time_elapsed     | 6539     |
|    total_timesteps  | 331813   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 80453    |
----------------------------------
Eval num_timesteps=332000, episode_reward=96.13 +/- 29.57
Episode length: 98.54 +/- 127.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.5     |
|    mean_reward      | 96.1     |
| rollout/            |          |
|    exploration_rate | 0.739    |
| time/               |          |
|    total_timesteps  | 332000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 80499    |
----------------------------------
Eval num_timesteps=332500, episode_reward=87.41 +/- 0.00
Episode length: 65.02 +/- 24.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65       |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.738    |
| time/               |          |
|    total_timesteps  | 332500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.44     |
|    n_updates        | 80624    |
----------------------------------
Eval num_timesteps=333000, episode_reward=89.59 +/- 15.26
Episode length: 76.92 +/- 68.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.9     |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.737    |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 80749    |
----------------------------------
Eval num_timesteps=333500, episode_reward=105.00 +/- 44.90
Episode length: 155.20 +/- 178.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.736    |
| time/               |          |
|    total_timesteps  | 333500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.28     |
|    n_updates        | 80874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 408      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 50       |
|    time_elapsed     | 6552     |
|    total_timesteps  | 333913   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.11     |
|    n_updates        | 80978    |
----------------------------------
Eval num_timesteps=334000, episode_reward=41.87 +/- 49.34
Episode length: 56.00 +/- 24.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56       |
|    mean_reward      | 41.9     |
| rollout/            |          |
|    exploration_rate | 0.735    |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.81     |
|    n_updates        | 80999    |
----------------------------------
Eval num_timesteps=334500, episode_reward=87.41 +/- 0.00
Episode length: 61.42 +/- 19.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.4     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.734    |
| time/               |          |
|    total_timesteps  | 334500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.13     |
|    n_updates        | 81124    |
----------------------------------
Eval num_timesteps=335000, episode_reward=10.19 +/- 41.01
Episode length: 49.70 +/- 16.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.733    |
| time/               |          |
|    total_timesteps  | 335000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.5      |
|    n_updates        | 81249    |
----------------------------------
Eval num_timesteps=335500, episode_reward=87.41 +/- 0.00
Episode length: 57.58 +/- 19.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.732    |
| time/               |          |
|    total_timesteps  | 335500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.81     |
|    n_updates        | 81374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 403      |
|    ep_rew_mean      | 145      |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 51       |
|    time_elapsed     | 6560     |
|    total_timesteps  | 335547   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.694    |
|    n_updates        | 81386    |
----------------------------------
Eval num_timesteps=336000, episode_reward=28.01 +/- 48.50
Episode length: 50.36 +/- 21.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.4     |
|    mean_reward      | 28       |
| rollout/            |          |
|    exploration_rate | 0.731    |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.92     |
|    n_updates        | 81499    |
----------------------------------
Eval num_timesteps=336500, episode_reward=104.85 +/- 39.96
Episode length: 133.88 +/- 171.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.73     |
| time/               |          |
|    total_timesteps  | 336500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 81624    |
----------------------------------
Eval num_timesteps=337000, episode_reward=79.49 +/- 26.86
Episode length: 55.18 +/- 20.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.2     |
|    mean_reward      | 79.5     |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 337000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.48     |
|    n_updates        | 81749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 51       |
|    time_elapsed     | 6568     |
|    total_timesteps  | 337184   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.88     |
|    n_updates        | 81795    |
----------------------------------
Eval num_timesteps=337500, episode_reward=32.17 +/- 53.58
Episode length: 64.70 +/- 69.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.7     |
|    mean_reward      | 32.2     |
| rollout/            |          |
|    exploration_rate | 0.728    |
| time/               |          |
|    total_timesteps  | 337500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 81874    |
----------------------------------
Eval num_timesteps=338000, episode_reward=116.03 +/- 48.30
Episode length: 178.64 +/- 206.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.727    |
| time/               |          |
|    total_timesteps  | 338000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.16     |
|    n_updates        | 81999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 404      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.726    |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 51       |
|    time_elapsed     | 6576     |
|    total_timesteps  | 338324   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.37     |
|    n_updates        | 82080    |
----------------------------------
Eval num_timesteps=338500, episode_reward=104.88 +/- 40.03
Episode length: 134.74 +/- 171.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.726    |
| time/               |          |
|    total_timesteps  | 338500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 82124    |
----------------------------------
Eval num_timesteps=339000, episode_reward=87.41 +/- 0.01
Episode length: 61.30 +/- 17.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.3     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.725    |
| time/               |          |
|    total_timesteps  | 339000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.959    |
|    n_updates        | 82249    |
----------------------------------
Eval num_timesteps=339500, episode_reward=-9.61 +/- 13.86
Episode length: 46.80 +/- 15.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | -9.61    |
| rollout/            |          |
|    exploration_rate | 0.724    |
| time/               |          |
|    total_timesteps  | 339500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 82374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 51       |
|    time_elapsed     | 6584     |
|    total_timesteps  | 339938   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 82484    |
----------------------------------
Eval num_timesteps=340000, episode_reward=87.41 +/- 0.00
Episode length: 64.46 +/- 22.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.5     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.723    |
| time/               |          |
|    total_timesteps  | 340000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.456    |
|    n_updates        | 82499    |
----------------------------------
Eval num_timesteps=340500, episode_reward=86.88 +/- 1.98
Episode length: 200.04 +/- 65.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 86.9     |
| rollout/            |          |
|    exploration_rate | 0.722    |
| time/               |          |
|    total_timesteps  | 340500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.969    |
|    n_updates        | 82624    |
----------------------------------
Eval num_timesteps=341000, episode_reward=22.07 +/- 46.90
Episode length: 58.56 +/- 38.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.6     |
|    mean_reward      | 22.1     |
| rollout/            |          |
|    exploration_rate | 0.721    |
| time/               |          |
|    total_timesteps  | 341000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 82749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 410      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.721    |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 51       |
|    time_elapsed     | 6595     |
|    total_timesteps  | 341137   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 82784    |
----------------------------------
Eval num_timesteps=341500, episode_reward=-11.60 +/- 0.00
Episode length: 49.54 +/- 20.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.5     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.72     |
| time/               |          |
|    total_timesteps  | 341500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.29     |
|    n_updates        | 82874    |
----------------------------------
Eval num_timesteps=342000, episode_reward=129.44 +/- 56.15
Episode length: 273.98 +/- 216.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 274      |
|    mean_reward      | 129      |
| rollout/            |          |
|    exploration_rate | 0.719    |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.77     |
|    n_updates        | 82999    |
----------------------------------
Eval num_timesteps=342500, episode_reward=89.62 +/- 15.48
Episode length: 73.60 +/- 67.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.718    |
| time/               |          |
|    total_timesteps  | 342500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.35     |
|    n_updates        | 83124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 414      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 51       |
|    time_elapsed     | 6607     |
|    total_timesteps  | 342737   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.83     |
|    n_updates        | 83184    |
----------------------------------
Eval num_timesteps=343000, episode_reward=22.04 +/- 46.87
Episode length: 59.46 +/- 39.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.5     |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.717    |
| time/               |          |
|    total_timesteps  | 343000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 83249    |
----------------------------------
Eval num_timesteps=343500, episode_reward=87.41 +/- 0.01
Episode length: 68.40 +/- 24.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.4     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.716    |
| time/               |          |
|    total_timesteps  | 343500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 83374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 405      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 52       |
|    time_elapsed     | 6612     |
|    total_timesteps  | 343914   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 83478    |
----------------------------------
Eval num_timesteps=344000, episode_reward=37.91 +/- 49.50
Episode length: 60.62 +/- 26.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.6     |
|    mean_reward      | 37.9     |
| rollout/            |          |
|    exploration_rate | 0.715    |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 83499    |
----------------------------------
Eval num_timesteps=344500, episode_reward=122.44 +/- 51.07
Episode length: 208.88 +/- 217.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.714    |
| time/               |          |
|    total_timesteps  | 344500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.44     |
|    n_updates        | 83624    |
----------------------------------
Eval num_timesteps=345000, episode_reward=87.40 +/- 0.00
Episode length: 115.02 +/- 67.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.713    |
| time/               |          |
|    total_timesteps  | 345000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.53     |
|    n_updates        | 83749    |
----------------------------------
Eval num_timesteps=345500, episode_reward=131.17 +/- 53.60
Episode length: 244.70 +/- 229.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 245      |
|    mean_reward      | 131      |
| rollout/            |          |
|    exploration_rate | 0.712    |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.31     |
|    n_updates        | 83874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 52       |
|    time_elapsed     | 6631     |
|    total_timesteps  | 345539   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 83884    |
----------------------------------
Eval num_timesteps=346000, episode_reward=114.78 +/- 46.30
Episode length: 183.10 +/- 203.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 183      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 0.711    |
| time/               |          |
|    total_timesteps  | 346000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.65     |
|    n_updates        | 83999    |
----------------------------------
Eval num_timesteps=346500, episode_reward=159.67 +/- 51.87
Episode length: 368.14 +/- 218.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 368      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.71     |
| time/               |          |
|    total_timesteps  | 346500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.79     |
|    n_updates        | 84124    |
----------------------------------
Eval num_timesteps=347000, episode_reward=91.83 +/- 21.67
Episode length: 77.30 +/- 93.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.3     |
|    mean_reward      | 91.8     |
| rollout/            |          |
|    exploration_rate | 0.709    |
| time/               |          |
|    total_timesteps  | 347000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.85     |
|    n_updates        | 84249    |
----------------------------------
Eval num_timesteps=347500, episode_reward=147.49 +/- 55.14
Episode length: 331.86 +/- 220.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 332      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 347500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 84374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 52       |
|    time_elapsed     | 6660     |
|    total_timesteps  | 347639   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.82     |
|    n_updates        | 84409    |
----------------------------------
Eval num_timesteps=348000, episode_reward=116.09 +/- 52.27
Episode length: 188.82 +/- 210.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.707    |
| time/               |          |
|    total_timesteps  | 348000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.12     |
|    n_updates        | 84499    |
----------------------------------
Eval num_timesteps=348500, episode_reward=6.21 +/- 37.99
Episode length: 54.70 +/- 22.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.7     |
|    mean_reward      | 6.21     |
| rollout/            |          |
|    exploration_rate | 0.706    |
| time/               |          |
|    total_timesteps  | 348500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.908    |
|    n_updates        | 84624    |
----------------------------------
Eval num_timesteps=349000, episode_reward=45.96 +/- 53.18
Episode length: 94.58 +/- 91.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 46       |
| rollout/            |          |
|    exploration_rate | 0.705    |
| time/               |          |
|    total_timesteps  | 349000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.67     |
|    n_updates        | 84749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 408      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.704    |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 52       |
|    time_elapsed     | 6671     |
|    total_timesteps  | 349247   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.31     |
|    n_updates        | 84811    |
----------------------------------
Eval num_timesteps=349500, episode_reward=97.64 +/- 32.16
Episode length: 238.48 +/- 117.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 97.6     |
| rollout/            |          |
|    exploration_rate | 0.704    |
| time/               |          |
|    total_timesteps  | 349500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.78     |
|    n_updates        | 84874    |
----------------------------------
Eval num_timesteps=350000, episode_reward=22.02 +/- 46.88
Episode length: 58.86 +/- 20.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.9     |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.703    |
| time/               |          |
|    total_timesteps  | 350000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.92     |
|    n_updates        | 84999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 407      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 52       |
|    time_elapsed     | 6680     |
|    total_timesteps  | 350464   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.22     |
|    n_updates        | 85115    |
----------------------------------
Eval num_timesteps=350500, episode_reward=71.75 +/- 41.91
Episode length: 66.52 +/- 69.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.5     |
|    mean_reward      | 71.7     |
| rollout/            |          |
|    exploration_rate | 0.702    |
| time/               |          |
|    total_timesteps  | 350500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.27     |
|    n_updates        | 85124    |
----------------------------------
Eval num_timesteps=351000, episode_reward=124.45 +/- 51.61
Episode length: 218.20 +/- 220.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 124      |
| rollout/            |          |
|    exploration_rate | 0.701    |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.934    |
|    n_updates        | 85249    |
----------------------------------
Eval num_timesteps=351500, episode_reward=134.19 +/- 54.27
Episode length: 258.76 +/- 236.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 259      |
|    mean_reward      | 134      |
| rollout/            |          |
|    exploration_rate | 0.7      |
| time/               |          |
|    total_timesteps  | 351500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.05     |
|    n_updates        | 85374    |
----------------------------------
Eval num_timesteps=352000, episode_reward=89.12 +/- 39.08
Episode length: 202.60 +/- 128.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 89.1     |
| rollout/            |          |
|    exploration_rate | 0.699    |
| time/               |          |
|    total_timesteps  | 352000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.09     |
|    n_updates        | 85499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 407      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.699    |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 52       |
|    time_elapsed     | 6703     |
|    total_timesteps  | 352087   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.46     |
|    n_updates        | 85521    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 394      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 52       |
|    time_elapsed     | 6703     |
|    total_timesteps  | 352469   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.42     |
|    n_updates        | 85617    |
----------------------------------
Eval num_timesteps=352500, episode_reward=40.06 +/- 53.86
Episode length: 77.00 +/- 79.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77       |
|    mean_reward      | 40.1     |
| rollout/            |          |
|    exploration_rate | 0.698    |
| time/               |          |
|    total_timesteps  | 352500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.92     |
|    n_updates        | 85624    |
----------------------------------
Eval num_timesteps=353000, episode_reward=22.25 +/- 51.54
Episode length: 62.06 +/- 70.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.1     |
|    mean_reward      | 22.2     |
| rollout/            |          |
|    exploration_rate | 0.697    |
| time/               |          |
|    total_timesteps  | 353000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 85749    |
----------------------------------
Eval num_timesteps=353500, episode_reward=-1.70 +/- 29.70
Episode length: 47.68 +/- 17.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | -1.7     |
| rollout/            |          |
|    exploration_rate | 0.696    |
| time/               |          |
|    total_timesteps  | 353500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.13     |
|    n_updates        | 85874    |
----------------------------------
Eval num_timesteps=354000, episode_reward=107.19 +/- 42.24
Episode length: 151.02 +/- 177.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.695    |
| time/               |          |
|    total_timesteps  | 354000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 85999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 390      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 52       |
|    time_elapsed     | 6715     |
|    total_timesteps  | 354109   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.52     |
|    n_updates        | 86027    |
----------------------------------
Eval num_timesteps=354500, episode_reward=47.78 +/- 48.48
Episode length: 86.52 +/- 57.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.5     |
|    mean_reward      | 47.8     |
| rollout/            |          |
|    exploration_rate | 0.694    |
| time/               |          |
|    total_timesteps  | 354500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 86124    |
----------------------------------
Eval num_timesteps=355000, episode_reward=51.71 +/- 47.52
Episode length: 94.56 +/- 66.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 51.7     |
| rollout/            |          |
|    exploration_rate | 0.693    |
| time/               |          |
|    total_timesteps  | 355000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 86249    |
----------------------------------
Eval num_timesteps=355500, episode_reward=-3.64 +/- 26.85
Episode length: 52.50 +/- 17.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.5     |
|    mean_reward      | -3.64    |
| rollout/            |          |
|    exploration_rate | 0.692    |
| time/               |          |
|    total_timesteps  | 355500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.4      |
|    n_updates        | 86374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 389      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 52       |
|    time_elapsed     | 6723     |
|    total_timesteps  | 355727   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.35     |
|    n_updates        | 86431    |
----------------------------------
Eval num_timesteps=356000, episode_reward=34.32 +/- 57.81
Episode length: 91.20 +/- 104.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.2     |
|    mean_reward      | 34.3     |
| rollout/            |          |
|    exploration_rate | 0.691    |
| time/               |          |
|    total_timesteps  | 356000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.67     |
|    n_updates        | 86499    |
----------------------------------
Eval num_timesteps=356500, episode_reward=89.44 +/- 15.29
Episode length: 189.36 +/- 75.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 89.4     |
| rollout/            |          |
|    exploration_rate | 0.69     |
| time/               |          |
|    total_timesteps  | 356500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.76     |
|    n_updates        | 86624    |
----------------------------------
Eval num_timesteps=357000, episode_reward=93.88 +/- 25.82
Episode length: 141.14 +/- 124.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 93.9     |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 357000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.85     |
|    n_updates        | 86749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 145      |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 53       |
|    time_elapsed     | 6736     |
|    total_timesteps  | 357389   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 86847    |
----------------------------------
Eval num_timesteps=357500, episode_reward=86.82 +/- 1.23
Episode length: 186.00 +/- 70.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 86.8     |
| rollout/            |          |
|    exploration_rate | 0.688    |
| time/               |          |
|    total_timesteps  | 357500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.15     |
|    n_updates        | 86874    |
----------------------------------
Eval num_timesteps=358000, episode_reward=97.48 +/- 31.71
Episode length: 237.98 +/- 119.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 97.5     |
| rollout/            |          |
|    exploration_rate | 0.687    |
| time/               |          |
|    total_timesteps  | 358000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.985    |
|    n_updates        | 86999    |
----------------------------------
Eval num_timesteps=358500, episode_reward=57.67 +/- 45.34
Episode length: 90.76 +/- 56.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 57.7     |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.84     |
|    n_updates        | 87124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 376      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 53       |
|    time_elapsed     | 6752     |
|    total_timesteps  | 358605   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 87151    |
----------------------------------
Eval num_timesteps=359000, episode_reward=28.01 +/- 48.50
Episode length: 94.66 +/- 83.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 28       |
| rollout/            |          |
|    exploration_rate | 0.685    |
| time/               |          |
|    total_timesteps  | 359000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.79     |
|    n_updates        | 87249    |
----------------------------------
Eval num_timesteps=359500, episode_reward=8.21 +/- 39.60
Episode length: 58.02 +/- 28.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58       |
|    mean_reward      | 8.21     |
| rollout/            |          |
|    exploration_rate | 0.684    |
| time/               |          |
|    total_timesteps  | 359500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.934    |
|    n_updates        | 87374    |
----------------------------------
Eval num_timesteps=360000, episode_reward=104.39 +/- 40.00
Episode length: 134.72 +/- 171.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.683    |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.74     |
|    n_updates        | 87499    |
----------------------------------
Eval num_timesteps=360500, episode_reward=65.90 +/- 58.85
Episode length: 155.86 +/- 143.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 65.9     |
| rollout/            |          |
|    exploration_rate | 0.682    |
| time/               |          |
|    total_timesteps  | 360500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.8      |
|    n_updates        | 87624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 145      |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 53       |
|    time_elapsed     | 6766     |
|    total_timesteps  | 360705   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 87676    |
----------------------------------
Eval num_timesteps=361000, episode_reward=115.24 +/- 46.92
Episode length: 249.24 +/- 182.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 249      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 0.681    |
| time/               |          |
|    total_timesteps  | 361000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.47     |
|    n_updates        | 87749    |
----------------------------------
Eval num_timesteps=361500, episode_reward=92.94 +/- 24.42
Episode length: 194.96 +/- 99.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 92.9     |
| rollout/            |          |
|    exploration_rate | 0.68     |
| time/               |          |
|    total_timesteps  | 361500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.83     |
|    n_updates        | 87874    |
----------------------------------
Eval num_timesteps=362000, episode_reward=89.56 +/- 15.26
Episode length: 94.46 +/- 87.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.5     |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.679    |
| time/               |          |
|    total_timesteps  | 362000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.19     |
|    n_updates        | 87999    |
----------------------------------
Eval num_timesteps=362500, episode_reward=18.10 +/- 45.37
Episode length: 62.26 +/- 42.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.3     |
|    mean_reward      | 18.1     |
| rollout/            |          |
|    exploration_rate | 0.678    |
| time/               |          |
|    total_timesteps  | 362500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.61     |
|    n_updates        | 88124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 395      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 53       |
|    time_elapsed     | 6785     |
|    total_timesteps  | 362805   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.2      |
|    n_updates        | 88201    |
----------------------------------
Eval num_timesteps=363000, episode_reward=79.96 +/- 49.72
Episode length: 147.92 +/- 141.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 80       |
| rollout/            |          |
|    exploration_rate | 0.677    |
| time/               |          |
|    total_timesteps  | 363000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.5      |
|    n_updates        | 88249    |
----------------------------------
Eval num_timesteps=363500, episode_reward=117.94 +/- 48.96
Episode length: 192.86 +/- 207.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 118      |
| rollout/            |          |
|    exploration_rate | 0.676    |
| time/               |          |
|    total_timesteps  | 363500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.12     |
|    n_updates        | 88374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 386      |
|    ep_rew_mean      | 144      |
|    exploration_rate | 0.675    |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 53       |
|    time_elapsed     | 6796     |
|    total_timesteps  | 363974   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.99     |
|    n_updates        | 88493    |
----------------------------------
Eval num_timesteps=364000, episode_reward=0.27 +/- 32.18
Episode length: 58.62 +/- 45.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.6     |
|    mean_reward      | 0.267    |
| rollout/            |          |
|    exploration_rate | 0.675    |
| time/               |          |
|    total_timesteps  | 364000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.48     |
|    n_updates        | 88499    |
----------------------------------
Eval num_timesteps=364500, episode_reward=59.89 +/- 49.20
Episode length: 112.24 +/- 87.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 59.9     |
| rollout/            |          |
|    exploration_rate | 0.674    |
| time/               |          |
|    total_timesteps  | 364500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.73     |
|    n_updates        | 88624    |
----------------------------------
Eval num_timesteps=365000, episode_reward=164.61 +/- 49.67
Episode length: 428.14 +/- 158.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 428      |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 365000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.878    |
|    n_updates        | 88749    |
----------------------------------
Eval num_timesteps=365500, episode_reward=111.25 +/- 49.69
Episode length: 207.60 +/- 187.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.672    |
| time/               |          |
|    total_timesteps  | 365500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.53     |
|    n_updates        | 88874    |
----------------------------------
Eval num_timesteps=366000, episode_reward=61.81 +/- 48.32
Episode length: 101.08 +/- 86.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 61.8     |
| rollout/            |          |
|    exploration_rate | 0.671    |
| time/               |          |
|    total_timesteps  | 366000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.42     |
|    n_updates        | 88999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 386      |
|    ep_rew_mean      | 145      |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 53       |
|    time_elapsed     | 6824     |
|    total_timesteps  | 366074   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 89018    |
----------------------------------
Eval num_timesteps=366500, episode_reward=116.04 +/- 66.59
Episode length: 279.82 +/- 194.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 280      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.67     |
| time/               |          |
|    total_timesteps  | 366500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.9      |
|    n_updates        | 89124    |
----------------------------------
Eval num_timesteps=367000, episode_reward=141.70 +/- 54.51
Episode length: 299.70 +/- 229.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 300      |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.669    |
| time/               |          |
|    total_timesteps  | 367000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.39     |
|    n_updates        | 89249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 391      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 53       |
|    time_elapsed     | 6841     |
|    total_timesteps  | 367223   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.68     |
|    n_updates        | 89305    |
----------------------------------
Eval num_timesteps=367500, episode_reward=47.59 +/- 52.54
Episode length: 67.76 +/- 70.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.8     |
|    mean_reward      | 47.6     |
| rollout/            |          |
|    exploration_rate | 0.668    |
| time/               |          |
|    total_timesteps  | 367500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.73     |
|    n_updates        | 89374    |
----------------------------------
Eval num_timesteps=368000, episode_reward=45.80 +/- 48.88
Episode length: 92.76 +/- 78.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 45.8     |
| rollout/            |          |
|    exploration_rate | 0.667    |
| time/               |          |
|    total_timesteps  | 368000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.32     |
|    n_updates        | 89499    |
----------------------------------
Eval num_timesteps=368500, episode_reward=126.63 +/- 56.50
Episode length: 317.24 +/- 172.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 317      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.666    |
| time/               |          |
|    total_timesteps  | 368500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 89624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 391      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 53       |
|    time_elapsed     | 6856     |
|    total_timesteps  | 368857   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.08     |
|    n_updates        | 89714    |
----------------------------------
Eval num_timesteps=369000, episode_reward=93.90 +/- 25.90
Episode length: 185.24 +/- 108.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 185      |
|    mean_reward      | 93.9     |
| rollout/            |          |
|    exploration_rate | 0.665    |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.53     |
|    n_updates        | 89749    |
----------------------------------
Eval num_timesteps=369500, episode_reward=49.66 +/- 48.00
Episode length: 104.26 +/- 71.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 49.7     |
| rollout/            |          |
|    exploration_rate | 0.664    |
| time/               |          |
|    total_timesteps  | 369500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.33     |
|    n_updates        | 89874    |
----------------------------------
Eval num_timesteps=370000, episode_reward=-7.64 +/- 19.40
Episode length: 51.98 +/- 18.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52       |
|    mean_reward      | -7.64    |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 370000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.99     |
|    n_updates        | 89999    |
----------------------------------
Eval num_timesteps=370500, episode_reward=87.08 +/- 0.58
Episode length: 171.50 +/- 64.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 87.1     |
| rollout/            |          |
|    exploration_rate | 0.662    |
| time/               |          |
|    total_timesteps  | 370500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 90124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 391      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 53       |
|    time_elapsed     | 6873     |
|    total_timesteps  | 370957   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.972    |
|    n_updates        | 90239    |
----------------------------------
Eval num_timesteps=371000, episode_reward=20.06 +/- 46.19
Episode length: 80.48 +/- 56.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.5     |
|    mean_reward      | 20.1     |
| rollout/            |          |
|    exploration_rate | 0.661    |
| time/               |          |
|    total_timesteps  | 371000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.91     |
|    n_updates        | 90249    |
----------------------------------
Eval num_timesteps=371500, episode_reward=115.32 +/- 52.34
Episode length: 244.50 +/- 187.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 244      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 0.66     |
| time/               |          |
|    total_timesteps  | 371500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.38     |
|    n_updates        | 90374    |
----------------------------------
Eval num_timesteps=372000, episode_reward=49.97 +/- 52.50
Episode length: 124.88 +/- 115.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 50       |
| rollout/            |          |
|    exploration_rate | 0.659    |
| time/               |          |
|    total_timesteps  | 372000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 90499    |
----------------------------------
Eval num_timesteps=372500, episode_reward=-11.59 +/- 0.00
Episode length: 46.74 +/- 15.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | -11.6    |
| rollout/            |          |
|    exploration_rate | 0.658    |
| time/               |          |
|    total_timesteps  | 372500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.67     |
|    n_updates        | 90624    |
----------------------------------
Eval num_timesteps=373000, episode_reward=101.14 +/- 35.99
Episode length: 233.80 +/- 132.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 101      |
| rollout/            |          |
|    exploration_rate | 0.657    |
| time/               |          |
|    total_timesteps  | 373000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.16     |
|    n_updates        | 90749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 391      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.657    |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 54       |
|    time_elapsed     | 6895     |
|    total_timesteps  | 373057   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.22     |
|    n_updates        | 90764    |
----------------------------------
Eval num_timesteps=373500, episode_reward=158.78 +/- 51.78
Episode length: 393.12 +/- 189.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 393      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.656    |
| time/               |          |
|    total_timesteps  | 373500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.28     |
|    n_updates        | 90874    |
----------------------------------
Eval num_timesteps=374000, episode_reward=107.19 +/- 42.23
Episode length: 244.56 +/- 157.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 245      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.655    |
| time/               |          |
|    total_timesteps  | 374000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 90999    |
----------------------------------
Eval num_timesteps=374500, episode_reward=132.66 +/- 53.92
Episode length: 259.30 +/- 227.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 259      |
|    mean_reward      | 133      |
| rollout/            |          |
|    exploration_rate | 0.654    |
| time/               |          |
|    total_timesteps  | 374500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.95     |
|    n_updates        | 91124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 391      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 54       |
|    time_elapsed     | 6922     |
|    total_timesteps  | 374666   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.69     |
|    n_updates        | 91166    |
----------------------------------
Eval num_timesteps=375000, episode_reward=79.38 +/- 26.83
Episode length: 165.44 +/- 88.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 79.4     |
| rollout/            |          |
|    exploration_rate | 0.654    |
| time/               |          |
|    total_timesteps  | 375000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.1      |
|    n_updates        | 91249    |
----------------------------------
Eval num_timesteps=375500, episode_reward=135.36 +/- 54.09
Episode length: 333.48 +/- 191.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 333      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 375500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 91374    |
----------------------------------
Eval num_timesteps=376000, episode_reward=121.93 +/- 50.88
Episode length: 293.10 +/- 165.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 293      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.652    |
| time/               |          |
|    total_timesteps  | 376000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.83     |
|    n_updates        | 91499    |
----------------------------------
Eval num_timesteps=376500, episode_reward=187.74 +/- 28.72
Episode length: 499.48 +/- 101.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 499      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.651    |
| time/               |          |
|    total_timesteps  | 376500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.76     |
|    n_updates        | 91624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 396      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 54       |
|    time_elapsed     | 6961     |
|    total_timesteps  | 376766   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.22     |
|    n_updates        | 91691    |
----------------------------------
Eval num_timesteps=377000, episode_reward=193.33 +/- 21.63
Episode length: 509.84 +/- 74.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 510      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.65     |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 91749    |
----------------------------------
Eval num_timesteps=377500, episode_reward=185.37 +/- 33.52
Episode length: 493.24 +/- 99.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 493      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.649    |
| time/               |          |
|    total_timesteps  | 377500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.09     |
|    n_updates        | 91874    |
----------------------------------
Eval num_timesteps=378000, episode_reward=100.86 +/- 46.03
Episode length: 214.36 +/- 151.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 101      |
| rollout/            |          |
|    exploration_rate | 0.648    |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.66     |
|    n_updates        | 91999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 401      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 54       |
|    time_elapsed     | 6996     |
|    total_timesteps  | 378406   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.908    |
|    n_updates        | 92101    |
----------------------------------
Eval num_timesteps=378500, episode_reward=143.84 +/- 54.69
Episode length: 358.70 +/- 177.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 359      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.647    |
| time/               |          |
|    total_timesteps  | 378500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.32     |
|    n_updates        | 92124    |
----------------------------------
Eval num_timesteps=379000, episode_reward=151.70 +/- 56.07
Episode length: 385.74 +/- 173.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 386      |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.646    |
| time/               |          |
|    total_timesteps  | 379000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.45     |
|    n_updates        | 92249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 392      |
|    ep_rew_mean      | 145      |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 54       |
|    time_elapsed     | 7018     |
|    total_timesteps  | 379099   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.51     |
|    n_updates        | 92274    |
----------------------------------
Eval num_timesteps=379500, episode_reward=53.67 +/- 46.84
Episode length: 64.06 +/- 36.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.1     |
|    mean_reward      | 53.7     |
| rollout/            |          |
|    exploration_rate | 0.645    |
| time/               |          |
|    total_timesteps  | 379500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.75     |
|    n_updates        | 92374    |
----------------------------------
Eval num_timesteps=380000, episode_reward=163.96 +/- 51.06
Episode length: 429.40 +/- 158.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.644    |
| time/               |          |
|    total_timesteps  | 380000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.48     |
|    n_updates        | 92499    |
----------------------------------
Eval num_timesteps=380500, episode_reward=61.66 +/- 43.42
Episode length: 123.64 +/- 79.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 61.7     |
| rollout/            |          |
|    exploration_rate | 0.643    |
| time/               |          |
|    total_timesteps  | 380500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 92624    |
----------------------------------
Eval num_timesteps=381000, episode_reward=109.48 +/- 56.51
Episode length: 259.56 +/- 175.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 260      |
|    mean_reward      | 109      |
| rollout/            |          |
|    exploration_rate | 0.642    |
| time/               |          |
|    total_timesteps  | 381000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.09     |
|    n_updates        | 92749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 401      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 54       |
|    time_elapsed     | 7044     |
|    total_timesteps  | 381199   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.844    |
|    n_updates        | 92799    |
----------------------------------
Eval num_timesteps=381500, episode_reward=87.28 +/- 0.32
Episode length: 203.88 +/- 68.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 87.3     |
| rollout/            |          |
|    exploration_rate | 0.641    |
| time/               |          |
|    total_timesteps  | 381500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 92874    |
----------------------------------
Eval num_timesteps=382000, episode_reward=51.54 +/- 47.36
Episode length: 76.38 +/- 47.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.4     |
|    mean_reward      | 51.5     |
| rollout/            |          |
|    exploration_rate | 0.64     |
| time/               |          |
|    total_timesteps  | 382000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.38     |
|    n_updates        | 92999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 396      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.639    |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 54       |
|    time_elapsed     | 7053     |
|    total_timesteps  | 382323   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.5      |
|    n_updates        | 93080    |
----------------------------------
Eval num_timesteps=382500, episode_reward=110.45 +/- 45.61
Episode length: 250.72 +/- 159.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 251      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.639    |
| time/               |          |
|    total_timesteps  | 382500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.57     |
|    n_updates        | 93124    |
----------------------------------
Eval num_timesteps=383000, episode_reward=90.89 +/- 22.13
Episode length: 188.38 +/- 96.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 188      |
|    mean_reward      | 90.9     |
| rollout/            |          |
|    exploration_rate | 0.638    |
| time/               |          |
|    total_timesteps  | 383000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.56     |
|    n_updates        | 93249    |
----------------------------------
Eval num_timesteps=383500, episode_reward=131.15 +/- 53.91
Episode length: 282.80 +/- 206.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 283      |
|    mean_reward      | 131      |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 383500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.34     |
|    n_updates        | 93374    |
----------------------------------
Eval num_timesteps=384000, episode_reward=37.91 +/- 49.50
Episode length: 76.14 +/- 50.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.1     |
|    mean_reward      | 37.9     |
| rollout/            |          |
|    exploration_rate | 0.636    |
| time/               |          |
|    total_timesteps  | 384000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.41     |
|    n_updates        | 93499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 405      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.635    |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 54       |
|    time_elapsed     | 7078     |
|    total_timesteps  | 384423   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 93605    |
----------------------------------
Eval num_timesteps=384500, episode_reward=92.43 +/- 51.10
Episode length: 190.20 +/- 153.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 92.4     |
| rollout/            |          |
|    exploration_rate | 0.635    |
| time/               |          |
|    total_timesteps  | 384500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 93624    |
----------------------------------
Eval num_timesteps=385000, episode_reward=135.01 +/- 56.03
Episode length: 342.18 +/- 179.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 342      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.634    |
| time/               |          |
|    total_timesteps  | 385000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.02     |
|    n_updates        | 93749    |
----------------------------------
Eval num_timesteps=385500, episode_reward=115.76 +/- 47.84
Episode length: 179.60 +/- 205.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 180      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.633    |
| time/               |          |
|    total_timesteps  | 385500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 93874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 402      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.632    |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 54       |
|    time_elapsed     | 7099     |
|    total_timesteps  | 385695   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.4      |
|    n_updates        | 93923    |
----------------------------------
Eval num_timesteps=386000, episode_reward=81.47 +/- 23.51
Episode length: 68.92 +/- 39.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.9     |
|    mean_reward      | 81.5     |
| rollout/            |          |
|    exploration_rate | 0.632    |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 93999    |
----------------------------------
Eval num_timesteps=386500, episode_reward=135.58 +/- 54.34
Episode length: 264.76 +/- 231.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 265      |
|    mean_reward      | 136      |
| rollout/            |          |
|    exploration_rate | 0.631    |
| time/               |          |
|    total_timesteps  | 386500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 94124    |
----------------------------------
Eval num_timesteps=387000, episode_reward=156.32 +/- 54.38
Episode length: 415.24 +/- 153.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 415      |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.63     |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.64     |
|    n_updates        | 94249    |
----------------------------------
Eval num_timesteps=387500, episode_reward=139.12 +/- 55.21
Episode length: 308.36 +/- 219.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 308      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.629    |
| time/               |          |
|    total_timesteps  | 387500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.719    |
|    n_updates        | 94374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 402      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.628    |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 54       |
|    time_elapsed     | 7131     |
|    total_timesteps  | 387795   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.42     |
|    n_updates        | 94448    |
----------------------------------
Eval num_timesteps=388000, episode_reward=110.35 +/- 45.39
Episode length: 166.40 +/- 191.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.628    |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 94499    |
----------------------------------
Eval num_timesteps=388500, episode_reward=110.78 +/- 45.45
Episode length: 178.96 +/- 190.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.627    |
| time/               |          |
|    total_timesteps  | 388500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.15     |
|    n_updates        | 94624    |
----------------------------------
Eval num_timesteps=389000, episode_reward=97.61 +/- 39.21
Episode length: 208.80 +/- 132.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 97.6     |
| rollout/            |          |
|    exploration_rate | 0.626    |
| time/               |          |
|    total_timesteps  | 389000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.27     |
|    n_updates        | 94749    |
----------------------------------
Eval num_timesteps=389500, episode_reward=178.18 +/- 40.47
Episode length: 464.84 +/- 141.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 465      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.625    |
| time/               |          |
|    total_timesteps  | 389500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 94874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 404      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.625    |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 54       |
|    time_elapsed     | 7162     |
|    total_timesteps  | 389607   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 94901    |
----------------------------------
Eval num_timesteps=390000, episode_reward=113.15 +/- 53.65
Episode length: 297.20 +/- 168.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 297      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.624    |
| time/               |          |
|    total_timesteps  | 390000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.16     |
|    n_updates        | 94999    |
----------------------------------
Eval num_timesteps=390500, episode_reward=165.58 +/- 51.32
Episode length: 427.82 +/- 159.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 428      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.623    |
| time/               |          |
|    total_timesteps  | 390500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 95124    |
----------------------------------
Eval num_timesteps=391000, episode_reward=54.34 +/- 59.50
Episode length: 121.02 +/- 121.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 54.3     |
| rollout/            |          |
|    exploration_rate | 0.622    |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.26     |
|    n_updates        | 95249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 408      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.621    |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 54       |
|    time_elapsed     | 7187     |
|    total_timesteps  | 391224   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.416    |
|    n_updates        | 95305    |
----------------------------------
Eval num_timesteps=391500, episode_reward=144.40 +/- 54.64
Episode length: 341.60 +/- 207.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 342      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.621    |
| time/               |          |
|    total_timesteps  | 391500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.72     |
|    n_updates        | 95374    |
----------------------------------
Eval num_timesteps=392000, episode_reward=90.99 +/- 30.43
Episode length: 197.30 +/- 108.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 91       |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 392000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.937    |
|    n_updates        | 95499    |
----------------------------------
Eval num_timesteps=392500, episode_reward=129.74 +/- 58.90
Episode length: 337.58 +/- 169.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 338      |
|    mean_reward      | 130      |
| rollout/            |          |
|    exploration_rate | 0.619    |
| time/               |          |
|    total_timesteps  | 392500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 95624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 408      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 54       |
|    time_elapsed     | 7213     |
|    total_timesteps  | 392887   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.63     |
|    n_updates        | 95721    |
----------------------------------
Eval num_timesteps=393000, episode_reward=18.52 +/- 54.59
Episode length: 77.36 +/- 99.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.4     |
|    mean_reward      | 18.5     |
| rollout/            |          |
|    exploration_rate | 0.618    |
| time/               |          |
|    total_timesteps  | 393000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 95749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 407      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 54       |
|    time_elapsed     | 7216     |
|    total_timesteps  | 393202   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 95800    |
----------------------------------
Eval num_timesteps=393500, episode_reward=74.12 +/- 49.97
Episode length: 151.54 +/- 118.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 74.1     |
| rollout/            |          |
|    exploration_rate | 0.617    |
| time/               |          |
|    total_timesteps  | 393500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.18     |
|    n_updates        | 95874    |
----------------------------------
Eval num_timesteps=394000, episode_reward=80.16 +/- 23.65
Episode length: 167.20 +/- 81.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 167      |
|    mean_reward      | 80.2     |
| rollout/            |          |
|    exploration_rate | 0.616    |
| time/               |          |
|    total_timesteps  | 394000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.84     |
|    n_updates        | 95999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 400      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 54       |
|    time_elapsed     | 7226     |
|    total_timesteps  | 394062   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 96015    |
----------------------------------
Eval num_timesteps=394500, episode_reward=25.96 +/- 47.98
Episode length: 77.72 +/- 56.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.615    |
| time/               |          |
|    total_timesteps  | 394500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.66     |
|    n_updates        | 96124    |
----------------------------------
Eval num_timesteps=395000, episode_reward=139.12 +/- 63.39
Episode length: 364.88 +/- 171.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 365      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.614    |
| time/               |          |
|    total_timesteps  | 395000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 96249    |
----------------------------------
Eval num_timesteps=395500, episode_reward=136.17 +/- 55.02
Episode length: 299.24 +/- 216.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 299      |
|    mean_reward      | 136      |
| rollout/            |          |
|    exploration_rate | 0.613    |
| time/               |          |
|    total_timesteps  | 395500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 96374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 401      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.612    |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 54       |
|    time_elapsed     | 7248     |
|    total_timesteps  | 395801   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.22     |
|    n_updates        | 96450    |
----------------------------------
Eval num_timesteps=396000, episode_reward=168.65 +/- 51.93
Episode length: 454.30 +/- 139.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 454      |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.612    |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.74     |
|    n_updates        | 96499    |
----------------------------------
Eval num_timesteps=396500, episode_reward=147.38 +/- 55.61
Episode length: 362.38 +/- 193.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 362      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.611    |
| time/               |          |
|    total_timesteps  | 396500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.2      |
|    n_updates        | 96624    |
----------------------------------
Eval num_timesteps=397000, episode_reward=93.27 +/- 26.45
Episode length: 219.64 +/- 112.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 93.3     |
| rollout/            |          |
|    exploration_rate | 0.61     |
| time/               |          |
|    total_timesteps  | 397000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.6      |
|    n_updates        | 96749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 397      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.61     |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 54       |
|    time_elapsed     | 7280     |
|    total_timesteps  | 397094   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 96773    |
----------------------------------
Eval num_timesteps=397500, episode_reward=137.61 +/- 56.33
Episode length: 350.66 +/- 181.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 351      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 0.609    |
| time/               |          |
|    total_timesteps  | 397500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.52     |
|    n_updates        | 96874    |
----------------------------------
Eval num_timesteps=398000, episode_reward=98.94 +/- 56.52
Episode length: 186.04 +/- 185.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 98.9     |
| rollout/            |          |
|    exploration_rate | 0.608    |
| time/               |          |
|    total_timesteps  | 398000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.16     |
|    n_updates        | 96999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 396      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.608    |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 54       |
|    time_elapsed     | 7296     |
|    total_timesteps  | 398225   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 97056    |
----------------------------------
Eval num_timesteps=398500, episode_reward=125.27 +/- 60.22
Episode length: 321.22 +/- 167.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 321      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.607    |
| time/               |          |
|    total_timesteps  | 398500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.37     |
|    n_updates        | 97124    |
----------------------------------
Eval num_timesteps=399000, episode_reward=10.33 +/- 46.22
Episode length: 63.52 +/- 73.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.5     |
|    mean_reward      | 10.3     |
| rollout/            |          |
|    exploration_rate | 0.606    |
| time/               |          |
|    total_timesteps  | 399000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 97249    |
----------------------------------
Eval num_timesteps=399500, episode_reward=143.45 +/- 53.52
Episode length: 366.94 +/- 179.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 367      |
|    mean_reward      | 143      |
| rollout/            |          |
|    exploration_rate | 0.605    |
| time/               |          |
|    total_timesteps  | 399500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.64     |
|    n_updates        | 97374    |
----------------------------------
Eval num_timesteps=400000, episode_reward=113.49 +/- 46.85
Episode length: 276.86 +/- 155.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 277      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.604    |
| time/               |          |
|    total_timesteps  | 400000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.66     |
|    n_updates        | 97499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 396      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.603    |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 54       |
|    time_elapsed     | 7327     |
|    total_timesteps  | 400325   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.1      |
|    n_updates        | 97581    |
----------------------------------
Eval num_timesteps=400500, episode_reward=111.21 +/- 45.16
Episode length: 163.10 +/- 193.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 163      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.603    |
| time/               |          |
|    total_timesteps  | 400500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 97624    |
----------------------------------
Eval num_timesteps=401000, episode_reward=62.65 +/- 67.00
Episode length: 117.06 +/- 153.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 62.6     |
| rollout/            |          |
|    exploration_rate | 0.602    |
| time/               |          |
|    total_timesteps  | 401000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 97749    |
----------------------------------
Eval num_timesteps=401500, episode_reward=191.62 +/- 22.67
Episode length: 511.04 +/- 68.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 511      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.601    |
| time/               |          |
|    total_timesteps  | 401500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 97874    |
----------------------------------
Eval num_timesteps=402000, episode_reward=154.23 +/- 52.57
Episode length: 347.70 +/- 226.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 348      |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.6      |
| time/               |          |
|    total_timesteps  | 402000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 97999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 392      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.6      |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 54       |
|    time_elapsed     | 7361     |
|    total_timesteps  | 402017   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.38     |
|    n_updates        | 98004    |
----------------------------------
Eval num_timesteps=402500, episode_reward=156.50 +/- 57.14
Episode length: 412.68 +/- 170.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 413      |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.599    |
| time/               |          |
|    total_timesteps  | 402500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.92     |
|    n_updates        | 98124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 390      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.598    |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 54       |
|    time_elapsed     | 7374     |
|    total_timesteps  | 402973   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 98243    |
----------------------------------
Eval num_timesteps=403000, episode_reward=188.88 +/- 25.76
Episode length: 496.92 +/- 111.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.598    |
| time/               |          |
|    total_timesteps  | 403000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.33     |
|    n_updates        | 98249    |
----------------------------------
Eval num_timesteps=403500, episode_reward=155.03 +/- 58.97
Episode length: 403.32 +/- 173.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 403      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.597    |
| time/               |          |
|    total_timesteps  | 403500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.97     |
|    n_updates        | 98374    |
----------------------------------
Eval num_timesteps=404000, episode_reward=158.65 +/- 51.49
Episode length: 365.92 +/- 221.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 366      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.596    |
| time/               |          |
|    total_timesteps  | 404000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.775    |
|    n_updates        | 98499    |
----------------------------------
Eval num_timesteps=404500, episode_reward=175.33 +/- 42.91
Episode length: 442.66 +/- 177.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 443      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.595    |
| time/               |          |
|    total_timesteps  | 404500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.18     |
|    n_updates        | 98624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 54       |
|    time_elapsed     | 7423     |
|    total_timesteps  | 404575   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 98643    |
----------------------------------
Eval num_timesteps=405000, episode_reward=123.68 +/- 51.20
Episode length: 245.28 +/- 211.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 245      |
|    mean_reward      | 124      |
| rollout/            |          |
|    exploration_rate | 0.594    |
| time/               |          |
|    total_timesteps  | 405000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.52     |
|    n_updates        | 98749    |
----------------------------------
Eval num_timesteps=405500, episode_reward=178.02 +/- 41.63
Episode length: 471.80 +/- 123.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 472      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.593    |
| time/               |          |
|    total_timesteps  | 405500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.64     |
|    n_updates        | 98874    |
----------------------------------
Eval num_timesteps=406000, episode_reward=148.30 +/- 54.24
Episode length: 352.44 +/- 206.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 352      |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.592    |
| time/               |          |
|    total_timesteps  | 406000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.06     |
|    n_updates        | 98999    |
----------------------------------
Eval num_timesteps=406500, episode_reward=149.62 +/- 53.80
Episode length: 361.94 +/- 197.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 362      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.591    |
| time/               |          |
|    total_timesteps  | 406500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.25     |
|    n_updates        | 99124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 395      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.591    |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 54       |
|    time_elapsed     | 7465     |
|    total_timesteps  | 406675   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.72     |
|    n_updates        | 99168    |
----------------------------------
Eval num_timesteps=407000, episode_reward=127.67 +/- 53.40
Episode length: 312.62 +/- 174.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 313      |
|    mean_reward      | 128      |
| rollout/            |          |
|    exploration_rate | 0.59     |
| time/               |          |
|    total_timesteps  | 407000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.4      |
|    n_updates        | 99249    |
----------------------------------
Eval num_timesteps=407500, episode_reward=157.69 +/- 52.16
Episode length: 393.18 +/- 189.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 393      |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 407500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 99374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 390      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.588    |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 54       |
|    time_elapsed     | 7486     |
|    total_timesteps  | 407859   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.39     |
|    n_updates        | 99464    |
----------------------------------
Eval num_timesteps=408000, episode_reward=44.43 +/- 73.94
Episode length: 126.72 +/- 167.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 44.4     |
| rollout/            |          |
|    exploration_rate | 0.588    |
| time/               |          |
|    total_timesteps  | 408000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.25     |
|    n_updates        | 99499    |
----------------------------------
Eval num_timesteps=408500, episode_reward=124.12 +/- 51.72
Episode length: 220.88 +/- 218.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 124      |
| rollout/            |          |
|    exploration_rate | 0.587    |
| time/               |          |
|    total_timesteps  | 408500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.73     |
|    n_updates        | 99624    |
----------------------------------
Eval num_timesteps=409000, episode_reward=179.71 +/- 40.68
Episode length: 465.54 +/- 141.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 466      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.586    |
| time/               |          |
|    total_timesteps  | 409000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.71     |
|    n_updates        | 99749    |
----------------------------------
Eval num_timesteps=409500, episode_reward=52.50 +/- 66.69
Episode length: 104.64 +/- 143.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 52.5     |
| rollout/            |          |
|    exploration_rate | 0.585    |
| time/               |          |
|    total_timesteps  | 409500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.32     |
|    n_updates        | 99874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 386      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.585    |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 54       |
|    time_elapsed     | 7513     |
|    total_timesteps  | 409507   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.944    |
|    n_updates        | 99876    |
----------------------------------
Eval num_timesteps=410000, episode_reward=197.72 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.584    |
| time/               |          |
|    total_timesteps  | 410000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.756    |
|    n_updates        | 99999    |
----------------------------------
Eval num_timesteps=410500, episode_reward=193.44 +/- 21.65
Episode length: 512.62 +/- 61.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 513      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.583    |
| time/               |          |
|    total_timesteps  | 410500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.01     |
|    n_updates        | 100124   |
----------------------------------
Eval num_timesteps=411000, episode_reward=171.82 +/- 46.81
Episode length: 429.88 +/- 186.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 430      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.582    |
| time/               |          |
|    total_timesteps  | 411000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.09     |
|    n_updates        | 100249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 381      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.582    |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 54       |
|    time_elapsed     | 7555     |
|    total_timesteps  | 411124   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.19     |
|    n_updates        | 100280   |
----------------------------------
Eval num_timesteps=411500, episode_reward=-3.67 +/- 26.86
Episode length: 51.36 +/- 20.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -3.67    |
| rollout/            |          |
|    exploration_rate | 0.581    |
| time/               |          |
|    total_timesteps  | 411500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 100374   |
----------------------------------
Eval num_timesteps=412000, episode_reward=190.56 +/- 27.31
Episode length: 503.50 +/- 86.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 504      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.58     |
| time/               |          |
|    total_timesteps  | 412000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.985    |
|    n_updates        | 100499   |
----------------------------------
Eval num_timesteps=412500, episode_reward=171.91 +/- 46.51
Episode length: 436.76 +/- 173.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 437      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.579    |
| time/               |          |
|    total_timesteps  | 412500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 100624   |
----------------------------------
Eval num_timesteps=413000, episode_reward=2.20 +/- 34.20
Episode length: 51.14 +/- 17.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.1     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.578    |
| time/               |          |
|    total_timesteps  | 413000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 100749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 386      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.578    |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 54       |
|    time_elapsed     | 7588     |
|    total_timesteps  | 413224   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.19     |
|    n_updates        | 100805   |
----------------------------------
Eval num_timesteps=413500, episode_reward=152.60 +/- 70.81
Episode length: 395.94 +/- 200.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 396      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.577    |
| time/               |          |
|    total_timesteps  | 413500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.29     |
|    n_updates        | 100874   |
----------------------------------
Eval num_timesteps=414000, episode_reward=170.27 +/- 49.56
Episode length: 459.12 +/- 133.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.576    |
| time/               |          |
|    total_timesteps  | 414000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.39     |
|    n_updates        | 100999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 376      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 54       |
|    time_elapsed     | 7613     |
|    total_timesteps  | 414354   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 101088   |
----------------------------------
Eval num_timesteps=414500, episode_reward=104.99 +/- 41.76
Episode length: 148.40 +/- 178.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 414500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.826    |
|    n_updates        | 101124   |
----------------------------------
Eval num_timesteps=415000, episode_reward=188.47 +/- 22.31
Episode length: 508.30 +/- 81.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 508      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.574    |
| time/               |          |
|    total_timesteps  | 415000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 101249   |
----------------------------------
Eval num_timesteps=415500, episode_reward=178.66 +/- 41.56
Episode length: 456.90 +/- 158.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 457      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.573    |
| time/               |          |
|    total_timesteps  | 415500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.03     |
|    n_updates        | 101374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 375      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 54       |
|    time_elapsed     | 7646     |
|    total_timesteps  | 415948   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.921    |
|    n_updates        | 101486   |
----------------------------------
Eval num_timesteps=416000, episode_reward=66.25 +/- 68.40
Episode length: 129.24 +/- 165.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 66.2     |
| rollout/            |          |
|    exploration_rate | 0.572    |
| time/               |          |
|    total_timesteps  | 416000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 101499   |
----------------------------------
Eval num_timesteps=416500, episode_reward=52.41 +/- 66.47
Episode length: 102.40 +/- 145.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 52.4     |
| rollout/            |          |
|    exploration_rate | 0.571    |
| time/               |          |
|    total_timesteps  | 416500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.23     |
|    n_updates        | 101624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 376      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.571    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 54       |
|    time_elapsed     | 7653     |
|    total_timesteps  | 416670   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.01     |
|    n_updates        | 101667   |
----------------------------------
Eval num_timesteps=417000, episode_reward=118.31 +/- 50.21
Episode length: 197.92 +/- 214.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 118      |
| rollout/            |          |
|    exploration_rate | 0.57     |
| time/               |          |
|    total_timesteps  | 417000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.76     |
|    n_updates        | 101749   |
----------------------------------
Eval num_timesteps=417500, episode_reward=162.30 +/- 53.84
Episode length: 405.30 +/- 203.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 405      |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.569    |
| time/               |          |
|    total_timesteps  | 417500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 101874   |
----------------------------------
Eval num_timesteps=418000, episode_reward=107.47 +/- 41.56
Episode length: 160.64 +/- 185.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 161      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.568    |
| time/               |          |
|    total_timesteps  | 418000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.21     |
|    n_updates        | 101999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 371      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.568    |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 54       |
|    time_elapsed     | 7676     |
|    total_timesteps  | 418318   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.6      |
|    n_updates        | 102079   |
----------------------------------
Eval num_timesteps=418500, episode_reward=120.16 +/- 61.52
Episode length: 237.20 +/- 220.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 120      |
| rollout/            |          |
|    exploration_rate | 0.567    |
| time/               |          |
|    total_timesteps  | 418500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.63     |
|    n_updates        | 102124   |
----------------------------------
Eval num_timesteps=419000, episode_reward=185.44 +/- 34.72
Episode length: 487.76 +/- 121.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 488      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.566    |
| time/               |          |
|    total_timesteps  | 419000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.49     |
|    n_updates        | 102249   |
----------------------------------
Eval num_timesteps=419500, episode_reward=121.68 +/- 58.49
Episode length: 238.84 +/- 219.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.565    |
| time/               |          |
|    total_timesteps  | 419500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3        |
|    n_updates        | 102374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 376      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.564    |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 54       |
|    time_elapsed     | 7705     |
|    total_timesteps  | 419966   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 102491   |
----------------------------------
Eval num_timesteps=420000, episode_reward=84.20 +/- 81.50
Episode length: 199.88 +/- 209.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 84.2     |
| rollout/            |          |
|    exploration_rate | 0.564    |
| time/               |          |
|    total_timesteps  | 420000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.39     |
|    n_updates        | 102499   |
----------------------------------
Eval num_timesteps=420500, episode_reward=151.93 +/- 53.04
Episode length: 339.00 +/- 228.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 339      |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.563    |
| time/               |          |
|    total_timesteps  | 420500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.864    |
|    n_updates        | 102624   |
----------------------------------
Eval num_timesteps=421000, episode_reward=181.20 +/- 43.48
Episode length: 476.78 +/- 133.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.562    |
| time/               |          |
|    total_timesteps  | 421000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 102749   |
----------------------------------
Eval num_timesteps=421500, episode_reward=168.87 +/- 48.35
Episode length: 440.78 +/- 160.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.561    |
| time/               |          |
|    total_timesteps  | 421500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.46     |
|    n_updates        | 102874   |
----------------------------------
Eval num_timesteps=422000, episode_reward=149.73 +/- 54.27
Episode length: 384.08 +/- 176.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 384      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.56     |
| time/               |          |
|    total_timesteps  | 422000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.945    |
|    n_updates        | 102999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 376      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 54       |
|    time_elapsed     | 7759     |
|    total_timesteps  | 422066   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 103016   |
----------------------------------
Eval num_timesteps=422500, episode_reward=187.58 +/- 27.92
Episode length: 503.92 +/- 83.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 504      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.559    |
| time/               |          |
|    total_timesteps  | 422500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.86     |
|    n_updates        | 103124   |
----------------------------------
Eval num_timesteps=423000, episode_reward=197.78 +/- 0.86
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.558    |
| time/               |          |
|    total_timesteps  | 423000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.686    |
|    n_updates        | 103249   |
----------------------------------
Eval num_timesteps=423500, episode_reward=178.36 +/- 41.24
Episode length: 470.86 +/- 130.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 471      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.557    |
| time/               |          |
|    total_timesteps  | 423500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.651    |
|    n_updates        | 103374   |
----------------------------------
Eval num_timesteps=424000, episode_reward=172.86 +/- 47.49
Episode length: 458.38 +/- 135.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 458      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.556    |
| time/               |          |
|    total_timesteps  | 424000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.655    |
|    n_updates        | 103499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 144      |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 54       |
|    time_elapsed     | 7816     |
|    total_timesteps  | 424166   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 103541   |
----------------------------------
Eval num_timesteps=424500, episode_reward=150.99 +/- 55.84
Episode length: 387.44 +/- 176.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 387      |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.555    |
| time/               |          |
|    total_timesteps  | 424500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.78     |
|    n_updates        | 103624   |
----------------------------------
Eval num_timesteps=425000, episode_reward=197.68 +/- 25.79
Episode length: 518.14 +/- 48.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 518      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.555    |
| time/               |          |
|    total_timesteps  | 425000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.03     |
|    n_updates        | 103749   |
----------------------------------
Eval num_timesteps=425500, episode_reward=189.91 +/- 16.95
Episode length: 515.06 +/- 69.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.554    |
| time/               |          |
|    total_timesteps  | 425500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.25     |
|    n_updates        | 103874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 380      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.553    |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 54       |
|    time_elapsed     | 7858     |
|    total_timesteps  | 425760   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.4      |
|    n_updates        | 103939   |
----------------------------------
Eval num_timesteps=426000, episode_reward=161.13 +/- 51.25
Episode length: 373.54 +/- 220.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 374      |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.553    |
| time/               |          |
|    total_timesteps  | 426000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.874    |
|    n_updates        | 103999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 368      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.552    |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 54       |
|    time_elapsed     | 7869     |
|    total_timesteps  | 426448   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 104111   |
----------------------------------
Eval num_timesteps=426500, episode_reward=197.58 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.552    |
| time/               |          |
|    total_timesteps  | 426500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.34     |
|    n_updates        | 104124   |
----------------------------------
Eval num_timesteps=427000, episode_reward=191.94 +/- 22.52
Episode length: 507.16 +/- 87.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.551    |
| time/               |          |
|    total_timesteps  | 427000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.09     |
|    n_updates        | 104249   |
----------------------------------
Eval num_timesteps=427500, episode_reward=138.86 +/- 54.69
Episode length: 281.38 +/- 234.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 281      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.55     |
| time/               |          |
|    total_timesteps  | 427500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.2      |
|    n_updates        | 104374   |
----------------------------------
Eval num_timesteps=428000, episode_reward=157.37 +/- 60.71
Episode length: 403.88 +/- 188.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 404      |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.549    |
| time/               |          |
|    total_timesteps  | 428000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 104499   |
----------------------------------
Eval num_timesteps=428500, episode_reward=142.05 +/- 53.36
Episode length: 309.62 +/- 229.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 310      |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.548    |
| time/               |          |
|    total_timesteps  | 428500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 104624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 373      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.547    |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 54       |
|    time_elapsed     | 7928     |
|    total_timesteps  | 428548   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.15     |
|    n_updates        | 104636   |
----------------------------------
Eval num_timesteps=429000, episode_reward=193.14 +/- 21.59
Episode length: 505.58 +/- 95.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.547    |
| time/               |          |
|    total_timesteps  | 429000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.35     |
|    n_updates        | 104749   |
----------------------------------
Eval num_timesteps=429500, episode_reward=197.20 +/- 0.90
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.546    |
| time/               |          |
|    total_timesteps  | 429500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.42     |
|    n_updates        | 104874   |
----------------------------------
Eval num_timesteps=430000, episode_reward=187.22 +/- 26.49
Episode length: 496.56 +/- 112.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.545    |
| time/               |          |
|    total_timesteps  | 430000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 104999   |
----------------------------------
Eval num_timesteps=430500, episode_reward=111.05 +/- 45.16
Episode length: 160.78 +/- 194.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 161      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.544    |
| time/               |          |
|    total_timesteps  | 430500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.87     |
|    n_updates        | 105124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 378      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.543    |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 53       |
|    time_elapsed     | 7979     |
|    total_timesteps  | 430648   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.16     |
|    n_updates        | 105161   |
----------------------------------
Eval num_timesteps=431000, episode_reward=177.06 +/- 42.02
Episode length: 441.36 +/- 178.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.543    |
| time/               |          |
|    total_timesteps  | 431000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.68     |
|    n_updates        | 105249   |
----------------------------------
Eval num_timesteps=431500, episode_reward=139.11 +/- 54.59
Episode length: 283.16 +/- 232.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 283      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.542    |
| time/               |          |
|    total_timesteps  | 431500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 105374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 386      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.541    |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 53       |
|    time_elapsed     | 8000     |
|    total_timesteps  | 431798   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.21     |
|    n_updates        | 105449   |
----------------------------------
Eval num_timesteps=432000, episode_reward=185.41 +/- 38.88
Episode length: 487.02 +/- 128.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.541    |
| time/               |          |
|    total_timesteps  | 432000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 105499   |
----------------------------------
Eval num_timesteps=432500, episode_reward=190.83 +/- 32.76
Episode length: 504.98 +/- 98.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.54     |
| time/               |          |
|    total_timesteps  | 432500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.584    |
|    n_updates        | 105624   |
----------------------------------
Eval num_timesteps=433000, episode_reward=195.29 +/- 15.45
Episode length: 514.98 +/- 70.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.539    |
| time/               |          |
|    total_timesteps  | 433000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.41     |
|    n_updates        | 105749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 394      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.538    |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 53       |
|    time_elapsed     | 8044     |
|    total_timesteps  | 433423   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.06     |
|    n_updates        | 105855   |
----------------------------------
Eval num_timesteps=433500, episode_reward=193.96 +/- 15.28
Episode length: 515.74 +/- 64.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.538    |
| time/               |          |
|    total_timesteps  | 433500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 105874   |
----------------------------------
Eval num_timesteps=434000, episode_reward=70.69 +/- 94.85
Episode length: 206.26 +/- 229.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 70.7     |
| rollout/            |          |
|    exploration_rate | 0.537    |
| time/               |          |
|    total_timesteps  | 434000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 105999   |
----------------------------------
Eval num_timesteps=434500, episode_reward=181.15 +/- 32.35
Episode length: 479.62 +/- 136.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 480      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 434500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.54     |
|    n_updates        | 106124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 388      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 53       |
|    time_elapsed     | 8079     |
|    total_timesteps  | 434631   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 106157   |
----------------------------------
Eval num_timesteps=435000, episode_reward=100.15 +/- 83.14
Episode length: 234.70 +/- 227.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 100      |
| rollout/            |          |
|    exploration_rate | 0.535    |
| time/               |          |
|    total_timesteps  | 435000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.722    |
|    n_updates        | 106249   |
----------------------------------
Eval num_timesteps=435500, episode_reward=45.45 +/- 48.54
Episode length: 51.18 +/- 20.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | 45.4     |
| rollout/            |          |
|    exploration_rate | 0.534    |
| time/               |          |
|    total_timesteps  | 435500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.458    |
|    n_updates        | 106374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 387      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.533    |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 53       |
|    time_elapsed     | 8088     |
|    total_timesteps  | 435785   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.03     |
|    n_updates        | 106446   |
----------------------------------
Eval num_timesteps=436000, episode_reward=188.67 +/- 30.17
Episode length: 490.04 +/- 118.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 490      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.533    |
| time/               |          |
|    total_timesteps  | 436000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.835    |
|    n_updates        | 106499   |
----------------------------------
Eval num_timesteps=436500, episode_reward=113.50 +/- 46.62
Episode length: 174.80 +/- 197.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.532    |
| time/               |          |
|    total_timesteps  | 436500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.917    |
|    n_updates        | 106624   |
----------------------------------
Eval num_timesteps=437000, episode_reward=168.14 +/- 48.47
Episode length: 403.48 +/- 205.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 403      |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.531    |
| time/               |          |
|    total_timesteps  | 437000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.96     |
|    n_updates        | 106749   |
----------------------------------
Eval num_timesteps=437500, episode_reward=186.86 +/- 25.93
Episode length: 496.44 +/- 113.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.53     |
| time/               |          |
|    total_timesteps  | 437500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.39     |
|    n_updates        | 106874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 397      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.529    |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 53       |
|    time_elapsed     | 8134     |
|    total_timesteps  | 437885   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 106971   |
----------------------------------
Eval num_timesteps=438000, episode_reward=120.75 +/- 53.88
Episode length: 220.64 +/- 219.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 121      |
| rollout/            |          |
|    exploration_rate | 0.529    |
| time/               |          |
|    total_timesteps  | 438000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 106999   |
----------------------------------
Eval num_timesteps=438500, episode_reward=173.62 +/- 49.68
Episode length: 463.58 +/- 132.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 464      |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.528    |
| time/               |          |
|    total_timesteps  | 438500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.38     |
|    n_updates        | 107124   |
----------------------------------
Eval num_timesteps=439000, episode_reward=83.95 +/- 86.68
Episode length: 217.52 +/- 215.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 83.9     |
| rollout/            |          |
|    exploration_rate | 0.527    |
| time/               |          |
|    total_timesteps  | 439000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 107249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 389      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.526    |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 53       |
|    time_elapsed     | 8161     |
|    total_timesteps  | 439242   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.984    |
|    n_updates        | 107310   |
----------------------------------
Eval num_timesteps=439500, episode_reward=124.59 +/- 50.49
Episode length: 231.78 +/- 221.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.526    |
| time/               |          |
|    total_timesteps  | 439500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.731    |
|    n_updates        | 107374   |
----------------------------------
Eval num_timesteps=440000, episode_reward=185.40 +/- 36.68
Episode length: 490.34 +/- 107.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 490      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.525    |
| time/               |          |
|    total_timesteps  | 440000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.12     |
|    n_updates        | 107499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 380      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.525    |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 53       |
|    time_elapsed     | 8182     |
|    total_timesteps  | 440007   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.22     |
|    n_updates        | 107501   |
----------------------------------
Eval num_timesteps=440500, episode_reward=98.34 +/- 77.54
Episode length: 207.58 +/- 218.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 98.3     |
| rollout/            |          |
|    exploration_rate | 0.524    |
| time/               |          |
|    total_timesteps  | 440500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.66     |
|    n_updates        | 107624   |
----------------------------------
Eval num_timesteps=441000, episode_reward=130.84 +/- 57.22
Episode length: 254.36 +/- 231.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 254      |
|    mean_reward      | 131      |
| rollout/            |          |
|    exploration_rate | 0.523    |
| time/               |          |
|    total_timesteps  | 441000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.74     |
|    n_updates        | 107749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 382      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 53       |
|    time_elapsed     | 8196     |
|    total_timesteps  | 441135   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.15     |
|    n_updates        | 107783   |
----------------------------------
Eval num_timesteps=441500, episode_reward=180.37 +/- 32.20
Episode length: 476.24 +/- 146.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.522    |
| time/               |          |
|    total_timesteps  | 441500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.67     |
|    n_updates        | 107874   |
----------------------------------
Eval num_timesteps=442000, episode_reward=155.86 +/- 57.35
Episode length: 390.48 +/- 192.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 390      |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.521    |
| time/               |          |
|    total_timesteps  | 442000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 107999   |
----------------------------------
Eval num_timesteps=442500, episode_reward=187.95 +/- 26.79
Episode length: 501.62 +/- 94.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 502      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.52     |
| time/               |          |
|    total_timesteps  | 442500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.82     |
|    n_updates        | 108124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 381      |
|    ep_rew_mean      | 145      |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 53       |
|    time_elapsed     | 8236     |
|    total_timesteps  | 442647   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.4      |
|    n_updates        | 108161   |
----------------------------------
Eval num_timesteps=443000, episode_reward=146.98 +/- 56.44
Episode length: 370.42 +/- 193.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 370      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.519    |
| time/               |          |
|    total_timesteps  | 443000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.02     |
|    n_updates        | 108249   |
----------------------------------
Eval num_timesteps=443500, episode_reward=85.95 +/- 48.16
Episode length: 99.94 +/- 142.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.9     |
|    mean_reward      | 85.9     |
| rollout/            |          |
|    exploration_rate | 0.518    |
| time/               |          |
|    total_timesteps  | 443500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.28     |
|    n_updates        | 108374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 372      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.517    |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 53       |
|    time_elapsed     | 8250     |
|    total_timesteps  | 443852   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 108462   |
----------------------------------
Eval num_timesteps=444000, episode_reward=102.95 +/- 47.62
Episode length: 144.24 +/- 179.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.517    |
| time/               |          |
|    total_timesteps  | 444000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.01     |
|    n_updates        | 108499   |
----------------------------------
Eval num_timesteps=444500, episode_reward=104.48 +/- 83.43
Episode length: 273.18 +/- 219.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 273      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.516    |
| time/               |          |
|    total_timesteps  | 444500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.85     |
|    n_updates        | 108624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 371      |
|    ep_rew_mean      | 139      |
|    exploration_rate | 0.515    |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 53       |
|    time_elapsed     | 8263     |
|    total_timesteps  | 444985   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.58     |
|    n_updates        | 108746   |
----------------------------------
Eval num_timesteps=445000, episode_reward=170.58 +/- 49.41
Episode length: 443.34 +/- 156.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 443      |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.515    |
| time/               |          |
|    total_timesteps  | 445000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 108749   |
----------------------------------
Eval num_timesteps=445500, episode_reward=58.15 +/- 74.10
Episode length: 141.94 +/- 179.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 58.1     |
| rollout/            |          |
|    exploration_rate | 0.514    |
| time/               |          |
|    total_timesteps  | 445500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 108874   |
----------------------------------
Eval num_timesteps=446000, episode_reward=90.28 +/- 88.51
Episode length: 250.34 +/- 221.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 250      |
|    mean_reward      | 90.3     |
| rollout/            |          |
|    exploration_rate | 0.513    |
| time/               |          |
|    total_timesteps  | 446000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.2      |
|    n_updates        | 108999   |
----------------------------------
Eval num_timesteps=446500, episode_reward=103.34 +/- 82.63
Episode length: 254.46 +/- 225.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 254      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.512    |
| time/               |          |
|    total_timesteps  | 446500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.43     |
|    n_updates        | 109124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 372      |
|    ep_rew_mean      | 139      |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 53       |
|    time_elapsed     | 8295     |
|    total_timesteps  | 446723   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 109180   |
----------------------------------
Eval num_timesteps=447000, episode_reward=110.80 +/- 61.19
Episode length: 275.46 +/- 185.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 275      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.511    |
| time/               |          |
|    total_timesteps  | 447000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.65     |
|    n_updates        | 109249   |
----------------------------------
Eval num_timesteps=447500, episode_reward=88.93 +/- 74.68
Episode length: 207.54 +/- 195.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 88.9     |
| rollout/            |          |
|    exploration_rate | 0.51     |
| time/               |          |
|    total_timesteps  | 447500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 109374   |
----------------------------------
Eval num_timesteps=448000, episode_reward=127.91 +/- 79.55
Episode length: 326.92 +/- 226.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 327      |
|    mean_reward      | 128      |
| rollout/            |          |
|    exploration_rate | 0.509    |
| time/               |          |
|    total_timesteps  | 448000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.712    |
|    n_updates        | 109499   |
----------------------------------
Eval num_timesteps=448500, episode_reward=39.65 +/- 53.48
Episode length: 70.60 +/- 71.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.6     |
|    mean_reward      | 39.6     |
| rollout/            |          |
|    exploration_rate | 0.508    |
| time/               |          |
|    total_timesteps  | 448500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 109624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 377      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.507    |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 53       |
|    time_elapsed     | 8321     |
|    total_timesteps  | 448823   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.941    |
|    n_updates        | 109705   |
----------------------------------
Eval num_timesteps=449000, episode_reward=115.51 +/- 59.33
Episode length: 241.28 +/- 211.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 241      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.507    |
| time/               |          |
|    total_timesteps  | 449000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.833    |
|    n_updates        | 109749   |
----------------------------------
Eval num_timesteps=449500, episode_reward=169.96 +/- 46.96
Episode length: 414.82 +/- 196.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 415      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.506    |
| time/               |          |
|    total_timesteps  | 449500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 109874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 368      |
|    ep_rew_mean      | 139      |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 53       |
|    time_elapsed     | 8342     |
|    total_timesteps  | 449978   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.28     |
|    n_updates        | 109994   |
----------------------------------
Eval num_timesteps=450000, episode_reward=162.88 +/- 38.92
Episode length: 455.16 +/- 154.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 455      |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.505    |
| time/               |          |
|    total_timesteps  | 450000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.11     |
|    n_updates        | 109999   |
----------------------------------
Eval num_timesteps=450500, episode_reward=32.63 +/- 64.95
Episode length: 89.18 +/- 129.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.2     |
|    mean_reward      | 32.6     |
| rollout/            |          |
|    exploration_rate | 0.504    |
| time/               |          |
|    total_timesteps  | 450500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.868    |
|    n_updates        | 110124   |
----------------------------------
Eval num_timesteps=451000, episode_reward=188.55 +/- 30.43
Episode length: 485.96 +/- 132.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.503    |
| time/               |          |
|    total_timesteps  | 451000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.3      |
|    n_updates        | 110249   |
----------------------------------
Eval num_timesteps=451500, episode_reward=190.91 +/- 26.40
Episode length: 510.30 +/- 72.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 510      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.502    |
| time/               |          |
|    total_timesteps  | 451500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.954    |
|    n_updates        | 110374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 373      |
|    ep_rew_mean      | 139      |
|    exploration_rate | 0.502    |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 53       |
|    time_elapsed     | 8389     |
|    total_timesteps  | 451640   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.08     |
|    n_updates        | 110409   |
----------------------------------
Eval num_timesteps=452000, episode_reward=98.27 +/- 32.58
Episode length: 110.16 +/- 140.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 98.3     |
| rollout/            |          |
|    exploration_rate | 0.501    |
| time/               |          |
|    total_timesteps  | 452000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.826    |
|    n_updates        | 110499   |
----------------------------------
Eval num_timesteps=452500, episode_reward=132.08 +/- 64.61
Episode length: 307.32 +/- 215.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 307      |
|    mean_reward      | 132      |
| rollout/            |          |
|    exploration_rate | 0.5      |
| time/               |          |
|    total_timesteps  | 452500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.426    |
|    n_updates        | 110624   |
----------------------------------
Eval num_timesteps=453000, episode_reward=107.07 +/- 78.39
Episode length: 268.86 +/- 220.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 269      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.499    |
| time/               |          |
|    total_timesteps  | 453000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.04     |
|    n_updates        | 110749   |
----------------------------------
Eval num_timesteps=453500, episode_reward=133.92 +/- 59.19
Episode length: 319.02 +/- 197.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 319      |
|    mean_reward      | 134      |
| rollout/            |          |
|    exploration_rate | 0.498    |
| time/               |          |
|    total_timesteps  | 453500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.31     |
|    n_updates        | 110874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 378      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.498    |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 53       |
|    time_elapsed     | 8419     |
|    total_timesteps  | 453740   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.765    |
|    n_updates        | 110934   |
----------------------------------
Eval num_timesteps=454000, episode_reward=181.42 +/- 40.41
Episode length: 476.84 +/- 121.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.497    |
| time/               |          |
|    total_timesteps  | 454000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 110999   |
----------------------------------
Eval num_timesteps=454500, episode_reward=170.58 +/- 46.44
Episode length: 449.16 +/- 145.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.496    |
| time/               |          |
|    total_timesteps  | 454500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.4      |
|    n_updates        | 111124   |
----------------------------------
Eval num_timesteps=455000, episode_reward=148.16 +/- 72.97
Episode length: 380.92 +/- 213.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 381      |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.495    |
| time/               |          |
|    total_timesteps  | 455000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 111249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.495    |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 53       |
|    time_elapsed     | 8457     |
|    total_timesteps  | 455213   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.97     |
|    n_updates        | 111303   |
----------------------------------
Eval num_timesteps=455500, episode_reward=127.29 +/- 62.45
Episode length: 320.74 +/- 196.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 321      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.494    |
| time/               |          |
|    total_timesteps  | 455500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.51     |
|    n_updates        | 111374   |
----------------------------------
Eval num_timesteps=456000, episode_reward=123.53 +/- 64.58
Episode length: 249.54 +/- 235.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 250      |
|    mean_reward      | 124      |
| rollout/            |          |
|    exploration_rate | 0.493    |
| time/               |          |
|    total_timesteps  | 456000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.89     |
|    n_updates        | 111499   |
----------------------------------
Eval num_timesteps=456500, episode_reward=127.77 +/- 80.74
Episode length: 341.70 +/- 212.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 342      |
|    mean_reward      | 128      |
| rollout/            |          |
|    exploration_rate | 0.492    |
| time/               |          |
|    total_timesteps  | 456500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.797    |
|    n_updates        | 111624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.491    |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 53       |
|    time_elapsed     | 8484     |
|    total_timesteps  | 456822   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 111705   |
----------------------------------
Eval num_timesteps=457000, episode_reward=170.11 +/- 34.51
Episode length: 461.40 +/- 158.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 461      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.491    |
| time/               |          |
|    total_timesteps  | 457000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.93     |
|    n_updates        | 111749   |
----------------------------------
Eval num_timesteps=457500, episode_reward=79.19 +/- 69.32
Episode length: 135.82 +/- 182.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 79.2     |
| rollout/            |          |
|    exploration_rate | 0.49     |
| time/               |          |
|    total_timesteps  | 457500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.85     |
|    n_updates        | 111874   |
----------------------------------
Eval num_timesteps=458000, episode_reward=164.49 +/- 49.49
Episode length: 432.56 +/- 154.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 433      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.489    |
| time/               |          |
|    total_timesteps  | 458000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.61     |
|    n_updates        | 111999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 53       |
|    time_elapsed     | 8515     |
|    total_timesteps  | 458434   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.929    |
|    n_updates        | 112108   |
----------------------------------
Eval num_timesteps=458500, episode_reward=126.55 +/- 66.28
Episode length: 300.78 +/- 204.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 301      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 458500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.679    |
|    n_updates        | 112124   |
----------------------------------
Eval num_timesteps=459000, episode_reward=67.67 +/- 80.74
Episode length: 177.44 +/- 193.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 177      |
|    mean_reward      | 67.7     |
| rollout/            |          |
|    exploration_rate | 0.487    |
| time/               |          |
|    total_timesteps  | 459000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.842    |
|    n_updates        | 112249   |
----------------------------------
Eval num_timesteps=459500, episode_reward=156.15 +/- 53.44
Episode length: 388.08 +/- 189.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 388      |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.486    |
| time/               |          |
|    total_timesteps  | 459500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.798    |
|    n_updates        | 112374   |
----------------------------------
Eval num_timesteps=460000, episode_reward=127.86 +/- 70.67
Episode length: 343.88 +/- 193.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 344      |
|    mean_reward      | 128      |
| rollout/            |          |
|    exploration_rate | 0.485    |
| time/               |          |
|    total_timesteps  | 460000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.724    |
|    n_updates        | 112499   |
----------------------------------
Eval num_timesteps=460500, episode_reward=44.41 +/- 64.70
Episode length: 91.02 +/- 131.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91       |
|    mean_reward      | 44.4     |
| rollout/            |          |
|    exploration_rate | 0.484    |
| time/               |          |
|    total_timesteps  | 460500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.863    |
|    n_updates        | 112624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 139      |
|    exploration_rate | 0.484    |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 53       |
|    time_elapsed     | 8553     |
|    total_timesteps  | 460534   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.721    |
|    n_updates        | 112633   |
----------------------------------
Eval num_timesteps=461000, episode_reward=193.78 +/- 15.28
Episode length: 517.38 +/- 53.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 517      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.483    |
| time/               |          |
|    total_timesteps  | 461000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 112749   |
----------------------------------
Eval num_timesteps=461500, episode_reward=135.49 +/- 66.95
Episode length: 356.42 +/- 190.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 356      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.482    |
| time/               |          |
|    total_timesteps  | 461500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.39     |
|    n_updates        | 112874   |
----------------------------------
Eval num_timesteps=462000, episode_reward=132.18 +/- 60.60
Episode length: 308.16 +/- 214.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 308      |
|    mean_reward      | 132      |
| rollout/            |          |
|    exploration_rate | 0.481    |
| time/               |          |
|    total_timesteps  | 462000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.06     |
|    n_updates        | 112999   |
----------------------------------
Eval num_timesteps=462500, episode_reward=110.12 +/- 69.59
Episode length: 259.18 +/- 200.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 259      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.48     |
| time/               |          |
|    total_timesteps  | 462500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.57     |
|    n_updates        | 113124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 139      |
|    exploration_rate | 0.48     |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 53       |
|    time_elapsed     | 8596     |
|    total_timesteps  | 462634   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.83     |
|    n_updates        | 113158   |
----------------------------------
Eval num_timesteps=463000, episode_reward=122.69 +/- 56.55
Episode length: 299.64 +/- 180.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 300      |
|    mean_reward      | 123      |
| rollout/            |          |
|    exploration_rate | 0.479    |
| time/               |          |
|    total_timesteps  | 463000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.665    |
|    n_updates        | 113249   |
----------------------------------
Eval num_timesteps=463500, episode_reward=149.89 +/- 54.96
Episode length: 374.22 +/- 183.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 374      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.478    |
| time/               |          |
|    total_timesteps  | 463500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.878    |
|    n_updates        | 113374   |
----------------------------------
Eval num_timesteps=464000, episode_reward=91.81 +/- 85.99
Episode length: 237.78 +/- 221.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 91.8     |
| rollout/            |          |
|    exploration_rate | 0.477    |
| time/               |          |
|    total_timesteps  | 464000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 113499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 385      |
|    ep_rew_mean      | 139      |
|    exploration_rate | 0.477    |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 53       |
|    time_elapsed     | 8623     |
|    total_timesteps  | 464297   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.908    |
|    n_updates        | 113574   |
----------------------------------
Eval num_timesteps=464500, episode_reward=189.44 +/- 26.45
Episode length: 510.88 +/- 69.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 511      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.476    |
| time/               |          |
|    total_timesteps  | 464500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.82     |
|    n_updates        | 113624   |
----------------------------------
Eval num_timesteps=465000, episode_reward=178.50 +/- 40.17
Episode length: 469.60 +/- 139.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 470      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.475    |
| time/               |          |
|    total_timesteps  | 465000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.92     |
|    n_updates        | 113749   |
----------------------------------
Eval num_timesteps=465500, episode_reward=114.79 +/- 71.40
Episode length: 230.80 +/- 230.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 0.474    |
| time/               |          |
|    total_timesteps  | 465500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.28     |
|    n_updates        | 113874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 395      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 53       |
|    time_elapsed     | 8658     |
|    total_timesteps  | 465924   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.601    |
|    n_updates        | 113980   |
----------------------------------
Eval num_timesteps=466000, episode_reward=182.92 +/- 37.48
Episode length: 484.68 +/- 110.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 485      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.473    |
| time/               |          |
|    total_timesteps  | 466000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.872    |
|    n_updates        | 113999   |
----------------------------------
Eval num_timesteps=466500, episode_reward=152.21 +/- 56.47
Episode length: 394.06 +/- 177.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 394      |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.472    |
| time/               |          |
|    total_timesteps  | 466500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.67     |
|    n_updates        | 114124   |
----------------------------------
Eval num_timesteps=467000, episode_reward=148.28 +/- 64.79
Episode length: 372.90 +/- 198.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 373      |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.471    |
| time/               |          |
|    total_timesteps  | 467000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.737    |
|    n_updates        | 114249   |
----------------------------------
Eval num_timesteps=467500, episode_reward=144.18 +/- 60.04
Episode length: 398.40 +/- 161.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 398      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.47     |
| time/               |          |
|    total_timesteps  | 467500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.01     |
|    n_updates        | 114374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 390      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.47     |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 53       |
|    time_elapsed     | 8706     |
|    total_timesteps  | 467546   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.846    |
|    n_updates        | 114386   |
----------------------------------
Eval num_timesteps=468000, episode_reward=164.64 +/- 55.29
Episode length: 429.46 +/- 191.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.469    |
| time/               |          |
|    total_timesteps  | 468000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.2      |
|    n_updates        | 114499   |
----------------------------------
Eval num_timesteps=468500, episode_reward=176.26 +/- 41.97
Episode length: 448.46 +/- 166.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.468    |
| time/               |          |
|    total_timesteps  | 468500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.32     |
|    n_updates        | 114624   |
----------------------------------
Eval num_timesteps=469000, episode_reward=74.64 +/- 73.73
Episode length: 178.56 +/- 188.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 74.6     |
| rollout/            |          |
|    exploration_rate | 0.467    |
| time/               |          |
|    total_timesteps  | 469000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.949    |
|    n_updates        | 114749   |
----------------------------------
Eval num_timesteps=469500, episode_reward=186.91 +/- 36.15
Episode length: 481.30 +/- 134.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 481      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.466    |
| time/               |          |
|    total_timesteps  | 469500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.479    |
|    n_updates        | 114874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 390      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.466    |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 53       |
|    time_elapsed     | 8751     |
|    total_timesteps  | 469646   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.19     |
|    n_updates        | 114911   |
----------------------------------
Eval num_timesteps=470000, episode_reward=125.81 +/- 83.55
Episode length: 320.58 +/- 240.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 321      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.465    |
| time/               |          |
|    total_timesteps  | 470000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 114999   |
----------------------------------
Eval num_timesteps=470500, episode_reward=107.00 +/- 86.33
Episode length: 259.00 +/- 236.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 259      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.464    |
| time/               |          |
|    total_timesteps  | 470500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.393    |
|    n_updates        | 115124   |
----------------------------------
Eval num_timesteps=471000, episode_reward=51.11 +/- 76.81
Episode length: 125.68 +/- 175.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 51.1     |
| rollout/            |          |
|    exploration_rate | 0.463    |
| time/               |          |
|    total_timesteps  | 471000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.609    |
|    n_updates        | 115249   |
----------------------------------
Eval num_timesteps=471500, episode_reward=10.27 +/- 50.73
Episode length: 72.78 +/- 95.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.8     |
|    mean_reward      | 10.3     |
| rollout/            |          |
|    exploration_rate | 0.462    |
| time/               |          |
|    total_timesteps  | 471500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.642    |
|    n_updates        | 115374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 399      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.462    |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 53       |
|    time_elapsed     | 8775     |
|    total_timesteps  | 471746   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.32     |
|    n_updates        | 115436   |
----------------------------------
Eval num_timesteps=472000, episode_reward=16.35 +/- 53.76
Episode length: 68.98 +/- 94.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69       |
|    mean_reward      | 16.3     |
| rollout/            |          |
|    exploration_rate | 0.461    |
| time/               |          |
|    total_timesteps  | 472000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.709    |
|    n_updates        | 115499   |
----------------------------------
Eval num_timesteps=472500, episode_reward=55.78 +/- 53.18
Episode length: 104.08 +/- 103.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 55.8     |
| rollout/            |          |
|    exploration_rate | 0.46     |
| time/               |          |
|    total_timesteps  | 472500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.756    |
|    n_updates        | 115624   |
----------------------------------
Eval num_timesteps=473000, episode_reward=150.22 +/- 53.59
Episode length: 332.12 +/- 227.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 332      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.459    |
| time/               |          |
|    total_timesteps  | 473000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.07     |
|    n_updates        | 115749   |
----------------------------------
Eval num_timesteps=473500, episode_reward=141.29 +/- 58.70
Episode length: 325.86 +/- 213.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 326      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.458    |
| time/               |          |
|    total_timesteps  | 473500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.85     |
|    n_updates        | 115874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 404      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 53       |
|    time_elapsed     | 8800     |
|    total_timesteps  | 473846   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 115961   |
----------------------------------
Eval num_timesteps=474000, episode_reward=42.26 +/- 74.07
Episode length: 120.86 +/- 165.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 42.3     |
| rollout/            |          |
|    exploration_rate | 0.457    |
| time/               |          |
|    total_timesteps  | 474000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 115999   |
----------------------------------
Eval num_timesteps=474500, episode_reward=169.54 +/- 47.56
Episode length: 416.90 +/- 194.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 417      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.456    |
| time/               |          |
|    total_timesteps  | 474500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.47     |
|    n_updates        | 116124   |
----------------------------------
Eval num_timesteps=475000, episode_reward=160.40 +/- 44.07
Episode length: 448.96 +/- 145.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.456    |
| time/               |          |
|    total_timesteps  | 475000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.52     |
|    n_updates        | 116249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 404      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.455    |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 53       |
|    time_elapsed     | 8829     |
|    total_timesteps  | 475044   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.655    |
|    n_updates        | 116260   |
----------------------------------
Eval num_timesteps=475500, episode_reward=152.11 +/- 52.84
Episode length: 341.20 +/- 225.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 341      |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.455    |
| time/               |          |
|    total_timesteps  | 475500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.62     |
|    n_updates        | 116374   |
----------------------------------
Eval num_timesteps=476000, episode_reward=2.41 +/- 40.41
Episode length: 60.44 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.4     |
|    mean_reward      | 2.41     |
| rollout/            |          |
|    exploration_rate | 0.454    |
| time/               |          |
|    total_timesteps  | 476000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.917    |
|    n_updates        | 116499   |
----------------------------------
Eval num_timesteps=476500, episode_reward=176.85 +/- 41.91
Episode length: 438.06 +/- 185.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 438      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.453    |
| time/               |          |
|    total_timesteps  | 476500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.24     |
|    n_updates        | 116624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 53       |
|    time_elapsed     | 8854     |
|    total_timesteps  | 476687   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 116671   |
----------------------------------
Eval num_timesteps=477000, episode_reward=150.35 +/- 53.58
Episode length: 329.34 +/- 230.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 329      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.452    |
| time/               |          |
|    total_timesteps  | 477000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.587    |
|    n_updates        | 116749   |
----------------------------------
Eval num_timesteps=477500, episode_reward=141.51 +/- 54.13
Episode length: 291.82 +/- 233.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 292      |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.451    |
| time/               |          |
|    total_timesteps  | 477500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.652    |
|    n_updates        | 116874   |
----------------------------------
Eval num_timesteps=478000, episode_reward=116.33 +/- 66.31
Episode length: 217.04 +/- 231.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.45     |
| time/               |          |
|    total_timesteps  | 478000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.07     |
|    n_updates        | 116999   |
----------------------------------
Eval num_timesteps=478500, episode_reward=193.72 +/- 15.44
Episode length: 515.48 +/- 66.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.449    |
| time/               |          |
|    total_timesteps  | 478500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.74     |
|    n_updates        | 117124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.448    |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 53       |
|    time_elapsed     | 8894     |
|    total_timesteps  | 478787   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.929    |
|    n_updates        | 117196   |
----------------------------------
Eval num_timesteps=479000, episode_reward=198.46 +/- 13.91
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.448    |
| time/               |          |
|    total_timesteps  | 479000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.84     |
|    n_updates        | 117249   |
----------------------------------
New best mean reward!
Eval num_timesteps=479500, episode_reward=185.59 +/- 32.74
Episode length: 478.66 +/- 139.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.447    |
| time/               |          |
|    total_timesteps  | 479500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 117374   |
----------------------------------
Eval num_timesteps=480000, episode_reward=47.19 +/- 77.08
Episode length: 126.16 +/- 174.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 47.2     |
| rollout/            |          |
|    exploration_rate | 0.446    |
| time/               |          |
|    total_timesteps  | 480000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.92     |
|    n_updates        | 117499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 412      |
|    ep_rew_mean      | 144      |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 53       |
|    time_elapsed     | 8928     |
|    total_timesteps  | 480394   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.961    |
|    n_updates        | 117598   |
----------------------------------
Eval num_timesteps=480500, episode_reward=187.92 +/- 28.48
Episode length: 496.70 +/- 112.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.445    |
| time/               |          |
|    total_timesteps  | 480500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.11     |
|    n_updates        | 117624   |
----------------------------------
Eval num_timesteps=481000, episode_reward=150.51 +/- 53.70
Episode length: 328.64 +/- 230.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 329      |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.444    |
| time/               |          |
|    total_timesteps  | 481000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.28     |
|    n_updates        | 117749   |
----------------------------------
Eval num_timesteps=481500, episode_reward=38.49 +/- 65.03
Episode length: 86.46 +/- 129.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.5     |
|    mean_reward      | 38.5     |
| rollout/            |          |
|    exploration_rate | 0.443    |
| time/               |          |
|    total_timesteps  | 481500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.71     |
|    n_updates        | 117874   |
----------------------------------
Eval num_timesteps=482000, episode_reward=154.65 +/- 52.65
Episode length: 345.22 +/- 229.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 345      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.442    |
| time/               |          |
|    total_timesteps  | 482000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.24     |
|    n_updates        | 117999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 425      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.441    |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 53       |
|    time_elapsed     | 8965     |
|    total_timesteps  | 482494   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.82     |
|    n_updates        | 118123   |
----------------------------------
Eval num_timesteps=482500, episode_reward=53.85 +/- 62.64
Episode length: 93.02 +/- 128.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93       |
|    mean_reward      | 53.8     |
| rollout/            |          |
|    exploration_rate | 0.441    |
| time/               |          |
|    total_timesteps  | 482500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.899    |
|    n_updates        | 118124   |
----------------------------------
Eval num_timesteps=483000, episode_reward=127.43 +/- 72.23
Episode length: 277.20 +/- 238.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 277      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.44     |
| time/               |          |
|    total_timesteps  | 483000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.39     |
|    n_updates        | 118249   |
----------------------------------
Eval num_timesteps=483500, episode_reward=188.62 +/- 21.31
Episode length: 505.56 +/- 95.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.439    |
| time/               |          |
|    total_timesteps  | 483500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.969    |
|    n_updates        | 118374   |
----------------------------------
Eval num_timesteps=484000, episode_reward=18.69 +/- 58.64
Episode length: 82.36 +/- 113.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.4     |
|    mean_reward      | 18.7     |
| rollout/            |          |
|    exploration_rate | 0.438    |
| time/               |          |
|    total_timesteps  | 484000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.739    |
|    n_updates        | 118499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 430      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 53       |
|    time_elapsed     | 8994     |
|    total_timesteps  | 484132   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.21     |
|    n_updates        | 118532   |
----------------------------------
Eval num_timesteps=484500, episode_reward=183.36 +/- 32.84
Episode length: 477.22 +/- 143.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.437    |
| time/               |          |
|    total_timesteps  | 484500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.62     |
|    n_updates        | 118624   |
----------------------------------
Eval num_timesteps=485000, episode_reward=22.59 +/- 59.77
Episode length: 79.92 +/- 113.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.9     |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.436    |
| time/               |          |
|    total_timesteps  | 485000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.433    |
|    n_updates        | 118749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 426      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.435    |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 53       |
|    time_elapsed     | 9010     |
|    total_timesteps  | 485245   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.666    |
|    n_updates        | 118811   |
----------------------------------
Eval num_timesteps=485500, episode_reward=68.35 +/- 73.67
Episode length: 134.74 +/- 183.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 68.3     |
| rollout/            |          |
|    exploration_rate | 0.435    |
| time/               |          |
|    total_timesteps  | 485500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.828    |
|    n_updates        | 118874   |
----------------------------------
Eval num_timesteps=486000, episode_reward=193.25 +/- 15.62
Episode length: 514.94 +/- 70.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.434    |
| time/               |          |
|    total_timesteps  | 486000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.883    |
|    n_updates        | 118999   |
----------------------------------
Eval num_timesteps=486500, episode_reward=191.11 +/- 21.23
Episode length: 505.32 +/- 96.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.433    |
| time/               |          |
|    total_timesteps  | 486500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.808    |
|    n_updates        | 119124   |
----------------------------------
Eval num_timesteps=487000, episode_reward=183.07 +/- 35.45
Episode length: 466.42 +/- 158.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 466      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.432    |
| time/               |          |
|    total_timesteps  | 487000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.761    |
|    n_updates        | 119249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 435      |
|    ep_rew_mean      | 152      |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 53       |
|    time_elapsed     | 9059     |
|    total_timesteps  | 487345   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.355    |
|    n_updates        | 119336   |
----------------------------------
Eval num_timesteps=487500, episode_reward=80.15 +/- 86.23
Episode length: 192.34 +/- 218.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 80.1     |
| rollout/            |          |
|    exploration_rate | 0.431    |
| time/               |          |
|    total_timesteps  | 487500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.06     |
|    n_updates        | 119374   |
----------------------------------
Eval num_timesteps=488000, episode_reward=196.36 +/- 2.47
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.43     |
| time/               |          |
|    total_timesteps  | 488000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.42     |
|    n_updates        | 119499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 435      |
|    ep_rew_mean      | 153      |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 53       |
|    time_elapsed     | 9080     |
|    total_timesteps  | 488479   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.68     |
|    n_updates        | 119619   |
----------------------------------
Eval num_timesteps=488500, episode_reward=193.15 +/- 16.07
Episode length: 515.28 +/- 68.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.429    |
| time/               |          |
|    total_timesteps  | 488500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 119624   |
----------------------------------
Eval num_timesteps=489000, episode_reward=103.87 +/- 71.99
Episode length: 204.24 +/- 220.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.428    |
| time/               |          |
|    total_timesteps  | 489000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.771    |
|    n_updates        | 119749   |
----------------------------------
Eval num_timesteps=489500, episode_reward=181.36 +/- 42.80
Episode length: 468.14 +/- 154.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 468      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.427    |
| time/               |          |
|    total_timesteps  | 489500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.529    |
|    n_updates        | 119874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 429      |
|    ep_rew_mean      | 152      |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 53       |
|    time_elapsed     | 9115     |
|    total_timesteps  | 489614   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.766    |
|    n_updates        | 119903   |
----------------------------------
Eval num_timesteps=490000, episode_reward=135.90 +/- 75.71
Episode length: 346.46 +/- 223.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 346      |
|    mean_reward      | 136      |
| rollout/            |          |
|    exploration_rate | 0.426    |
| time/               |          |
|    total_timesteps  | 490000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.27     |
|    n_updates        | 119999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 416      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.425    |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 53       |
|    time_elapsed     | 9125     |
|    total_timesteps  | 490467   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.68     |
|    n_updates        | 120116   |
----------------------------------
Eval num_timesteps=490500, episode_reward=45.01 +/- 76.83
Episode length: 126.98 +/- 174.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 45       |
| rollout/            |          |
|    exploration_rate | 0.425    |
| time/               |          |
|    total_timesteps  | 490500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.728    |
|    n_updates        | 120124   |
----------------------------------
Eval num_timesteps=491000, episode_reward=186.48 +/- 5.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.424    |
| time/               |          |
|    total_timesteps  | 491000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.289    |
|    n_updates        | 120249   |
----------------------------------
Eval num_timesteps=491500, episode_reward=193.53 +/- 15.31
Episode length: 515.02 +/- 69.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 491500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.327    |
|    n_updates        | 120374   |
----------------------------------
Eval num_timesteps=492000, episode_reward=165.72 +/- 49.02
Episode length: 392.40 +/- 212.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 392      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.422    |
| time/               |          |
|    total_timesteps  | 492000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.789    |
|    n_updates        | 120499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 421      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 53       |
|    time_elapsed     | 9170     |
|    total_timesteps  | 492093   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.758    |
|    n_updates        | 120523   |
----------------------------------
Eval num_timesteps=492500, episode_reward=105.27 +/- 53.38
Episode length: 163.84 +/- 192.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.421    |
| time/               |          |
|    total_timesteps  | 492500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 120624   |
----------------------------------
Eval num_timesteps=493000, episode_reward=177.29 +/- 34.37
Episode length: 468.56 +/- 153.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 469      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.42     |
| time/               |          |
|    total_timesteps  | 493000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.728    |
|    n_updates        | 120749   |
----------------------------------
Eval num_timesteps=493500, episode_reward=191.25 +/- 21.43
Episode length: 505.50 +/- 95.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.419    |
| time/               |          |
|    total_timesteps  | 493500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.798    |
|    n_updates        | 120874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 421      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.418    |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 53       |
|    time_elapsed     | 9204     |
|    total_timesteps  | 493707   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 120926   |
----------------------------------
Eval num_timesteps=494000, episode_reward=191.88 +/- 21.54
Episode length: 506.08 +/- 92.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.418    |
| time/               |          |
|    total_timesteps  | 494000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 120999   |
----------------------------------
Eval num_timesteps=494500, episode_reward=189.25 +/- 33.57
Episode length: 487.20 +/- 128.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.417    |
| time/               |          |
|    total_timesteps  | 494500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.847    |
|    n_updates        | 121124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 412      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.416    |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 53       |
|    time_elapsed     | 9233     |
|    total_timesteps  | 494891   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.54     |
|    n_updates        | 121222   |
----------------------------------
Eval num_timesteps=495000, episode_reward=176.62 +/- 46.80
Episode length: 428.88 +/- 192.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.416    |
| time/               |          |
|    total_timesteps  | 495000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.342    |
|    n_updates        | 121249   |
----------------------------------
Eval num_timesteps=495500, episode_reward=78.45 +/- 91.90
Episode length: 209.12 +/- 227.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 78.4     |
| rollout/            |          |
|    exploration_rate | 0.415    |
| time/               |          |
|    total_timesteps  | 495500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.38     |
|    n_updates        | 121374   |
----------------------------------
Eval num_timesteps=496000, episode_reward=195.07 +/- 15.41
Episode length: 515.68 +/- 65.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.414    |
| time/               |          |
|    total_timesteps  | 496000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.442    |
|    n_updates        | 121499   |
----------------------------------
Eval num_timesteps=496500, episode_reward=22.60 +/- 59.72
Episode length: 83.84 +/- 114.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.8     |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.413    |
| time/               |          |
|    total_timesteps  | 496500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.24     |
|    n_updates        | 121624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 413      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.413    |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 53       |
|    time_elapsed     | 9270     |
|    total_timesteps  | 496506   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.802    |
|    n_updates        | 121626   |
----------------------------------
Eval num_timesteps=497000, episode_reward=91.88 +/- 46.60
Episode length: 114.82 +/- 153.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 91.9     |
| rollout/            |          |
|    exploration_rate | 0.412    |
| time/               |          |
|    total_timesteps  | 497000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 121749   |
----------------------------------
Eval num_timesteps=497500, episode_reward=86.91 +/- 20.88
Episode length: 71.66 +/- 68.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.7     |
|    mean_reward      | 86.9     |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 497500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.651    |
|    n_updates        | 121874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.411    |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 53       |
|    time_elapsed     | 9276     |
|    total_timesteps  | 497698   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.37     |
|    n_updates        | 121924   |
----------------------------------
Eval num_timesteps=498000, episode_reward=152.75 +/- 63.82
Episode length: 344.30 +/- 231.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 344      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.41     |
| time/               |          |
|    total_timesteps  | 498000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.931    |
|    n_updates        | 121999   |
----------------------------------
Eval num_timesteps=498500, episode_reward=130.10 +/- 53.61
Episode length: 245.78 +/- 228.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 130      |
| rollout/            |          |
|    exploration_rate | 0.409    |
| time/               |          |
|    total_timesteps  | 498500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.97     |
|    n_updates        | 122124   |
----------------------------------
Eval num_timesteps=499000, episode_reward=120.76 +/- 54.41
Episode length: 228.28 +/- 223.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 121      |
| rollout/            |          |
|    exploration_rate | 0.408    |
| time/               |          |
|    total_timesteps  | 499000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.57     |
|    n_updates        | 122249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 409      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.407    |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 53       |
|    time_elapsed     | 9301     |
|    total_timesteps  | 499304   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.98     |
|    n_updates        | 122325   |
----------------------------------
Eval num_timesteps=499500, episode_reward=125.90 +/- 52.52
Episode length: 226.06 +/- 225.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.407    |
| time/               |          |
|    total_timesteps  | 499500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.511    |
|    n_updates        | 122374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 394      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 53       |
|    time_elapsed     | 9308     |
|    total_timesteps  | 499942   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.577    |
|    n_updates        | 122485   |
----------------------------------
Eval num_timesteps=500000, episode_reward=101.98 +/- 43.70
Episode length: 255.18 +/- 139.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 255      |
|    mean_reward      | 102      |
| rollout/            |          |
|    exploration_rate | 0.406    |
| time/               |          |
|    total_timesteps  | 500000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 122499   |
----------------------------------
Eval num_timesteps=500500, episode_reward=93.89 +/- 30.66
Episode length: 117.38 +/- 151.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 93.9     |
| rollout/            |          |
|    exploration_rate | 0.405    |
| time/               |          |
|    total_timesteps  | 500500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.21     |
|    n_updates        | 122624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 381      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.404    |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 53       |
|    time_elapsed     | 9319     |
|    total_timesteps  | 500772   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.535    |
|    n_updates        | 122692   |
----------------------------------
Eval num_timesteps=501000, episode_reward=113.44 +/- 50.72
Episode length: 190.72 +/- 201.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.404    |
| time/               |          |
|    total_timesteps  | 501000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.708    |
|    n_updates        | 122749   |
----------------------------------
Eval num_timesteps=501500, episode_reward=186.04 +/- 29.44
Episode length: 486.98 +/- 128.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.403    |
| time/               |          |
|    total_timesteps  | 501500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.781    |
|    n_updates        | 122874   |
----------------------------------
Eval num_timesteps=502000, episode_reward=147.26 +/- 55.74
Episode length: 322.06 +/- 229.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 322      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.402    |
| time/               |          |
|    total_timesteps  | 502000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.833    |
|    n_updates        | 122999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 381      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 53       |
|    time_elapsed     | 9349     |
|    total_timesteps  | 502428   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.768    |
|    n_updates        | 123106   |
----------------------------------
Eval num_timesteps=502500, episode_reward=102.49 +/- 43.29
Episode length: 131.92 +/- 172.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 102      |
| rollout/            |          |
|    exploration_rate | 0.401    |
| time/               |          |
|    total_timesteps  | 502500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.508    |
|    n_updates        | 123124   |
----------------------------------
Eval num_timesteps=503000, episode_reward=100.07 +/- 35.22
Episode length: 123.44 +/- 150.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 100      |
| rollout/            |          |
|    exploration_rate | 0.4      |
| time/               |          |
|    total_timesteps  | 503000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.71     |
|    n_updates        | 123249   |
----------------------------------
Eval num_timesteps=503500, episode_reward=91.75 +/- 21.36
Episode length: 73.84 +/- 93.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.8     |
|    mean_reward      | 91.7     |
| rollout/            |          |
|    exploration_rate | 0.399    |
| time/               |          |
|    total_timesteps  | 503500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.22     |
|    n_updates        | 123374   |
----------------------------------
Eval num_timesteps=504000, episode_reward=133.25 +/- 58.67
Episode length: 300.22 +/- 224.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 300      |
|    mean_reward      | 133      |
| rollout/            |          |
|    exploration_rate | 0.398    |
| time/               |          |
|    total_timesteps  | 504000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 123499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 382      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 53       |
|    time_elapsed     | 9368     |
|    total_timesteps  | 504081   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.864    |
|    n_updates        | 123520   |
----------------------------------
Eval num_timesteps=504500, episode_reward=154.06 +/- 53.39
Episode length: 346.96 +/- 227.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 347      |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.397    |
| time/               |          |
|    total_timesteps  | 504500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.762    |
|    n_updates        | 123624   |
----------------------------------
Eval num_timesteps=505000, episode_reward=48.57 +/- 70.78
Episode length: 107.06 +/- 155.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 48.6     |
| rollout/            |          |
|    exploration_rate | 0.396    |
| time/               |          |
|    total_timesteps  | 505000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.89     |
|    n_updates        | 123749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 377      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.396    |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 53       |
|    time_elapsed     | 9382     |
|    total_timesteps  | 505267   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.439    |
|    n_updates        | 123816   |
----------------------------------
Eval num_timesteps=505500, episode_reward=130.31 +/- 52.68
Episode length: 259.50 +/- 235.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 260      |
|    mean_reward      | 130      |
| rollout/            |          |
|    exploration_rate | 0.395    |
| time/               |          |
|    total_timesteps  | 505500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 123874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 363      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 53       |
|    time_elapsed     | 9390     |
|    total_timesteps  | 505948   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.09     |
|    n_updates        | 123986   |
----------------------------------
Eval num_timesteps=506000, episode_reward=21.95 +/- 46.86
Episode length: 53.18 +/- 17.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.2     |
|    mean_reward      | 21.9     |
| rollout/            |          |
|    exploration_rate | 0.394    |
| time/               |          |
|    total_timesteps  | 506000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.87     |
|    n_updates        | 123999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 344      |
|    ep_rew_mean      | 136      |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 53       |
|    time_elapsed     | 9392     |
|    total_timesteps  | 506158   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.684    |
|    n_updates        | 124039   |
----------------------------------
Eval num_timesteps=506500, episode_reward=154.23 +/- 54.13
Episode length: 345.80 +/- 229.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 346      |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.393    |
| time/               |          |
|    total_timesteps  | 506500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.711    |
|    n_updates        | 124124   |
----------------------------------
Eval num_timesteps=507000, episode_reward=81.35 +/- 37.68
Episode length: 76.58 +/- 94.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.6     |
|    mean_reward      | 81.3     |
| rollout/            |          |
|    exploration_rate | 0.392    |
| time/               |          |
|    total_timesteps  | 507000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.903    |
|    n_updates        | 124249   |
----------------------------------
Eval num_timesteps=507500, episode_reward=89.85 +/- 29.92
Episode length: 89.52 +/- 129.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 89.8     |
| rollout/            |          |
|    exploration_rate | 0.391    |
| time/               |          |
|    total_timesteps  | 507500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 124374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 339      |
|    ep_rew_mean      | 135      |
|    exploration_rate | 0.391    |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 53       |
|    time_elapsed     | 9408     |
|    total_timesteps  | 507783   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.882    |
|    n_updates        | 124445   |
----------------------------------
Eval num_timesteps=508000, episode_reward=163.35 +/- 54.11
Episode length: 383.78 +/- 215.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 384      |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.39     |
| time/               |          |
|    total_timesteps  | 508000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.362    |
|    n_updates        | 124499   |
----------------------------------
Eval num_timesteps=508500, episode_reward=119.96 +/- 63.50
Episode length: 334.04 +/- 172.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 334      |
|    mean_reward      | 120      |
| rollout/            |          |
|    exploration_rate | 0.389    |
| time/               |          |
|    total_timesteps  | 508500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.386    |
|    n_updates        | 124624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 335      |
|    ep_rew_mean      | 134      |
|    exploration_rate | 0.389    |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 53       |
|    time_elapsed     | 9428     |
|    total_timesteps  | 508576   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.598    |
|    n_updates        | 124643   |
----------------------------------
Eval num_timesteps=509000, episode_reward=164.17 +/- 42.81
Episode length: 467.82 +/- 135.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 468      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.388    |
| time/               |          |
|    total_timesteps  | 509000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.99     |
|    n_updates        | 124749   |
----------------------------------
Eval num_timesteps=509500, episode_reward=102.52 +/- 56.28
Episode length: 282.46 +/- 164.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 282      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.387    |
| time/               |          |
|    total_timesteps  | 509500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.28     |
|    n_updates        | 124874   |
----------------------------------
Eval num_timesteps=510000, episode_reward=172.75 +/- 43.38
Episode length: 431.26 +/- 187.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.386    |
| time/               |          |
|    total_timesteps  | 510000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 124999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 336      |
|    ep_rew_mean      | 134      |
|    exploration_rate | 0.386    |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 53       |
|    time_elapsed     | 9463     |
|    total_timesteps  | 510258   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 125064   |
----------------------------------
Eval num_timesteps=510500, episode_reward=154.24 +/- 53.03
Episode length: 346.14 +/- 228.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 346      |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.385    |
| time/               |          |
|    total_timesteps  | 510500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.849    |
|    n_updates        | 125124   |
----------------------------------
Eval num_timesteps=511000, episode_reward=175.28 +/- 46.72
Episode length: 453.38 +/- 156.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 453      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.384    |
| time/               |          |
|    total_timesteps  | 511000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.76     |
|    n_updates        | 125249   |
----------------------------------
Eval num_timesteps=511500, episode_reward=141.46 +/- 55.06
Episode length: 310.74 +/- 221.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 311      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 511500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.3      |
|    n_updates        | 125374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 328      |
|    ep_rew_mean      | 133      |
|    exploration_rate | 0.383    |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 53       |
|    time_elapsed     | 9495     |
|    total_timesteps  | 511539   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.702    |
|    n_updates        | 125384   |
----------------------------------
Eval num_timesteps=512000, episode_reward=34.03 +/- 61.09
Episode length: 118.44 +/- 124.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 34       |
| rollout/            |          |
|    exploration_rate | 0.382    |
| time/               |          |
|    total_timesteps  | 512000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.819    |
|    n_updates        | 125499   |
----------------------------------
Eval num_timesteps=512500, episode_reward=149.53 +/- 74.05
Episode length: 399.76 +/- 202.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 400      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.381    |
| time/               |          |
|    total_timesteps  | 512500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.697    |
|    n_updates        | 125624   |
----------------------------------
Eval num_timesteps=513000, episode_reward=183.41 +/- 35.47
Episode length: 468.52 +/- 152.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 469      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.38     |
| time/               |          |
|    total_timesteps  | 513000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.915    |
|    n_updates        | 125749   |
----------------------------------
Eval num_timesteps=513500, episode_reward=196.58 +/- 1.10
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.379    |
| time/               |          |
|    total_timesteps  | 513500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.39     |
|    n_updates        | 125874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 332      |
|    ep_rew_mean      | 133      |
|    exploration_rate | 0.379    |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 53       |
|    time_elapsed     | 9539     |
|    total_timesteps  | 513639   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.34     |
|    n_updates        | 125909   |
----------------------------------
Eval num_timesteps=514000, episode_reward=196.94 +/- 1.45
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.378    |
| time/               |          |
|    total_timesteps  | 514000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.897    |
|    n_updates        | 125999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 318      |
|    ep_rew_mean      | 131      |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 53       |
|    time_elapsed     | 9555     |
|    total_timesteps  | 514282   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 126070   |
----------------------------------
Eval num_timesteps=514500, episode_reward=179.09 +/- 40.19
Episode length: 451.68 +/- 168.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 452      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.377    |
| time/               |          |
|    total_timesteps  | 514500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.462    |
|    n_updates        | 126124   |
----------------------------------
Eval num_timesteps=515000, episode_reward=194.23 +/- 15.59
Episode length: 515.60 +/- 65.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.376    |
| time/               |          |
|    total_timesteps  | 515000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.31     |
|    n_updates        | 126249   |
----------------------------------
Eval num_timesteps=515500, episode_reward=146.27 +/- 52.45
Episode length: 318.76 +/- 233.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 319      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.375    |
| time/               |          |
|    total_timesteps  | 515500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.488    |
|    n_updates        | 126374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 314      |
|    ep_rew_mean      | 130      |
|    exploration_rate | 0.375    |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 53       |
|    time_elapsed     | 9595     |
|    total_timesteps  | 515523   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.765    |
|    n_updates        | 126380   |
----------------------------------
Eval num_timesteps=516000, episode_reward=143.99 +/- 54.40
Episode length: 301.44 +/- 233.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 301      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.374    |
| time/               |          |
|    total_timesteps  | 516000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.97     |
|    n_updates        | 126499   |
----------------------------------
Eval num_timesteps=516500, episode_reward=102.13 +/- 79.38
Episode length: 218.06 +/- 230.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 102      |
| rollout/            |          |
|    exploration_rate | 0.373    |
| time/               |          |
|    total_timesteps  | 516500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.739    |
|    n_updates        | 126624   |
----------------------------------
Eval num_timesteps=517000, episode_reward=159.96 +/- 50.81
Episode length: 375.24 +/- 218.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 375      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.372    |
| time/               |          |
|    total_timesteps  | 517000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 126749   |
----------------------------------
Eval num_timesteps=517500, episode_reward=146.23 +/- 54.37
Episode length: 311.10 +/- 232.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 311      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.371    |
| time/               |          |
|    total_timesteps  | 517500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.515    |
|    n_updates        | 126874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 324      |
|    ep_rew_mean      | 133      |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 53       |
|    time_elapsed     | 9631     |
|    total_timesteps  | 517623   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 126905   |
----------------------------------
Eval num_timesteps=518000, episode_reward=135.07 +/- 53.89
Episode length: 265.26 +/- 230.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 265      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.37     |
| time/               |          |
|    total_timesteps  | 518000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.565    |
|    n_updates        | 126999   |
----------------------------------
Eval num_timesteps=518500, episode_reward=117.10 +/- 52.77
Episode length: 222.96 +/- 206.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 117      |
| rollout/            |          |
|    exploration_rate | 0.369    |
| time/               |          |
|    total_timesteps  | 518500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.02     |
|    n_updates        | 127124   |
----------------------------------
Eval num_timesteps=519000, episode_reward=91.77 +/- 21.36
Episode length: 80.64 +/- 93.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.6     |
|    mean_reward      | 91.8     |
| rollout/            |          |
|    exploration_rate | 0.368    |
| time/               |          |
|    total_timesteps  | 519000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.631    |
|    n_updates        | 127249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 319      |
|    ep_rew_mean      | 131      |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 53       |
|    time_elapsed     | 9648     |
|    total_timesteps  | 519245   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.22     |
|    n_updates        | 127311   |
----------------------------------
Eval num_timesteps=519500, episode_reward=154.57 +/- 53.58
Episode length: 365.56 +/- 208.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 366      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.367    |
| time/               |          |
|    total_timesteps  | 519500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.961    |
|    n_updates        | 127374   |
----------------------------------
Eval num_timesteps=520000, episode_reward=184.69 +/- 29.56
Episode length: 486.20 +/- 131.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.366    |
| time/               |          |
|    total_timesteps  | 520000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.871    |
|    n_updates        | 127499   |
----------------------------------
Eval num_timesteps=520500, episode_reward=144.75 +/- 54.61
Episode length: 380.60 +/- 165.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 381      |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 0.365    |
| time/               |          |
|    total_timesteps  | 520500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 127624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 324      |
|    ep_rew_mean      | 132      |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 53       |
|    time_elapsed     | 9684     |
|    total_timesteps  | 520885   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.905    |
|    n_updates        | 127721   |
----------------------------------
Eval num_timesteps=521000, episode_reward=189.20 +/- 15.07
Episode length: 515.28 +/- 68.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.364    |
| time/               |          |
|    total_timesteps  | 521000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 127749   |
----------------------------------
Eval num_timesteps=521500, episode_reward=8.21 +/- 39.60
Episode length: 48.44 +/- 16.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | 8.21     |
| rollout/            |          |
|    exploration_rate | 0.363    |
| time/               |          |
|    total_timesteps  | 521500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.768    |
|    n_updates        | 127874   |
----------------------------------
Eval num_timesteps=522000, episode_reward=94.05 +/- 29.62
Episode length: 95.40 +/- 128.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.4     |
|    mean_reward      | 94       |
| rollout/            |          |
|    exploration_rate | 0.362    |
| time/               |          |
|    total_timesteps  | 522000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.12     |
|    n_updates        | 127999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 329      |
|    ep_rew_mean      | 132      |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 53       |
|    time_elapsed     | 9704     |
|    total_timesteps  | 522495   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 128123   |
----------------------------------
Eval num_timesteps=522500, episode_reward=61.44 +/- 80.04
Episode length: 167.44 +/- 194.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 167      |
|    mean_reward      | 61.4     |
| rollout/            |          |
|    exploration_rate | 0.361    |
| time/               |          |
|    total_timesteps  | 522500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.827    |
|    n_updates        | 128124   |
----------------------------------
Eval num_timesteps=523000, episode_reward=85.17 +/- 32.49
Episode length: 79.04 +/- 95.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79       |
|    mean_reward      | 85.2     |
| rollout/            |          |
|    exploration_rate | 0.36     |
| time/               |          |
|    total_timesteps  | 523000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 128249   |
----------------------------------
Eval num_timesteps=523500, episode_reward=111.13 +/- 81.04
Episode length: 270.84 +/- 221.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 271      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.359    |
| time/               |          |
|    total_timesteps  | 523500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 128374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 332      |
|    ep_rew_mean      | 134      |
|    exploration_rate | 0.359    |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 53       |
|    time_elapsed     | 9719     |
|    total_timesteps  | 523677   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.454    |
|    n_updates        | 128419   |
----------------------------------
Eval num_timesteps=524000, episode_reward=81.89 +/- 79.91
Episode length: 175.16 +/- 207.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 81.9     |
| rollout/            |          |
|    exploration_rate | 0.358    |
| time/               |          |
|    total_timesteps  | 524000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.02     |
|    n_updates        | 128499   |
----------------------------------
Eval num_timesteps=524500, episode_reward=152.81 +/- 53.69
Episode length: 335.64 +/- 232.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 336      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.357    |
| time/               |          |
|    total_timesteps  | 524500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.512    |
|    n_updates        | 128624   |
----------------------------------
Eval num_timesteps=525000, episode_reward=189.13 +/- 26.76
Episode length: 499.30 +/- 102.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 499      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.357    |
| time/               |          |
|    total_timesteps  | 525000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 128749   |
----------------------------------
Eval num_timesteps=525500, episode_reward=134.49 +/- 83.55
Episode length: 345.92 +/- 229.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 346      |
|    mean_reward      | 134      |
| rollout/            |          |
|    exploration_rate | 0.356    |
| time/               |          |
|    total_timesteps  | 525500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.824    |
|    n_updates        | 128874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 337      |
|    ep_rew_mean      | 134      |
|    exploration_rate | 0.355    |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 53       |
|    time_elapsed     | 9760     |
|    total_timesteps  | 525777   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.415    |
|    n_updates        | 128944   |
----------------------------------
Eval num_timesteps=526000, episode_reward=180.91 +/- 15.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.355    |
| time/               |          |
|    total_timesteps  | 526000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.47     |
|    n_updates        | 128999   |
----------------------------------
Eval num_timesteps=526500, episode_reward=175.65 +/- 42.93
Episode length: 448.72 +/- 166.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.354    |
| time/               |          |
|    total_timesteps  | 526500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.14     |
|    n_updates        | 129124   |
----------------------------------
Eval num_timesteps=527000, episode_reward=171.59 +/- 44.97
Episode length: 421.84 +/- 194.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 422      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.353    |
| time/               |          |
|    total_timesteps  | 527000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.313    |
|    n_updates        | 129249   |
----------------------------------
Eval num_timesteps=527500, episode_reward=123.66 +/- 75.44
Episode length: 332.48 +/- 211.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 332      |
|    mean_reward      | 124      |
| rollout/            |          |
|    exploration_rate | 0.352    |
| time/               |          |
|    total_timesteps  | 527500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.775    |
|    n_updates        | 129374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 342      |
|    ep_rew_mean      | 135      |
|    exploration_rate | 0.351    |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 53       |
|    time_elapsed     | 9810     |
|    total_timesteps  | 527877   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.82     |
|    n_updates        | 129469   |
----------------------------------
Eval num_timesteps=528000, episode_reward=171.25 +/- 43.76
Episode length: 419.70 +/- 198.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 420      |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.351    |
| time/               |          |
|    total_timesteps  | 528000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.21     |
|    n_updates        | 129499   |
----------------------------------
Eval num_timesteps=528500, episode_reward=190.07 +/- 25.41
Episode length: 509.98 +/- 74.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 510      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.35     |
| time/               |          |
|    total_timesteps  | 528500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 129624   |
----------------------------------
Eval num_timesteps=529000, episode_reward=149.65 +/- 55.48
Episode length: 392.06 +/- 182.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 392      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.349    |
| time/               |          |
|    total_timesteps  | 529000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.599    |
|    n_updates        | 129749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 342      |
|    ep_rew_mean      | 135      |
|    exploration_rate | 0.348    |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 53       |
|    time_elapsed     | 9848     |
|    total_timesteps  | 529132   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.2      |
|    n_updates        | 129782   |
----------------------------------
Eval num_timesteps=529500, episode_reward=112.86 +/- 52.95
Episode length: 218.48 +/- 220.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.348    |
| time/               |          |
|    total_timesteps  | 529500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 129874   |
----------------------------------
Eval num_timesteps=530000, episode_reward=100.67 +/- 45.88
Episode length: 140.42 +/- 169.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 101      |
| rollout/            |          |
|    exploration_rate | 0.347    |
| time/               |          |
|    total_timesteps  | 530000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 129999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 337      |
|    ep_rew_mean      | 133      |
|    exploration_rate | 0.346    |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 53       |
|    time_elapsed     | 9859     |
|    total_timesteps  | 530254   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.822    |
|    n_updates        | 130063   |
----------------------------------
Eval num_timesteps=530500, episode_reward=171.51 +/- 45.89
Episode length: 434.62 +/- 176.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 435      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.346    |
| time/               |          |
|    total_timesteps  | 530500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.25     |
|    n_updates        | 130124   |
----------------------------------
Eval num_timesteps=531000, episode_reward=143.99 +/- 58.07
Episode length: 309.94 +/- 234.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 310      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.345    |
| time/               |          |
|    total_timesteps  | 531000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.8      |
|    n_updates        | 130249   |
----------------------------------
Eval num_timesteps=531500, episode_reward=169.13 +/- 46.99
Episode length: 416.84 +/- 194.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 417      |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.344    |
| time/               |          |
|    total_timesteps  | 531500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.811    |
|    n_updates        | 130374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 342      |
|    ep_rew_mean      | 135      |
|    exploration_rate | 0.343    |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 53       |
|    time_elapsed     | 9893     |
|    total_timesteps  | 531910   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.739    |
|    n_updates        | 130477   |
----------------------------------
Eval num_timesteps=532000, episode_reward=145.96 +/- 54.49
Episode length: 314.92 +/- 229.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 315      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.343    |
| time/               |          |
|    total_timesteps  | 532000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.621    |
|    n_updates        | 130499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 329      |
|    ep_rew_mean      | 132      |
|    exploration_rate | 0.342    |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 53       |
|    time_elapsed     | 9902     |
|    total_timesteps  | 532178   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.75     |
|    n_updates        | 130544   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 325      |
|    ep_rew_mean      | 132      |
|    exploration_rate | 0.342    |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 53       |
|    time_elapsed     | 9902     |
|    total_timesteps  | 532415   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.62     |
|    n_updates        | 130603   |
----------------------------------
Eval num_timesteps=532500, episode_reward=110.17 +/- 45.21
Episode length: 162.40 +/- 193.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 162      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.342    |
| time/               |          |
|    total_timesteps  | 532500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.99     |
|    n_updates        | 130624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 319      |
|    ep_rew_mean      | 131      |
|    exploration_rate | 0.341    |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 53       |
|    time_elapsed     | 9907     |
|    total_timesteps  | 532637   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.589    |
|    n_updates        | 130659   |
----------------------------------
Eval num_timesteps=533000, episode_reward=123.01 +/- 51.91
Episode length: 220.12 +/- 219.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 123      |
| rollout/            |          |
|    exploration_rate | 0.341    |
| time/               |          |
|    total_timesteps  | 533000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.48     |
|    n_updates        | 130749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 309      |
|    ep_rew_mean      | 129      |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 53       |
|    time_elapsed     | 9914     |
|    total_timesteps  | 533305   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.63     |
|    n_updates        | 130826   |
----------------------------------
Eval num_timesteps=533500, episode_reward=111.17 +/- 48.07
Episode length: 182.80 +/- 204.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 183      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.34     |
| time/               |          |
|    total_timesteps  | 533500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.987    |
|    n_updates        | 130874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 295      |
|    ep_rew_mean      | 126      |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 53       |
|    time_elapsed     | 9919     |
|    total_timesteps  | 533550   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.83     |
|    n_updates        | 130887   |
----------------------------------
Eval num_timesteps=534000, episode_reward=145.20 +/- 53.32
Episode length: 345.80 +/- 214.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 346      |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 0.339    |
| time/               |          |
|    total_timesteps  | 534000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.04     |
|    n_updates        | 130999   |
----------------------------------
Eval num_timesteps=534500, episode_reward=18.05 +/- 45.36
Episode length: 51.34 +/- 16.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.3     |
|    mean_reward      | 18       |
| rollout/            |          |
|    exploration_rate | 0.338    |
| time/               |          |
|    total_timesteps  | 534500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.29     |
|    n_updates        | 131124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 295      |
|    ep_rew_mean      | 126      |
|    exploration_rate | 0.337    |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 53       |
|    time_elapsed     | 9932     |
|    total_timesteps  | 534721   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.635    |
|    n_updates        | 131180   |
----------------------------------
Eval num_timesteps=535000, episode_reward=140.22 +/- 55.50
Episode length: 316.92 +/- 222.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 317      |
|    mean_reward      | 140      |
| rollout/            |          |
|    exploration_rate | 0.337    |
| time/               |          |
|    total_timesteps  | 535000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.529    |
|    n_updates        | 131249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 295      |
|    ep_rew_mean      | 127      |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 53       |
|    time_elapsed     | 9941     |
|    total_timesteps  | 535446   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.55     |
|    n_updates        | 131361   |
----------------------------------
Eval num_timesteps=535500, episode_reward=127.25 +/- 69.14
Episode length: 269.58 +/- 236.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 270      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.336    |
| time/               |          |
|    total_timesteps  | 535500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.48     |
|    n_updates        | 131374   |
----------------------------------
Eval num_timesteps=536000, episode_reward=102.44 +/- 59.60
Episode length: 170.18 +/- 200.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 102      |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 536000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.506    |
|    n_updates        | 131499   |
----------------------------------
Eval num_timesteps=536500, episode_reward=85.45 +/- 67.20
Episode length: 189.82 +/- 179.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 85.4     |
| rollout/            |          |
|    exploration_rate | 0.334    |
| time/               |          |
|    total_timesteps  | 536500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 131624   |
----------------------------------
Eval num_timesteps=537000, episode_reward=58.97 +/- 75.00
Episode length: 134.88 +/- 172.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 59       |
| rollout/            |          |
|    exploration_rate | 0.333    |
| time/               |          |
|    total_timesteps  | 537000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.741    |
|    n_updates        | 131749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 310      |
|    ep_rew_mean      | 130      |
|    exploration_rate | 0.332    |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 53       |
|    time_elapsed     | 9964     |
|    total_timesteps  | 537128   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.776    |
|    n_updates        | 131781   |
----------------------------------
Eval num_timesteps=537500, episode_reward=150.26 +/- 64.46
Episode length: 362.64 +/- 220.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 363      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.332    |
| time/               |          |
|    total_timesteps  | 537500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.992    |
|    n_updates        | 131874   |
----------------------------------
Eval num_timesteps=538000, episode_reward=115.17 +/- 49.95
Episode length: 207.92 +/- 218.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 538000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.15     |
|    n_updates        | 131999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 305      |
|    ep_rew_mean      | 129      |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 53       |
|    time_elapsed     | 9981     |
|    total_timesteps  | 538298   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.744    |
|    n_updates        | 132074   |
----------------------------------
Eval num_timesteps=538500, episode_reward=154.44 +/- 60.45
Episode length: 386.18 +/- 200.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 386      |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.33     |
| time/               |          |
|    total_timesteps  | 538500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.84     |
|    n_updates        | 132124   |
----------------------------------
Eval num_timesteps=539000, episode_reward=75.33 +/- 80.25
Episode length: 169.82 +/- 200.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 75.3     |
| rollout/            |          |
|    exploration_rate | 0.329    |
| time/               |          |
|    total_timesteps  | 539000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.5      |
|    n_updates        | 132249   |
----------------------------------
Eval num_timesteps=539500, episode_reward=190.81 +/- 21.71
Episode length: 505.48 +/- 95.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.328    |
| time/               |          |
|    total_timesteps  | 539500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.975    |
|    n_updates        | 132374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 314      |
|    ep_rew_mean      | 131      |
|    exploration_rate | 0.327    |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 53       |
|    time_elapsed     | 10012    |
|    total_timesteps  | 539930   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.11     |
|    n_updates        | 132482   |
----------------------------------
Eval num_timesteps=540000, episode_reward=179.29 +/- 43.10
Episode length: 448.26 +/- 175.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.327    |
| time/               |          |
|    total_timesteps  | 540000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.591    |
|    n_updates        | 132499   |
----------------------------------
Eval num_timesteps=540500, episode_reward=69.65 +/- 90.30
Episode length: 196.08 +/- 217.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 196      |
|    mean_reward      | 69.6     |
| rollout/            |          |
|    exploration_rate | 0.326    |
| time/               |          |
|    total_timesteps  | 540500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.884    |
|    n_updates        | 132624   |
----------------------------------
Eval num_timesteps=541000, episode_reward=181.67 +/- 42.15
Episode length: 485.30 +/- 125.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 485      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.325    |
| time/               |          |
|    total_timesteps  | 541000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.792    |
|    n_updates        | 132749   |
----------------------------------
Eval num_timesteps=541500, episode_reward=142.79 +/- 58.82
Episode length: 343.94 +/- 197.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 344      |
|    mean_reward      | 143      |
| rollout/            |          |
|    exploration_rate | 0.324    |
| time/               |          |
|    total_timesteps  | 541500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.888    |
|    n_updates        | 132874   |
----------------------------------
Eval num_timesteps=542000, episode_reward=169.68 +/- 45.92
Episode length: 420.94 +/- 196.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 421      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.323    |
| time/               |          |
|    total_timesteps  | 542000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.5      |
|    n_updates        | 132999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 318      |
|    ep_rew_mean      | 132      |
|    exploration_rate | 0.323    |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 53       |
|    time_elapsed     | 10067    |
|    total_timesteps  | 542030   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.76     |
|    n_updates        | 133007   |
----------------------------------
Eval num_timesteps=542500, episode_reward=136.57 +/- 60.57
Episode length: 290.94 +/- 234.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 291      |
|    mean_reward      | 137      |
| rollout/            |          |
|    exploration_rate | 0.322    |
| time/               |          |
|    total_timesteps  | 542500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.996    |
|    n_updates        | 133124   |
----------------------------------
Eval num_timesteps=543000, episode_reward=66.87 +/- 71.98
Episode length: 132.22 +/- 173.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 66.9     |
| rollout/            |          |
|    exploration_rate | 0.321    |
| time/               |          |
|    total_timesteps  | 543000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.923    |
|    n_updates        | 133249   |
----------------------------------
Eval num_timesteps=543500, episode_reward=138.65 +/- 57.48
Episode length: 378.46 +/- 158.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 378      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.32     |
| time/               |          |
|    total_timesteps  | 543500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.11     |
|    n_updates        | 133374   |
----------------------------------
Eval num_timesteps=544000, episode_reward=102.68 +/- 46.90
Episode length: 243.36 +/- 165.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 243      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.319    |
| time/               |          |
|    total_timesteps  | 544000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.723    |
|    n_updates        | 133499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 326      |
|    ep_rew_mean      | 134      |
|    exploration_rate | 0.319    |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 53       |
|    time_elapsed     | 10099    |
|    total_timesteps  | 544130   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.43     |
|    n_updates        | 133532   |
----------------------------------
Eval num_timesteps=544500, episode_reward=86.31 +/- 52.96
Episode length: 145.58 +/- 155.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 86.3     |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 544500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.73     |
|    n_updates        | 133624   |
----------------------------------
Eval num_timesteps=545000, episode_reward=164.95 +/- 57.46
Episode length: 413.16 +/- 199.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 413      |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.317    |
| time/               |          |
|    total_timesteps  | 545000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.569    |
|    n_updates        | 133749   |
----------------------------------
Eval num_timesteps=545500, episode_reward=165.09 +/- 54.08
Episode length: 436.84 +/- 162.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 437      |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.316    |
| time/               |          |
|    total_timesteps  | 545500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.52     |
|    n_updates        | 133874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 323      |
|    ep_rew_mean      | 133      |
|    exploration_rate | 0.315    |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 53       |
|    time_elapsed     | 10128    |
|    total_timesteps  | 545919   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 133979   |
----------------------------------
Eval num_timesteps=546000, episode_reward=187.75 +/- 34.13
Episode length: 494.76 +/- 103.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.315    |
| time/               |          |
|    total_timesteps  | 546000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.26     |
|    n_updates        | 133999   |
----------------------------------
Eval num_timesteps=546500, episode_reward=109.56 +/- 74.43
Episode length: 240.10 +/- 226.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 240      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.314    |
| time/               |          |
|    total_timesteps  | 546500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.511    |
|    n_updates        | 134124   |
----------------------------------
Eval num_timesteps=547000, episode_reward=160.90 +/- 50.85
Episode length: 395.86 +/- 208.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 396      |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.313    |
| time/               |          |
|    total_timesteps  | 547000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.692    |
|    n_updates        | 134249   |
----------------------------------
Eval num_timesteps=547500, episode_reward=185.99 +/- 29.83
Episode length: 476.16 +/- 146.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.312    |
| time/               |          |
|    total_timesteps  | 547500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.785    |
|    n_updates        | 134374   |
----------------------------------
Eval num_timesteps=548000, episode_reward=187.81 +/- 16.45
Episode length: 515.80 +/- 64.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.311    |
| time/               |          |
|    total_timesteps  | 548000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.03     |
|    n_updates        | 134499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 337      |
|    ep_rew_mean      | 136      |
|    exploration_rate | 0.311    |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 53       |
|    time_elapsed     | 10190    |
|    total_timesteps  | 548019   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.747    |
|    n_updates        | 134504   |
----------------------------------
Eval num_timesteps=548500, episode_reward=178.66 +/- 34.43
Episode length: 470.08 +/- 149.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 470      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.31     |
| time/               |          |
|    total_timesteps  | 548500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.06     |
|    n_updates        | 134624   |
----------------------------------
Eval num_timesteps=549000, episode_reward=76.49 +/- 60.13
Episode length: 111.24 +/- 154.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 76.5     |
| rollout/            |          |
|    exploration_rate | 0.309    |
| time/               |          |
|    total_timesteps  | 549000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.804    |
|    n_updates        | 134749   |
----------------------------------
Eval num_timesteps=549500, episode_reward=128.03 +/- 55.15
Episode length: 326.94 +/- 173.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 327      |
|    mean_reward      | 128      |
| rollout/            |          |
|    exploration_rate | 0.308    |
| time/               |          |
|    total_timesteps  | 549500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.596    |
|    n_updates        | 134874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 341      |
|    ep_rew_mean      | 137      |
|    exploration_rate | 0.308    |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 53       |
|    time_elapsed     | 10217    |
|    total_timesteps  | 549645   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 134911   |
----------------------------------
Eval num_timesteps=550000, episode_reward=190.05 +/- 26.31
Episode length: 495.64 +/- 116.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.307    |
| time/               |          |
|    total_timesteps  | 550000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.692    |
|    n_updates        | 134999   |
----------------------------------
Eval num_timesteps=550500, episode_reward=179.94 +/- 38.16
Episode length: 464.60 +/- 152.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 465      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.306    |
| time/               |          |
|    total_timesteps  | 550500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.06     |
|    n_updates        | 135124   |
----------------------------------
Eval num_timesteps=551000, episode_reward=175.02 +/- 44.87
Episode length: 450.76 +/- 163.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 451      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.305    |
| time/               |          |
|    total_timesteps  | 551000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.08     |
|    n_updates        | 135249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 336      |
|    ep_rew_mean      | 136      |
|    exploration_rate | 0.305    |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 53       |
|    time_elapsed     | 10258    |
|    total_timesteps  | 551242   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.26     |
|    n_updates        | 135310   |
----------------------------------
Eval num_timesteps=551500, episode_reward=128.90 +/- 51.34
Episode length: 245.52 +/- 228.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 129      |
| rollout/            |          |
|    exploration_rate | 0.304    |
| time/               |          |
|    total_timesteps  | 551500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.714    |
|    n_updates        | 135374   |
----------------------------------
Eval num_timesteps=552000, episode_reward=134.62 +/- 57.55
Episode length: 300.34 +/- 215.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 300      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.303    |
| time/               |          |
|    total_timesteps  | 552000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.894    |
|    n_updates        | 135499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 332      |
|    ep_rew_mean      | 135      |
|    exploration_rate | 0.302    |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 53       |
|    time_elapsed     | 10274    |
|    total_timesteps  | 552436   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.868    |
|    n_updates        | 135608   |
----------------------------------
Eval num_timesteps=552500, episode_reward=171.18 +/- 43.81
Episode length: 429.36 +/- 191.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.302    |
| time/               |          |
|    total_timesteps  | 552500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.24     |
|    n_updates        | 135624   |
----------------------------------
Eval num_timesteps=553000, episode_reward=145.71 +/- 63.89
Episode length: 337.82 +/- 229.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 338      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.301    |
| time/               |          |
|    total_timesteps  | 553000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.939    |
|    n_updates        | 135749   |
----------------------------------
Eval num_timesteps=553500, episode_reward=152.97 +/- 55.74
Episode length: 428.96 +/- 162.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.3      |
| time/               |          |
|    total_timesteps  | 553500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.969    |
|    n_updates        | 135874   |
----------------------------------
Eval num_timesteps=554000, episode_reward=197.24 +/- 1.22
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.299    |
| time/               |          |
|    total_timesteps  | 554000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.65     |
|    n_updates        | 135999   |
----------------------------------
Eval num_timesteps=554500, episode_reward=134.97 +/- 58.67
Episode length: 282.44 +/- 225.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 282      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.298    |
| time/               |          |
|    total_timesteps  | 554500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.567    |
|    n_updates        | 136124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 337      |
|    ep_rew_mean      | 136      |
|    exploration_rate | 0.298    |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 53       |
|    time_elapsed     | 10332    |
|    total_timesteps  | 554536   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.771    |
|    n_updates        | 136133   |
----------------------------------
Eval num_timesteps=555000, episode_reward=163.35 +/- 67.16
Episode length: 429.70 +/- 190.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 430      |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.297    |
| time/               |          |
|    total_timesteps  | 555000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.14     |
|    n_updates        | 136249   |
----------------------------------
Eval num_timesteps=555500, episode_reward=133.41 +/- 83.35
Episode length: 347.98 +/- 227.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 348      |
|    mean_reward      | 133      |
| rollout/            |          |
|    exploration_rate | 0.296    |
| time/               |          |
|    total_timesteps  | 555500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.425    |
|    n_updates        | 136374   |
----------------------------------
Eval num_timesteps=556000, episode_reward=174.86 +/- 34.72
Episode length: 481.90 +/- 130.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 482      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.295    |
| time/               |          |
|    total_timesteps  | 556000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.621    |
|    n_updates        | 136499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 337      |
|    ep_rew_mean      | 136      |
|    exploration_rate | 0.295    |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 53       |
|    time_elapsed     | 10369    |
|    total_timesteps  | 556153   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.41     |
|    n_updates        | 136538   |
----------------------------------
Eval num_timesteps=556500, episode_reward=149.65 +/- 81.24
Episode length: 399.58 +/- 211.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 400      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.294    |
| time/               |          |
|    total_timesteps  | 556500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.21     |
|    n_updates        | 136624   |
----------------------------------
Eval num_timesteps=557000, episode_reward=178.95 +/- 49.03
Episode length: 470.06 +/- 149.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 470      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.293    |
| time/               |          |
|    total_timesteps  | 557000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.794    |
|    n_updates        | 136749   |
----------------------------------
Eval num_timesteps=557500, episode_reward=194.69 +/- 15.68
Episode length: 515.60 +/- 65.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.292    |
| time/               |          |
|    total_timesteps  | 557500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.756    |
|    n_updates        | 136874   |
----------------------------------
Eval num_timesteps=558000, episode_reward=184.62 +/- 32.65
Episode length: 479.34 +/- 137.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.291    |
| time/               |          |
|    total_timesteps  | 558000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.51     |
|    n_updates        | 136999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 346      |
|    ep_rew_mean      | 137      |
|    exploration_rate | 0.291    |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 53       |
|    time_elapsed     | 10423    |
|    total_timesteps  | 558253   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.727    |
|    n_updates        | 137063   |
----------------------------------
Eval num_timesteps=558500, episode_reward=176.51 +/- 42.03
Episode length: 440.18 +/- 181.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.29     |
| time/               |          |
|    total_timesteps  | 558500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.665    |
|    n_updates        | 137124   |
----------------------------------
Eval num_timesteps=559000, episode_reward=90.33 +/- 80.54
Episode length: 188.26 +/- 220.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 188      |
|    mean_reward      | 90.3     |
| rollout/            |          |
|    exploration_rate | 0.289    |
| time/               |          |
|    total_timesteps  | 559000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.621    |
|    n_updates        | 137249   |
----------------------------------
Eval num_timesteps=559500, episode_reward=155.80 +/- 66.96
Episode length: 381.54 +/- 219.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 382      |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.288    |
| time/               |          |
|    total_timesteps  | 559500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.09     |
|    n_updates        | 137374   |
----------------------------------
Eval num_timesteps=560000, episode_reward=92.15 +/- 25.62
Episode length: 88.78 +/- 112.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.8     |
|    mean_reward      | 92.1     |
| rollout/            |          |
|    exploration_rate | 0.287    |
| time/               |          |
|    total_timesteps  | 560000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.509    |
|    n_updates        | 137499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 346      |
|    ep_rew_mean      | 137      |
|    exploration_rate | 0.287    |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 53       |
|    time_elapsed     | 10456    |
|    total_timesteps  | 560353   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.439    |
|    n_updates        | 137588   |
----------------------------------
Eval num_timesteps=560500, episode_reward=153.64 +/- 67.06
Episode length: 378.44 +/- 214.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 378      |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.286    |
| time/               |          |
|    total_timesteps  | 560500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.342    |
|    n_updates        | 137624   |
----------------------------------
Eval num_timesteps=561000, episode_reward=81.71 +/- 77.22
Episode length: 181.76 +/- 198.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 182      |
|    mean_reward      | 81.7     |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 561000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.22     |
|    n_updates        | 137749   |
----------------------------------
Eval num_timesteps=561500, episode_reward=104.53 +/- 80.90
Episode length: 233.52 +/- 230.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.284    |
| time/               |          |
|    total_timesteps  | 561500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.23     |
|    n_updates        | 137874   |
----------------------------------
Eval num_timesteps=562000, episode_reward=144.49 +/- 73.78
Episode length: 365.86 +/- 216.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 366      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.283    |
| time/               |          |
|    total_timesteps  | 562000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.484    |
|    n_updates        | 137999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 346      |
|    ep_rew_mean      | 137      |
|    exploration_rate | 0.282    |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 53       |
|    time_elapsed     | 10490    |
|    total_timesteps  | 562453   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.853    |
|    n_updates        | 138113   |
----------------------------------
Eval num_timesteps=562500, episode_reward=170.87 +/- 49.78
Episode length: 409.90 +/- 204.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 410      |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.282    |
| time/               |          |
|    total_timesteps  | 562500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.32     |
|    n_updates        | 138124   |
----------------------------------
Eval num_timesteps=563000, episode_reward=190.64 +/- 16.42
Episode length: 515.30 +/- 67.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.281    |
| time/               |          |
|    total_timesteps  | 563000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.61     |
|    n_updates        | 138249   |
----------------------------------
Eval num_timesteps=563500, episode_reward=91.03 +/- 41.99
Episode length: 106.74 +/- 141.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 91       |
| rollout/            |          |
|    exploration_rate | 0.28     |
| time/               |          |
|    total_timesteps  | 563500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.452    |
|    n_updates        | 138374   |
----------------------------------
Eval num_timesteps=564000, episode_reward=134.60 +/- 67.88
Episode length: 293.38 +/- 241.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 293      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.279    |
| time/               |          |
|    total_timesteps  | 564000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.562    |
|    n_updates        | 138499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 350      |
|    ep_rew_mean      | 138      |
|    exploration_rate | 0.279    |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 53       |
|    time_elapsed     | 10530    |
|    total_timesteps  | 564082   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.04     |
|    n_updates        | 138520   |
----------------------------------
Eval num_timesteps=564500, episode_reward=125.07 +/- 82.02
Episode length: 306.38 +/- 237.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 306      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.278    |
| time/               |          |
|    total_timesteps  | 564500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.612    |
|    n_updates        | 138624   |
----------------------------------
Eval num_timesteps=565000, episode_reward=185.15 +/- 14.22
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.277    |
| time/               |          |
|    total_timesteps  | 565000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 138749   |
----------------------------------
Eval num_timesteps=565500, episode_reward=195.78 +/- 3.02
Episode length: 515.58 +/- 65.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.276    |
| time/               |          |
|    total_timesteps  | 565500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.466    |
|    n_updates        | 138874   |
----------------------------------
Eval num_timesteps=566000, episode_reward=181.06 +/- 47.15
Episode length: 476.08 +/- 146.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.275    |
| time/               |          |
|    total_timesteps  | 566000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.22     |
|    n_updates        | 138999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 359      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 53       |
|    time_elapsed     | 10582    |
|    total_timesteps  | 566182   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.334    |
|    n_updates        | 139045   |
----------------------------------
Eval num_timesteps=566500, episode_reward=194.54 +/- 2.43
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.274    |
| time/               |          |
|    total_timesteps  | 566500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.861    |
|    n_updates        | 139124   |
----------------------------------
Eval num_timesteps=567000, episode_reward=182.52 +/- 39.31
Episode length: 466.92 +/- 157.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 467      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.273    |
| time/               |          |
|    total_timesteps  | 567000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.673    |
|    n_updates        | 139249   |
----------------------------------
Eval num_timesteps=567500, episode_reward=138.45 +/- 78.85
Episode length: 345.24 +/- 229.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 345      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 0.272    |
| time/               |          |
|    total_timesteps  | 567500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.465    |
|    n_updates        | 139374   |
----------------------------------
Eval num_timesteps=568000, episode_reward=166.44 +/- 51.63
Episode length: 430.64 +/- 171.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.271    |
| time/               |          |
|    total_timesteps  | 568000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.935    |
|    n_updates        | 139499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 364      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.271    |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 53       |
|    time_elapsed     | 10634    |
|    total_timesteps  | 568282   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.2      |
|    n_updates        | 139570   |
----------------------------------
Eval num_timesteps=568500, episode_reward=184.49 +/- 27.13
Episode length: 503.72 +/- 84.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 504      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.27     |
| time/               |          |
|    total_timesteps  | 568500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.668    |
|    n_updates        | 139624   |
----------------------------------
Eval num_timesteps=569000, episode_reward=194.70 +/- 15.66
Episode length: 515.18 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.269    |
| time/               |          |
|    total_timesteps  | 569000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.403    |
|    n_updates        | 139749   |
----------------------------------
Eval num_timesteps=569500, episode_reward=181.91 +/- 36.80
Episode length: 469.92 +/- 150.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 470      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.268    |
| time/               |          |
|    total_timesteps  | 569500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.72     |
|    n_updates        | 139874   |
----------------------------------
Eval num_timesteps=570000, episode_reward=178.79 +/- 40.63
Episode length: 449.22 +/- 173.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.267    |
| time/               |          |
|    total_timesteps  | 570000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.417    |
|    n_updates        | 139999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 382      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.267    |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 53       |
|    time_elapsed     | 10690    |
|    total_timesteps  | 570382   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.208    |
|    n_updates        | 140095   |
----------------------------------
Eval num_timesteps=570500, episode_reward=165.06 +/- 62.78
Episode length: 423.48 +/- 193.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 423      |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.266    |
| time/               |          |
|    total_timesteps  | 570500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.35     |
|    n_updates        | 140124   |
----------------------------------
Eval num_timesteps=571000, episode_reward=146.16 +/- 61.47
Episode length: 362.40 +/- 195.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 362      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.265    |
| time/               |          |
|    total_timesteps  | 571000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.19     |
|    n_updates        | 140249   |
----------------------------------
Eval num_timesteps=571500, episode_reward=159.14 +/- 51.66
Episode length: 408.30 +/- 164.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 408      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.264    |
| time/               |          |
|    total_timesteps  | 571500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.5      |
|    n_updates        | 140374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 396      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.263    |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 53       |
|    time_elapsed     | 10725    |
|    total_timesteps  | 571978   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.684    |
|    n_updates        | 140494   |
----------------------------------
Eval num_timesteps=572000, episode_reward=187.78 +/- 29.62
Episode length: 495.02 +/- 102.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.263    |
| time/               |          |
|    total_timesteps  | 572000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.41     |
|    n_updates        | 140499   |
----------------------------------
Eval num_timesteps=572500, episode_reward=172.39 +/- 44.73
Episode length: 430.40 +/- 189.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 430      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.262    |
| time/               |          |
|    total_timesteps  | 572500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.23     |
|    n_updates        | 140624   |
----------------------------------
Eval num_timesteps=573000, episode_reward=189.81 +/- 25.88
Episode length: 496.24 +/- 113.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.261    |
| time/               |          |
|    total_timesteps  | 573000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.497    |
|    n_updates        | 140749   |
----------------------------------
Eval num_timesteps=573500, episode_reward=115.55 +/- 82.41
Episode length: 267.02 +/- 238.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 267      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.26     |
| time/               |          |
|    total_timesteps  | 573500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 140874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 410      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.26     |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 53       |
|    time_elapsed     | 10775    |
|    total_timesteps  | 573613   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.201    |
|    n_updates        | 140903   |
----------------------------------
Eval num_timesteps=574000, episode_reward=154.92 +/- 53.44
Episode length: 361.44 +/- 214.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 361      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.259    |
| time/               |          |
|    total_timesteps  | 574000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.656    |
|    n_updates        | 140999   |
----------------------------------
Eval num_timesteps=574500, episode_reward=16.25 +/- 49.34
Episode length: 60.88 +/- 68.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.9     |
|    mean_reward      | 16.2     |
| rollout/            |          |
|    exploration_rate | 0.258    |
| time/               |          |
|    total_timesteps  | 574500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.632    |
|    n_updates        | 141124   |
----------------------------------
Eval num_timesteps=575000, episode_reward=185.46 +/- 32.69
Episode length: 475.78 +/- 147.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.258    |
| time/               |          |
|    total_timesteps  | 575000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.15     |
|    n_updates        | 141249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 419      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.257    |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 53       |
|    time_elapsed     | 10802    |
|    total_timesteps  | 575233   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.41     |
|    n_updates        | 141308   |
----------------------------------
Eval num_timesteps=575500, episode_reward=187.84 +/- 29.69
Episode length: 485.68 +/- 133.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.257    |
| time/               |          |
|    total_timesteps  | 575500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.242    |
|    n_updates        | 141374   |
----------------------------------
Eval num_timesteps=576000, episode_reward=123.42 +/- 73.66
Episode length: 274.10 +/- 232.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 274      |
|    mean_reward      | 123      |
| rollout/            |          |
|    exploration_rate | 0.256    |
| time/               |          |
|    total_timesteps  | 576000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.422    |
|    n_updates        | 141499   |
----------------------------------
Eval num_timesteps=576500, episode_reward=181.23 +/- 37.92
Episode length: 458.32 +/- 165.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 458      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.255    |
| time/               |          |
|    total_timesteps  | 576500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.17     |
|    n_updates        | 141624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 433      |
|    ep_rew_mean      | 154      |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 53       |
|    time_elapsed     | 10838    |
|    total_timesteps  | 576833   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.423    |
|    n_updates        | 141708   |
----------------------------------
Eval num_timesteps=577000, episode_reward=55.40 +/- 81.23
Episode length: 142.26 +/- 191.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 55.4     |
| rollout/            |          |
|    exploration_rate | 0.254    |
| time/               |          |
|    total_timesteps  | 577000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.258    |
|    n_updates        | 141749   |
----------------------------------
Eval num_timesteps=577500, episode_reward=174.47 +/- 43.63
Episode length: 431.46 +/- 187.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.253    |
| time/               |          |
|    total_timesteps  | 577500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.693    |
|    n_updates        | 141874   |
----------------------------------
Eval num_timesteps=578000, episode_reward=196.70 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.252    |
| time/               |          |
|    total_timesteps  | 578000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.809    |
|    n_updates        | 141999   |
----------------------------------
Eval num_timesteps=578500, episode_reward=126.64 +/- 52.40
Episode length: 226.58 +/- 224.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 578500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.509    |
|    n_updates        | 142124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 442      |
|    ep_rew_mean      | 156      |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 53       |
|    time_elapsed     | 10878    |
|    total_timesteps  | 578933   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.423    |
|    n_updates        | 142233   |
----------------------------------
Eval num_timesteps=579000, episode_reward=187.52 +/- 29.53
Episode length: 485.60 +/- 133.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.25     |
| time/               |          |
|    total_timesteps  | 579000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.951    |
|    n_updates        | 142249   |
----------------------------------
Eval num_timesteps=579500, episode_reward=191.36 +/- 21.57
Episode length: 506.50 +/- 90.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.249    |
| time/               |          |
|    total_timesteps  | 579500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.02     |
|    n_updates        | 142374   |
----------------------------------
Eval num_timesteps=580000, episode_reward=113.45 +/- 46.44
Episode length: 165.14 +/- 202.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.248    |
| time/               |          |
|    total_timesteps  | 580000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.241    |
|    n_updates        | 142499   |
----------------------------------
Eval num_timesteps=580500, episode_reward=174.97 +/- 43.78
Episode length: 430.60 +/- 188.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.247    |
| time/               |          |
|    total_timesteps  | 580500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.334    |
|    n_updates        | 142624   |
----------------------------------
Eval num_timesteps=581000, episode_reward=69.19 +/- 90.72
Episode length: 202.94 +/- 221.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 69.2     |
| rollout/            |          |
|    exploration_rate | 0.246    |
| time/               |          |
|    total_timesteps  | 581000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.505    |
|    n_updates        | 142749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 456      |
|    ep_rew_mean      | 157      |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 53       |
|    time_elapsed     | 10930    |
|    total_timesteps  | 581033   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.29     |
|    n_updates        | 142758   |
----------------------------------
Eval num_timesteps=581500, episode_reward=196.71 +/- 1.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.245    |
| time/               |          |
|    total_timesteps  | 581500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.443    |
|    n_updates        | 142874   |
----------------------------------
Eval num_timesteps=582000, episode_reward=18.41 +/- 54.50
Episode length: 72.42 +/- 93.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.4     |
|    mean_reward      | 18.4     |
| rollout/            |          |
|    exploration_rate | 0.244    |
| time/               |          |
|    total_timesteps  | 582000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.334    |
|    n_updates        | 142999   |
----------------------------------
Eval num_timesteps=582500, episode_reward=188.79 +/- 25.73
Episode length: 495.78 +/- 115.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.243    |
| time/               |          |
|    total_timesteps  | 582500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.274    |
|    n_updates        | 143124   |
----------------------------------
Eval num_timesteps=583000, episode_reward=194.73 +/- 15.79
Episode length: 515.04 +/- 69.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.242    |
| time/               |          |
|    total_timesteps  | 583000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.258    |
|    n_updates        | 143249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 460      |
|    ep_rew_mean      | 158      |
|    exploration_rate | 0.241    |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 53       |
|    time_elapsed     | 10977    |
|    total_timesteps  | 583133   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.133    |
|    n_updates        | 143283   |
----------------------------------
Eval num_timesteps=583500, episode_reward=46.37 +/- 64.18
Episode length: 90.40 +/- 128.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.4     |
|    mean_reward      | 46.4     |
| rollout/            |          |
|    exploration_rate | 0.241    |
| time/               |          |
|    total_timesteps  | 583500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.374    |
|    n_updates        | 143374   |
----------------------------------
Eval num_timesteps=584000, episode_reward=29.19 +/- 74.39
Episode length: 114.98 +/- 166.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 29.2     |
| rollout/            |          |
|    exploration_rate | 0.24     |
| time/               |          |
|    total_timesteps  | 584000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.373    |
|    n_updates        | 143499   |
----------------------------------
Eval num_timesteps=584500, episode_reward=161.70 +/- 55.79
Episode length: 389.32 +/- 217.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 389      |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.239    |
| time/               |          |
|    total_timesteps  | 584500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.478    |
|    n_updates        | 143624   |
----------------------------------
Eval num_timesteps=585000, episode_reward=125.57 +/- 51.26
Episode length: 221.74 +/- 227.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.238    |
| time/               |          |
|    total_timesteps  | 585000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.614    |
|    n_updates        | 143749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.237    |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 53       |
|    time_elapsed     | 11002    |
|    total_timesteps  | 585233   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.194    |
|    n_updates        | 143808   |
----------------------------------
Eval num_timesteps=585500, episode_reward=180.02 +/- 34.18
Episode length: 495.60 +/- 116.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.237    |
| time/               |          |
|    total_timesteps  | 585500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.25     |
|    n_updates        | 143874   |
----------------------------------
Eval num_timesteps=586000, episode_reward=197.23 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.236    |
| time/               |          |
|    total_timesteps  | 586000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.145    |
|    n_updates        | 143999   |
----------------------------------
Eval num_timesteps=586500, episode_reward=185.36 +/- 13.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.235    |
| time/               |          |
|    total_timesteps  | 586500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.227    |
|    n_updates        | 144124   |
----------------------------------
Eval num_timesteps=587000, episode_reward=190.68 +/- 21.16
Episode length: 505.58 +/- 95.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.234    |
| time/               |          |
|    total_timesteps  | 587000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0603   |
|    n_updates        | 144249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.233    |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 53       |
|    time_elapsed     | 11061    |
|    total_timesteps  | 587333   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.303    |
|    n_updates        | 144333   |
----------------------------------
Eval num_timesteps=587500, episode_reward=84.69 +/- 24.82
Episode length: 73.64 +/- 70.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | 84.7     |
| rollout/            |          |
|    exploration_rate | 0.233    |
| time/               |          |
|    total_timesteps  | 587500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.308    |
|    n_updates        | 144374   |
----------------------------------
Eval num_timesteps=588000, episode_reward=106.97 +/- 44.40
Episode length: 140.48 +/- 181.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.232    |
| time/               |          |
|    total_timesteps  | 588000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.213    |
|    n_updates        | 144499   |
----------------------------------
Eval num_timesteps=588500, episode_reward=111.73 +/- 46.72
Episode length: 170.32 +/- 200.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 112      |
| rollout/            |          |
|    exploration_rate | 0.231    |
| time/               |          |
|    total_timesteps  | 588500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.313    |
|    n_updates        | 144624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 53       |
|    time_elapsed     | 11074    |
|    total_timesteps  | 588939   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.127    |
|    n_updates        | 144734   |
----------------------------------
Eval num_timesteps=589000, episode_reward=39.21 +/- 74.97
Episode length: 113.72 +/- 166.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 39.2     |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 589000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.284    |
|    n_updates        | 144749   |
----------------------------------
Eval num_timesteps=589500, episode_reward=117.87 +/- 60.41
Episode length: 214.56 +/- 223.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 118      |
| rollout/            |          |
|    exploration_rate | 0.229    |
| time/               |          |
|    total_timesteps  | 589500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.228    |
|    n_updates        | 144874   |
----------------------------------
Eval num_timesteps=590000, episode_reward=93.00 +/- 93.72
Episode length: 249.04 +/- 235.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 249      |
|    mean_reward      | 93       |
| rollout/            |          |
|    exploration_rate | 0.228    |
| time/               |          |
|    total_timesteps  | 590000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.744    |
|    n_updates        | 144999   |
----------------------------------
Eval num_timesteps=590500, episode_reward=187.76 +/- 20.95
Episode length: 505.90 +/- 93.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.227    |
| time/               |          |
|    total_timesteps  | 590500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.335    |
|    n_updates        | 145124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 464      |
|    ep_rew_mean      | 159      |
|    exploration_rate | 0.227    |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 53       |
|    time_elapsed     | 11105    |
|    total_timesteps  | 590543   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.278    |
|    n_updates        | 145135   |
----------------------------------
Eval num_timesteps=591000, episode_reward=167.11 +/- 59.68
Episode length: 437.82 +/- 165.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 438      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.226    |
| time/               |          |
|    total_timesteps  | 591000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.514    |
|    n_updates        | 145249   |
----------------------------------
Eval num_timesteps=591500, episode_reward=147.43 +/- 52.12
Episode length: 330.38 +/- 229.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 330      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.225    |
| time/               |          |
|    total_timesteps  | 591500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.254    |
|    n_updates        | 145374   |
----------------------------------
Eval num_timesteps=592000, episode_reward=125.14 +/- 51.94
Episode length: 235.96 +/- 227.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.224    |
| time/               |          |
|    total_timesteps  | 592000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.405    |
|    n_updates        | 145499   |
----------------------------------
Eval num_timesteps=592500, episode_reward=185.55 +/- 25.33
Episode length: 505.06 +/- 97.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.223    |
| time/               |          |
|    total_timesteps  | 592500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.499    |
|    n_updates        | 145624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.223    |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 53       |
|    time_elapsed     | 11150    |
|    total_timesteps  | 592643   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.328    |
|    n_updates        | 145660   |
----------------------------------
Eval num_timesteps=593000, episode_reward=33.09 +/- 71.57
Episode length: 107.74 +/- 154.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 33.1     |
| rollout/            |          |
|    exploration_rate | 0.222    |
| time/               |          |
|    total_timesteps  | 593000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.271    |
|    n_updates        | 145749   |
----------------------------------
Eval num_timesteps=593500, episode_reward=23.65 +/- 76.66
Episode length: 125.20 +/- 175.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.221    |
| time/               |          |
|    total_timesteps  | 593500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.6      |
|    n_updates        | 145874   |
----------------------------------
Eval num_timesteps=594000, episode_reward=196.15 +/- 2.37
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.22     |
| time/               |          |
|    total_timesteps  | 594000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.257    |
|    n_updates        | 145999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 159      |
|    exploration_rate | 0.219    |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 53       |
|    time_elapsed     | 11173    |
|    total_timesteps  | 594262   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.42     |
|    n_updates        | 146065   |
----------------------------------
Eval num_timesteps=594500, episode_reward=36.87 +/- 68.53
Episode length: 97.86 +/- 143.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 36.9     |
| rollout/            |          |
|    exploration_rate | 0.219    |
| time/               |          |
|    total_timesteps  | 594500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.857    |
|    n_updates        | 146124   |
----------------------------------
Eval num_timesteps=595000, episode_reward=192.83 +/- 15.52
Episode length: 514.88 +/- 70.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.218    |
| time/               |          |
|    total_timesteps  | 595000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.627    |
|    n_updates        | 146249   |
----------------------------------
Eval num_timesteps=595500, episode_reward=155.73 +/- 51.88
Episode length: 372.10 +/- 222.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 372      |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.217    |
| time/               |          |
|    total_timesteps  | 595500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.361    |
|    n_updates        | 146374   |
----------------------------------
Eval num_timesteps=596000, episode_reward=190.48 +/- 26.14
Episode length: 495.22 +/- 117.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.216    |
| time/               |          |
|    total_timesteps  | 596000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0546   |
|    n_updates        | 146499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.215    |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 53       |
|    time_elapsed     | 11217    |
|    total_timesteps  | 596362   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.78     |
|    n_updates        | 146590   |
----------------------------------
Eval num_timesteps=596500, episode_reward=175.70 +/- 40.08
Episode length: 448.74 +/- 174.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.215    |
| time/               |          |
|    total_timesteps  | 596500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.281    |
|    n_updates        | 146624   |
----------------------------------
Eval num_timesteps=597000, episode_reward=185.54 +/- 33.13
Episode length: 480.78 +/- 133.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 481      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.214    |
| time/               |          |
|    total_timesteps  | 597000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.484    |
|    n_updates        | 146749   |
----------------------------------
Eval num_timesteps=597500, episode_reward=136.69 +/- 81.59
Episode length: 358.22 +/- 218.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 358      |
|    mean_reward      | 137      |
| rollout/            |          |
|    exploration_rate | 0.213    |
| time/               |          |
|    total_timesteps  | 597500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.092    |
|    n_updates        | 146874   |
----------------------------------
Eval num_timesteps=598000, episode_reward=197.19 +/- 1.31
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.212    |
| time/               |          |
|    total_timesteps  | 598000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.185    |
|    n_updates        | 146999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | 162      |
|    exploration_rate | 0.211    |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 53       |
|    time_elapsed     | 11270    |
|    total_timesteps  | 598462   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.377    |
|    n_updates        | 147115   |
----------------------------------
Eval num_timesteps=598500, episode_reward=189.10 +/- 21.99
Episode length: 510.74 +/- 69.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 511      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.211    |
| time/               |          |
|    total_timesteps  | 598500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.169    |
|    n_updates        | 147124   |
----------------------------------
Eval num_timesteps=599000, episode_reward=178.01 +/- 30.52
Episode length: 505.06 +/- 97.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.21     |
| time/               |          |
|    total_timesteps  | 599000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.665    |
|    n_updates        | 147249   |
----------------------------------
Eval num_timesteps=599500, episode_reward=194.74 +/- 15.41
Episode length: 515.32 +/- 67.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.209    |
| time/               |          |
|    total_timesteps  | 599500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.293    |
|    n_updates        | 147374   |
----------------------------------
Eval num_timesteps=600000, episode_reward=64.83 +/- 69.72
Episode length: 116.54 +/- 165.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 64.8     |
| rollout/            |          |
|    exploration_rate | 0.208    |
| time/               |          |
|    total_timesteps  | 600000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.184    |
|    n_updates        | 147499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | 163      |
|    exploration_rate | 0.208    |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 53       |
|    time_elapsed     | 11318    |
|    total_timesteps  | 600122   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.622    |
|    n_updates        | 147530   |
----------------------------------
Eval num_timesteps=600500, episode_reward=97.53 +/- 68.58
Episode length: 170.34 +/- 210.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 97.5     |
| rollout/            |          |
|    exploration_rate | 0.207    |
| time/               |          |
|    total_timesteps  | 600500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.225    |
|    n_updates        | 147624   |
----------------------------------
Eval num_timesteps=601000, episode_reward=195.92 +/- 1.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.206    |
| time/               |          |
|    total_timesteps  | 601000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.449    |
|    n_updates        | 147749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.205    |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 53       |
|    time_elapsed     | 11339    |
|    total_timesteps  | 601278   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.221    |
|    n_updates        | 147819   |
----------------------------------
Eval num_timesteps=601500, episode_reward=112.21 +/- 49.17
Episode length: 184.82 +/- 212.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 185      |
|    mean_reward      | 112      |
| rollout/            |          |
|    exploration_rate | 0.205    |
| time/               |          |
|    total_timesteps  | 601500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.193    |
|    n_updates        | 147874   |
----------------------------------
Eval num_timesteps=602000, episode_reward=142.42 +/- 58.55
Episode length: 314.36 +/- 237.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 314      |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.204    |
| time/               |          |
|    total_timesteps  | 602000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.527    |
|    n_updates        | 147999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 463      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 53       |
|    time_elapsed     | 11354    |
|    total_timesteps  | 602461   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.306    |
|    n_updates        | 148115   |
----------------------------------
Eval num_timesteps=602500, episode_reward=179.98 +/- 40.29
Episode length: 467.52 +/- 155.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 468      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.203    |
| time/               |          |
|    total_timesteps  | 602500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.268    |
|    n_updates        | 148124   |
----------------------------------
Eval num_timesteps=603000, episode_reward=176.04 +/- 45.65
Episode length: 439.60 +/- 182.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.202    |
| time/               |          |
|    total_timesteps  | 603000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.211    |
|    n_updates        | 148249   |
----------------------------------
Eval num_timesteps=603500, episode_reward=166.35 +/- 63.40
Episode length: 427.64 +/- 194.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 428      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.201    |
| time/               |          |
|    total_timesteps  | 603500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.643    |
|    n_updates        | 148374   |
----------------------------------
Eval num_timesteps=604000, episode_reward=178.30 +/- 43.67
Episode length: 446.88 +/- 179.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 447      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 604000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.219    |
|    n_updates        | 148499   |
----------------------------------
Eval num_timesteps=604500, episode_reward=159.95 +/- 68.72
Episode length: 408.46 +/- 207.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 408      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.199    |
| time/               |          |
|    total_timesteps  | 604500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.215    |
|    n_updates        | 148624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 463      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.199    |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 52       |
|    time_elapsed     | 11417    |
|    total_timesteps  | 604561   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.167    |
|    n_updates        | 148640   |
----------------------------------
Eval num_timesteps=605000, episode_reward=192.14 +/- 21.43
Episode length: 505.80 +/- 94.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.198    |
| time/               |          |
|    total_timesteps  | 605000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.735    |
|    n_updates        | 148749   |
----------------------------------
Eval num_timesteps=605500, episode_reward=180.27 +/- 38.60
Episode length: 457.48 +/- 167.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 457      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.197    |
| time/               |          |
|    total_timesteps  | 605500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.282    |
|    n_updates        | 148874   |
----------------------------------
Eval num_timesteps=606000, episode_reward=116.93 +/- 49.66
Episode length: 194.74 +/- 216.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 117      |
| rollout/            |          |
|    exploration_rate | 0.196    |
| time/               |          |
|    total_timesteps  | 606000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.201    |
|    n_updates        | 148999   |
----------------------------------
Eval num_timesteps=606500, episode_reward=58.29 +/- 88.98
Episode length: 174.00 +/- 208.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 58.3     |
| rollout/            |          |
|    exploration_rate | 0.195    |
| time/               |          |
|    total_timesteps  | 606500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.238    |
|    n_updates        | 149124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 463      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.195    |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 52       |
|    time_elapsed     | 11457    |
|    total_timesteps  | 606661   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.126    |
|    n_updates        | 149165   |
----------------------------------
Eval num_timesteps=607000, episode_reward=165.27 +/- 72.98
Episode length: 447.88 +/- 176.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.194    |
| time/               |          |
|    total_timesteps  | 607000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.386    |
|    n_updates        | 149249   |
----------------------------------
Eval num_timesteps=607500, episode_reward=192.41 +/- 29.16
Episode length: 515.08 +/- 69.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.193    |
| time/               |          |
|    total_timesteps  | 607500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0655   |
|    n_updates        | 149374   |
----------------------------------
Eval num_timesteps=608000, episode_reward=192.36 +/- 21.57
Episode length: 505.70 +/- 94.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.192    |
| time/               |          |
|    total_timesteps  | 608000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.439    |
|    n_updates        | 149499   |
----------------------------------
Eval num_timesteps=608500, episode_reward=193.81 +/- 16.13
Episode length: 516.02 +/- 62.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.191    |
| time/               |          |
|    total_timesteps  | 608500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.332    |
|    n_updates        | 149624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 463      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.191    |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 52       |
|    time_elapsed     | 11515    |
|    total_timesteps  | 608761   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.21     |
|    n_updates        | 149690   |
----------------------------------
Eval num_timesteps=609000, episode_reward=194.50 +/- 2.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.19     |
| time/               |          |
|    total_timesteps  | 609000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.251    |
|    n_updates        | 149749   |
----------------------------------
Eval num_timesteps=609500, episode_reward=192.12 +/- 22.69
Episode length: 506.36 +/- 91.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.189    |
| time/               |          |
|    total_timesteps  | 609500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.548    |
|    n_updates        | 149874   |
----------------------------------
Eval num_timesteps=610000, episode_reward=110.63 +/- 75.63
Episode length: 230.66 +/- 231.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.188    |
| time/               |          |
|    total_timesteps  | 610000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.15     |
|    n_updates        | 149999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 463      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 52       |
|    time_elapsed     | 11552    |
|    total_timesteps  | 610369   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.283    |
|    n_updates        | 150092   |
----------------------------------
Eval num_timesteps=610500, episode_reward=138.73 +/- 54.96
Episode length: 283.16 +/- 232.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 283      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.187    |
| time/               |          |
|    total_timesteps  | 610500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.367    |
|    n_updates        | 150124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 157      |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 52       |
|    time_elapsed     | 11560    |
|    total_timesteps  | 610567   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.06     |
|    n_updates        | 150141   |
----------------------------------
Eval num_timesteps=611000, episode_reward=167.04 +/- 52.67
Episode length: 423.54 +/- 185.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 424      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.186    |
| time/               |          |
|    total_timesteps  | 611000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.365    |
|    n_updates        | 150249   |
----------------------------------
Eval num_timesteps=611500, episode_reward=118.31 +/- 49.48
Episode length: 195.76 +/- 216.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 196      |
|    mean_reward      | 118      |
| rollout/            |          |
|    exploration_rate | 0.185    |
| time/               |          |
|    total_timesteps  | 611500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.229    |
|    n_updates        | 150374   |
----------------------------------
Eval num_timesteps=612000, episode_reward=196.70 +/- 1.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.184    |
| time/               |          |
|    total_timesteps  | 612000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.514    |
|    n_updates        | 150499   |
----------------------------------
Eval num_timesteps=612500, episode_reward=196.48 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.183    |
| time/               |          |
|    total_timesteps  | 612500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.414    |
|    n_updates        | 150624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 157      |
|    exploration_rate | 0.183    |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 52       |
|    time_elapsed     | 11609    |
|    total_timesteps  | 612667   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.996    |
|    n_updates        | 150666   |
----------------------------------
Eval num_timesteps=613000, episode_reward=102.82 +/- 87.24
Episode length: 247.44 +/- 236.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 247      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.182    |
| time/               |          |
|    total_timesteps  | 613000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0849   |
|    n_updates        | 150749   |
----------------------------------
Eval num_timesteps=613500, episode_reward=200.29 +/- 19.09
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 200      |
| rollout/            |          |
|    exploration_rate | 0.181    |
| time/               |          |
|    total_timesteps  | 613500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.205    |
|    n_updates        | 150874   |
----------------------------------
New best mean reward!
Eval num_timesteps=614000, episode_reward=196.49 +/- 1.93
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.18     |
| time/               |          |
|    total_timesteps  | 614000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.404    |
|    n_updates        | 150999   |
----------------------------------
Eval num_timesteps=614500, episode_reward=175.34 +/- 41.66
Episode length: 439.62 +/- 182.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.179    |
| time/               |          |
|    total_timesteps  | 614500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.243    |
|    n_updates        | 151124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 157      |
|    exploration_rate | 0.179    |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 52       |
|    time_elapsed     | 11659    |
|    total_timesteps  | 614767   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.658    |
|    n_updates        | 151191   |
----------------------------------
Eval num_timesteps=615000, episode_reward=194.51 +/- 23.26
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 615000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.588    |
|    n_updates        | 151249   |
----------------------------------
Eval num_timesteps=615500, episode_reward=197.51 +/- 0.83
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.177    |
| time/               |          |
|    total_timesteps  | 615500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.447    |
|    n_updates        | 151374   |
----------------------------------
Eval num_timesteps=616000, episode_reward=197.39 +/- 0.95
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.176    |
| time/               |          |
|    total_timesteps  | 616000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.408    |
|    n_updates        | 151499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 158      |
|    exploration_rate | 0.176    |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 52       |
|    time_elapsed     | 11705    |
|    total_timesteps  | 616390   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.464    |
|    n_updates        | 151597   |
----------------------------------
Eval num_timesteps=616500, episode_reward=197.16 +/- 0.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.175    |
| time/               |          |
|    total_timesteps  | 616500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.872    |
|    n_updates        | 151624   |
----------------------------------
Eval num_timesteps=617000, episode_reward=110.07 +/- 45.07
Episode length: 165.06 +/- 192.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 617000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.237    |
|    n_updates        | 151749   |
----------------------------------
Eval num_timesteps=617500, episode_reward=182.78 +/- 35.80
Episode length: 466.46 +/- 158.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 466      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.173    |
| time/               |          |
|    total_timesteps  | 617500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.141    |
|    n_updates        | 151874   |
----------------------------------
Eval num_timesteps=618000, episode_reward=197.24 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.172    |
| time/               |          |
|    total_timesteps  | 618000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.158    |
|    n_updates        | 151999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 449      |
|    ep_rew_mean      | 159      |
|    exploration_rate | 0.171    |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 52       |
|    time_elapsed     | 11754    |
|    total_timesteps  | 618490   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.459    |
|    n_updates        | 152122   |
----------------------------------
Eval num_timesteps=618500, episode_reward=185.96 +/- 33.54
Episode length: 476.76 +/- 144.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.171    |
| time/               |          |
|    total_timesteps  | 618500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.847    |
|    n_updates        | 152124   |
----------------------------------
Eval num_timesteps=619000, episode_reward=197.49 +/- 1.08
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.17     |
| time/               |          |
|    total_timesteps  | 619000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.143    |
|    n_updates        | 152249   |
----------------------------------
Eval num_timesteps=619500, episode_reward=193.77 +/- 15.70
Episode length: 515.04 +/- 69.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.169    |
| time/               |          |
|    total_timesteps  | 619500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.244    |
|    n_updates        | 152374   |
----------------------------------
Eval num_timesteps=620000, episode_reward=195.15 +/- 15.41
Episode length: 515.72 +/- 64.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.168    |
| time/               |          |
|    total_timesteps  | 620000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0992   |
|    n_updates        | 152499   |
----------------------------------
Eval num_timesteps=620500, episode_reward=195.91 +/- 1.94
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.167    |
| time/               |          |
|    total_timesteps  | 620500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.315    |
|    n_updates        | 152624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 454      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.167    |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 52       |
|    time_elapsed     | 11828    |
|    total_timesteps  | 620590   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.172    |
|    n_updates        | 152647   |
----------------------------------
Eval num_timesteps=621000, episode_reward=192.57 +/- 21.51
Episode length: 505.10 +/- 97.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.166    |
| time/               |          |
|    total_timesteps  | 621000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.158    |
|    n_updates        | 152749   |
----------------------------------
Eval num_timesteps=621500, episode_reward=195.83 +/- 2.07
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.165    |
| time/               |          |
|    total_timesteps  | 621500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.099    |
|    n_updates        | 152874   |
----------------------------------
Eval num_timesteps=622000, episode_reward=187.44 +/- 29.97
Episode length: 487.04 +/- 128.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.164    |
| time/               |          |
|    total_timesteps  | 622000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.27     |
|    n_updates        | 152999   |
----------------------------------
Eval num_timesteps=622500, episode_reward=196.38 +/- 1.16
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.163    |
| time/               |          |
|    total_timesteps  | 622500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.171    |
|    n_updates        | 153124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 459      |
|    ep_rew_mean      | 162      |
|    exploration_rate | 0.163    |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 52       |
|    time_elapsed     | 11887    |
|    total_timesteps  | 622690   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.186    |
|    n_updates        | 153172   |
----------------------------------
Eval num_timesteps=623000, episode_reward=169.87 +/- 46.83
Episode length: 411.86 +/- 201.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 412      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.162    |
| time/               |          |
|    total_timesteps  | 623000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.11     |
|    n_updates        | 153249   |
----------------------------------
Eval num_timesteps=623500, episode_reward=195.07 +/- 2.98
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.161    |
| time/               |          |
|    total_timesteps  | 623500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.43     |
|    n_updates        | 153374   |
----------------------------------
Eval num_timesteps=624000, episode_reward=142.65 +/- 55.33
Episode length: 301.20 +/- 233.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 301      |
|    mean_reward      | 143      |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 624000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.255    |
|    n_updates        | 153499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 454      |
|    ep_rew_mean      | 161      |
|    exploration_rate | 0.16     |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 52       |
|    time_elapsed     | 11924    |
|    total_timesteps  | 624327   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.346    |
|    n_updates        | 153581   |
----------------------------------
Eval num_timesteps=624500, episode_reward=197.47 +/- 0.92
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.159    |
| time/               |          |
|    total_timesteps  | 624500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.377    |
|    n_updates        | 153624   |
----------------------------------
Eval num_timesteps=625000, episode_reward=197.37 +/- 1.48
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.159    |
| time/               |          |
|    total_timesteps  | 625000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.118    |
|    n_updates        | 153749   |
----------------------------------
Eval num_timesteps=625500, episode_reward=185.35 +/- 30.03
Episode length: 486.52 +/- 130.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.158    |
| time/               |          |
|    total_timesteps  | 625500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0501   |
|    n_updates        | 153874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 449      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.157    |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 52       |
|    time_elapsed     | 11969    |
|    total_timesteps  | 625948   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.104    |
|    n_updates        | 153986   |
----------------------------------
Eval num_timesteps=626000, episode_reward=197.61 +/- 0.74
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.157    |
| time/               |          |
|    total_timesteps  | 626000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.16     |
|    n_updates        | 153999   |
----------------------------------
Eval num_timesteps=626500, episode_reward=167.22 +/- 58.72
Episode length: 438.60 +/- 175.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 439      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.156    |
| time/               |          |
|    total_timesteps  | 626500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0746   |
|    n_updates        | 154124   |
----------------------------------
Eval num_timesteps=627000, episode_reward=106.91 +/- 81.99
Episode length: 248.98 +/- 227.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 249      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.155    |
| time/               |          |
|    total_timesteps  | 627000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.446    |
|    n_updates        | 154249   |
----------------------------------
Eval num_timesteps=627500, episode_reward=183.87 +/- 19.36
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.154    |
| time/               |          |
|    total_timesteps  | 627500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.309    |
|    n_updates        | 154374   |
----------------------------------
Eval num_timesteps=628000, episode_reward=179.97 +/- 38.43
Episode length: 460.28 +/- 160.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 460      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.153    |
| time/               |          |
|    total_timesteps  | 628000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.438    |
|    n_updates        | 154499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 449      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.152    |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 52       |
|    time_elapsed     | 12033    |
|    total_timesteps  | 628048   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.146    |
|    n_updates        | 154511   |
----------------------------------
Eval num_timesteps=628500, episode_reward=194.82 +/- 15.39
Episode length: 517.28 +/- 54.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 517      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.152    |
| time/               |          |
|    total_timesteps  | 628500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.44     |
|    n_updates        | 154624   |
----------------------------------
Eval num_timesteps=629000, episode_reward=128.76 +/- 55.92
Episode length: 312.32 +/- 183.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 312      |
|    mean_reward      | 129      |
| rollout/            |          |
|    exploration_rate | 0.151    |
| time/               |          |
|    total_timesteps  | 629000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.545    |
|    n_updates        | 154749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 440      |
|    ep_rew_mean      | 159      |
|    exploration_rate | 0.15     |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 52       |
|    time_elapsed     | 12058    |
|    total_timesteps  | 629225   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0674   |
|    n_updates        | 154806   |
----------------------------------
Eval num_timesteps=629500, episode_reward=188.25 +/- 33.55
Episode length: 486.08 +/- 132.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.15     |
| time/               |          |
|    total_timesteps  | 629500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.465    |
|    n_updates        | 154874   |
----------------------------------
Eval num_timesteps=630000, episode_reward=27.65 +/- 48.32
Episode length: 51.50 +/- 21.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.5     |
|    mean_reward      | 27.6     |
| rollout/            |          |
|    exploration_rate | 0.149    |
| time/               |          |
|    total_timesteps  | 630000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.179    |
|    n_updates        | 154999   |
----------------------------------
Eval num_timesteps=630500, episode_reward=197.51 +/- 1.22
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.148    |
| time/               |          |
|    total_timesteps  | 630500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.573    |
|    n_updates        | 155124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 435      |
|    ep_rew_mean      | 158      |
|    exploration_rate | 0.147    |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 52       |
|    time_elapsed     | 12089    |
|    total_timesteps  | 630866   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.127    |
|    n_updates        | 155216   |
----------------------------------
Eval num_timesteps=631000, episode_reward=106.20 +/- 42.08
Episode length: 156.08 +/- 177.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 106      |
| rollout/            |          |
|    exploration_rate | 0.147    |
| time/               |          |
|    total_timesteps  | 631000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.271    |
|    n_updates        | 155249   |
----------------------------------
Eval num_timesteps=631500, episode_reward=4.14 +/- 36.07
Episode length: 49.06 +/- 13.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | 4.14     |
| rollout/            |          |
|    exploration_rate | 0.146    |
| time/               |          |
|    total_timesteps  | 631500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.224    |
|    n_updates        | 155374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 426      |
|    ep_rew_mean      | 155      |
|    exploration_rate | 0.146    |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 52       |
|    time_elapsed     | 12095    |
|    total_timesteps  | 631521   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.227    |
|    n_updates        | 155380   |
----------------------------------
Eval num_timesteps=632000, episode_reward=184.19 +/- 33.39
Episode length: 476.06 +/- 146.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.145    |
| time/               |          |
|    total_timesteps  | 632000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.196    |
|    n_updates        | 155499   |
----------------------------------
Eval num_timesteps=632500, episode_reward=195.45 +/- 2.63
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.144    |
| time/               |          |
|    total_timesteps  | 632500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.216    |
|    n_updates        | 155624   |
----------------------------------
Eval num_timesteps=633000, episode_reward=160.95 +/- 86.01
Episode length: 431.02 +/- 188.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.143    |
| time/               |          |
|    total_timesteps  | 633000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.25     |
|    n_updates        | 155749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 426      |
|    ep_rew_mean      | 155      |
|    exploration_rate | 0.142    |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 52       |
|    time_elapsed     | 12137    |
|    total_timesteps  | 633182   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.5      |
|    n_updates        | 155795   |
----------------------------------
Eval num_timesteps=633500, episode_reward=89.19 +/- 69.06
Episode length: 155.56 +/- 196.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 89.2     |
| rollout/            |          |
|    exploration_rate | 0.142    |
| time/               |          |
|    total_timesteps  | 633500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.564    |
|    n_updates        | 155874   |
----------------------------------
Eval num_timesteps=634000, episode_reward=189.79 +/- 26.35
Episode length: 495.96 +/- 115.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.141    |
| time/               |          |
|    total_timesteps  | 634000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.231    |
|    n_updates        | 155999   |
----------------------------------
Eval num_timesteps=634500, episode_reward=16.39 +/- 53.78
Episode length: 70.80 +/- 94.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.8     |
|    mean_reward      | 16.4     |
| rollout/            |          |
|    exploration_rate | 0.14     |
| time/               |          |
|    total_timesteps  | 634500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.117    |
|    n_updates        | 156124   |
----------------------------------
Eval num_timesteps=635000, episode_reward=192.66 +/- 21.49
Episode length: 505.78 +/- 94.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.139    |
| time/               |          |
|    total_timesteps  | 635000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.239    |
|    n_updates        | 156249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 426      |
|    ep_rew_mean      | 155      |
|    exploration_rate | 0.138    |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 52       |
|    time_elapsed     | 12173    |
|    total_timesteps  | 635282   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.442    |
|    n_updates        | 156320   |
----------------------------------
Eval num_timesteps=635500, episode_reward=111.38 +/- 48.89
Episode length: 188.64 +/- 210.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.138    |
| time/               |          |
|    total_timesteps  | 635500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.476    |
|    n_updates        | 156374   |
----------------------------------
Eval num_timesteps=636000, episode_reward=122.82 +/- 89.39
Episode length: 312.90 +/- 239.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 313      |
|    mean_reward      | 123      |
| rollout/            |          |
|    exploration_rate | 0.137    |
| time/               |          |
|    total_timesteps  | 636000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.209    |
|    n_updates        | 156499   |
----------------------------------
Eval num_timesteps=636500, episode_reward=192.19 +/- 21.95
Episode length: 505.06 +/- 97.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.136    |
| time/               |          |
|    total_timesteps  | 636500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.24     |
|    n_updates        | 156624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 427      |
|    ep_rew_mean      | 156      |
|    exploration_rate | 0.135    |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 52       |
|    time_elapsed     | 12203    |
|    total_timesteps  | 636984   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.235    |
|    n_updates        | 156745   |
----------------------------------
Eval num_timesteps=637000, episode_reward=196.56 +/- 1.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.135    |
| time/               |          |
|    total_timesteps  | 637000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.03     |
|    n_updates        | 156749   |
----------------------------------
Eval num_timesteps=637500, episode_reward=104.39 +/- 40.50
Episode length: 137.76 +/- 170.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.134    |
| time/               |          |
|    total_timesteps  | 637500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.237    |
|    n_updates        | 156874   |
----------------------------------
Eval num_timesteps=638000, episode_reward=91.55 +/- 25.52
Episode length: 89.82 +/- 111.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.8     |
|    mean_reward      | 91.6     |
| rollout/            |          |
|    exploration_rate | 0.133    |
| time/               |          |
|    total_timesteps  | 638000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.526    |
|    n_updates        | 156999   |
----------------------------------
Eval num_timesteps=638500, episode_reward=179.32 +/- 37.70
Episode length: 460.80 +/- 159.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 461      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.132    |
| time/               |          |
|    total_timesteps  | 638500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.258    |
|    n_updates        | 157124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 425      |
|    ep_rew_mean      | 155      |
|    exploration_rate | 0.131    |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 52       |
|    time_elapsed     | 12239    |
|    total_timesteps  | 638851   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.247    |
|    n_updates        | 157212   |
----------------------------------
Eval num_timesteps=639000, episode_reward=193.23 +/- 21.61
Episode length: 505.44 +/- 95.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.131    |
| time/               |          |
|    total_timesteps  | 639000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.18     |
|    n_updates        | 157249   |
----------------------------------
Eval num_timesteps=639500, episode_reward=193.56 +/- 5.30
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.13     |
| time/               |          |
|    total_timesteps  | 639500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.657    |
|    n_updates        | 157374   |
----------------------------------
Eval num_timesteps=640000, episode_reward=194.84 +/- 3.17
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.129    |
| time/               |          |
|    total_timesteps  | 640000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.303    |
|    n_updates        | 157499   |
----------------------------------
Eval num_timesteps=640500, episode_reward=138.54 +/- 54.87
Episode length: 286.38 +/- 238.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 286      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.128    |
| time/               |          |
|    total_timesteps  | 640500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.213    |
|    n_updates        | 157624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 420      |
|    ep_rew_mean      | 153      |
|    exploration_rate | 0.128    |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 52       |
|    time_elapsed     | 12292    |
|    total_timesteps  | 640502   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.144    |
|    n_updates        | 157625   |
----------------------------------
Eval num_timesteps=641000, episode_reward=174.32 +/- 43.59
Episode length: 431.52 +/- 187.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 432      |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.127    |
| time/               |          |
|    total_timesteps  | 641000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0903   |
|    n_updates        | 157749   |
----------------------------------
Eval num_timesteps=641500, episode_reward=30.93 +/- 68.28
Episode length: 99.72 +/- 143.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.7     |
|    mean_reward      | 30.9     |
| rollout/            |          |
|    exploration_rate | 0.126    |
| time/               |          |
|    total_timesteps  | 641500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.154    |
|    n_updates        | 157874   |
----------------------------------
Eval num_timesteps=642000, episode_reward=196.28 +/- 1.59
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.125    |
| time/               |          |
|    total_timesteps  | 642000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.12     |
|    n_updates        | 157999   |
----------------------------------
Eval num_timesteps=642500, episode_reward=197.18 +/- 1.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.124    |
| time/               |          |
|    total_timesteps  | 642500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.125    |
|    n_updates        | 158124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 425      |
|    ep_rew_mean      | 154      |
|    exploration_rate | 0.124    |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 52       |
|    time_elapsed     | 12338    |
|    total_timesteps  | 642602   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.271    |
|    n_updates        | 158150   |
----------------------------------
Eval num_timesteps=643000, episode_reward=181.18 +/- 6.62
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.123    |
| time/               |          |
|    total_timesteps  | 643000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.662    |
|    n_updates        | 158249   |
----------------------------------
Eval num_timesteps=643500, episode_reward=87.09 +/- 15.10
Episode length: 73.46 +/- 68.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.5     |
|    mean_reward      | 87.1     |
| rollout/            |          |
|    exploration_rate | 0.122    |
| time/               |          |
|    total_timesteps  | 643500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.317    |
|    n_updates        | 158374   |
----------------------------------
Eval num_timesteps=644000, episode_reward=177.01 +/- 42.04
Episode length: 441.38 +/- 178.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.121    |
| time/               |          |
|    total_timesteps  | 644000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.105    |
|    n_updates        | 158499   |
----------------------------------
Eval num_timesteps=644500, episode_reward=197.38 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.12     |
| time/               |          |
|    total_timesteps  | 644500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.169    |
|    n_updates        | 158624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 434      |
|    ep_rew_mean      | 156      |
|    exploration_rate | 0.119    |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 52       |
|    time_elapsed     | 12385    |
|    total_timesteps  | 644702   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.119    |
|    n_updates        | 158675   |
----------------------------------
Eval num_timesteps=645000, episode_reward=194.43 +/- 16.23
Episode length: 515.92 +/- 63.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.119    |
| time/               |          |
|    total_timesteps  | 645000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.772    |
|    n_updates        | 158749   |
----------------------------------
Eval num_timesteps=645500, episode_reward=173.48 +/- 43.22
Episode length: 430.88 +/- 188.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.118    |
| time/               |          |
|    total_timesteps  | 645500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.353    |
|    n_updates        | 158874   |
----------------------------------
Eval num_timesteps=646000, episode_reward=177.99 +/- 18.74
Episode length: 506.02 +/- 92.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.117    |
| time/               |          |
|    total_timesteps  | 646000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.736    |
|    n_updates        | 158999   |
----------------------------------
Eval num_timesteps=646500, episode_reward=197.29 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.116    |
| time/               |          |
|    total_timesteps  | 646500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.122    |
|    n_updates        | 159124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 443      |
|    ep_rew_mean      | 158      |
|    exploration_rate | 0.115    |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 51       |
|    time_elapsed     | 12442    |
|    total_timesteps  | 646802   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.122    |
|    n_updates        | 159200   |
----------------------------------
Eval num_timesteps=647000, episode_reward=196.86 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.115    |
| time/               |          |
|    total_timesteps  | 647000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.255    |
|    n_updates        | 159249   |
----------------------------------
Eval num_timesteps=647500, episode_reward=192.42 +/- 30.34
Episode length: 515.18 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.114    |
| time/               |          |
|    total_timesteps  | 647500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.274    |
|    n_updates        | 159374   |
----------------------------------
Eval num_timesteps=648000, episode_reward=146.52 +/- 54.60
Episode length: 327.52 +/- 218.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 328      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.113    |
| time/               |          |
|    total_timesteps  | 648000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0702   |
|    n_updates        | 159499   |
----------------------------------
Eval num_timesteps=648500, episode_reward=197.34 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.112    |
| time/               |          |
|    total_timesteps  | 648500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.129    |
|    n_updates        | 159624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 443      |
|    ep_rew_mean      | 158      |
|    exploration_rate | 0.111    |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 51       |
|    time_elapsed     | 12497    |
|    total_timesteps  | 648902   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.333    |
|    n_updates        | 159725   |
----------------------------------
Eval num_timesteps=649000, episode_reward=196.76 +/- 2.34
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.111    |
| time/               |          |
|    total_timesteps  | 649000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.085    |
|    n_updates        | 159749   |
----------------------------------
Eval num_timesteps=649500, episode_reward=197.75 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.11     |
| time/               |          |
|    total_timesteps  | 649500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.122    |
|    n_updates        | 159874   |
----------------------------------
Eval num_timesteps=650000, episode_reward=192.57 +/- 29.42
Episode length: 514.88 +/- 70.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.109    |
| time/               |          |
|    total_timesteps  | 650000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.35     |
|    n_updates        | 159999   |
----------------------------------
Eval num_timesteps=650500, episode_reward=166.72 +/- 69.93
Episode length: 447.48 +/- 177.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 447      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.108    |
| time/               |          |
|    total_timesteps  | 650500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.209    |
|    n_updates        | 160124   |
----------------------------------
Eval num_timesteps=651000, episode_reward=182.60 +/- 35.84
Episode length: 477.06 +/- 132.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.107    |
| time/               |          |
|    total_timesteps  | 651000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.381    |
|    n_updates        | 160249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 443      |
|    ep_rew_mean      | 159      |
|    exploration_rate | 0.107    |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 51       |
|    time_elapsed     | 12570    |
|    total_timesteps  | 651002   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.851    |
|    n_updates        | 160250   |
----------------------------------
Eval num_timesteps=651500, episode_reward=114.93 +/- 47.97
Episode length: 283.26 +/- 156.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 283      |
|    mean_reward      | 115      |
| rollout/            |          |
|    exploration_rate | 0.106    |
| time/               |          |
|    total_timesteps  | 651500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.557    |
|    n_updates        | 160374   |
----------------------------------
Eval num_timesteps=652000, episode_reward=125.09 +/- 54.62
Episode length: 334.30 +/- 171.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 334      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.105    |
| time/               |          |
|    total_timesteps  | 652000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.21     |
|    n_updates        | 160499   |
----------------------------------
Eval num_timesteps=652500, episode_reward=167.48 +/- 48.03
Episode length: 424.36 +/- 182.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 424      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.104    |
| time/               |          |
|    total_timesteps  | 652500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.517    |
|    n_updates        | 160624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 438      |
|    ep_rew_mean      | 158      |
|    exploration_rate | 0.104    |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 51       |
|    time_elapsed     | 12601    |
|    total_timesteps  | 652609   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0636   |
|    n_updates        | 160652   |
----------------------------------
Eval num_timesteps=653000, episode_reward=186.93 +/- 30.67
Episode length: 492.32 +/- 112.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 492      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.103    |
| time/               |          |
|    total_timesteps  | 653000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.251    |
|    n_updates        | 160749   |
----------------------------------
Eval num_timesteps=653500, episode_reward=179.09 +/- 40.35
Episode length: 448.44 +/- 175.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.102    |
| time/               |          |
|    total_timesteps  | 653500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.276    |
|    n_updates        | 160874   |
----------------------------------
Eval num_timesteps=654000, episode_reward=125.80 +/- 52.25
Episode length: 222.36 +/- 227.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.101    |
| time/               |          |
|    total_timesteps  | 654000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.748    |
|    n_updates        | 160999   |
----------------------------------
Eval num_timesteps=654500, episode_reward=195.81 +/- 1.36
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.1      |
| time/               |          |
|    total_timesteps  | 654500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0923   |
|    n_updates        | 161124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 443      |
|    ep_rew_mean      | 159      |
|    exploration_rate | 0.0997   |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 51       |
|    time_elapsed     | 12650    |
|    total_timesteps  | 654709   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.165    |
|    n_updates        | 161177   |
----------------------------------
Eval num_timesteps=655000, episode_reward=197.56 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0991   |
| time/               |          |
|    total_timesteps  | 655000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.24     |
|    n_updates        | 161249   |
----------------------------------
Eval num_timesteps=655500, episode_reward=173.22 +/- 42.02
Episode length: 436.96 +/- 187.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 437      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.0981   |
| time/               |          |
|    total_timesteps  | 655500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.192    |
|    n_updates        | 161374   |
----------------------------------
Eval num_timesteps=656000, episode_reward=197.10 +/- 1.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0971   |
| time/               |          |
|    total_timesteps  | 656000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.949    |
|    n_updates        | 161499   |
----------------------------------
Eval num_timesteps=656500, episode_reward=196.77 +/- 1.03
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0961   |
| time/               |          |
|    total_timesteps  | 656500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.346    |
|    n_updates        | 161624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 164      |
|    exploration_rate | 0.0955   |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 51       |
|    time_elapsed     | 12708    |
|    total_timesteps  | 656809   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.194    |
|    n_updates        | 161702   |
----------------------------------
Eval num_timesteps=657000, episode_reward=197.57 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0951   |
| time/               |          |
|    total_timesteps  | 657000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.252    |
|    n_updates        | 161749   |
----------------------------------
Eval num_timesteps=657500, episode_reward=196.65 +/- 1.57
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0942   |
| time/               |          |
|    total_timesteps  | 657500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.111    |
|    n_updates        | 161874   |
----------------------------------
Eval num_timesteps=658000, episode_reward=195.26 +/- 15.60
Episode length: 514.86 +/- 70.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.0932   |
| time/               |          |
|    total_timesteps  | 658000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.38     |
|    n_updates        | 161999   |
----------------------------------
Eval num_timesteps=658500, episode_reward=197.69 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0922   |
| time/               |          |
|    total_timesteps  | 658500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.142    |
|    n_updates        | 162124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 164      |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 51       |
|    time_elapsed     | 12769    |
|    total_timesteps  | 658909   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.125    |
|    n_updates        | 162227   |
----------------------------------
Eval num_timesteps=659000, episode_reward=188.69 +/- 35.82
Episode length: 498.34 +/- 106.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.0912   |
| time/               |          |
|    total_timesteps  | 659000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.136    |
|    n_updates        | 162249   |
----------------------------------
Eval num_timesteps=659500, episode_reward=190.84 +/- 6.07
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.0902   |
| time/               |          |
|    total_timesteps  | 659500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.18     |
|    n_updates        | 162374   |
----------------------------------
Eval num_timesteps=660000, episode_reward=197.81 +/- 0.51
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0892   |
| time/               |          |
|    total_timesteps  | 660000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.132    |
|    n_updates        | 162499   |
----------------------------------
Eval num_timesteps=660500, episode_reward=196.68 +/- 1.86
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0882   |
| time/               |          |
|    total_timesteps  | 660500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.123    |
|    n_updates        | 162624   |
----------------------------------
Eval num_timesteps=661000, episode_reward=135.37 +/- 54.48
Episode length: 266.54 +/- 229.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 267      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 661000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0734   |
|    n_updates        | 162749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 165      |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 51       |
|    time_elapsed     | 12836    |
|    total_timesteps  | 661009   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.201    |
|    n_updates        | 162752   |
----------------------------------
Eval num_timesteps=661500, episode_reward=197.30 +/- 0.99
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0862   |
| time/               |          |
|    total_timesteps  | 661500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.11     |
|    n_updates        | 162874   |
----------------------------------
Eval num_timesteps=662000, episode_reward=196.62 +/- 1.37
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0852   |
| time/               |          |
|    total_timesteps  | 662000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.247    |
|    n_updates        | 162999   |
----------------------------------
Eval num_timesteps=662500, episode_reward=153.33 +/- 53.91
Episode length: 338.14 +/- 229.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 338      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.0843   |
| time/               |          |
|    total_timesteps  | 662500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.288    |
|    n_updates        | 163124   |
----------------------------------
Eval num_timesteps=663000, episode_reward=98.27 +/- 32.58
Episode length: 112.26 +/- 138.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 98.3     |
| rollout/            |          |
|    exploration_rate | 0.0833   |
| time/               |          |
|    total_timesteps  | 663000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.153    |
|    n_updates        | 163249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | 166      |
|    exploration_rate | 0.083    |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 51       |
|    time_elapsed     | 12881    |
|    total_timesteps  | 663109   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0543   |
|    n_updates        | 163277   |
----------------------------------
Eval num_timesteps=663500, episode_reward=190.61 +/- 30.25
Episode length: 515.24 +/- 68.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.0823   |
| time/               |          |
|    total_timesteps  | 663500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.847    |
|    n_updates        | 163374   |
----------------------------------
Eval num_timesteps=664000, episode_reward=197.13 +/- 1.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0813   |
| time/               |          |
|    total_timesteps  | 664000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.414    |
|    n_updates        | 163499   |
----------------------------------
Eval num_timesteps=664500, episode_reward=197.52 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0803   |
| time/               |          |
|    total_timesteps  | 664500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.126    |
|    n_updates        | 163624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 165      |
|    exploration_rate | 0.0799   |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 51       |
|    time_elapsed     | 12926    |
|    total_timesteps  | 664711   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.174    |
|    n_updates        | 163677   |
----------------------------------
Eval num_timesteps=665000, episode_reward=196.61 +/- 1.23
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0793   |
| time/               |          |
|    total_timesteps  | 665000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.232    |
|    n_updates        | 163749   |
----------------------------------
Eval num_timesteps=665500, episode_reward=146.22 +/- 54.40
Episode length: 312.04 +/- 231.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 312      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.0783   |
| time/               |          |
|    total_timesteps  | 665500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.358    |
|    n_updates        | 163874   |
----------------------------------
Eval num_timesteps=666000, episode_reward=197.20 +/- 1.28
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0773   |
| time/               |          |
|    total_timesteps  | 666000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0994   |
|    n_updates        | 163999   |
----------------------------------
Eval num_timesteps=666500, episode_reward=196.75 +/- 1.19
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0763   |
| time/               |          |
|    total_timesteps  | 666500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.03     |
|    n_updates        | 164124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 166      |
|    exploration_rate | 0.0757   |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 51       |
|    time_elapsed     | 12981    |
|    total_timesteps  | 666811   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.199    |
|    n_updates        | 164202   |
----------------------------------
Eval num_timesteps=667000, episode_reward=196.66 +/- 13.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0753   |
| time/               |          |
|    total_timesteps  | 667000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.291    |
|    n_updates        | 164249   |
----------------------------------
Eval num_timesteps=667500, episode_reward=149.37 +/- 75.94
Episode length: 381.42 +/- 219.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 381      |
|    mean_reward      | 149      |
| rollout/            |          |
|    exploration_rate | 0.0744   |
| time/               |          |
|    total_timesteps  | 667500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0711   |
|    n_updates        | 164374   |
----------------------------------
Eval num_timesteps=668000, episode_reward=196.27 +/- 13.22
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.0734   |
| time/               |          |
|    total_timesteps  | 668000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0944   |
|    n_updates        | 164499   |
----------------------------------
Eval num_timesteps=668500, episode_reward=197.59 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0724   |
| time/               |          |
|    total_timesteps  | 668500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.101    |
|    n_updates        | 164624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 166      |
|    exploration_rate | 0.0716   |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 51       |
|    time_elapsed     | 13038    |
|    total_timesteps  | 668911   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0658   |
|    n_updates        | 164727   |
----------------------------------
Eval num_timesteps=669000, episode_reward=193.79 +/- 15.48
Episode length: 518.22 +/- 47.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 518      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.0714   |
| time/               |          |
|    total_timesteps  | 669000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.147    |
|    n_updates        | 164749   |
----------------------------------
Eval num_timesteps=669500, episode_reward=179.81 +/- 40.08
Episode length: 457.70 +/- 166.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 458      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.0704   |
| time/               |          |
|    total_timesteps  | 669500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0644   |
|    n_updates        | 164874   |
----------------------------------
Eval num_timesteps=670000, episode_reward=180.45 +/- 46.83
Episode length: 486.64 +/- 130.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.0694   |
| time/               |          |
|    total_timesteps  | 670000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.126    |
|    n_updates        | 164999   |
----------------------------------
Eval num_timesteps=670500, episode_reward=194.23 +/- 4.83
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.0684   |
| time/               |          |
|    total_timesteps  | 670500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0788   |
|    n_updates        | 165124   |
----------------------------------
Eval num_timesteps=671000, episode_reward=189.22 +/- 35.12
Episode length: 507.76 +/- 85.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 508      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.0674   |
| time/               |          |
|    total_timesteps  | 671000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.211    |
|    n_updates        | 165249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | 167      |
|    exploration_rate | 0.0674   |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 51       |
|    time_elapsed     | 13109    |
|    total_timesteps  | 671011   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.228    |
|    n_updates        | 165252   |
----------------------------------
Eval num_timesteps=671500, episode_reward=188.26 +/- 26.56
Episode length: 497.24 +/- 110.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 671500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.119    |
|    n_updates        | 165374   |
----------------------------------
Eval num_timesteps=672000, episode_reward=197.65 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0654   |
| time/               |          |
|    total_timesteps  | 672000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.497    |
|    n_updates        | 165499   |
----------------------------------
Eval num_timesteps=672500, episode_reward=197.66 +/- 0.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0645   |
| time/               |          |
|    total_timesteps  | 672500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.16     |
|    n_updates        | 165624   |
----------------------------------
Eval num_timesteps=673000, episode_reward=134.02 +/- 62.31
Episode length: 269.50 +/- 236.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 270      |
|    mean_reward      | 134      |
| rollout/            |          |
|    exploration_rate | 0.0635   |
| time/               |          |
|    total_timesteps  | 673000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.165    |
|    n_updates        | 165749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | 168      |
|    exploration_rate | 0.0632   |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 51       |
|    time_elapsed     | 13162    |
|    total_timesteps  | 673111   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.195    |
|    n_updates        | 165777   |
----------------------------------
Eval num_timesteps=673500, episode_reward=184.16 +/- 33.10
Episode length: 483.24 +/- 126.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 483      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.0625   |
| time/               |          |
|    total_timesteps  | 673500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.046    |
|    n_updates        | 165874   |
----------------------------------
Eval num_timesteps=674000, episode_reward=197.42 +/- 1.08
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0615   |
| time/               |          |
|    total_timesteps  | 674000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0953   |
|    n_updates        | 165999   |
----------------------------------
Eval num_timesteps=674500, episode_reward=120.09 +/- 50.05
Episode length: 197.20 +/- 215.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 120      |
| rollout/            |          |
|    exploration_rate | 0.0605   |
| time/               |          |
|    total_timesteps  | 674500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.141    |
|    n_updates        | 166124   |
----------------------------------
Eval num_timesteps=675000, episode_reward=103.47 +/- 87.57
Episode length: 250.68 +/- 233.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 251      |
|    mean_reward      | 103      |
| rollout/            |          |
|    exploration_rate | 0.0595   |
| time/               |          |
|    total_timesteps  | 675000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0997   |
|    n_updates        | 166249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | 169      |
|    exploration_rate | 0.0591   |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 51       |
|    time_elapsed     | 13205    |
|    total_timesteps  | 675211   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0717   |
|    n_updates        | 166302   |
----------------------------------
Eval num_timesteps=675500, episode_reward=197.70 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0585   |
| time/               |          |
|    total_timesteps  | 675500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 166374   |
----------------------------------
Eval num_timesteps=676000, episode_reward=190.95 +/- 26.17
Episode length: 496.06 +/- 114.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 676000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.267    |
|    n_updates        | 166499   |
----------------------------------
Eval num_timesteps=676500, episode_reward=188.74 +/- 6.37
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.0565   |
| time/               |          |
|    total_timesteps  | 676500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.135    |
|    n_updates        | 166624   |
----------------------------------
Eval num_timesteps=677000, episode_reward=179.49 +/- 44.64
Episode length: 449.58 +/- 172.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 450      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.0555   |
| time/               |          |
|    total_timesteps  | 677000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0692   |
|    n_updates        | 166749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | 171      |
|    exploration_rate | 0.0549   |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 51       |
|    time_elapsed     | 13263    |
|    total_timesteps  | 677311   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.197    |
|    n_updates        | 166827   |
----------------------------------
Eval num_timesteps=677500, episode_reward=145.40 +/- 79.38
Episode length: 382.48 +/- 218.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 382      |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 0.0546   |
| time/               |          |
|    total_timesteps  | 677500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.16     |
|    n_updates        | 166874   |
----------------------------------
Eval num_timesteps=678000, episode_reward=196.66 +/- 2.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0536   |
| time/               |          |
|    total_timesteps  | 678000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.175    |
|    n_updates        | 166999   |
----------------------------------
Eval num_timesteps=678500, episode_reward=197.59 +/- 0.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0526   |
| time/               |          |
|    total_timesteps  | 678500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.184    |
|    n_updates        | 167124   |
----------------------------------
Eval num_timesteps=679000, episode_reward=197.20 +/- 0.94
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0516   |
| time/               |          |
|    total_timesteps  | 679000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.162    |
|    n_updates        | 167249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | 172      |
|    exploration_rate | 0.0508   |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 51       |
|    time_elapsed     | 13320    |
|    total_timesteps  | 679411   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.124    |
|    n_updates        | 167352   |
----------------------------------
Eval num_timesteps=679500, episode_reward=197.31 +/- 1.19
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0506   |
| time/               |          |
|    total_timesteps  | 679500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.1      |
|    n_updates        | 167374   |
----------------------------------
Eval num_timesteps=680000, episode_reward=197.62 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0496   |
| time/               |          |
|    total_timesteps  | 680000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.233    |
|    n_updates        | 167499   |
----------------------------------
Eval num_timesteps=680500, episode_reward=194.42 +/- 15.91
Episode length: 518.74 +/- 43.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 519      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.0486   |
| time/               |          |
|    total_timesteps  | 680500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.183    |
|    n_updates        | 167624   |
----------------------------------
Eval num_timesteps=681000, episode_reward=197.39 +/- 0.91
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0476   |
| time/               |          |
|    total_timesteps  | 681000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0956   |
|    n_updates        | 167749   |
----------------------------------
Eval num_timesteps=681500, episode_reward=191.19 +/- 21.62
Episode length: 505.72 +/- 94.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.0466   |
| time/               |          |
|    total_timesteps  | 681500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0446   |
|    n_updates        | 167874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | 176      |
|    exploration_rate | 0.0466   |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 50       |
|    time_elapsed     | 13394    |
|    total_timesteps  | 681511   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.108    |
|    n_updates        | 167877   |
----------------------------------
Eval num_timesteps=682000, episode_reward=148.36 +/- 54.07
Episode length: 320.04 +/- 231.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 320      |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.0456   |
| time/               |          |
|    total_timesteps  | 682000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0288   |
|    n_updates        | 167999   |
----------------------------------
Eval num_timesteps=682500, episode_reward=137.68 +/- 54.47
Episode length: 273.30 +/- 232.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 273      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 0.0447   |
| time/               |          |
|    total_timesteps  | 682500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.136    |
|    n_updates        | 168124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | 176      |
|    exploration_rate | 0.0443   |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 50       |
|    time_elapsed     | 13412    |
|    total_timesteps  | 682659   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0251   |
|    n_updates        | 168164   |
----------------------------------
Eval num_timesteps=683000, episode_reward=197.16 +/- 1.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0437   |
| time/               |          |
|    total_timesteps  | 683000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.183    |
|    n_updates        | 168249   |
----------------------------------
Eval num_timesteps=683500, episode_reward=190.18 +/- 26.58
Episode length: 496.04 +/- 114.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.0427   |
| time/               |          |
|    total_timesteps  | 683500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.351    |
|    n_updates        | 168374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | 173      |
|    exploration_rate | 0.042    |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 50       |
|    time_elapsed     | 13442    |
|    total_timesteps  | 683827   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.121    |
|    n_updates        | 168456   |
----------------------------------
Eval num_timesteps=684000, episode_reward=188.41 +/- 26.19
Episode length: 495.42 +/- 117.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.0417   |
| time/               |          |
|    total_timesteps  | 684000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.112    |
|    n_updates        | 168499   |
----------------------------------
Eval num_timesteps=684500, episode_reward=197.62 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0407   |
| time/               |          |
|    total_timesteps  | 684500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0423   |
|    n_updates        | 168624   |
----------------------------------
Eval num_timesteps=685000, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0397   |
| time/               |          |
|    total_timesteps  | 685000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.194    |
|    n_updates        | 168749   |
----------------------------------
Eval num_timesteps=685500, episode_reward=197.62 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0387   |
| time/               |          |
|    total_timesteps  | 685500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.149    |
|    n_updates        | 168874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | 174      |
|    exploration_rate | 0.0379   |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 50       |
|    time_elapsed     | 13502    |
|    total_timesteps  | 685927   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.177    |
|    n_updates        | 168981   |
----------------------------------
Eval num_timesteps=686000, episode_reward=141.81 +/- 54.44
Episode length: 286.20 +/- 239.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 286      |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.0377   |
| time/               |          |
|    total_timesteps  | 686000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.128    |
|    n_updates        | 168999   |
----------------------------------
Eval num_timesteps=686500, episode_reward=145.45 +/- 55.08
Episode length: 313.72 +/- 229.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 314      |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 0.0367   |
| time/               |          |
|    total_timesteps  | 686500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0661   |
|    n_updates        | 169124   |
----------------------------------
Eval num_timesteps=687000, episode_reward=194.96 +/- 15.38
Episode length: 515.10 +/- 69.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.0357   |
| time/               |          |
|    total_timesteps  | 687000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0659   |
|    n_updates        | 169249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | 174      |
|    exploration_rate | 0.0356   |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 50       |
|    time_elapsed     | 13535    |
|    total_timesteps  | 687088   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.148    |
|    n_updates        | 169271   |
----------------------------------
Eval num_timesteps=687500, episode_reward=192.53 +/- 21.48
Episode length: 505.52 +/- 95.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.0348   |
| time/               |          |
|    total_timesteps  | 687500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.105    |
|    n_updates        | 169374   |
----------------------------------
Eval num_timesteps=688000, episode_reward=152.27 +/- 52.95
Episode length: 343.64 +/- 231.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 344      |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.0338   |
| time/               |          |
|    total_timesteps  | 688000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.239    |
|    n_updates        | 169499   |
----------------------------------
Eval num_timesteps=688500, episode_reward=197.74 +/- 0.98
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0328   |
| time/               |          |
|    total_timesteps  | 688500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.104    |
|    n_updates        | 169624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | 174      |
|    exploration_rate | 0.0324   |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 50       |
|    time_elapsed     | 13575    |
|    total_timesteps  | 688699   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0635   |
|    n_updates        | 169674   |
----------------------------------
Eval num_timesteps=689000, episode_reward=193.39 +/- 15.80
Episode length: 515.34 +/- 67.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.0318   |
| time/               |          |
|    total_timesteps  | 689000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0506   |
|    n_updates        | 169749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | 171      |
|    exploration_rate | 0.0311   |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 50       |
|    time_elapsed     | 13590    |
|    total_timesteps  | 689362   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0634   |
|    n_updates        | 169840   |
----------------------------------
Eval num_timesteps=689500, episode_reward=196.73 +/- 27.10
Episode length: 517.00 +/- 56.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 517      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0308   |
| time/               |          |
|    total_timesteps  | 689500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.148    |
|    n_updates        | 169874   |
----------------------------------
Eval num_timesteps=690000, episode_reward=93.99 +/- 26.14
Episode length: 93.70 +/- 110.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.7     |
|    mean_reward      | 94       |
| rollout/            |          |
|    exploration_rate | 0.0298   |
| time/               |          |
|    total_timesteps  | 690000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.848    |
|    n_updates        | 169999   |
----------------------------------
Eval num_timesteps=690500, episode_reward=137.56 +/- 54.34
Episode length: 276.58 +/- 229.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 277      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 0.0288   |
| time/               |          |
|    total_timesteps  | 690500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.173    |
|    n_updates        | 170124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 458      |
|    ep_rew_mean      | 170      |
|    exploration_rate | 0.0287   |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 50       |
|    time_elapsed     | 13616    |
|    total_timesteps  | 690543   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.223    |
|    n_updates        | 170135   |
----------------------------------
Eval num_timesteps=691000, episode_reward=197.82 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0278   |
| time/               |          |
|    total_timesteps  | 691000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0813   |
|    n_updates        | 170249   |
----------------------------------
Eval num_timesteps=691500, episode_reward=187.33 +/- 35.80
Episode length: 497.80 +/- 108.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.0268   |
| time/               |          |
|    total_timesteps  | 691500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.157    |
|    n_updates        | 170374   |
----------------------------------
Eval num_timesteps=692000, episode_reward=185.97 +/- 32.86
Episode length: 482.10 +/- 129.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 482      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.0258   |
| time/               |          |
|    total_timesteps  | 692000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0476   |
|    n_updates        | 170499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 454      |
|    ep_rew_mean      | 169      |
|    exploration_rate | 0.0255   |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 50       |
|    time_elapsed     | 13660    |
|    total_timesteps  | 692153   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.283    |
|    n_updates        | 170538   |
----------------------------------
Eval num_timesteps=692500, episode_reward=189.78 +/- 25.99
Episode length: 503.46 +/- 86.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 503      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.0249   |
| time/               |          |
|    total_timesteps  | 692500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.103    |
|    n_updates        | 170624   |
----------------------------------
Eval num_timesteps=693000, episode_reward=104.27 +/- 96.88
Episode length: 283.02 +/- 242.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 283      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 693000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.117    |
|    n_updates        | 170749   |
----------------------------------
Eval num_timesteps=693500, episode_reward=156.98 +/- 52.53
Episode length: 355.90 +/- 225.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 356      |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.0229   |
| time/               |          |
|    total_timesteps  | 693500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.143    |
|    n_updates        | 170874   |
----------------------------------
Eval num_timesteps=694000, episode_reward=23.19 +/- 72.99
Episode length: 113.92 +/- 166.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 23.2     |
| rollout/            |          |
|    exploration_rate | 0.0219   |
| time/               |          |
|    total_timesteps  | 694000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0791   |
|    n_updates        | 170999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 454      |
|    ep_rew_mean      | 170      |
|    exploration_rate | 0.0214   |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 50       |
|    time_elapsed     | 13698    |
|    total_timesteps  | 694253   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.416    |
|    n_updates        | 171063   |
----------------------------------
Eval num_timesteps=694500, episode_reward=184.34 +/- 42.24
Episode length: 485.60 +/- 133.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.0209   |
| time/               |          |
|    total_timesteps  | 694500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.201    |
|    n_updates        | 171124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 439      |
|    ep_rew_mean      | 166      |
|    exploration_rate | 0.0201   |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 50       |
|    time_elapsed     | 13712    |
|    total_timesteps  | 694906   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.142    |
|    n_updates        | 171226   |
----------------------------------
Eval num_timesteps=695000, episode_reward=197.29 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.0199   |
| time/               |          |
|    total_timesteps  | 695000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.419    |
|    n_updates        | 171249   |
----------------------------------
Eval num_timesteps=695500, episode_reward=197.61 +/- 0.75
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0189   |
| time/               |          |
|    total_timesteps  | 695500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.177    |
|    n_updates        | 171374   |
----------------------------------
Eval num_timesteps=696000, episode_reward=190.71 +/- 15.51
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.0179   |
| time/               |          |
|    total_timesteps  | 696000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0869   |
|    n_updates        | 171499   |
----------------------------------
Eval num_timesteps=696500, episode_reward=197.94 +/- 0.30
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.0169   |
| time/               |          |
|    total_timesteps  | 696500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.222    |
|    n_updates        | 171624   |
----------------------------------
Eval num_timesteps=697000, episode_reward=192.59 +/- 28.98
Episode length: 514.84 +/- 71.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.0159   |
| time/               |          |
|    total_timesteps  | 697000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0697   |
|    n_updates        | 171749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 167      |
|    exploration_rate | 0.0159   |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 50       |
|    time_elapsed     | 13787    |
|    total_timesteps  | 697006   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0379   |
|    n_updates        | 171751   |
----------------------------------
Eval num_timesteps=697500, episode_reward=192.15 +/- 30.73
Episode length: 496.00 +/- 114.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.015    |
| time/               |          |
|    total_timesteps  | 697500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.406    |
|    n_updates        | 171874   |
----------------------------------
Eval num_timesteps=698000, episode_reward=188.36 +/- 21.14
Episode length: 505.82 +/- 94.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.014    |
| time/               |          |
|    total_timesteps  | 698000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.313    |
|    n_updates        | 171999   |
----------------------------------
Eval num_timesteps=698500, episode_reward=167.85 +/- 57.47
Episode length: 399.06 +/- 212.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 399      |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.013    |
| time/               |          |
|    total_timesteps  | 698500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0726   |
|    n_updates        | 172124   |
----------------------------------
Eval num_timesteps=699000, episode_reward=187.86 +/- 29.64
Episode length: 485.14 +/- 135.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 485      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.012    |
| time/               |          |
|    total_timesteps  | 699000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0574   |
|    n_updates        | 172249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 168      |
|    exploration_rate | 0.0118   |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 50       |
|    time_elapsed     | 13842    |
|    total_timesteps  | 699106   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.133    |
|    n_updates        | 172276   |
----------------------------------
Eval num_timesteps=699500, episode_reward=197.68 +/- 1.18
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.011    |
| time/               |          |
|    total_timesteps  | 699500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.11     |
|    n_updates        | 172374   |
----------------------------------
Eval num_timesteps=700000, episode_reward=198.84 +/- 23.25
Episode length: 515.28 +/- 68.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 700000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0905   |
|    n_updates        | 172499   |
----------------------------------
Eval num_timesteps=700500, episode_reward=187.95 +/- 29.65
Episode length: 487.14 +/- 128.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 700500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.833    |
|    n_updates        | 172624   |
----------------------------------
Eval num_timesteps=701000, episode_reward=196.76 +/- 2.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 701000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0627   |
|    n_updates        | 172749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 167      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 50       |
|    time_elapsed     | 13902    |
|    total_timesteps  | 701206   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0449   |
|    n_updates        | 172801   |
----------------------------------
Eval num_timesteps=701500, episode_reward=135.28 +/- 54.11
Episode length: 260.56 +/- 234.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 261      |
|    mean_reward      | 135      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 701500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0742   |
|    n_updates        | 172874   |
----------------------------------
Eval num_timesteps=702000, episode_reward=106.93 +/- 89.09
Episode length: 272.76 +/- 234.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 273      |
|    mean_reward      | 107      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 702000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.157    |
|    n_updates        | 172999   |
----------------------------------
Eval num_timesteps=702500, episode_reward=150.76 +/- 66.72
Episode length: 428.02 +/- 194.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 428      |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 702500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.833    |
|    n_updates        | 173124   |
----------------------------------
Eval num_timesteps=703000, episode_reward=197.79 +/- 0.57
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 703000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.112    |
|    n_updates        | 173249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 444      |
|    ep_rew_mean      | 168      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 50       |
|    time_elapsed     | 13946    |
|    total_timesteps  | 703306   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0548   |
|    n_updates        | 173326   |
----------------------------------
Eval num_timesteps=703500, episode_reward=116.31 +/- 98.06
Episode length: 334.06 +/- 234.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 334      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 703500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.117    |
|    n_updates        | 173374   |
----------------------------------
Eval num_timesteps=704000, episode_reward=197.88 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 704000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.104    |
|    n_updates        | 173499   |
----------------------------------
Eval num_timesteps=704500, episode_reward=178.62 +/- 39.93
Episode length: 450.20 +/- 171.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 450      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 704500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.355    |
|    n_updates        | 173624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 439      |
|    ep_rew_mean      | 166      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 50       |
|    time_elapsed     | 13984    |
|    total_timesteps  | 704917   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.134    |
|    n_updates        | 173729   |
----------------------------------
Eval num_timesteps=705000, episode_reward=168.51 +/- 65.35
Episode length: 451.74 +/- 168.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 452      |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 705000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0406   |
|    n_updates        | 173749   |
----------------------------------
Eval num_timesteps=705500, episode_reward=179.12 +/- 48.86
Episode length: 479.38 +/- 131.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 705500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.121    |
|    n_updates        | 173874   |
----------------------------------
Eval num_timesteps=706000, episode_reward=45.45 +/- 82.61
Episode length: 149.78 +/- 189.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 45.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 706000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.127    |
|    n_updates        | 173999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 430      |
|    ep_rew_mean      | 162      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 50       |
|    time_elapsed     | 14016    |
|    total_timesteps  | 706075   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.182    |
|    n_updates        | 174018   |
----------------------------------
Eval num_timesteps=706500, episode_reward=189.73 +/- 25.86
Episode length: 495.96 +/- 115.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 706500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.155    |
|    n_updates        | 174124   |
----------------------------------
Eval num_timesteps=707000, episode_reward=196.52 +/- 2.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 707000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.896    |
|    n_updates        | 174249   |
----------------------------------
Eval num_timesteps=707500, episode_reward=109.73 +/- 45.14
Episode length: 159.10 +/- 195.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 707500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.61     |
|    n_updates        | 174374   |
----------------------------------
Eval num_timesteps=708000, episode_reward=175.44 +/- 38.84
Episode length: 456.42 +/- 170.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 456      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 708000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.129    |
|    n_updates        | 174499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 435      |
|    ep_rew_mean      | 163      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 50       |
|    time_elapsed     | 14063    |
|    total_timesteps  | 708175   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0756   |
|    n_updates        | 174543   |
----------------------------------
Eval num_timesteps=708500, episode_reward=104.30 +/- 38.91
Episode length: 134.12 +/- 171.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 708500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.952    |
|    n_updates        | 174624   |
----------------------------------
Eval num_timesteps=709000, episode_reward=197.81 +/- 0.56
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 709000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.059    |
|    n_updates        | 174749   |
----------------------------------
Eval num_timesteps=709500, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 709500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.161    |
|    n_updates        | 174874   |
----------------------------------
Eval num_timesteps=710000, episode_reward=197.62 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 710000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0913   |
|    n_updates        | 174999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 435      |
|    ep_rew_mean      | 164      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 50       |
|    time_elapsed     | 14114    |
|    total_timesteps  | 710275   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0871   |
|    n_updates        | 175068   |
----------------------------------
Eval num_timesteps=710500, episode_reward=161.71 +/- 50.97
Episode length: 369.18 +/- 227.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 369      |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 710500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0912   |
|    n_updates        | 175124   |
----------------------------------
Eval num_timesteps=711000, episode_reward=197.66 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 711000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.106    |
|    n_updates        | 175249   |
----------------------------------
Eval num_timesteps=711500, episode_reward=197.92 +/- 0.32
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 711500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0607   |
|    n_updates        | 175374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 430      |
|    ep_rew_mean      | 163      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 50       |
|    time_elapsed     | 14155    |
|    total_timesteps  | 711921   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.54     |
|    n_updates        | 175480   |
----------------------------------
Eval num_timesteps=712000, episode_reward=189.03 +/- 33.02
Episode length: 486.08 +/- 132.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 712000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.12     |
|    n_updates        | 175499   |
----------------------------------
Eval num_timesteps=712500, episode_reward=197.76 +/- 0.59
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 712500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0877   |
|    n_updates        | 175624   |
----------------------------------
Eval num_timesteps=713000, episode_reward=197.67 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 713000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.115    |
|    n_updates        | 175749   |
----------------------------------
Eval num_timesteps=713500, episode_reward=193.17 +/- 29.84
Episode length: 514.88 +/- 70.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 713500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0735   |
|    n_updates        | 175874   |
----------------------------------
Eval num_timesteps=714000, episode_reward=196.47 +/- 0.50
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 714000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.1      |
|    n_updates        | 175999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 430      |
|    ep_rew_mean      | 164      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 50       |
|    time_elapsed     | 14229    |
|    total_timesteps  | 714021   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.116    |
|    n_updates        | 176005   |
----------------------------------
Eval num_timesteps=714500, episode_reward=196.73 +/- 1.29
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 714500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.171    |
|    n_updates        | 176124   |
----------------------------------
Eval num_timesteps=715000, episode_reward=197.78 +/- 0.54
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 715000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.184    |
|    n_updates        | 176249   |
----------------------------------
Eval num_timesteps=715500, episode_reward=122.06 +/- 50.52
Episode length: 210.58 +/- 216.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 715500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.137    |
|    n_updates        | 176374   |
----------------------------------
Eval num_timesteps=716000, episode_reward=197.80 +/- 0.52
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 716000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.29     |
|    n_updates        | 176499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 430      |
|    ep_rew_mean      | 164      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 50       |
|    time_elapsed     | 14281    |
|    total_timesteps  | 716121   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.668    |
|    n_updates        | 176530   |
----------------------------------
Eval num_timesteps=716500, episode_reward=197.59 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 716500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.784    |
|    n_updates        | 176624   |
----------------------------------
Eval num_timesteps=717000, episode_reward=197.85 +/- 0.45
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 717000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.247    |
|    n_updates        | 176749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 421      |
|    ep_rew_mean      | 162      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 50       |
|    time_elapsed     | 14311    |
|    total_timesteps  | 717282   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.856    |
|    n_updates        | 176820   |
----------------------------------
Eval num_timesteps=717500, episode_reward=197.78 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 717500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.994    |
|    n_updates        | 176874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 407      |
|    ep_rew_mean      | 159      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 50       |
|    time_elapsed     | 14326    |
|    total_timesteps  | 717990   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.27     |
|    n_updates        | 176997   |
----------------------------------
Eval num_timesteps=718000, episode_reward=159.70 +/- 51.90
Episode length: 366.50 +/- 221.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 366      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 718000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.28     |
|    n_updates        | 176999   |
----------------------------------
Eval num_timesteps=718500, episode_reward=126.30 +/- 51.99
Episode length: 223.90 +/- 226.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 718500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.27     |
|    n_updates        | 177124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 393      |
|    ep_rew_mean      | 156      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 50       |
|    time_elapsed     | 14344    |
|    total_timesteps  | 718675   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.14     |
|    n_updates        | 177168   |
----------------------------------
Eval num_timesteps=719000, episode_reward=97.96 +/- 32.75
Episode length: 112.72 +/- 139.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 98       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 719000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.3      |
|    n_updates        | 177249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 379      |
|    ep_rew_mean      | 153      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 50       |
|    time_elapsed     | 14347    |
|    total_timesteps  | 719374   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.82     |
|    n_updates        | 177343   |
----------------------------------
Eval num_timesteps=719500, episode_reward=87.37 +/- 0.20
Episode length: 58.34 +/- 20.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.3     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 719500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.03     |
|    n_updates        | 177374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 369      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 50       |
|    time_elapsed     | 14349    |
|    total_timesteps  | 719589   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.9      |
|    n_updates        | 177397   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 361      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 50       |
|    time_elapsed     | 14350    |
|    total_timesteps  | 719881   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.36     |
|    n_updates        | 177470   |
----------------------------------
Eval num_timesteps=720000, episode_reward=81.39 +/- 23.49
Episode length: 58.46 +/- 19.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.5     |
|    mean_reward      | 81.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 720000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.83     |
|    n_updates        | 177499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 343      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 50       |
|    time_elapsed     | 14352    |
|    total_timesteps  | 720178   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.06     |
|    n_updates        | 177544   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 334      |
|    ep_rew_mean      | 144      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 50       |
|    time_elapsed     | 14352    |
|    total_timesteps  | 720466   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.43     |
|    n_updates        | 177616   |
----------------------------------
Eval num_timesteps=720500, episode_reward=86.87 +/- 0.83
Episode length: 55.30 +/- 21.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.3     |
|    mean_reward      | 86.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 720500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.87     |
|    n_updates        | 177624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 320      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 50       |
|    time_elapsed     | 14354    |
|    total_timesteps  | 720700   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.09     |
|    n_updates        | 177674   |
----------------------------------
Eval num_timesteps=721000, episode_reward=86.85 +/- 1.00
Episode length: 66.32 +/- 23.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.3     |
|    mean_reward      | 86.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 721000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.63     |
|    n_updates        | 177749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 316      |
|    ep_rew_mean      | 140      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 50       |
|    time_elapsed     | 14356    |
|    total_timesteps  | 721005   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.39     |
|    n_updates        | 177751   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 307      |
|    ep_rew_mean      | 138      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 50       |
|    time_elapsed     | 14356    |
|    total_timesteps  | 721253   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.36     |
|    n_updates        | 177813   |
----------------------------------
Eval num_timesteps=721500, episode_reward=87.09 +/- 0.81
Episode length: 68.74 +/- 21.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.7     |
|    mean_reward      | 87.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 721500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.17     |
|    n_updates        | 177874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 294      |
|    ep_rew_mean      | 135      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 50       |
|    time_elapsed     | 14359    |
|    total_timesteps  | 721591   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.59     |
|    n_updates        | 177897   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 276      |
|    ep_rew_mean      | 131      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 50       |
|    time_elapsed     | 14359    |
|    total_timesteps  | 721838   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 177959   |
----------------------------------
Eval num_timesteps=722000, episode_reward=87.11 +/- 0.76
Episode length: 67.36 +/- 23.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.4     |
|    mean_reward      | 87.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 722000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.6      |
|    n_updates        | 177999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 272      |
|    ep_rew_mean      | 131      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 50       |
|    time_elapsed     | 14361    |
|    total_timesteps  | 722119   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.37     |
|    n_updates        | 178029   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 254      |
|    ep_rew_mean      | 127      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 50       |
|    time_elapsed     | 14361    |
|    total_timesteps  | 722411   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.04     |
|    n_updates        | 178102   |
----------------------------------
Eval num_timesteps=722500, episode_reward=87.41 +/- 0.00
Episode length: 61.44 +/- 20.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.4     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 722500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.07     |
|    n_updates        | 178124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 236      |
|    ep_rew_mean      | 123      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 50       |
|    time_elapsed     | 14363    |
|    total_timesteps  | 722718   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.43     |
|    n_updates        | 178179   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 116      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 50       |
|    time_elapsed     | 14364    |
|    total_timesteps  | 722901   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.59     |
|    n_updates        | 178225   |
----------------------------------
Eval num_timesteps=723000, episode_reward=86.79 +/- 0.74
Episode length: 64.86 +/- 21.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.9     |
|    mean_reward      | 86.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 723000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.62     |
|    n_updates        | 178249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 112      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 50       |
|    time_elapsed     | 14366    |
|    total_timesteps  | 723080   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.47     |
|    n_updates        | 178269   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 108      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 50       |
|    time_elapsed     | 14366    |
|    total_timesteps  | 723353   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.69     |
|    n_updates        | 178338   |
----------------------------------
Eval num_timesteps=723500, episode_reward=56.91 +/- 45.03
Episode length: 54.64 +/- 19.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.6     |
|    mean_reward      | 56.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 723500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.72     |
|    n_updates        | 178374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 175      |
|    ep_rew_mean      | 106      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 50       |
|    time_elapsed     | 14368    |
|    total_timesteps  | 723555   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4        |
|    n_updates        | 178388   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 156      |
|    ep_rew_mean      | 102      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 50       |
|    time_elapsed     | 14368    |
|    total_timesteps  | 723805   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.35     |
|    n_updates        | 178451   |
----------------------------------
Eval num_timesteps=724000, episode_reward=86.55 +/- 1.25
Episode length: 58.96 +/- 19.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | 86.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 724000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.81     |
|    n_updates        | 178499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 97.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 50       |
|    time_elapsed     | 14370    |
|    total_timesteps  | 724032   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.27     |
|    n_updates        | 178507   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 93.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 50       |
|    time_elapsed     | 14370    |
|    total_timesteps  | 724249   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.32     |
|    n_updates        | 178562   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 89.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 50       |
|    time_elapsed     | 14370    |
|    total_timesteps  | 724482   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.23     |
|    n_updates        | 178620   |
----------------------------------
Eval num_timesteps=724500, episode_reward=87.11 +/- 0.67
Episode length: 63.88 +/- 22.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.9     |
|    mean_reward      | 87.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 724500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.7      |
|    n_updates        | 178624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 84.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 50       |
|    time_elapsed     | 14373    |
|    total_timesteps  | 724697   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.7      |
|    n_updates        | 178674   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 82.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2456     |
|    fps              | 50       |
|    time_elapsed     | 14373    |
|    total_timesteps  | 724909   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.75     |
|    n_updates        | 178727   |
----------------------------------
Eval num_timesteps=725000, episode_reward=86.85 +/- 0.85
Episode length: 61.02 +/- 18.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61       |
|    mean_reward      | 86.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 725000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.98     |
|    n_updates        | 178749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.6     |
|    ep_rew_mean      | 81.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2460     |
|    fps              | 50       |
|    time_elapsed     | 14375    |
|    total_timesteps  | 725148   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.33     |
|    n_updates        | 178786   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.6     |
|    ep_rew_mean      | 80.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2464     |
|    fps              | 50       |
|    time_elapsed     | 14375    |
|    total_timesteps  | 725439   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.93     |
|    n_updates        | 178859   |
----------------------------------
Eval num_timesteps=725500, episode_reward=85.05 +/- 13.82
Episode length: 61.24 +/- 23.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.2     |
|    mean_reward      | 85       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 725500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.24     |
|    n_updates        | 178874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.4     |
|    ep_rew_mean      | 79.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 50       |
|    time_elapsed     | 14377    |
|    total_timesteps  | 725715   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.52     |
|    n_updates        | 178928   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.3     |
|    ep_rew_mean      | 79.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 50       |
|    time_elapsed     | 14377    |
|    total_timesteps  | 725918   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.17     |
|    n_updates        | 178979   |
----------------------------------
Eval num_timesteps=726000, episode_reward=87.25 +/- 0.51
Episode length: 61.82 +/- 20.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.8     |
|    mean_reward      | 87.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 726000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.58     |
|    n_updates        | 178999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.1     |
|    ep_rew_mean      | 79.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2476     |
|    fps              | 50       |
|    time_elapsed     | 14379    |
|    total_timesteps  | 726189   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.52     |
|    n_updates        | 179047   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.3     |
|    ep_rew_mean      | 79.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2480     |
|    fps              | 50       |
|    time_elapsed     | 14380    |
|    total_timesteps  | 726407   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.86     |
|    n_updates        | 179101   |
----------------------------------
Eval num_timesteps=726500, episode_reward=87.37 +/- 0.28
Episode length: 71.66 +/- 25.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.7     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 726500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.59     |
|    n_updates        | 179124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.1     |
|    ep_rew_mean      | 79.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2484     |
|    fps              | 50       |
|    time_elapsed     | 14382    |
|    total_timesteps  | 726677   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.75     |
|    n_updates        | 179169   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.7     |
|    ep_rew_mean      | 78.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2488     |
|    fps              | 50       |
|    time_elapsed     | 14382    |
|    total_timesteps  | 726872   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.17     |
|    n_updates        | 179217   |
----------------------------------
Eval num_timesteps=727000, episode_reward=87.41 +/- 0.00
Episode length: 63.28 +/- 18.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.3     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 727000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.63     |
|    n_updates        | 179249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.6     |
|    ep_rew_mean      | 78.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2492     |
|    fps              | 50       |
|    time_elapsed     | 14384    |
|    total_timesteps  | 727170   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.96     |
|    n_updates        | 179292   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61       |
|    ep_rew_mean      | 78.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 50       |
|    time_elapsed     | 14385    |
|    total_timesteps  | 727351   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.36     |
|    n_updates        | 179337   |
----------------------------------
Eval num_timesteps=727500, episode_reward=94.33 +/- 35.40
Episode length: 109.60 +/- 139.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 94.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 727500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.03     |
|    n_updates        | 179374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.8     |
|    ep_rew_mean      | 78.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2500     |
|    fps              | 50       |
|    time_elapsed     | 14388    |
|    total_timesteps  | 727570   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.51     |
|    n_updates        | 179392   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.4     |
|    ep_rew_mean      | 77.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2504     |
|    fps              | 50       |
|    time_elapsed     | 14388    |
|    total_timesteps  | 727774   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.6      |
|    n_updates        | 179443   |
----------------------------------
Eval num_timesteps=728000, episode_reward=87.29 +/- 0.38
Episode length: 58.82 +/- 19.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.8     |
|    mean_reward      | 87.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 728000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.56     |
|    n_updates        | 179499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.5     |
|    ep_rew_mean      | 77.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2508     |
|    fps              | 50       |
|    time_elapsed     | 14390    |
|    total_timesteps  | 728067   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.16     |
|    n_updates        | 179516   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.4     |
|    ep_rew_mean      | 76.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2512     |
|    fps              | 50       |
|    time_elapsed     | 14390    |
|    total_timesteps  | 728350   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.34     |
|    n_updates        | 179587   |
----------------------------------
Eval num_timesteps=728500, episode_reward=53.81 +/- 59.33
Episode length: 82.32 +/- 113.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.3     |
|    mean_reward      | 53.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 728500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.21     |
|    n_updates        | 179624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.4     |
|    ep_rew_mean      | 76.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2516     |
|    fps              | 50       |
|    time_elapsed     | 14393    |
|    total_timesteps  | 728561   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.18     |
|    n_updates        | 179640   |
----------------------------------
Eval num_timesteps=729000, episode_reward=41.13 +/- 49.43
Episode length: 52.64 +/- 17.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.6     |
|    mean_reward      | 41.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 729000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.98     |
|    n_updates        | 179749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.6     |
|    ep_rew_mean      | 77.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2520     |
|    fps              | 50       |
|    time_elapsed     | 14395    |
|    total_timesteps  | 729258   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.33     |
|    n_updates        | 179814   |
----------------------------------
Eval num_timesteps=729500, episode_reward=55.29 +/- 75.53
Episode length: 132.60 +/- 177.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 55.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 729500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.15     |
|    n_updates        | 179874   |
----------------------------------
Eval num_timesteps=730000, episode_reward=87.25 +/- 0.37
Episode length: 62.94 +/- 24.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.9     |
|    mean_reward      | 87.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 730000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.2      |
|    n_updates        | 179999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.6     |
|    ep_rew_mean      | 77.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2524     |
|    fps              | 50       |
|    time_elapsed     | 14402    |
|    total_timesteps  | 730038   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.65     |
|    n_updates        | 180009   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 79.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2528     |
|    fps              | 50       |
|    time_elapsed     | 14402    |
|    total_timesteps  | 730294   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.05     |
|    n_updates        | 180073   |
----------------------------------
Eval num_timesteps=730500, episode_reward=92.13 +/- 55.12
Episode length: 139.20 +/- 173.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 92.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 730500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.64     |
|    n_updates        | 180124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 81.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2532     |
|    fps              | 50       |
|    time_elapsed     | 14406    |
|    total_timesteps  | 730560   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.76     |
|    n_updates        | 180139   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.5     |
|    ep_rew_mean      | 81.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2536     |
|    fps              | 50       |
|    time_elapsed     | 14406    |
|    total_timesteps  | 730850   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.56     |
|    n_updates        | 180212   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 80.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2540     |
|    fps              | 50       |
|    time_elapsed     | 14407    |
|    total_timesteps  | 730974   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.3      |
|    n_updates        | 180243   |
----------------------------------
Eval num_timesteps=731000, episode_reward=79.47 +/- 26.85
Episode length: 60.48 +/- 22.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.5     |
|    mean_reward      | 79.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 731000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.92     |
|    n_updates        | 180249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 80.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2544     |
|    fps              | 50       |
|    time_elapsed     | 14409    |
|    total_timesteps  | 731189   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.9      |
|    n_updates        | 180297   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 80.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2548     |
|    fps              | 50       |
|    time_elapsed     | 14409    |
|    total_timesteps  | 731484   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.09     |
|    n_updates        | 180370   |
----------------------------------
Eval num_timesteps=731500, episode_reward=86.79 +/- 1.31
Episode length: 56.46 +/- 17.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.5     |
|    mean_reward      | 86.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 731500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.35     |
|    n_updates        | 180374   |
----------------------------------
Eval num_timesteps=732000, episode_reward=94.50 +/- 64.68
Episode length: 165.78 +/- 191.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 94.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 732000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.86     |
|    n_updates        | 180499   |
----------------------------------
Eval num_timesteps=732500, episode_reward=163.50 +/- 50.62
Episode length: 401.46 +/- 201.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 401      |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 732500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.46     |
|    n_updates        | 180624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 82.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2552     |
|    fps              | 50       |
|    time_elapsed     | 14428    |
|    total_timesteps  | 732647   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.8      |
|    n_updates        | 180661   |
----------------------------------
Eval num_timesteps=733000, episode_reward=140.29 +/- 55.43
Episode length: 293.56 +/- 232.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 294      |
|    mean_reward      | 140      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 733000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.19     |
|    n_updates        | 180749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | 83.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2556     |
|    fps              | 50       |
|    time_elapsed     | 14437    |
|    total_timesteps  | 733346   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.16     |
|    n_updates        | 180836   |
----------------------------------
Eval num_timesteps=733500, episode_reward=87.37 +/- 0.28
Episode length: 70.28 +/- 30.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.3     |
|    mean_reward      | 87.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 733500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.06     |
|    n_updates        | 180874   |
----------------------------------
Eval num_timesteps=734000, episode_reward=136.30 +/- 54.64
Episode length: 271.48 +/- 234.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 271      |
|    mean_reward      | 136      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 734000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.51     |
|    n_updates        | 180999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.3     |
|    ep_rew_mean      | 84.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2560     |
|    fps              | 50       |
|    time_elapsed     | 14447    |
|    total_timesteps  | 734082   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.58     |
|    n_updates        | 181020   |
----------------------------------
Eval num_timesteps=734500, episode_reward=87.23 +/- 0.79
Episode length: 67.10 +/- 23.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.1     |
|    mean_reward      | 87.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 734500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.28     |
|    n_updates        | 181124   |
----------------------------------
Eval num_timesteps=735000, episode_reward=159.20 +/- 52.30
Episode length: 365.70 +/- 222.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 366      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 735000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.26     |
|    n_updates        | 181249   |
----------------------------------
Eval num_timesteps=735500, episode_reward=146.03 +/- 58.77
Episode length: 325.10 +/- 235.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 325      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 735500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.78     |
|    n_updates        | 181374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 87.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2564     |
|    fps              | 50       |
|    time_elapsed     | 14470    |
|    total_timesteps  | 735698   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.52     |
|    n_updates        | 181424   |
----------------------------------
Eval num_timesteps=736000, episode_reward=125.18 +/- 53.12
Episode length: 228.06 +/- 223.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 736000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.07     |
|    n_updates        | 181499   |
----------------------------------
Eval num_timesteps=736500, episode_reward=190.19 +/- 26.31
Episode length: 494.88 +/- 119.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 736500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.88     |
|    n_updates        | 181624   |
----------------------------------
Eval num_timesteps=737000, episode_reward=197.86 +/- 0.44
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 737000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.82     |
|    n_updates        | 181749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 90.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2568     |
|    fps              | 50       |
|    time_elapsed     | 14507    |
|    total_timesteps  | 737349   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.05     |
|    n_updates        | 181837   |
----------------------------------
Eval num_timesteps=737500, episode_reward=192.81 +/- 21.57
Episode length: 506.44 +/- 90.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 737500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.73     |
|    n_updates        | 181874   |
----------------------------------
Eval num_timesteps=738000, episode_reward=173.33 +/- 48.78
Episode length: 421.52 +/- 195.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 422      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 738000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.6      |
|    n_updates        | 181999   |
----------------------------------
Eval num_timesteps=738500, episode_reward=199.44 +/- 13.99
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 738500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.24     |
|    n_updates        | 182124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 93.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2572     |
|    fps              | 50       |
|    time_elapsed     | 14549    |
|    total_timesteps  | 738953   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.72     |
|    n_updates        | 182238   |
----------------------------------
Eval num_timesteps=739000, episode_reward=143.47 +/- 48.50
Episode length: 363.66 +/- 224.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 364      |
|    mean_reward      | 143      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 739000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.27     |
|    n_updates        | 182249   |
----------------------------------
Eval num_timesteps=739500, episode_reward=183.55 +/- 36.15
Episode length: 468.54 +/- 153.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 469      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 739500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.93     |
|    n_updates        | 182374   |
----------------------------------
Eval num_timesteps=740000, episode_reward=139.42 +/- 64.82
Episode length: 289.66 +/- 235.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 290      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 740000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.23     |
|    n_updates        | 182499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 95.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2576     |
|    fps              | 50       |
|    time_elapsed     | 14582    |
|    total_timesteps  | 740095   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.58     |
|    n_updates        | 182523   |
----------------------------------
Eval num_timesteps=740500, episode_reward=169.77 +/- 46.94
Episode length: 423.78 +/- 190.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 424      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 740500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.99     |
|    n_updates        | 182624   |
----------------------------------
Eval num_timesteps=741000, episode_reward=128.57 +/- 53.08
Episode length: 239.00 +/- 224.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 129      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 741000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.17     |
|    n_updates        | 182749   |
----------------------------------
Eval num_timesteps=741500, episode_reward=197.52 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 741500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.31     |
|    n_updates        | 182874   |
----------------------------------
Eval num_timesteps=742000, episode_reward=190.34 +/- 26.19
Episode length: 497.16 +/- 110.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 742000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.36     |
|    n_updates        | 182999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 100      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2580     |
|    fps              | 50       |
|    time_elapsed     | 14631    |
|    total_timesteps  | 742195   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.88     |
|    n_updates        | 183048   |
----------------------------------
Eval num_timesteps=742500, episode_reward=117.71 +/- 50.26
Episode length: 202.36 +/- 212.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 118      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 742500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.97     |
|    n_updates        | 183124   |
----------------------------------
Eval num_timesteps=743000, episode_reward=189.26 +/- 27.11
Episode length: 496.28 +/- 113.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 743000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.14     |
|    n_updates        | 183249   |
----------------------------------
Eval num_timesteps=743500, episode_reward=181.27 +/- 40.24
Episode length: 466.04 +/- 139.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 466      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 743500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.95     |
|    n_updates        | 183374   |
----------------------------------
Eval num_timesteps=744000, episode_reward=190.20 +/- 14.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 744000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.05     |
|    n_updates        | 183499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 104      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2584     |
|    fps              | 50       |
|    time_elapsed     | 14680    |
|    total_timesteps  | 744295   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.46     |
|    n_updates        | 183573   |
----------------------------------
Eval num_timesteps=744500, episode_reward=192.12 +/- 21.40
Episode length: 506.20 +/- 92.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 744500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.05     |
|    n_updates        | 183624   |
----------------------------------
Eval num_timesteps=745000, episode_reward=195.31 +/- 15.87
Episode length: 516.04 +/- 62.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 745000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.17     |
|    n_updates        | 183749   |
----------------------------------
Eval num_timesteps=745500, episode_reward=93.48 +/- 26.10
Episode length: 94.14 +/- 112.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.1     |
|    mean_reward      | 93.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 745500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.92     |
|    n_updates        | 183874   |
----------------------------------
Eval num_timesteps=746000, episode_reward=190.10 +/- 23.34
Episode length: 507.20 +/- 87.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 746000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.77     |
|    n_updates        | 183999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | 109      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2588     |
|    fps              | 50       |
|    time_elapsed     | 14727    |
|    total_timesteps  | 746395   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.15     |
|    n_updates        | 184098   |
----------------------------------
Eval num_timesteps=746500, episode_reward=197.29 +/- 1.42
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 746500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.2      |
|    n_updates        | 184124   |
----------------------------------
Eval num_timesteps=747000, episode_reward=191.84 +/- 22.73
Episode length: 507.82 +/- 84.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 508      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 747000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.48     |
|    n_updates        | 184249   |
----------------------------------
Eval num_timesteps=747500, episode_reward=190.47 +/- 21.95
Episode length: 506.38 +/- 91.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 747500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.76     |
|    n_updates        | 184374   |
----------------------------------
Eval num_timesteps=748000, episode_reward=195.05 +/- 16.41
Episode length: 515.20 +/- 68.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 748000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.78     |
|    n_updates        | 184499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 113      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2592     |
|    fps              | 50       |
|    time_elapsed     | 14787    |
|    total_timesteps  | 748495   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.04     |
|    n_updates        | 184623   |
----------------------------------
Eval num_timesteps=748500, episode_reward=197.18 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 748500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.85     |
|    n_updates        | 184624   |
----------------------------------
Eval num_timesteps=749000, episode_reward=111.78 +/- 46.35
Episode length: 169.48 +/- 200.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 169      |
|    mean_reward      | 112      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 749000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.64     |
|    n_updates        | 184749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 114      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2596     |
|    fps              | 50       |
|    time_elapsed     | 14807    |
|    total_timesteps  | 749177   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.1      |
|    n_updates        | 184794   |
----------------------------------
Eval num_timesteps=749500, episode_reward=187.33 +/- 29.86
Episode length: 487.48 +/- 127.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 749500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.8      |
|    n_updates        | 184874   |
----------------------------------
Eval num_timesteps=750000, episode_reward=109.65 +/- 45.68
Episode length: 160.68 +/- 194.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 161      |
|    mean_reward      | 110      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 750000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.55     |
|    n_updates        | 184999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 228      |
|    ep_rew_mean      | 116      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2600     |
|    fps              | 50       |
|    time_elapsed     | 14827    |
|    total_timesteps  | 750341   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.87     |
|    n_updates        | 185085   |
----------------------------------
Eval num_timesteps=750500, episode_reward=190.29 +/- 26.09
Episode length: 496.72 +/- 112.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 750500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.17     |
|    n_updates        | 185124   |
----------------------------------
Eval num_timesteps=751000, episode_reward=196.68 +/- 2.17
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 751000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.14     |
|    n_updates        | 185249   |
----------------------------------
Eval num_timesteps=751500, episode_reward=181.58 +/- 35.75
Episode length: 467.42 +/- 156.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 467      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 751500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.68     |
|    n_updates        | 185374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 242      |
|    ep_rew_mean      | 120      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2604     |
|    fps              | 50       |
|    time_elapsed     | 14870    |
|    total_timesteps  | 751971   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 10.7     |
|    n_updates        | 185492   |
----------------------------------
Eval num_timesteps=752000, episode_reward=189.80 +/- 33.08
Episode length: 505.34 +/- 96.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 752000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.8      |
|    n_updates        | 185499   |
----------------------------------
Eval num_timesteps=752500, episode_reward=173.20 +/- 45.26
Episode length: 430.76 +/- 188.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 431      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 752500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.79     |
|    n_updates        | 185624   |
----------------------------------
Eval num_timesteps=753000, episode_reward=190.36 +/- 26.02
Episode length: 495.72 +/- 115.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 753000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.08     |
|    n_updates        | 185749   |
----------------------------------
Eval num_timesteps=753500, episode_reward=156.26 +/- 55.83
Episode length: 347.64 +/- 227.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 348      |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 753500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.45     |
|    n_updates        | 185874   |
----------------------------------
Eval num_timesteps=754000, episode_reward=181.56 +/- 38.84
Episode length: 475.56 +/- 137.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 754000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.72     |
|    n_updates        | 185999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 260      |
|    ep_rew_mean      | 124      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2608     |
|    fps              | 50       |
|    time_elapsed     | 14935    |
|    total_timesteps  | 754071   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.13     |
|    n_updates        | 186017   |
----------------------------------
Eval num_timesteps=754500, episode_reward=160.39 +/- 51.61
Episode length: 380.60 +/- 211.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 381      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 754500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.38     |
|    n_updates        | 186124   |
----------------------------------
Eval num_timesteps=755000, episode_reward=141.12 +/- 55.51
Episode length: 292.94 +/- 232.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 293      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 755000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 12.2     |
|    n_updates        | 186249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 269      |
|    ep_rew_mean      | 128      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 50       |
|    time_elapsed     | 14955    |
|    total_timesteps  | 755280   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.27     |
|    n_updates        | 186319   |
----------------------------------
Eval num_timesteps=755500, episode_reward=195.59 +/- 15.61
Episode length: 515.18 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 755500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.68     |
|    n_updates        | 186374   |
----------------------------------
Eval num_timesteps=756000, episode_reward=192.67 +/- 21.52
Episode length: 505.84 +/- 93.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 756000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 10.2     |
|    n_updates        | 186499   |
----------------------------------
Eval num_timesteps=756500, episode_reward=192.41 +/- 22.00
Episode length: 507.00 +/- 88.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 756500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.94     |
|    n_updates        | 186624   |
----------------------------------
Eval num_timesteps=757000, episode_reward=174.77 +/- 48.90
Episode length: 439.78 +/- 174.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 757000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.67     |
|    n_updates        | 186749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 288      |
|    ep_rew_mean      | 132      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 50       |
|    time_elapsed     | 15013    |
|    total_timesteps  | 757380   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.68     |
|    n_updates        | 186844   |
----------------------------------
Eval num_timesteps=757500, episode_reward=190.65 +/- 26.80
Episode length: 499.14 +/- 104.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 499      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 757500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.43     |
|    n_updates        | 186874   |
----------------------------------
Eval num_timesteps=758000, episode_reward=189.36 +/- 26.30
Episode length: 495.98 +/- 114.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 758000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.06     |
|    n_updates        | 186999   |
----------------------------------
Eval num_timesteps=758500, episode_reward=154.75 +/- 52.28
Episode length: 355.24 +/- 226.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 355      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 758500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.51     |
|    n_updates        | 187124   |
----------------------------------
Eval num_timesteps=759000, episode_reward=190.53 +/- 26.17
Episode length: 496.44 +/- 113.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 759000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.63     |
|    n_updates        | 187249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 302      |
|    ep_rew_mean      | 138      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2620     |
|    fps              | 50       |
|    time_elapsed     | 15067    |
|    total_timesteps  | 759480   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 11.1     |
|    n_updates        | 187369   |
----------------------------------
Eval num_timesteps=759500, episode_reward=97.07 +/- 32.49
Episode length: 107.30 +/- 140.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 97.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 759500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.83     |
|    n_updates        | 187374   |
----------------------------------
Eval num_timesteps=760000, episode_reward=181.73 +/- 25.59
Episode length: 496.66 +/- 112.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 760000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.25     |
|    n_updates        | 187499   |
----------------------------------
Eval num_timesteps=760500, episode_reward=192.94 +/- 21.59
Episode length: 506.32 +/- 91.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 760500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.57     |
|    n_updates        | 187624   |
----------------------------------
Eval num_timesteps=761000, episode_reward=197.38 +/- 0.70
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 761000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.59     |
|    n_updates        | 187749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 311      |
|    ep_rew_mean      | 141      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2624     |
|    fps              | 50       |
|    time_elapsed     | 15114    |
|    total_timesteps  | 761102   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.27     |
|    n_updates        | 187775   |
----------------------------------
Eval num_timesteps=761500, episode_reward=130.47 +/- 53.64
Episode length: 247.32 +/- 227.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 247      |
|    mean_reward      | 130      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 761500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.64     |
|    n_updates        | 187874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 316      |
|    ep_rew_mean      | 142      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2628     |
|    fps              | 50       |
|    time_elapsed     | 15122    |
|    total_timesteps  | 761865   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 13.8     |
|    n_updates        | 187966   |
----------------------------------
Eval num_timesteps=762000, episode_reward=98.08 +/- 32.89
Episode length: 108.52 +/- 140.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 98.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 762000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.86     |
|    n_updates        | 187999   |
----------------------------------
Eval num_timesteps=762500, episode_reward=90.23 +/- 21.05
Episode length: 83.74 +/- 93.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.7     |
|    mean_reward      | 90.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 762500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.01     |
|    n_updates        | 188124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 320      |
|    ep_rew_mean      | 143      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2632     |
|    fps              | 50       |
|    time_elapsed     | 15128    |
|    total_timesteps  | 762529   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.79     |
|    n_updates        | 188132   |
----------------------------------
Eval num_timesteps=763000, episode_reward=93.03 +/- 22.50
Episode length: 91.96 +/- 111.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 93       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 763000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.34     |
|    n_updates        | 188249   |
----------------------------------
Eval num_timesteps=763500, episode_reward=158.67 +/- 52.88
Episode length: 364.88 +/- 223.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 365      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 763500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 10.2     |
|    n_updates        | 188374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 329      |
|    ep_rew_mean      | 145      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2636     |
|    fps              | 50       |
|    time_elapsed     | 15142    |
|    total_timesteps  | 763716   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 8.29     |
|    n_updates        | 188428   |
----------------------------------
Eval num_timesteps=764000, episode_reward=185.89 +/- 33.05
Episode length: 478.00 +/- 141.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 764000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7        |
|    n_updates        | 188499   |
----------------------------------
Eval num_timesteps=764500, episode_reward=182.25 +/- 42.47
Episode length: 459.54 +/- 162.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 460      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 764500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.46     |
|    n_updates        | 188624   |
----------------------------------
Eval num_timesteps=765000, episode_reward=150.05 +/- 53.84
Episode length: 325.34 +/- 234.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 325      |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 765000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.1      |
|    n_updates        | 188749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 344      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2640     |
|    fps              | 50       |
|    time_elapsed     | 15179    |
|    total_timesteps  | 765368   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.65     |
|    n_updates        | 188841   |
----------------------------------
Eval num_timesteps=765500, episode_reward=184.66 +/- 33.15
Episode length: 478.70 +/- 139.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 765500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.86     |
|    n_updates        | 188874   |
----------------------------------
Eval num_timesteps=766000, episode_reward=164.32 +/- 50.36
Episode length: 381.44 +/- 219.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 381      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 766000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 9.94     |
|    n_updates        | 188999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 348      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2644     |
|    fps              | 50       |
|    time_elapsed     | 15204    |
|    total_timesteps  | 766017   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.56     |
|    n_updates        | 189004   |
----------------------------------
Eval num_timesteps=766500, episode_reward=139.33 +/- 55.19
Episode length: 281.68 +/- 234.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 282      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 766500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 7.97     |
|    n_updates        | 189124   |
----------------------------------
Eval num_timesteps=767000, episode_reward=190.31 +/- 26.40
Episode length: 496.94 +/- 111.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 767000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.26     |
|    n_updates        | 189249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 357      |
|    ep_rew_mean      | 153      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2648     |
|    fps              | 50       |
|    time_elapsed     | 15227    |
|    total_timesteps  | 767152   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.34     |
|    n_updates        | 189287   |
----------------------------------
Eval num_timesteps=767500, episode_reward=91.43 +/- 21.47
Episode length: 84.48 +/- 92.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.5     |
|    mean_reward      | 91.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 767500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 6.04     |
|    n_updates        | 189374   |
----------------------------------
Eval num_timesteps=768000, episode_reward=155.11 +/- 53.38
Episode length: 348.88 +/- 225.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 349      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 768000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.97     |
|    n_updates        | 189499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 356      |
|    ep_rew_mean      | 153      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2652     |
|    fps              | 50       |
|    time_elapsed     | 15241    |
|    total_timesteps  | 768287   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.56     |
|    n_updates        | 189571   |
----------------------------------
Eval num_timesteps=768500, episode_reward=197.63 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 768500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.54     |
|    n_updates        | 189624   |
----------------------------------
Eval num_timesteps=769000, episode_reward=194.84 +/- 15.67
Episode length: 516.18 +/- 61.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 769000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.95     |
|    n_updates        | 189749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 361      |
|    ep_rew_mean      | 155      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2656     |
|    fps              | 50       |
|    time_elapsed     | 15271    |
|    total_timesteps  | 769455   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.49     |
|    n_updates        | 189863   |
----------------------------------
Eval num_timesteps=769500, episode_reward=91.61 +/- 21.40
Episode length: 80.96 +/- 93.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81       |
|    mean_reward      | 91.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 769500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.11     |
|    n_updates        | 189874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 357      |
|    ep_rew_mean      | 154      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2660     |
|    fps              | 50       |
|    time_elapsed     | 15274    |
|    total_timesteps  | 769768   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.07     |
|    n_updates        | 189941   |
----------------------------------
Eval num_timesteps=770000, episode_reward=176.78 +/- 42.26
Episode length: 443.18 +/- 175.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 443      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 770000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.76     |
|    n_updates        | 189999   |
----------------------------------
Eval num_timesteps=770500, episode_reward=195.04 +/- 15.84
Episode length: 515.38 +/- 67.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 770500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.64     |
|    n_updates        | 190124   |
----------------------------------
Eval num_timesteps=771000, episode_reward=183.72 +/- 35.83
Episode length: 469.00 +/- 151.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 469      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 771000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.41     |
|    n_updates        | 190249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 357      |
|    ep_rew_mean      | 154      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2664     |
|    fps              | 50       |
|    time_elapsed     | 15316    |
|    total_timesteps  | 771393   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.27     |
|    n_updates        | 190348   |
----------------------------------
Eval num_timesteps=771500, episode_reward=188.05 +/- 30.45
Episode length: 488.32 +/- 124.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 488      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 771500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.06     |
|    n_updates        | 190374   |
----------------------------------
Eval num_timesteps=772000, episode_reward=133.39 +/- 57.65
Episode length: 266.48 +/- 229.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 266      |
|    mean_reward      | 133      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 772000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5        |
|    n_updates        | 190499   |
----------------------------------
Eval num_timesteps=772500, episode_reward=187.64 +/- 29.79
Episode length: 487.52 +/- 127.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 488      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 772500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 190624   |
----------------------------------
Eval num_timesteps=773000, episode_reward=188.60 +/- 29.85
Episode length: 487.70 +/- 126.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 488      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 773000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.37     |
|    n_updates        | 190749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 361      |
|    ep_rew_mean      | 154      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2668     |
|    fps              | 50       |
|    time_elapsed     | 15366    |
|    total_timesteps  | 773493   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.84     |
|    n_updates        | 190873   |
----------------------------------
Eval num_timesteps=773500, episode_reward=155.43 +/- 60.64
Episode length: 365.38 +/- 223.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 365      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 773500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.04     |
|    n_updates        | 190874   |
----------------------------------
Eval num_timesteps=774000, episode_reward=134.09 +/- 54.76
Episode length: 263.76 +/- 232.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 264      |
|    mean_reward      | 134      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 774000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.55     |
|    n_updates        | 190999   |
----------------------------------
Eval num_timesteps=774500, episode_reward=115.94 +/- 49.12
Episode length: 189.94 +/- 210.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 774500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 191124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 357      |
|    ep_rew_mean      | 153      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2672     |
|    fps              | 50       |
|    time_elapsed     | 15391    |
|    total_timesteps  | 774695   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.45     |
|    n_updates        | 191173   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 348      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2676     |
|    fps              | 50       |
|    time_elapsed     | 15391    |
|    total_timesteps  | 774941   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.44     |
|    n_updates        | 191235   |
----------------------------------
Eval num_timesteps=775000, episode_reward=192.71 +/- 21.94
Episode length: 507.18 +/- 87.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 775000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.985    |
|    n_updates        | 191249   |
----------------------------------
Eval num_timesteps=775500, episode_reward=141.05 +/- 54.60
Episode length: 293.72 +/- 231.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 294      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 775500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.05     |
|    n_updates        | 191374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 336      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2680     |
|    fps              | 50       |
|    time_elapsed     | 15414    |
|    total_timesteps  | 775748   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.17     |
|    n_updates        | 191436   |
----------------------------------
Eval num_timesteps=776000, episode_reward=166.28 +/- 49.37
Episode length: 395.24 +/- 208.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 395      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 776000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 191499   |
----------------------------------
Eval num_timesteps=776500, episode_reward=100.03 +/- 35.55
Episode length: 115.94 +/- 152.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 100      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 776500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.3      |
|    n_updates        | 191624   |
----------------------------------
Eval num_timesteps=777000, episode_reward=174.47 +/- 43.95
Episode length: 432.38 +/- 185.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 432      |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 777000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.778    |
|    n_updates        | 191749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 332      |
|    ep_rew_mean      | 148      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2684     |
|    fps              | 50       |
|    time_elapsed     | 15442    |
|    total_timesteps  | 777474   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.989    |
|    n_updates        | 191868   |
----------------------------------
Eval num_timesteps=777500, episode_reward=197.51 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 777500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 5.74     |
|    n_updates        | 191874   |
----------------------------------
Eval num_timesteps=778000, episode_reward=192.44 +/- 21.49
Episode length: 506.04 +/- 93.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 778000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.671    |
|    n_updates        | 191999   |
----------------------------------
Eval num_timesteps=778500, episode_reward=182.04 +/- 43.35
Episode length: 467.02 +/- 157.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 467      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 778500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.03     |
|    n_updates        | 192124   |
----------------------------------
Eval num_timesteps=779000, episode_reward=181.42 +/- 38.18
Episode length: 460.20 +/- 160.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 460      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 779000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.43     |
|    n_updates        | 192249   |
----------------------------------
Eval num_timesteps=779500, episode_reward=115.73 +/- 47.82
Episode length: 183.78 +/- 203.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 184      |
|    mean_reward      | 116      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 779500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 4.12     |
|    n_updates        | 192374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 332      |
|    ep_rew_mean      | 147      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2688     |
|    fps              | 50       |
|    time_elapsed     | 15505    |
|    total_timesteps  | 779574   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.28     |
|    n_updates        | 192393   |
----------------------------------
Eval num_timesteps=780000, episode_reward=164.43 +/- 49.11
Episode length: 393.36 +/- 211.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 393      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 780000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.572    |
|    n_updates        | 192499   |
----------------------------------
Eval num_timesteps=780500, episode_reward=50.35 +/- 60.51
Episode length: 83.98 +/- 112.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84       |
|    mean_reward      | 50.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 780500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.946    |
|    n_updates        | 192624   |
----------------------------------
Eval num_timesteps=781000, episode_reward=159.04 +/- 51.72
Episode length: 364.56 +/- 223.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 365      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 781000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.763    |
|    n_updates        | 192749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 327      |
|    ep_rew_mean      | 146      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2692     |
|    fps              | 50       |
|    time_elapsed     | 15530    |
|    total_timesteps  | 781212   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.255    |
|    n_updates        | 192802   |
----------------------------------
Eval num_timesteps=781500, episode_reward=187.00 +/- 3.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 781500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.491    |
|    n_updates        | 192874   |
----------------------------------
Eval num_timesteps=782000, episode_reward=160.80 +/- 51.12
Episode length: 364.74 +/- 223.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 365      |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 782000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.381    |
|    n_updates        | 192999   |
----------------------------------
Eval num_timesteps=782500, episode_reward=132.94 +/- 53.72
Episode length: 254.28 +/- 230.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 254      |
|    mean_reward      | 133      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 782500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.55     |
|    n_updates        | 193124   |
----------------------------------
Eval num_timesteps=783000, episode_reward=167.31 +/- 48.90
Episode length: 403.26 +/- 205.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 403      |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 783000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.67     |
|    n_updates        | 193249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 341      |
|    ep_rew_mean      | 149      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2696     |
|    fps              | 50       |
|    time_elapsed     | 15576    |
|    total_timesteps  | 783312   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.867    |
|    n_updates        | 193327   |
----------------------------------
Eval num_timesteps=783500, episode_reward=179.95 +/- 37.86
Episode length: 456.68 +/- 169.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 457      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 783500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.59     |
|    n_updates        | 193374   |
----------------------------------
Eval num_timesteps=784000, episode_reward=155.39 +/- 52.39
Episode length: 354.60 +/- 227.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 355      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 784000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 3.11     |
|    n_updates        | 193499   |
----------------------------------
Eval num_timesteps=784500, episode_reward=194.06 +/- 15.47
Episode length: 515.62 +/- 65.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 784500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.788    |
|    n_updates        | 193624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 346      |
|    ep_rew_mean      | 150      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2700     |
|    fps              | 50       |
|    time_elapsed     | 15615    |
|    total_timesteps  | 784962   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.63     |
|    n_updates        | 193740   |
----------------------------------
Eval num_timesteps=785000, episode_reward=193.18 +/- 22.02
Episode length: 507.28 +/- 86.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 785000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 193749   |
----------------------------------
Eval num_timesteps=785500, episode_reward=190.90 +/- 21.78
Episode length: 505.80 +/- 94.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 785500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.48     |
|    n_updates        | 193874   |
----------------------------------
Eval num_timesteps=786000, episode_reward=176.69 +/- 40.97
Episode length: 450.52 +/- 170.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 451      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 786000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.54     |
|    n_updates        | 193999   |
----------------------------------
Eval num_timesteps=786500, episode_reward=89.58 +/- 85.85
Episode length: 210.18 +/- 226.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 89.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 786500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.53     |
|    n_updates        | 194124   |
----------------------------------
Eval num_timesteps=787000, episode_reward=193.31 +/- 21.30
Episode length: 515.22 +/- 68.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 787000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.77     |
|    n_updates        | 194249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 351      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2704     |
|    fps              | 50       |
|    time_elapsed     | 15679    |
|    total_timesteps  | 787062   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.639    |
|    n_updates        | 194265   |
----------------------------------
Eval num_timesteps=787500, episode_reward=192.88 +/- 15.76
Episode length: 515.06 +/- 69.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 787500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.651    |
|    n_updates        | 194374   |
----------------------------------
Eval num_timesteps=788000, episode_reward=197.41 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 788000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.813    |
|    n_updates        | 194499   |
----------------------------------
Eval num_timesteps=788500, episode_reward=176.86 +/- 41.13
Episode length: 487.88 +/- 126.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 488      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 788500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.16     |
|    n_updates        | 194624   |
----------------------------------
Eval num_timesteps=789000, episode_reward=159.65 +/- 51.00
Episode length: 374.22 +/- 220.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 374      |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 789000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.728    |
|    n_updates        | 194749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 351      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2708     |
|    fps              | 50       |
|    time_elapsed     | 15734    |
|    total_timesteps  | 789162   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.16     |
|    n_updates        | 194790   |
----------------------------------
Eval num_timesteps=789500, episode_reward=194.51 +/- 15.33
Episode length: 515.12 +/- 69.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 789500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.403    |
|    n_updates        | 194874   |
----------------------------------
Eval num_timesteps=790000, episode_reward=193.68 +/- 15.29
Episode length: 515.02 +/- 69.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 790000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.34     |
|    n_updates        | 194999   |
----------------------------------
Eval num_timesteps=790500, episode_reward=154.68 +/- 52.80
Episode length: 345.56 +/- 229.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 346      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 790500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.538    |
|    n_updates        | 195124   |
----------------------------------
Eval num_timesteps=791000, episode_reward=188.12 +/- 29.71
Episode length: 489.08 +/- 121.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 489      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 791000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.453    |
|    n_updates        | 195249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 360      |
|    ep_rew_mean      | 152      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2712     |
|    fps              | 50       |
|    time_elapsed     | 15788    |
|    total_timesteps  | 791262   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.904    |
|    n_updates        | 195315   |
----------------------------------
Eval num_timesteps=791500, episode_reward=194.18 +/- 15.28
Episode length: 515.52 +/- 66.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 791500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.276    |
|    n_updates        | 195374   |
----------------------------------
Eval num_timesteps=792000, episode_reward=192.26 +/- 21.42
Episode length: 505.28 +/- 96.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 792000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.514    |
|    n_updates        | 195499   |
----------------------------------
Eval num_timesteps=792500, episode_reward=195.03 +/- 15.40
Episode length: 516.34 +/- 60.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 792500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.05     |
|    n_updates        | 195624   |
----------------------------------
Eval num_timesteps=793000, episode_reward=183.49 +/- 35.49
Episode length: 467.80 +/- 155.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 468      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 793000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.708    |
|    n_updates        | 195749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 360      |
|    ep_rew_mean      | 152      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2716     |
|    fps              | 50       |
|    time_elapsed     | 15847    |
|    total_timesteps  | 793362   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.274    |
|    n_updates        | 195840   |
----------------------------------
Eval num_timesteps=793500, episode_reward=194.76 +/- 15.38
Episode length: 515.66 +/- 65.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 793500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.71     |
|    n_updates        | 195874   |
----------------------------------
Eval num_timesteps=794000, episode_reward=196.25 +/- 1.11
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 794000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.23     |
|    n_updates        | 195999   |
----------------------------------
Eval num_timesteps=794500, episode_reward=139.67 +/- 54.43
Episode length: 281.80 +/- 233.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 282      |
|    mean_reward      | 140      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 794500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.614    |
|    n_updates        | 196124   |
----------------------------------
Eval num_timesteps=795000, episode_reward=192.53 +/- 21.47
Episode length: 506.86 +/- 88.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 795000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.658    |
|    n_updates        | 196249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 360      |
|    ep_rew_mean      | 152      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2720     |
|    fps              | 50       |
|    time_elapsed     | 15900    |
|    total_timesteps  | 795462   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.52     |
|    n_updates        | 196365   |
----------------------------------
Eval num_timesteps=795500, episode_reward=183.52 +/- 32.37
Episode length: 479.48 +/- 136.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 795500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.36     |
|    n_updates        | 196374   |
----------------------------------
Eval num_timesteps=796000, episode_reward=194.76 +/- 15.36
Episode length: 515.36 +/- 67.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 796000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.934    |
|    n_updates        | 196499   |
----------------------------------
Eval num_timesteps=796500, episode_reward=163.53 +/- 50.11
Episode length: 384.32 +/- 215.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 384      |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 796500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.675    |
|    n_updates        | 196624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 355      |
|    ep_rew_mean      | 151      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2724     |
|    fps              | 49       |
|    time_elapsed     | 15939    |
|    total_timesteps  | 796619   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.483    |
|    n_updates        | 196654   |
----------------------------------
Eval num_timesteps=797000, episode_reward=189.99 +/- 26.52
Episode length: 497.40 +/- 109.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 797000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.8      |
|    n_updates        | 196749   |
----------------------------------
Eval num_timesteps=797500, episode_reward=179.02 +/- 39.98
Episode length: 448.82 +/- 174.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 449      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 797500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.29     |
|    n_updates        | 196874   |
----------------------------------
Eval num_timesteps=798000, episode_reward=192.06 +/- 21.57
Episode length: 505.34 +/- 96.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 798000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.617    |
|    n_updates        | 196999   |
----------------------------------
Eval num_timesteps=798500, episode_reward=179.06 +/- 40.00
Episode length: 450.34 +/- 171.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 450      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 798500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.286    |
|    n_updates        | 197124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 369      |
|    ep_rew_mean      | 155      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2728     |
|    fps              | 49       |
|    time_elapsed     | 15995    |
|    total_timesteps  | 798719   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.54     |
|    n_updates        | 197179   |
----------------------------------
Eval num_timesteps=799000, episode_reward=191.75 +/- 21.62
Episode length: 505.40 +/- 96.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 799000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.478    |
|    n_updates        | 197249   |
----------------------------------
Eval num_timesteps=799500, episode_reward=197.63 +/- 0.90
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 799500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.653    |
|    n_updates        | 197374   |
----------------------------------
Eval num_timesteps=800000, episode_reward=195.18 +/- 15.42
Episode length: 514.90 +/- 70.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 800000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.26     |
|    n_updates        | 197499   |
----------------------------------
Eval num_timesteps=800500, episode_reward=196.57 +/- 1.71
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 800500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.369    |
|    n_updates        | 197624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 383      |
|    ep_rew_mean      | 158      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2732     |
|    fps              | 49       |
|    time_elapsed     | 16055    |
|    total_timesteps  | 800819   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.864    |
|    n_updates        | 197704   |
----------------------------------
Eval num_timesteps=801000, episode_reward=193.40 +/- 15.41
Episode length: 514.94 +/- 70.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 801000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.03     |
|    n_updates        | 197749   |
----------------------------------
Eval num_timesteps=801500, episode_reward=147.11 +/- 78.84
Episode length: 383.26 +/- 216.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 383      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 801500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.642    |
|    n_updates        | 197874   |
----------------------------------
Eval num_timesteps=802000, episode_reward=197.24 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 802000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.345    |
|    n_updates        | 197999   |
----------------------------------
Eval num_timesteps=802500, episode_reward=172.46 +/- 45.17
Episode length: 422.04 +/- 194.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 422      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 802500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.935    |
|    n_updates        | 198124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 392      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2736     |
|    fps              | 49       |
|    time_elapsed     | 16109    |
|    total_timesteps  | 802919   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 198229   |
----------------------------------
Eval num_timesteps=803000, episode_reward=167.99 +/- 47.86
Episode length: 398.60 +/- 213.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 399      |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 803000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.516    |
|    n_updates        | 198249   |
----------------------------------
Eval num_timesteps=803500, episode_reward=176.58 +/- 41.84
Episode length: 441.00 +/- 179.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 803500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.474    |
|    n_updates        | 198374   |
----------------------------------
Eval num_timesteps=804000, episode_reward=181.12 +/- 37.93
Episode length: 458.06 +/- 165.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 458      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 804000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.634    |
|    n_updates        | 198499   |
----------------------------------
Eval num_timesteps=804500, episode_reward=189.92 +/- 25.90
Episode length: 495.46 +/- 116.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 804500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.528    |
|    n_updates        | 198624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 392      |
|    ep_rew_mean      | 160      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2740     |
|    fps              | 49       |
|    time_elapsed     | 16161    |
|    total_timesteps  | 804551   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.825    |
|    n_updates        | 198637   |
----------------------------------
Eval num_timesteps=805000, episode_reward=196.99 +/- 0.74
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 805000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.972    |
|    n_updates        | 198749   |
----------------------------------
Eval num_timesteps=805500, episode_reward=193.26 +/- 15.35
Episode length: 515.10 +/- 69.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 805500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.511    |
|    n_updates        | 198874   |
----------------------------------
Eval num_timesteps=806000, episode_reward=185.62 +/- 32.81
Episode length: 481.88 +/- 131.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 482      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 806000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.88     |
|    n_updates        | 198999   |
----------------------------------
Eval num_timesteps=806500, episode_reward=197.71 +/- 0.71
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 806500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.508    |
|    n_updates        | 199124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 406      |
|    ep_rew_mean      | 163      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2744     |
|    fps              | 49       |
|    time_elapsed     | 16221    |
|    total_timesteps  | 806651   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.34     |
|    n_updates        | 199162   |
----------------------------------
Eval num_timesteps=807000, episode_reward=176.74 +/- 41.86
Episode length: 440.90 +/- 179.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 807000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 199249   |
----------------------------------
Eval num_timesteps=807500, episode_reward=197.37 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 807500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.533    |
|    n_updates        | 199374   |
----------------------------------
Eval num_timesteps=808000, episode_reward=197.24 +/- 1.13
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 808000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.447    |
|    n_updates        | 199499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 411      |
|    ep_rew_mean      | 165      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2748     |
|    fps              | 49       |
|    time_elapsed     | 16264    |
|    total_timesteps  | 808250   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.769    |
|    n_updates        | 199562   |
----------------------------------
Eval num_timesteps=808500, episode_reward=192.33 +/- 21.58
Episode length: 505.70 +/- 94.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 808500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.742    |
|    n_updates        | 199624   |
----------------------------------
Eval num_timesteps=809000, episode_reward=182.04 +/- 35.65
Episode length: 469.20 +/- 151.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 469      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 809000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.667    |
|    n_updates        | 199749   |
----------------------------------
Eval num_timesteps=809500, episode_reward=146.09 +/- 54.52
Episode length: 310.72 +/- 232.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 311      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 809500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.786    |
|    n_updates        | 199874   |
----------------------------------
Eval num_timesteps=810000, episode_reward=184.70 +/- 36.34
Episode length: 495.28 +/- 117.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 810000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.385    |
|    n_updates        | 199999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 421      |
|    ep_rew_mean      | 167      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2752     |
|    fps              | 49       |
|    time_elapsed     | 16316    |
|    total_timesteps  | 810350   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.18     |
|    n_updates        | 200087   |
----------------------------------
Eval num_timesteps=810500, episode_reward=131.04 +/- 53.44
Episode length: 250.18 +/- 225.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 250      |
|    mean_reward      | 131      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 810500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.355    |
|    n_updates        | 200124   |
----------------------------------
Eval num_timesteps=811000, episode_reward=79.41 +/- 83.39
Episode length: 191.62 +/- 212.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 79.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 811000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.43     |
|    n_updates        | 200249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 416      |
|    ep_rew_mean      | 166      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2756     |
|    fps              | 49       |
|    time_elapsed     | 16329    |
|    total_timesteps  | 811066   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.636    |
|    n_updates        | 200266   |
----------------------------------
Eval num_timesteps=811500, episode_reward=56.75 +/- 69.44
Episode length: 104.82 +/- 155.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 56.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 811500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.454    |
|    n_updates        | 200374   |
----------------------------------
Eval num_timesteps=812000, episode_reward=181.32 +/- 43.24
Episode length: 466.72 +/- 157.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 467      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 812000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.48     |
|    n_updates        | 200499   |
----------------------------------
Eval num_timesteps=812500, episode_reward=127.55 +/- 52.90
Episode length: 233.98 +/- 228.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 128      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 812500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.64     |
|    n_updates        | 200624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 429      |
|    ep_rew_mean      | 169      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2760     |
|    fps              | 49       |
|    time_elapsed     | 16354    |
|    total_timesteps  | 812669   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.638    |
|    n_updates        | 200667   |
----------------------------------
Eval num_timesteps=813000, episode_reward=192.99 +/- 30.90
Episode length: 507.04 +/- 87.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 813000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.771    |
|    n_updates        | 200749   |
----------------------------------
Eval num_timesteps=813500, episode_reward=197.19 +/- 1.29
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 813500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.886    |
|    n_updates        | 200874   |
----------------------------------
Eval num_timesteps=814000, episode_reward=173.65 +/- 26.85
Episode length: 486.06 +/- 132.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 814000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 200999   |
----------------------------------
Eval num_timesteps=814500, episode_reward=185.66 +/- 30.87
Episode length: 487.22 +/- 128.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 814500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.281    |
|    n_updates        | 201124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 434      |
|    ep_rew_mean      | 170      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 49       |
|    time_elapsed     | 16412    |
|    total_timesteps  | 814769   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.01     |
|    n_updates        | 201192   |
----------------------------------
Eval num_timesteps=815000, episode_reward=192.93 +/- 21.76
Episode length: 505.26 +/- 96.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 815000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.386    |
|    n_updates        | 201249   |
----------------------------------
Eval num_timesteps=815500, episode_reward=195.94 +/- 1.91
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 815500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.912    |
|    n_updates        | 201374   |
----------------------------------
Eval num_timesteps=816000, episode_reward=195.82 +/- 4.62
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 816000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.384    |
|    n_updates        | 201499   |
----------------------------------
Eval num_timesteps=816500, episode_reward=197.59 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 816500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.817    |
|    n_updates        | 201624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 434      |
|    ep_rew_mean      | 170      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2768     |
|    fps              | 49       |
|    time_elapsed     | 16473    |
|    total_timesteps  | 816869   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.07     |
|    n_updates        | 201717   |
----------------------------------
Eval num_timesteps=817000, episode_reward=139.31 +/- 81.74
Episode length: 350.92 +/- 232.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 351      |
|    mean_reward      | 139      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 817000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.318    |
|    n_updates        | 201749   |
----------------------------------
Eval num_timesteps=817500, episode_reward=170.31 +/- 46.59
Episode length: 410.42 +/- 204.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 410      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 817500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.15     |
|    n_updates        | 201874   |
----------------------------------
Eval num_timesteps=818000, episode_reward=195.23 +/- 15.57
Episode length: 515.80 +/- 64.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 818000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.649    |
|    n_updates        | 201999   |
----------------------------------
Eval num_timesteps=818500, episode_reward=197.49 +/- 0.91
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 818500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.327    |
|    n_updates        | 202124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 443      |
|    ep_rew_mean      | 172      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2772     |
|    fps              | 49       |
|    time_elapsed     | 16525    |
|    total_timesteps  | 818969   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.336    |
|    n_updates        | 202242   |
----------------------------------
Eval num_timesteps=819000, episode_reward=176.07 +/- 42.26
Episode length: 440.94 +/- 179.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 819000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.535    |
|    n_updates        | 202249   |
----------------------------------
Eval num_timesteps=819500, episode_reward=197.72 +/- 1.04
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 819500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 202374   |
----------------------------------
Eval num_timesteps=820000, episode_reward=195.05 +/- 15.56
Episode length: 515.50 +/- 66.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 820000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.487    |
|    n_updates        | 202499   |
----------------------------------
Eval num_timesteps=820500, episode_reward=175.36 +/- 41.37
Episode length: 440.30 +/- 181.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 440      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 820500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.427    |
|    n_updates        | 202624   |
----------------------------------
Eval num_timesteps=821000, episode_reward=195.46 +/- 1.86
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 821000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.431    |
|    n_updates        | 202749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | 177      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 49       |
|    time_elapsed     | 16596    |
|    total_timesteps  | 821069   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.383    |
|    n_updates        | 202767   |
----------------------------------
Eval num_timesteps=821500, episode_reward=167.91 +/- 64.02
Episode length: 444.22 +/- 174.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 444      |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 821500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.502    |
|    n_updates        | 202874   |
----------------------------------
Eval num_timesteps=822000, episode_reward=178.44 +/- 39.74
Episode length: 447.64 +/- 177.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 822000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.514    |
|    n_updates        | 202999   |
----------------------------------
Eval num_timesteps=822500, episode_reward=105.20 +/- 88.49
Episode length: 257.94 +/- 236.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 258      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 822500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.48     |
|    n_updates        | 203124   |
----------------------------------
Eval num_timesteps=823000, episode_reward=196.93 +/- 1.49
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 823000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.394    |
|    n_updates        | 203249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2780     |
|    fps              | 49       |
|    time_elapsed     | 16645    |
|    total_timesteps  | 823169   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.249    |
|    n_updates        | 203292   |
----------------------------------
Eval num_timesteps=823500, episode_reward=197.31 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 823500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.255    |
|    n_updates        | 203374   |
----------------------------------
Eval num_timesteps=824000, episode_reward=190.08 +/- 29.97
Episode length: 497.28 +/- 109.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 824000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.358    |
|    n_updates        | 203499   |
----------------------------------
Eval num_timesteps=824500, episode_reward=193.54 +/- 15.96
Episode length: 515.16 +/- 68.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 824500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 2.14     |
|    n_updates        | 203624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2784     |
|    fps              | 49       |
|    time_elapsed     | 16690    |
|    total_timesteps  | 824846   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.258    |
|    n_updates        | 203711   |
----------------------------------
Eval num_timesteps=825000, episode_reward=143.52 +/- 54.09
Episode length: 303.48 +/- 230.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 303      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 825000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.499    |
|    n_updates        | 203749   |
----------------------------------
Eval num_timesteps=825500, episode_reward=187.32 +/- 29.66
Episode length: 487.14 +/- 128.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 825500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.233    |
|    n_updates        | 203874   |
----------------------------------
Eval num_timesteps=826000, episode_reward=194.07 +/- 15.87
Episode length: 515.54 +/- 66.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 826000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.356    |
|    n_updates        | 203999   |
----------------------------------
Eval num_timesteps=826500, episode_reward=197.78 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 826500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.535    |
|    n_updates        | 204124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2788     |
|    fps              | 49       |
|    time_elapsed     | 16743    |
|    total_timesteps  | 826946   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.377    |
|    n_updates        | 204236   |
----------------------------------
Eval num_timesteps=827000, episode_reward=197.48 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 827000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.647    |
|    n_updates        | 204249   |
----------------------------------
Eval num_timesteps=827500, episode_reward=195.49 +/- 15.46
Episode length: 515.34 +/- 67.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 827500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.179    |
|    n_updates        | 204374   |
----------------------------------
Eval num_timesteps=828000, episode_reward=194.20 +/- 15.26
Episode length: 515.04 +/- 69.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 828000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.448    |
|    n_updates        | 204499   |
----------------------------------
Eval num_timesteps=828500, episode_reward=174.62 +/- 52.84
Episode length: 448.24 +/- 175.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 828500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.414    |
|    n_updates        | 204624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2792     |
|    fps              | 49       |
|    time_elapsed     | 16802    |
|    total_timesteps  | 828562   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.256    |
|    n_updates        | 204640   |
----------------------------------
Eval num_timesteps=829000, episode_reward=194.72 +/- 16.71
Episode length: 515.84 +/- 64.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 829000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.383    |
|    n_updates        | 204749   |
----------------------------------
Eval num_timesteps=829500, episode_reward=194.69 +/- 15.53
Episode length: 515.30 +/- 67.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 829500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.911    |
|    n_updates        | 204874   |
----------------------------------
Eval num_timesteps=830000, episode_reward=108.12 +/- 98.36
Episode length: 301.38 +/- 234.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 301      |
|    mean_reward      | 108      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 830000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.321    |
|    n_updates        | 204999   |
----------------------------------
Eval num_timesteps=830500, episode_reward=194.61 +/- 15.70
Episode length: 515.16 +/- 68.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 830500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.839    |
|    n_updates        | 205124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2796     |
|    fps              | 49       |
|    time_elapsed     | 16856    |
|    total_timesteps  | 830662   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.663    |
|    n_updates        | 205165   |
----------------------------------
Eval num_timesteps=831000, episode_reward=152.60 +/- 53.36
Episode length: 334.88 +/- 233.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 335      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 831000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.215    |
|    n_updates        | 205249   |
----------------------------------
Eval num_timesteps=831500, episode_reward=168.18 +/- 45.95
Episode length: 410.76 +/- 203.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 411      |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 831500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.201    |
|    n_updates        | 205374   |
----------------------------------
Eval num_timesteps=832000, episode_reward=192.69 +/- 15.94
Episode length: 514.98 +/- 70.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 832000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.239    |
|    n_updates        | 205499   |
----------------------------------
Eval num_timesteps=832500, episode_reward=197.74 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 832500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.597    |
|    n_updates        | 205624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2800     |
|    fps              | 49       |
|    time_elapsed     | 16908    |
|    total_timesteps  | 832762   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.403    |
|    n_updates        | 205690   |
----------------------------------
Eval num_timesteps=833000, episode_reward=168.84 +/- 46.24
Episode length: 411.88 +/- 201.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 412      |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 833000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.241    |
|    n_updates        | 205749   |
----------------------------------
Eval num_timesteps=833500, episode_reward=188.16 +/- 26.18
Episode length: 496.50 +/- 112.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 833500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.947    |
|    n_updates        | 205874   |
----------------------------------
Eval num_timesteps=834000, episode_reward=103.59 +/- 96.88
Episode length: 285.32 +/- 240.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 285      |
|    mean_reward      | 104      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 834000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.3      |
|    n_updates        | 205999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 49       |
|    time_elapsed     | 16943    |
|    total_timesteps  | 834375   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.436    |
|    n_updates        | 206093   |
----------------------------------
Eval num_timesteps=834500, episode_reward=120.82 +/- 89.89
Episode length: 320.40 +/- 226.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 320      |
|    mean_reward      | 121      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 834500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.4      |
|    n_updates        | 206124   |
----------------------------------
Eval num_timesteps=835000, episode_reward=197.60 +/- 1.11
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 835000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.243    |
|    n_updates        | 206249   |
----------------------------------
Eval num_timesteps=835500, episode_reward=194.44 +/- 15.34
Episode length: 515.28 +/- 68.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 835500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.752    |
|    n_updates        | 206374   |
----------------------------------
Eval num_timesteps=836000, episode_reward=197.45 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 836000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.201    |
|    n_updates        | 206499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2808     |
|    fps              | 49       |
|    time_elapsed     | 16998    |
|    total_timesteps  | 836475   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.342    |
|    n_updates        | 206618   |
----------------------------------
Eval num_timesteps=836500, episode_reward=197.78 +/- 0.52
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 836500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.378    |
|    n_updates        | 206624   |
----------------------------------
Eval num_timesteps=837000, episode_reward=108.57 +/- 85.17
Episode length: 263.38 +/- 232.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 263      |
|    mean_reward      | 109      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 837000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.349    |
|    n_updates        | 206749   |
----------------------------------
Eval num_timesteps=837500, episode_reward=197.57 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 837500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.6      |
|    n_updates        | 206874   |
----------------------------------
Eval num_timesteps=838000, episode_reward=156.76 +/- 74.03
Episode length: 428.40 +/- 188.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 428      |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 838000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.173    |
|    n_updates        | 206999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 179      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2812     |
|    fps              | 49       |
|    time_elapsed     | 17049    |
|    total_timesteps  | 838150   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.389    |
|    n_updates        | 207037   |
----------------------------------
Eval num_timesteps=838500, episode_reward=197.51 +/- 1.13
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 838500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.166    |
|    n_updates        | 207124   |
----------------------------------
Eval num_timesteps=839000, episode_reward=172.33 +/- 45.25
Episode length: 424.32 +/- 190.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 424      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 839000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.134    |
|    n_updates        | 207249   |
----------------------------------
Eval num_timesteps=839500, episode_reward=197.52 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 839500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.248    |
|    n_updates        | 207374   |
----------------------------------
Eval num_timesteps=840000, episode_reward=138.21 +/- 73.03
Episode length: 341.14 +/- 208.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 341      |
|    mean_reward      | 138      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 840000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.286    |
|    n_updates        | 207499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 179      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2816     |
|    fps              | 49       |
|    time_elapsed     | 17102    |
|    total_timesteps  | 840250   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.233    |
|    n_updates        | 207562   |
----------------------------------
Eval num_timesteps=840500, episode_reward=197.68 +/- 0.75
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 840500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.179    |
|    n_updates        | 207624   |
----------------------------------
Eval num_timesteps=841000, episode_reward=181.39 +/- 40.37
Episode length: 457.58 +/- 167.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 458      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 841000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.464    |
|    n_updates        | 207749   |
----------------------------------
Eval num_timesteps=841500, episode_reward=155.01 +/- 52.92
Episode length: 348.06 +/- 226.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 348      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 841500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.337    |
|    n_updates        | 207874   |
----------------------------------
Eval num_timesteps=842000, episode_reward=197.28 +/- 1.21
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 842000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.146    |
|    n_updates        | 207999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2820     |
|    fps              | 49       |
|    time_elapsed     | 17156    |
|    total_timesteps  | 842350   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.189    |
|    n_updates        | 208087   |
----------------------------------
Eval num_timesteps=842500, episode_reward=197.61 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 842500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.179    |
|    n_updates        | 208124   |
----------------------------------
Eval num_timesteps=843000, episode_reward=111.23 +/- 99.50
Episode length: 291.32 +/- 224.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 291      |
|    mean_reward      | 111      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 843000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.233    |
|    n_updates        | 208249   |
----------------------------------
Eval num_timesteps=843500, episode_reward=197.50 +/- 1.27
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 843500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.252    |
|    n_updates        | 208374   |
----------------------------------
Eval num_timesteps=844000, episode_reward=172.34 +/- 45.11
Episode length: 419.06 +/- 199.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 419      |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 844000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.193    |
|    n_updates        | 208499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | 182      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2824     |
|    fps              | 49       |
|    time_elapsed     | 17207    |
|    total_timesteps  | 844450   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.262    |
|    n_updates        | 208612   |
----------------------------------
Eval num_timesteps=844500, episode_reward=163.16 +/- 49.61
Episode length: 382.92 +/- 217.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 383      |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 844500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.158    |
|    n_updates        | 208624   |
----------------------------------
Eval num_timesteps=845000, episode_reward=197.10 +/- 1.27
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 845000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.663    |
|    n_updates        | 208749   |
----------------------------------
Eval num_timesteps=845500, episode_reward=197.88 +/- 0.39
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 845500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.285    |
|    n_updates        | 208874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2828     |
|    fps              | 49       |
|    time_elapsed     | 17248    |
|    total_timesteps  | 845617   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.927    |
|    n_updates        | 208904   |
----------------------------------
Eval num_timesteps=846000, episode_reward=187.89 +/- 29.63
Episode length: 486.66 +/- 130.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 846000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.165    |
|    n_updates        | 208999   |
----------------------------------
Eval num_timesteps=846500, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 846500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.485    |
|    n_updates        | 209124   |
----------------------------------
Eval num_timesteps=847000, episode_reward=148.80 +/- 76.21
Episode length: 384.64 +/- 215.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 385      |
|    mean_reward      | 149      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 847000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.137    |
|    n_updates        | 209249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 465      |
|    ep_rew_mean      | 178      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2832     |
|    fps              | 49       |
|    time_elapsed     | 17289    |
|    total_timesteps  | 847280   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.336    |
|    n_updates        | 209319   |
----------------------------------
Eval num_timesteps=847500, episode_reward=191.34 +/- 30.92
Episode length: 515.46 +/- 66.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 847500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.373    |
|    n_updates        | 209374   |
----------------------------------
Eval num_timesteps=848000, episode_reward=197.04 +/- 1.04
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 848000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.314    |
|    n_updates        | 209499   |
----------------------------------
Eval num_timesteps=848500, episode_reward=197.72 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 848500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0846   |
|    n_updates        | 209624   |
----------------------------------
Eval num_timesteps=849000, episode_reward=165.79 +/- 48.89
Episode length: 394.68 +/- 209.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 395      |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 849000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.362    |
|    n_updates        | 209749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 465      |
|    ep_rew_mean      | 179      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2836     |
|    fps              | 48       |
|    time_elapsed     | 17346    |
|    total_timesteps  | 849380   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.338    |
|    n_updates        | 209844   |
----------------------------------
Eval num_timesteps=849500, episode_reward=197.21 +/- 1.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 849500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.132    |
|    n_updates        | 209874   |
----------------------------------
Eval num_timesteps=850000, episode_reward=197.76 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 850000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.43     |
|    n_updates        | 209999   |
----------------------------------
Eval num_timesteps=850500, episode_reward=192.75 +/- 29.82
Episode length: 497.94 +/- 107.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 850500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.16     |
|    n_updates        | 210124   |
----------------------------------
Eval num_timesteps=851000, episode_reward=197.68 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 851000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.123    |
|    n_updates        | 210249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2840     |
|    fps              | 48       |
|    time_elapsed     | 17405    |
|    total_timesteps  | 851480   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.268    |
|    n_updates        | 210369   |
----------------------------------
Eval num_timesteps=851500, episode_reward=197.28 +/- 1.25
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 851500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.5      |
|    n_updates        | 210374   |
----------------------------------
Eval num_timesteps=852000, episode_reward=197.84 +/- 0.50
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 852000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0707   |
|    n_updates        | 210499   |
----------------------------------
Eval num_timesteps=852500, episode_reward=196.50 +/- 1.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 852500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.148    |
|    n_updates        | 210624   |
----------------------------------
Eval num_timesteps=853000, episode_reward=197.83 +/- 0.48
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 853000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.367    |
|    n_updates        | 210749   |
----------------------------------
Eval num_timesteps=853500, episode_reward=197.82 +/- 0.50
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 853500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0876   |
|    n_updates        | 210874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2844     |
|    fps              | 48       |
|    time_elapsed     | 17481    |
|    total_timesteps  | 853580   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.18     |
|    n_updates        | 210894   |
----------------------------------
Eval num_timesteps=854000, episode_reward=193.68 +/- 15.98
Episode length: 515.02 +/- 69.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 854000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.274    |
|    n_updates        | 210999   |
----------------------------------
Eval num_timesteps=854500, episode_reward=197.85 +/- 0.51
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 854500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.642    |
|    n_updates        | 211124   |
----------------------------------
Eval num_timesteps=855000, episode_reward=197.92 +/- 0.34
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 855000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.158    |
|    n_updates        | 211249   |
----------------------------------
Eval num_timesteps=855500, episode_reward=197.71 +/- 0.70
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 855500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.12     |
|    n_updates        | 211374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2848     |
|    fps              | 48       |
|    time_elapsed     | 17542    |
|    total_timesteps  | 855680   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.252    |
|    n_updates        | 211419   |
----------------------------------
Eval num_timesteps=856000, episode_reward=197.65 +/- 1.11
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 856000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.418    |
|    n_updates        | 211499   |
----------------------------------
Eval num_timesteps=856500, episode_reward=199.69 +/- 13.42
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 200      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 856500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.232    |
|    n_updates        | 211624   |
----------------------------------
Eval num_timesteps=857000, episode_reward=177.18 +/- 55.24
Episode length: 469.54 +/- 150.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 470      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 857000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.162    |
|    n_updates        | 211749   |
----------------------------------
Eval num_timesteps=857500, episode_reward=197.04 +/- 1.30
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 857500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.349    |
|    n_updates        | 211874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2852     |
|    fps              | 48       |
|    time_elapsed     | 17601    |
|    total_timesteps  | 857780   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.247    |
|    n_updates        | 211944   |
----------------------------------
Eval num_timesteps=858000, episode_reward=194.09 +/- 25.85
Episode length: 504.92 +/- 98.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 858000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.203    |
|    n_updates        | 211999   |
----------------------------------
Eval num_timesteps=858500, episode_reward=197.70 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 858500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.281    |
|    n_updates        | 212124   |
----------------------------------
Eval num_timesteps=859000, episode_reward=197.62 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 859000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.222    |
|    n_updates        | 212249   |
----------------------------------
Eval num_timesteps=859500, episode_reward=194.48 +/- 15.61
Episode length: 515.20 +/- 68.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 859500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.129    |
|    n_updates        | 212374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2856     |
|    fps              | 48       |
|    time_elapsed     | 17661    |
|    total_timesteps  | 859880   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.955    |
|    n_updates        | 212469   |
----------------------------------
Eval num_timesteps=860000, episode_reward=197.21 +/- 1.23
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 860000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.231    |
|    n_updates        | 212499   |
----------------------------------
Eval num_timesteps=860500, episode_reward=122.40 +/- 71.58
Episode length: 257.34 +/- 237.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 257      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 860500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.127    |
|    n_updates        | 212624   |
----------------------------------
Eval num_timesteps=861000, episode_reward=197.75 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 861000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.33     |
|    n_updates        | 212749   |
----------------------------------
Eval num_timesteps=861500, episode_reward=197.54 +/- 1.05
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 861500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0564   |
|    n_updates        | 212874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2860     |
|    fps              | 48       |
|    time_elapsed     | 17716    |
|    total_timesteps  | 861980   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.26     |
|    n_updates        | 212994   |
----------------------------------
Eval num_timesteps=862000, episode_reward=197.23 +/- 1.32
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 862000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.284    |
|    n_updates        | 212999   |
----------------------------------
Eval num_timesteps=862500, episode_reward=177.34 +/- 61.24
Episode length: 477.30 +/- 143.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 862500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.123    |
|    n_updates        | 213124   |
----------------------------------
Eval num_timesteps=863000, episode_reward=197.40 +/- 1.17
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 863000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.187    |
|    n_updates        | 213249   |
----------------------------------
Eval num_timesteps=863500, episode_reward=197.18 +/- 1.13
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 863500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0995   |
|    n_updates        | 213374   |
----------------------------------
Eval num_timesteps=864000, episode_reward=197.76 +/- 0.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 864000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.542    |
|    n_updates        | 213499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2864     |
|    fps              | 48       |
|    time_elapsed     | 17791    |
|    total_timesteps  | 864080   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.309    |
|    n_updates        | 213519   |
----------------------------------
Eval num_timesteps=864500, episode_reward=197.31 +/- 1.26
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 864500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.073    |
|    n_updates        | 213624   |
----------------------------------
Eval num_timesteps=865000, episode_reward=197.73 +/- 0.83
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 865000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.077    |
|    n_updates        | 213749   |
----------------------------------
Eval num_timesteps=865500, episode_reward=194.03 +/- 15.69
Episode length: 515.40 +/- 67.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 865500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.49     |
|    n_updates        | 213874   |
----------------------------------
Eval num_timesteps=866000, episode_reward=197.72 +/- 0.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 866000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.233    |
|    n_updates        | 213999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2868     |
|    fps              | 48       |
|    time_elapsed     | 17852    |
|    total_timesteps  | 866180   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.251    |
|    n_updates        | 214044   |
----------------------------------
Eval num_timesteps=866500, episode_reward=197.60 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 866500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0818   |
|    n_updates        | 214124   |
----------------------------------
Eval num_timesteps=867000, episode_reward=197.39 +/- 1.05
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 867000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.085    |
|    n_updates        | 214249   |
----------------------------------
Eval num_timesteps=867500, episode_reward=197.45 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 867500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.072    |
|    n_updates        | 214374   |
----------------------------------
Eval num_timesteps=868000, episode_reward=197.28 +/- 1.07
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 868000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.146    |
|    n_updates        | 214499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2872     |
|    fps              | 48       |
|    time_elapsed     | 17913    |
|    total_timesteps  | 868280   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0643   |
|    n_updates        | 214569   |
----------------------------------
Eval num_timesteps=868500, episode_reward=197.31 +/- 1.10
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 868500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.146    |
|    n_updates        | 214624   |
----------------------------------
Eval num_timesteps=869000, episode_reward=197.84 +/- 0.47
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 869000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.145    |
|    n_updates        | 214749   |
----------------------------------
Eval num_timesteps=869500, episode_reward=197.71 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 869500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.132    |
|    n_updates        | 214874   |
----------------------------------
Eval num_timesteps=870000, episode_reward=200.22 +/- 17.86
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 200      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 870000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0596   |
|    n_updates        | 214999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2876     |
|    fps              | 48       |
|    time_elapsed     | 17974    |
|    total_timesteps  | 870380   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.3      |
|    n_updates        | 215094   |
----------------------------------
Eval num_timesteps=870500, episode_reward=89.85 +/- 48.88
Episode length: 115.86 +/- 152.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 89.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 870500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.388    |
|    n_updates        | 215124   |
----------------------------------
Eval num_timesteps=871000, episode_reward=93.71 +/- 25.87
Episode length: 89.16 +/- 112.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.2     |
|    mean_reward      | 93.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 871000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0667   |
|    n_updates        | 215249   |
----------------------------------
Eval num_timesteps=871500, episode_reward=125.46 +/- 91.51
Episode length: 352.72 +/- 229.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 353      |
|    mean_reward      | 125      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 871500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.381    |
|    n_updates        | 215374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | 183      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2880     |
|    fps              | 48       |
|    time_elapsed     | 17991    |
|    total_timesteps  | 871563   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.228    |
|    n_updates        | 215390   |
----------------------------------
Eval num_timesteps=872000, episode_reward=158.81 +/- 80.58
Episode length: 420.02 +/- 197.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 420      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 872000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.08     |
|    n_updates        | 215499   |
----------------------------------
Eval num_timesteps=872500, episode_reward=197.81 +/- 0.68
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 872500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0878   |
|    n_updates        | 215624   |
----------------------------------
Eval num_timesteps=873000, episode_reward=196.60 +/- 1.03
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 873000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.156    |
|    n_updates        | 215749   |
----------------------------------
Eval num_timesteps=873500, episode_reward=195.55 +/- 4.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 873500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.264    |
|    n_updates        | 215874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2884     |
|    fps              | 48       |
|    time_elapsed     | 18049    |
|    total_timesteps  | 873663   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.733    |
|    n_updates        | 215915   |
----------------------------------
Eval num_timesteps=874000, episode_reward=187.90 +/- 29.64
Episode length: 486.86 +/- 129.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 874000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0916   |
|    n_updates        | 215999   |
----------------------------------
Eval num_timesteps=874500, episode_reward=197.06 +/- 1.34
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 874500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.525    |
|    n_updates        | 216124   |
----------------------------------
Eval num_timesteps=875000, episode_reward=197.26 +/- 1.17
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 875000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.211    |
|    n_updates        | 216249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | 183      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2888     |
|    fps              | 48       |
|    time_elapsed     | 18093    |
|    total_timesteps  | 875287   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.445    |
|    n_updates        | 216321   |
----------------------------------
Eval num_timesteps=875500, episode_reward=197.13 +/- 1.25
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 875500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.303    |
|    n_updates        | 216374   |
----------------------------------
Eval num_timesteps=876000, episode_reward=195.35 +/- 15.44
Episode length: 515.18 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 876000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.112    |
|    n_updates        | 216499   |
----------------------------------
Eval num_timesteps=876500, episode_reward=147.40 +/- 78.91
Episode length: 380.60 +/- 220.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 381      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 876500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.197    |
|    n_updates        | 216624   |
----------------------------------
Eval num_timesteps=877000, episode_reward=196.64 +/- 0.97
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 877000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.214    |
|    n_updates        | 216749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2892     |
|    fps              | 48       |
|    time_elapsed     | 18149    |
|    total_timesteps  | 877387   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.261    |
|    n_updates        | 216846   |
----------------------------------
Eval num_timesteps=877500, episode_reward=196.53 +/- 0.92
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 877500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.331    |
|    n_updates        | 216874   |
----------------------------------
Eval num_timesteps=878000, episode_reward=196.80 +/- 1.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 878000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.377    |
|    n_updates        | 216999   |
----------------------------------
Eval num_timesteps=878500, episode_reward=197.31 +/- 1.07
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 878500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.174    |
|    n_updates        | 217124   |
----------------------------------
Eval num_timesteps=879000, episode_reward=197.37 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 879000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.143    |
|    n_updates        | 217249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2896     |
|    fps              | 48       |
|    time_elapsed     | 18210    |
|    total_timesteps  | 879487   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.225    |
|    n_updates        | 217371   |
----------------------------------
Eval num_timesteps=879500, episode_reward=176.92 +/- 41.94
Episode length: 440.90 +/- 179.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 879500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.29     |
|    n_updates        | 217374   |
----------------------------------
Eval num_timesteps=880000, episode_reward=94.19 +/- 77.67
Episode length: 192.32 +/- 218.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 94.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 880000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0955   |
|    n_updates        | 217499   |
----------------------------------
Eval num_timesteps=880500, episode_reward=195.19 +/- 15.42
Episode length: 515.48 +/- 66.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 880500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.231    |
|    n_updates        | 217624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | 183      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2900     |
|    fps              | 48       |
|    time_elapsed     | 18243    |
|    total_timesteps  | 880660   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.327    |
|    n_updates        | 217664   |
----------------------------------
Eval num_timesteps=881000, episode_reward=197.63 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 881000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.121    |
|    n_updates        | 217749   |
----------------------------------
Eval num_timesteps=881500, episode_reward=197.17 +/- 1.08
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 881500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.165    |
|    n_updates        | 217874   |
----------------------------------
Eval num_timesteps=882000, episode_reward=182.03 +/- 20.31
Episode length: 505.74 +/- 94.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 882000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.179    |
|    n_updates        | 217999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | 183      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2904     |
|    fps              | 48       |
|    time_elapsed     | 18288    |
|    total_timesteps  | 882261   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.289    |
|    n_updates        | 218065   |
----------------------------------
Eval num_timesteps=882500, episode_reward=189.71 +/- 25.90
Episode length: 496.46 +/- 113.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 882500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.649    |
|    n_updates        | 218124   |
----------------------------------
Eval num_timesteps=883000, episode_reward=113.21 +/- 88.04
Episode length: 299.98 +/- 223.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 300      |
|    mean_reward      | 113      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 883000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.36     |
|    n_updates        | 218249   |
----------------------------------
Eval num_timesteps=883500, episode_reward=192.98 +/- 21.59
Episode length: 506.36 +/- 91.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 883500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.155    |
|    n_updates        | 218374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 182      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2908     |
|    fps              | 48       |
|    time_elapsed     | 18326    |
|    total_timesteps  | 883877   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.206    |
|    n_updates        | 218469   |
----------------------------------
Eval num_timesteps=884000, episode_reward=192.40 +/- 21.44
Episode length: 506.32 +/- 91.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 884000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.199    |
|    n_updates        | 218499   |
----------------------------------
Eval num_timesteps=884500, episode_reward=185.06 +/- 30.49
Episode length: 485.82 +/- 132.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 884500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.214    |
|    n_updates        | 218624   |
----------------------------------
Eval num_timesteps=885000, episode_reward=176.98 +/- 43.29
Episode length: 495.58 +/- 116.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 885000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.04     |
|    n_updates        | 218749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2912     |
|    fps              | 48       |
|    time_elapsed     | 18369    |
|    total_timesteps  | 885013   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.28     |
|    n_updates        | 218753   |
----------------------------------
Eval num_timesteps=885500, episode_reward=195.12 +/- 15.99
Episode length: 514.98 +/- 70.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 885500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.77     |
|    n_updates        | 218874   |
----------------------------------
Eval num_timesteps=886000, episode_reward=197.49 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 886000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.182    |
|    n_updates        | 218999   |
----------------------------------
Eval num_timesteps=886500, episode_reward=195.31 +/- 1.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 886500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0337   |
|    n_updates        | 219124   |
----------------------------------
Eval num_timesteps=887000, episode_reward=189.00 +/- 6.54
Episode length: 509.02 +/- 78.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 509      |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 887000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.44     |
|    n_updates        | 219249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2916     |
|    fps              | 48       |
|    time_elapsed     | 18429    |
|    total_timesteps  | 887113   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0767   |
|    n_updates        | 219278   |
----------------------------------
Eval num_timesteps=887500, episode_reward=190.94 +/- 2.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 887500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.119    |
|    n_updates        | 219374   |
----------------------------------
Eval num_timesteps=888000, episode_reward=197.67 +/- 0.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 888000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.149    |
|    n_updates        | 219499   |
----------------------------------
Eval num_timesteps=888500, episode_reward=197.71 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 888500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.197    |
|    n_updates        | 219624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 464      |
|    ep_rew_mean      | 179      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2920     |
|    fps              | 48       |
|    time_elapsed     | 18475    |
|    total_timesteps  | 888773   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.223    |
|    n_updates        | 219693   |
----------------------------------
Eval num_timesteps=889000, episode_reward=195.05 +/- 15.97
Episode length: 515.70 +/- 65.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 889000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.118    |
|    n_updates        | 219749   |
----------------------------------
Eval num_timesteps=889500, episode_reward=180.91 +/- 38.67
Episode length: 457.62 +/- 167.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 458      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 889500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.1      |
|    n_updates        | 219874   |
----------------------------------
Eval num_timesteps=890000, episode_reward=197.65 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 890000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.213    |
|    n_updates        | 219999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 459      |
|    ep_rew_mean      | 177      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2924     |
|    fps              | 48       |
|    time_elapsed     | 18518    |
|    total_timesteps  | 890395   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0848   |
|    n_updates        | 220098   |
----------------------------------
Eval num_timesteps=890500, episode_reward=197.44 +/- 0.83
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 890500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.125    |
|    n_updates        | 220124   |
----------------------------------
Eval num_timesteps=891000, episode_reward=194.92 +/- 16.13
Episode length: 515.06 +/- 69.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 891000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.138    |
|    n_updates        | 220249   |
----------------------------------
Eval num_timesteps=891500, episode_reward=146.25 +/- 54.35
Episode length: 308.68 +/- 234.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 309      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 891500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0551   |
|    n_updates        | 220374   |
----------------------------------
Eval num_timesteps=892000, episode_reward=197.83 +/- 0.48
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 892000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.379    |
|    n_updates        | 220499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 179      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2928     |
|    fps              | 48       |
|    time_elapsed     | 18572    |
|    total_timesteps  | 892495   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.175    |
|    n_updates        | 220623   |
----------------------------------
Eval num_timesteps=892500, episode_reward=197.81 +/- 0.51
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 892500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 1.04     |
|    n_updates        | 220624   |
----------------------------------
Eval num_timesteps=893000, episode_reward=180.29 +/- 44.23
Episode length: 461.34 +/- 158.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 461      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 893000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.126    |
|    n_updates        | 220749   |
----------------------------------
Eval num_timesteps=893500, episode_reward=173.77 +/- 56.92
Episode length: 459.78 +/- 161.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 460      |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 893500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.92     |
|    n_updates        | 220874   |
----------------------------------
Eval num_timesteps=894000, episode_reward=197.73 +/- 0.59
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 894000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.192    |
|    n_updates        | 220999   |
----------------------------------
Eval num_timesteps=894500, episode_reward=163.16 +/- 50.18
Episode length: 384.46 +/- 214.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 384      |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 894500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.066    |
|    n_updates        | 221124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2932     |
|    fps              | 47       |
|    time_elapsed     | 18640    |
|    total_timesteps  | 894595   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.111    |
|    n_updates        | 221148   |
----------------------------------
Eval num_timesteps=895000, episode_reward=191.91 +/- 22.63
Episode length: 505.54 +/- 95.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 895000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.103    |
|    n_updates        | 221249   |
----------------------------------
Eval num_timesteps=895500, episode_reward=192.74 +/- 22.15
Episode length: 505.32 +/- 96.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 895500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.344    |
|    n_updates        | 221374   |
----------------------------------
Eval num_timesteps=896000, episode_reward=197.63 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 896000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.254    |
|    n_updates        | 221499   |
----------------------------------
Eval num_timesteps=896500, episode_reward=197.77 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 896500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.182    |
|    n_updates        | 221624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2936     |
|    fps              | 47       |
|    time_elapsed     | 18700    |
|    total_timesteps  | 896695   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.152    |
|    n_updates        | 221673   |
----------------------------------
Eval num_timesteps=897000, episode_reward=197.67 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 897000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.243    |
|    n_updates        | 221749   |
----------------------------------
Eval num_timesteps=897500, episode_reward=197.82 +/- 0.46
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 897500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.295    |
|    n_updates        | 221874   |
----------------------------------
Eval num_timesteps=898000, episode_reward=196.26 +/- 1.90
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 898000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0338   |
|    n_updates        | 221999   |
----------------------------------
Eval num_timesteps=898500, episode_reward=181.54 +/- 48.71
Episode length: 491.44 +/- 116.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 491      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 898500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.38     |
|    n_updates        | 222124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2940     |
|    fps              | 47       |
|    time_elapsed     | 18761    |
|    total_timesteps  | 898795   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.201    |
|    n_updates        | 222198   |
----------------------------------
Eval num_timesteps=899000, episode_reward=191.78 +/- 2.99
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 899000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0967   |
|    n_updates        | 222249   |
----------------------------------
Eval num_timesteps=899500, episode_reward=186.54 +/- 38.34
Episode length: 498.50 +/- 105.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 899500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0728   |
|    n_updates        | 222374   |
----------------------------------
Eval num_timesteps=900000, episode_reward=197.20 +/- 1.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 900000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0702   |
|    n_updates        | 222499   |
----------------------------------
Eval num_timesteps=900500, episode_reward=174.70 +/- 43.70
Episode length: 429.14 +/- 191.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 900500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.314    |
|    n_updates        | 222624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2944     |
|    fps              | 47       |
|    time_elapsed     | 18821    |
|    total_timesteps  | 900895   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0211   |
|    n_updates        | 222723   |
----------------------------------
Eval num_timesteps=901000, episode_reward=196.53 +/- 3.24
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 901000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.808    |
|    n_updates        | 222749   |
----------------------------------
Eval num_timesteps=901500, episode_reward=190.04 +/- 26.13
Episode length: 497.06 +/- 110.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 901500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0643   |
|    n_updates        | 222874   |
----------------------------------
Eval num_timesteps=902000, episode_reward=197.45 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 902000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.24     |
|    n_updates        | 222999   |
----------------------------------
Eval num_timesteps=902500, episode_reward=154.56 +/- 53.00
Episode length: 349.94 +/- 223.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 350      |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 902500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.078    |
|    n_updates        | 223124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2948     |
|    fps              | 47       |
|    time_elapsed     | 18876    |
|    total_timesteps  | 902995   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.147    |
|    n_updates        | 223248   |
----------------------------------
Eval num_timesteps=903000, episode_reward=197.55 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 903000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.168    |
|    n_updates        | 223249   |
----------------------------------
Eval num_timesteps=903500, episode_reward=197.84 +/- 0.48
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 903500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0946   |
|    n_updates        | 223374   |
----------------------------------
Eval num_timesteps=904000, episode_reward=197.76 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 904000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0324   |
|    n_updates        | 223499   |
----------------------------------
Eval num_timesteps=904500, episode_reward=126.04 +/- 52.32
Episode length: 222.64 +/- 227.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 904500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.183    |
|    n_updates        | 223624   |
----------------------------------
Eval num_timesteps=905000, episode_reward=197.68 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 905000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.15     |
|    n_updates        | 223749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2952     |
|    fps              | 47       |
|    time_elapsed     | 18944    |
|    total_timesteps  | 905095   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.119    |
|    n_updates        | 223773   |
----------------------------------
Eval num_timesteps=905500, episode_reward=195.15 +/- 15.85
Episode length: 515.00 +/- 70.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 905500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0863   |
|    n_updates        | 223874   |
----------------------------------
Eval num_timesteps=906000, episode_reward=190.38 +/- 26.29
Episode length: 497.70 +/- 108.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 498      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 906000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.175    |
|    n_updates        | 223999   |
----------------------------------
Eval num_timesteps=906500, episode_reward=194.93 +/- 15.40
Episode length: 515.34 +/- 67.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 906500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0792   |
|    n_updates        | 224124   |
----------------------------------
Eval num_timesteps=907000, episode_reward=197.16 +/- 0.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 907000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0667   |
|    n_updates        | 224249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2956     |
|    fps              | 47       |
|    time_elapsed     | 19003    |
|    total_timesteps  | 907195   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0601   |
|    n_updates        | 224298   |
----------------------------------
Eval num_timesteps=907500, episode_reward=197.62 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 907500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.11     |
|    n_updates        | 224374   |
----------------------------------
Eval num_timesteps=908000, episode_reward=195.54 +/- 15.74
Episode length: 515.18 +/- 68.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 908000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0422   |
|    n_updates        | 224499   |
----------------------------------
Eval num_timesteps=908500, episode_reward=197.67 +/- 0.63
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 908500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.115    |
|    n_updates        | 224624   |
----------------------------------
Eval num_timesteps=909000, episode_reward=152.71 +/- 60.11
Episode length: 382.42 +/- 178.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 382      |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 909000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.158    |
|    n_updates        | 224749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2960     |
|    fps              | 47       |
|    time_elapsed     | 19060    |
|    total_timesteps  | 909295   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.133    |
|    n_updates        | 224823   |
----------------------------------
Eval num_timesteps=909500, episode_reward=196.62 +/- 1.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 909500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0607   |
|    n_updates        | 224874   |
----------------------------------
Eval num_timesteps=910000, episode_reward=197.74 +/- 0.62
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 910000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0692   |
|    n_updates        | 224999   |
----------------------------------
Eval num_timesteps=910500, episode_reward=197.49 +/- 0.85
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 910500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.121    |
|    n_updates        | 225124   |
----------------------------------
Eval num_timesteps=911000, episode_reward=190.02 +/- 25.93
Episode length: 495.82 +/- 115.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 911000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0585   |
|    n_updates        | 225249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2964     |
|    fps              | 47       |
|    time_elapsed     | 19119    |
|    total_timesteps  | 911395   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.335    |
|    n_updates        | 225348   |
----------------------------------
Eval num_timesteps=911500, episode_reward=195.17 +/- 15.71
Episode length: 515.66 +/- 65.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 516      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 911500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0708   |
|    n_updates        | 225374   |
----------------------------------
Eval num_timesteps=912000, episode_reward=197.40 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 912000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.137    |
|    n_updates        | 225499   |
----------------------------------
Eval num_timesteps=912500, episode_reward=194.86 +/- 16.84
Episode length: 515.40 +/- 67.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 912500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.131    |
|    n_updates        | 225624   |
----------------------------------
Eval num_timesteps=913000, episode_reward=197.77 +/- 0.54
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 913000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.112    |
|    n_updates        | 225749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2968     |
|    fps              | 47       |
|    time_elapsed     | 19180    |
|    total_timesteps  | 913495   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.109    |
|    n_updates        | 225873   |
----------------------------------
Eval num_timesteps=913500, episode_reward=186.28 +/- 32.97
Episode length: 478.12 +/- 140.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 913500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0377   |
|    n_updates        | 225874   |
----------------------------------
Eval num_timesteps=914000, episode_reward=181.50 +/- 38.26
Episode length: 458.90 +/- 164.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 914000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.109    |
|    n_updates        | 225999   |
----------------------------------
Eval num_timesteps=914500, episode_reward=190.53 +/- 26.08
Episode length: 497.14 +/- 110.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 914500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.117    |
|    n_updates        | 226124   |
----------------------------------
Eval num_timesteps=915000, episode_reward=121.55 +/- 59.23
Episode length: 275.74 +/- 157.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 276      |
|    mean_reward      | 122      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 915000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0586   |
|    n_updates        | 226249   |
----------------------------------
Eval num_timesteps=915500, episode_reward=195.66 +/- 15.47
Episode length: 515.00 +/- 70.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 915500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0897   |
|    n_updates        | 226374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 47       |
|    time_elapsed     | 19245    |
|    total_timesteps  | 915595   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0241   |
|    n_updates        | 226398   |
----------------------------------
Eval num_timesteps=916000, episode_reward=197.88 +/- 0.40
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 916000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.124    |
|    n_updates        | 226499   |
----------------------------------
Eval num_timesteps=916500, episode_reward=197.53 +/- 0.82
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 916500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0488   |
|    n_updates        | 226624   |
----------------------------------
Eval num_timesteps=917000, episode_reward=197.56 +/- 1.06
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 917000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.364    |
|    n_updates        | 226749   |
----------------------------------
Eval num_timesteps=917500, episode_reward=197.41 +/- 0.91
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 917500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0621   |
|    n_updates        | 226874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2976     |
|    fps              | 47       |
|    time_elapsed     | 19306    |
|    total_timesteps  | 917695   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.104    |
|    n_updates        | 226923   |
----------------------------------
Eval num_timesteps=918000, episode_reward=197.50 +/- 1.09
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 918000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0688   |
|    n_updates        | 226999   |
----------------------------------
Eval num_timesteps=918500, episode_reward=197.41 +/- 1.14
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 918500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0367   |
|    n_updates        | 227124   |
----------------------------------
Eval num_timesteps=919000, episode_reward=194.99 +/- 15.46
Episode length: 515.02 +/- 69.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 919000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.894    |
|    n_updates        | 227249   |
----------------------------------
Eval num_timesteps=919500, episode_reward=180.00 +/- 37.57
Episode length: 459.14 +/- 163.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 919500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.18     |
|    n_updates        | 227374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | 183      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 47       |
|    time_elapsed     | 19365    |
|    total_timesteps  | 919795   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.119    |
|    n_updates        | 227448   |
----------------------------------
Eval num_timesteps=920000, episode_reward=145.94 +/- 57.76
Episode length: 318.54 +/- 233.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 319      |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 920000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.945    |
|    n_updates        | 227499   |
----------------------------------
Eval num_timesteps=920500, episode_reward=191.47 +/- 21.30
Episode length: 505.68 +/- 94.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 920500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.105    |
|    n_updates        | 227624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | 180      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2984     |
|    fps              | 47       |
|    time_elapsed     | 19389    |
|    total_timesteps  | 920509   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0973   |
|    n_updates        | 227627   |
----------------------------------
Eval num_timesteps=921000, episode_reward=194.99 +/- 15.39
Episode length: 515.16 +/- 68.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 921000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0599   |
|    n_updates        | 227749   |
----------------------------------
Eval num_timesteps=921500, episode_reward=197.77 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 921500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.134    |
|    n_updates        | 227874   |
----------------------------------
Eval num_timesteps=922000, episode_reward=197.51 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 922000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0668   |
|    n_updates        | 227999   |
----------------------------------
Eval num_timesteps=922500, episode_reward=127.10 +/- 59.91
Episode length: 245.66 +/- 228.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 127      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 922500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.182    |
|    n_updates        | 228124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 47       |
|    time_elapsed     | 19442    |
|    total_timesteps  | 922609   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0132   |
|    n_updates        | 228152   |
----------------------------------
Eval num_timesteps=923000, episode_reward=195.57 +/- 16.03
Episode length: 515.00 +/- 70.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 923000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.151    |
|    n_updates        | 228249   |
----------------------------------
Eval num_timesteps=923500, episode_reward=196.92 +/- 1.42
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 923500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.218    |
|    n_updates        | 228374   |
----------------------------------
Eval num_timesteps=924000, episode_reward=197.80 +/- 0.51
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 924000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.166    |
|    n_updates        | 228499   |
----------------------------------
Eval num_timesteps=924500, episode_reward=185.60 +/- 32.74
Episode length: 477.42 +/- 142.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 924500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.223    |
|    n_updates        | 228624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2992     |
|    fps              | 47       |
|    time_elapsed     | 19501    |
|    total_timesteps  | 924709   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0815   |
|    n_updates        | 228677   |
----------------------------------
Eval num_timesteps=925000, episode_reward=197.56 +/- 1.16
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 925000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0776   |
|    n_updates        | 228749   |
----------------------------------
Eval num_timesteps=925500, episode_reward=195.56 +/- 15.46
Episode length: 515.06 +/- 69.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 925500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.149    |
|    n_updates        | 228874   |
----------------------------------
Eval num_timesteps=926000, episode_reward=197.59 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 926000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.152    |
|    n_updates        | 228999   |
----------------------------------
Eval num_timesteps=926500, episode_reward=197.50 +/- 0.92
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 926500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0475   |
|    n_updates        | 229124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 181      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 2996     |
|    fps              | 47       |
|    time_elapsed     | 19562    |
|    total_timesteps  | 926809   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0763   |
|    n_updates        | 229202   |
----------------------------------
Eval num_timesteps=927000, episode_reward=197.74 +/- 0.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 927000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.133    |
|    n_updates        | 229249   |
----------------------------------
Eval num_timesteps=927500, episode_reward=197.61 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 927500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0578   |
|    n_updates        | 229374   |
----------------------------------
Eval num_timesteps=928000, episode_reward=197.58 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 928000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.765    |
|    n_updates        | 229499   |
----------------------------------
Eval num_timesteps=928500, episode_reward=197.75 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 928500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0981   |
|    n_updates        | 229624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3000     |
|    fps              | 47       |
|    time_elapsed     | 19623    |
|    total_timesteps  | 928909   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.124    |
|    n_updates        | 229727   |
----------------------------------
Eval num_timesteps=929000, episode_reward=197.75 +/- 0.56
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 929000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0897   |
|    n_updates        | 229749   |
----------------------------------
Eval num_timesteps=929500, episode_reward=192.21 +/- 22.33
Episode length: 507.46 +/- 85.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 507      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 929500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0486   |
|    n_updates        | 229874   |
----------------------------------
Eval num_timesteps=930000, episode_reward=197.69 +/- 0.63
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 930000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0985   |
|    n_updates        | 229999   |
----------------------------------
Eval num_timesteps=930500, episode_reward=197.72 +/- 0.60
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 930500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.021    |
|    n_updates        | 230124   |
----------------------------------
Eval num_timesteps=931000, episode_reward=197.66 +/- 1.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 931000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.293    |
|    n_updates        | 230249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3004     |
|    fps              | 47       |
|    time_elapsed     | 19698    |
|    total_timesteps  | 931009   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.363    |
|    n_updates        | 230252   |
----------------------------------
Eval num_timesteps=931500, episode_reward=197.40 +/- 0.89
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 931500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.325    |
|    n_updates        | 230374   |
----------------------------------
Eval num_timesteps=932000, episode_reward=197.45 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 932000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0261   |
|    n_updates        | 230499   |
----------------------------------
Eval num_timesteps=932500, episode_reward=197.46 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 932500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0564   |
|    n_updates        | 230624   |
----------------------------------
Eval num_timesteps=933000, episode_reward=197.71 +/- 0.64
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 933000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.13     |
|    n_updates        | 230749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | 186      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3008     |
|    fps              | 47       |
|    time_elapsed     | 19759    |
|    total_timesteps  | 933109   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0636   |
|    n_updates        | 230777   |
----------------------------------
Eval num_timesteps=933500, episode_reward=197.88 +/- 0.42
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 933500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0784   |
|    n_updates        | 230874   |
----------------------------------
Eval num_timesteps=934000, episode_reward=197.52 +/- 0.85
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 934000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.096    |
|    n_updates        | 230999   |
----------------------------------
Eval num_timesteps=934500, episode_reward=197.59 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 934500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0942   |
|    n_updates        | 231124   |
----------------------------------
Eval num_timesteps=935000, episode_reward=197.40 +/- 1.19
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 935000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0437   |
|    n_updates        | 231249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3012     |
|    fps              | 47       |
|    time_elapsed     | 19820    |
|    total_timesteps  | 935209   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0291   |
|    n_updates        | 231302   |
----------------------------------
Eval num_timesteps=935500, episode_reward=197.10 +/- 1.25
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 935500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.034    |
|    n_updates        | 231374   |
----------------------------------
Eval num_timesteps=936000, episode_reward=197.60 +/- 0.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 936000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.286    |
|    n_updates        | 231499   |
----------------------------------
Eval num_timesteps=936500, episode_reward=157.91 +/- 60.10
Episode length: 408.58 +/- 167.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 409      |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 936500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0849   |
|    n_updates        | 231624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | 186      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3016     |
|    fps              | 47       |
|    time_elapsed     | 19862    |
|    total_timesteps  | 936835   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.135    |
|    n_updates        | 231708   |
----------------------------------
Eval num_timesteps=937000, episode_reward=196.89 +/- 1.40
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 937000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0843   |
|    n_updates        | 231749   |
----------------------------------
Eval num_timesteps=937500, episode_reward=91.78 +/- 21.45
Episode length: 80.44 +/- 92.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.4     |
|    mean_reward      | 91.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 937500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0561   |
|    n_updates        | 231874   |
----------------------------------
Eval num_timesteps=938000, episode_reward=146.50 +/- 54.55
Episode length: 308.74 +/- 234.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 309      |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 938000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0143   |
|    n_updates        | 231999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | 186      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3020     |
|    fps              | 47       |
|    time_elapsed     | 19890    |
|    total_timesteps  | 938444   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0796   |
|    n_updates        | 232110   |
----------------------------------
Eval num_timesteps=938500, episode_reward=197.80 +/- 0.52
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 938500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0943   |
|    n_updates        | 232124   |
----------------------------------
Eval num_timesteps=939000, episode_reward=197.83 +/- 0.48
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 939000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.475    |
|    n_updates        | 232249   |
----------------------------------
Eval num_timesteps=939500, episode_reward=197.49 +/- 0.84
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 939500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0465   |
|    n_updates        | 232374   |
----------------------------------
Eval num_timesteps=940000, episode_reward=197.82 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 940000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0632   |
|    n_updates        | 232499   |
----------------------------------
Eval num_timesteps=940500, episode_reward=197.60 +/- 0.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 940500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.209    |
|    n_updates        | 232624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3024     |
|    fps              | 47       |
|    time_elapsed     | 19965    |
|    total_timesteps  | 940544   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0115   |
|    n_updates        | 232635   |
----------------------------------
Eval num_timesteps=941000, episode_reward=197.70 +/- 0.69
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 941000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0527   |
|    n_updates        | 232749   |
----------------------------------
Eval num_timesteps=941500, episode_reward=197.76 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 941500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0717   |
|    n_updates        | 232874   |
----------------------------------
Eval num_timesteps=942000, episode_reward=194.39 +/- 1.06
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 942000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0514   |
|    n_updates        | 232999   |
----------------------------------
Eval num_timesteps=942500, episode_reward=170.24 +/- 46.60
Episode length: 414.82 +/- 196.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 415      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 942500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0282   |
|    n_updates        | 233124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3028     |
|    fps              | 47       |
|    time_elapsed     | 20023    |
|    total_timesteps  | 942644   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0588   |
|    n_updates        | 233160   |
----------------------------------
Eval num_timesteps=943000, episode_reward=197.60 +/- 0.79
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 943000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0787   |
|    n_updates        | 233249   |
----------------------------------
Eval num_timesteps=943500, episode_reward=197.74 +/- 0.56
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 943500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.112    |
|    n_updates        | 233374   |
----------------------------------
Eval num_timesteps=944000, episode_reward=187.19 +/- 26.12
Episode length: 495.02 +/- 118.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 944000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0455   |
|    n_updates        | 233499   |
----------------------------------
Eval num_timesteps=944500, episode_reward=197.29 +/- 1.04
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 944500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0769   |
|    n_updates        | 233624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3032     |
|    fps              | 47       |
|    time_elapsed     | 20083    |
|    total_timesteps  | 944744   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0419   |
|    n_updates        | 233685   |
----------------------------------
Eval num_timesteps=945000, episode_reward=197.49 +/- 0.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 945000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0891   |
|    n_updates        | 233749   |
----------------------------------
Eval num_timesteps=945500, episode_reward=192.56 +/- 22.85
Episode length: 506.18 +/- 92.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 945500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0307   |
|    n_updates        | 233874   |
----------------------------------
Eval num_timesteps=946000, episode_reward=197.27 +/- 1.87
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 946000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0495   |
|    n_updates        | 233999   |
----------------------------------
Eval num_timesteps=946500, episode_reward=197.84 +/- 0.52
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 946500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0454   |
|    n_updates        | 234124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3036     |
|    fps              | 47       |
|    time_elapsed     | 20143    |
|    total_timesteps  | 946844   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.931    |
|    n_updates        | 234210   |
----------------------------------
Eval num_timesteps=947000, episode_reward=173.19 +/- 43.97
Episode length: 428.70 +/- 192.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 947000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.013    |
|    n_updates        | 234249   |
----------------------------------
Eval num_timesteps=947500, episode_reward=194.58 +/- 19.53
Episode length: 510.28 +/- 72.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 510      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 947500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0699   |
|    n_updates        | 234374   |
----------------------------------
Eval num_timesteps=948000, episode_reward=197.22 +/- 0.95
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 948000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0626   |
|    n_updates        | 234499   |
----------------------------------
Eval num_timesteps=948500, episode_reward=197.61 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 948500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0137   |
|    n_updates        | 234624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 189      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 46       |
|    time_elapsed     | 20200    |
|    total_timesteps  | 948944   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0542   |
|    n_updates        | 234735   |
----------------------------------
Eval num_timesteps=949000, episode_reward=197.79 +/- 0.59
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 949000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0345   |
|    n_updates        | 234749   |
----------------------------------
Eval num_timesteps=949500, episode_reward=197.73 +/- 1.08
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 949500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0465   |
|    n_updates        | 234874   |
----------------------------------
Eval num_timesteps=950000, episode_reward=197.82 +/- 0.54
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 950000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.00829  |
|    n_updates        | 234999   |
----------------------------------
Eval num_timesteps=950500, episode_reward=197.66 +/- 0.67
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 950500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.328    |
|    n_updates        | 235124   |
----------------------------------
Eval num_timesteps=951000, episode_reward=192.86 +/- 1.30
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 951000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0247   |
|    n_updates        | 235249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 46       |
|    time_elapsed     | 20276    |
|    total_timesteps  | 951044   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.102    |
|    n_updates        | 235260   |
----------------------------------
Eval num_timesteps=951500, episode_reward=192.14 +/- 21.38
Episode length: 506.20 +/- 92.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 951500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0516   |
|    n_updates        | 235374   |
----------------------------------
Eval num_timesteps=952000, episode_reward=195.89 +/- 1.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 952000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0296   |
|    n_updates        | 235499   |
----------------------------------
Eval num_timesteps=952500, episode_reward=197.56 +/- 0.76
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 952500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0844   |
|    n_updates        | 235624   |
----------------------------------
Eval num_timesteps=953000, episode_reward=197.90 +/- 0.35
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 953000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0266   |
|    n_updates        | 235749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3048     |
|    fps              | 46       |
|    time_elapsed     | 20337    |
|    total_timesteps  | 953144   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0265   |
|    n_updates        | 235785   |
----------------------------------
Eval num_timesteps=953500, episode_reward=195.11 +/- 15.55
Episode length: 517.00 +/- 56.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 517      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 953500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0208   |
|    n_updates        | 235874   |
----------------------------------
Eval num_timesteps=954000, episode_reward=197.69 +/- 0.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 954000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0444   |
|    n_updates        | 235999   |
----------------------------------
Eval num_timesteps=954500, episode_reward=197.88 +/- 0.39
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 954500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0647   |
|    n_updates        | 236124   |
----------------------------------
Eval num_timesteps=955000, episode_reward=197.64 +/- 0.65
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 955000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0977   |
|    n_updates        | 236249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3052     |
|    fps              | 46       |
|    time_elapsed     | 20398    |
|    total_timesteps  | 955244   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0285   |
|    n_updates        | 236310   |
----------------------------------
Eval num_timesteps=955500, episode_reward=194.60 +/- 15.35
Episode length: 515.06 +/- 69.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 955500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.00826  |
|    n_updates        | 236374   |
----------------------------------
Eval num_timesteps=956000, episode_reward=194.95 +/- 4.80
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 956000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.372    |
|    n_updates        | 236499   |
----------------------------------
Eval num_timesteps=956500, episode_reward=197.25 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 956500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0908   |
|    n_updates        | 236624   |
----------------------------------
Eval num_timesteps=957000, episode_reward=194.87 +/- 15.37
Episode length: 515.38 +/- 67.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 957000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.037    |
|    n_updates        | 236749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3056     |
|    fps              | 46       |
|    time_elapsed     | 20458    |
|    total_timesteps  | 957344   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0325   |
|    n_updates        | 236835   |
----------------------------------
Eval num_timesteps=957500, episode_reward=179.04 +/- 41.65
Episode length: 472.08 +/- 123.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 472      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 957500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.028    |
|    n_updates        | 236874   |
----------------------------------
Eval num_timesteps=958000, episode_reward=194.10 +/- 4.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 958000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0479   |
|    n_updates        | 236999   |
----------------------------------
Eval num_timesteps=958500, episode_reward=195.67 +/- 1.27
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 958500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0925   |
|    n_updates        | 237124   |
----------------------------------
Eval num_timesteps=959000, episode_reward=196.44 +/- 2.66
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 959000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0668   |
|    n_updates        | 237249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3060     |
|    fps              | 46       |
|    time_elapsed     | 20518    |
|    total_timesteps  | 959444   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0231   |
|    n_updates        | 237360   |
----------------------------------
Eval num_timesteps=959500, episode_reward=197.13 +/- 1.07
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 959500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0242   |
|    n_updates        | 237374   |
----------------------------------
Eval num_timesteps=960000, episode_reward=187.96 +/- 30.92
Episode length: 496.10 +/- 100.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 496      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 960000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0628   |
|    n_updates        | 237499   |
----------------------------------
Eval num_timesteps=960500, episode_reward=157.22 +/- 59.82
Episode length: 371.52 +/- 223.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 372      |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 960500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0276   |
|    n_updates        | 237624   |
----------------------------------
Eval num_timesteps=961000, episode_reward=197.22 +/- 0.92
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 961000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.049    |
|    n_updates        | 237749   |
----------------------------------
Eval num_timesteps=961500, episode_reward=176.78 +/- 42.08
Episode length: 438.22 +/- 185.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 438      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 961500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0508   |
|    n_updates        | 237874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3064     |
|    fps              | 46       |
|    time_elapsed     | 20585    |
|    total_timesteps  | 961544   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0324   |
|    n_updates        | 237885   |
----------------------------------
Eval num_timesteps=962000, episode_reward=196.11 +/- 1.10
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 962000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.127    |
|    n_updates        | 237999   |
----------------------------------
Eval num_timesteps=962500, episode_reward=188.37 +/- 30.09
Episode length: 486.56 +/- 130.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 962500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0353   |
|    n_updates        | 238124   |
----------------------------------
Eval num_timesteps=963000, episode_reward=179.03 +/- 40.04
Episode length: 453.60 +/- 163.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 454      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 963000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0688   |
|    n_updates        | 238249   |
----------------------------------
Eval num_timesteps=963500, episode_reward=197.44 +/- 0.81
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 963500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0173   |
|    n_updates        | 238374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3068     |
|    fps              | 46       |
|    time_elapsed     | 20643    |
|    total_timesteps  | 963644   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0153   |
|    n_updates        | 238410   |
----------------------------------
Eval num_timesteps=964000, episode_reward=179.28 +/- 40.10
Episode length: 447.92 +/- 176.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 448      |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 964000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0474   |
|    n_updates        | 238499   |
----------------------------------
Eval num_timesteps=964500, episode_reward=188.20 +/- 29.74
Episode length: 499.36 +/- 88.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 499      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 964500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0247   |
|    n_updates        | 238624   |
----------------------------------
Eval num_timesteps=965000, episode_reward=195.63 +/- 1.28
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 965000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.113    |
|    n_updates        | 238749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | 187      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3072     |
|    fps              | 46       |
|    time_elapsed     | 20686    |
|    total_timesteps  | 965328   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.168    |
|    n_updates        | 238831   |
----------------------------------
Eval num_timesteps=965500, episode_reward=157.07 +/- 52.25
Episode length: 353.70 +/- 228.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 354      |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 965500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.126    |
|    n_updates        | 238874   |
----------------------------------
Eval num_timesteps=966000, episode_reward=197.73 +/- 0.83
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 966000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0546   |
|    n_updates        | 238999   |
----------------------------------
Eval num_timesteps=966500, episode_reward=197.30 +/- 0.83
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 966500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.348    |
|    n_updates        | 239124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 186      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3076     |
|    fps              | 46       |
|    time_elapsed     | 20727    |
|    total_timesteps  | 966986   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0386   |
|    n_updates        | 239246   |
----------------------------------
Eval num_timesteps=967000, episode_reward=196.52 +/- 1.70
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 967000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.22     |
|    n_updates        | 239249   |
----------------------------------
Eval num_timesteps=967500, episode_reward=197.64 +/- 0.73
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 967500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0292   |
|    n_updates        | 239374   |
----------------------------------
Eval num_timesteps=968000, episode_reward=191.94 +/- 3.35
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 968000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.16     |
|    n_updates        | 239499   |
----------------------------------
Eval num_timesteps=968500, episode_reward=197.74 +/- 0.70
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 968500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0456   |
|    n_updates        | 239624   |
----------------------------------
Eval num_timesteps=969000, episode_reward=195.67 +/- 15.47
Episode length: 515.16 +/- 68.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 969000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0298   |
|    n_updates        | 239749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 186      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3080     |
|    fps              | 46       |
|    time_elapsed     | 20802    |
|    total_timesteps  | 969086   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.13     |
|    n_updates        | 239771   |
----------------------------------
Eval num_timesteps=969500, episode_reward=197.87 +/- 13.74
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 969500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0473   |
|    n_updates        | 239874   |
----------------------------------
Eval num_timesteps=970000, episode_reward=197.90 +/- 0.38
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 970000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0589   |
|    n_updates        | 239999   |
----------------------------------
Eval num_timesteps=970500, episode_reward=197.75 +/- 0.63
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 970500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.224    |
|    n_updates        | 240124   |
----------------------------------
Eval num_timesteps=971000, episode_reward=193.91 +/- 18.87
Episode length: 520.20 +/- 33.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 520      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 971000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.196    |
|    n_updates        | 240249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | 189      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3084     |
|    fps              | 46       |
|    time_elapsed     | 20863    |
|    total_timesteps  | 971186   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.1      |
|    n_updates        | 240296   |
----------------------------------
Eval num_timesteps=971500, episode_reward=197.76 +/- 0.62
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 971500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0968   |
|    n_updates        | 240374   |
----------------------------------
Eval num_timesteps=972000, episode_reward=195.46 +/- 15.45
Episode length: 515.22 +/- 68.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 972000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0219   |
|    n_updates        | 240499   |
----------------------------------
Eval num_timesteps=972500, episode_reward=197.78 +/- 0.54
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 972500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0152   |
|    n_updates        | 240624   |
----------------------------------
Eval num_timesteps=973000, episode_reward=193.89 +/- 1.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 973000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.00448  |
|    n_updates        | 240749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | 189      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3088     |
|    fps              | 46       |
|    time_elapsed     | 20923    |
|    total_timesteps  | 973286   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0455   |
|    n_updates        | 240821   |
----------------------------------
Eval num_timesteps=973500, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 973500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0759   |
|    n_updates        | 240874   |
----------------------------------
Eval num_timesteps=974000, episode_reward=197.75 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 974000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0583   |
|    n_updates        | 240999   |
----------------------------------
Eval num_timesteps=974500, episode_reward=197.08 +/- 1.45
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 974500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.127    |
|    n_updates        | 241124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | 188      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3092     |
|    fps              | 46       |
|    time_elapsed     | 20969    |
|    total_timesteps  | 974912   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0247   |
|    n_updates        | 241227   |
----------------------------------
Eval num_timesteps=975000, episode_reward=197.26 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 975000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0869   |
|    n_updates        | 241249   |
----------------------------------
Eval num_timesteps=975500, episode_reward=196.83 +/- 1.18
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 975500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.056    |
|    n_updates        | 241374   |
----------------------------------
Eval num_timesteps=976000, episode_reward=169.66 +/- 46.23
Episode length: 413.18 +/- 199.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 413      |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 976000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.248    |
|    n_updates        | 241499   |
----------------------------------
Eval num_timesteps=976500, episode_reward=104.61 +/- 39.91
Episode length: 136.44 +/- 170.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 105      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 976500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.941    |
|    n_updates        | 241624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | 187      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3096     |
|    fps              | 46       |
|    time_elapsed     | 21016    |
|    total_timesteps  | 976540   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0588   |
|    n_updates        | 241634   |
----------------------------------
Eval num_timesteps=977000, episode_reward=100.33 +/- 45.63
Episode length: 135.20 +/- 171.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 100      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 977000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0393   |
|    n_updates        | 241749   |
----------------------------------
Eval num_timesteps=977500, episode_reward=179.59 +/- 40.35
Episode length: 446.98 +/- 178.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 447      |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 977500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.193    |
|    n_updates        | 241874   |
----------------------------------
Eval num_timesteps=978000, episode_reward=177.17 +/- 22.77
Episode length: 496.84 +/- 111.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 497      |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 978000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.05     |
|    n_updates        | 241999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3100     |
|    fps              | 46       |
|    time_elapsed     | 21047    |
|    total_timesteps  | 978172   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.118    |
|    n_updates        | 242042   |
----------------------------------
Eval num_timesteps=978500, episode_reward=192.42 +/- 15.09
Episode length: 514.88 +/- 70.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 978500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0935   |
|    n_updates        | 242124   |
----------------------------------
Eval num_timesteps=979000, episode_reward=25.63 +/- 76.95
Episode length: 127.70 +/- 174.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 979000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.021    |
|    n_updates        | 242249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | 182      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3104     |
|    fps              | 46       |
|    time_elapsed     | 21066    |
|    total_timesteps  | 979342   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0576   |
|    n_updates        | 242335   |
----------------------------------
Eval num_timesteps=979500, episode_reward=82.48 +/- 85.22
Episode length: 194.30 +/- 217.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 82.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 979500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.147    |
|    n_updates        | 242374   |
----------------------------------
Eval num_timesteps=980000, episode_reward=197.33 +/- 0.96
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 980000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0622   |
|    n_updates        | 242499   |
----------------------------------
Eval num_timesteps=980500, episode_reward=187.21 +/- 4.36
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 980500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0751   |
|    n_updates        | 242624   |
----------------------------------
Eval num_timesteps=981000, episode_reward=196.63 +/- 1.77
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 981000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0625   |
|    n_updates        | 242749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | 182      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3108     |
|    fps              | 46       |
|    time_elapsed     | 21121    |
|    total_timesteps  | 981442   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0657   |
|    n_updates        | 242860   |
----------------------------------
Eval num_timesteps=981500, episode_reward=196.86 +/- 2.88
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 981500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0261   |
|    n_updates        | 242874   |
----------------------------------
Eval num_timesteps=982000, episode_reward=194.87 +/- 15.38
Episode length: 514.92 +/- 70.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 515      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 982000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0139   |
|    n_updates        | 242999   |
----------------------------------
Eval num_timesteps=982500, episode_reward=141.14 +/- 54.88
Episode length: 293.76 +/- 231.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 294      |
|    mean_reward      | 141      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 982500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0945   |
|    n_updates        | 243124   |
----------------------------------
Eval num_timesteps=983000, episode_reward=197.12 +/- 1.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 983000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0484   |
|    n_updates        | 243249   |
----------------------------------
Eval num_timesteps=983500, episode_reward=195.28 +/- 5.37
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 983500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.118    |
|    n_updates        | 243374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | 182      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3112     |
|    fps              | 46       |
|    time_elapsed     | 21190    |
|    total_timesteps  | 983542   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.147    |
|    n_updates        | 243385   |
----------------------------------
Eval num_timesteps=984000, episode_reward=144.33 +/- 52.81
Episode length: 306.48 +/- 237.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 306      |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 984000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0595   |
|    n_updates        | 243499   |
----------------------------------
Eval num_timesteps=984500, episode_reward=183.57 +/- 35.78
Episode length: 467.22 +/- 156.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 467      |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 984500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0101   |
|    n_updates        | 243624   |
----------------------------------
Eval num_timesteps=985000, episode_reward=197.65 +/- 1.34
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 985000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.038    |
|    n_updates        | 243749   |
----------------------------------
Eval num_timesteps=985500, episode_reward=196.84 +/- 2.40
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 985500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.934    |
|    n_updates        | 243874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3116     |
|    fps              | 46       |
|    time_elapsed     | 21246    |
|    total_timesteps  | 985642   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.177    |
|    n_updates        | 243910   |
----------------------------------
Eval num_timesteps=986000, episode_reward=197.25 +/- 1.72
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 986000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0741   |
|    n_updates        | 243999   |
----------------------------------
Eval num_timesteps=986500, episode_reward=197.54 +/- 0.78
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 986500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0297   |
|    n_updates        | 244124   |
----------------------------------
Eval num_timesteps=987000, episode_reward=192.68 +/- 21.50
Episode length: 505.54 +/- 95.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 987000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.017    |
|    n_updates        | 244249   |
----------------------------------
Eval num_timesteps=987500, episode_reward=197.75 +/- 0.61
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 987500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.136    |
|    n_updates        | 244374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3120     |
|    fps              | 46       |
|    time_elapsed     | 21306    |
|    total_timesteps  | 987742   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0215   |
|    n_updates        | 244435   |
----------------------------------
Eval num_timesteps=988000, episode_reward=197.75 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 988000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0469   |
|    n_updates        | 244499   |
----------------------------------
Eval num_timesteps=988500, episode_reward=185.81 +/- 33.74
Episode length: 477.94 +/- 141.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 988500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0954   |
|    n_updates        | 244624   |
----------------------------------
Eval num_timesteps=989000, episode_reward=125.65 +/- 45.66
Episode length: 251.94 +/- 233.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 252      |
|    mean_reward      | 126      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 989000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.115    |
|    n_updates        | 244749   |
----------------------------------
Eval num_timesteps=989500, episode_reward=197.76 +/- 0.55
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 989500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.12     |
|    n_updates        | 244874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | 185      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3124     |
|    fps              | 46       |
|    time_elapsed     | 21358    |
|    total_timesteps  | 989842   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0756   |
|    n_updates        | 244960   |
----------------------------------
Eval num_timesteps=990000, episode_reward=171.30 +/- 56.19
Episode length: 427.64 +/- 194.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 428      |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 990000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0919   |
|    n_updates        | 244999   |
----------------------------------
Eval num_timesteps=990500, episode_reward=197.30 +/- 1.26
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 990500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.058    |
|    n_updates        | 245124   |
----------------------------------
Eval num_timesteps=991000, episode_reward=197.85 +/- 0.44
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 991000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0664   |
|    n_updates        | 245249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3128     |
|    fps              | 46       |
|    time_elapsed     | 21401    |
|    total_timesteps  | 991461   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0518   |
|    n_updates        | 245365   |
----------------------------------
Eval num_timesteps=991500, episode_reward=187.56 +/- 30.75
Episode length: 495.22 +/- 101.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 495      |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 991500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.136    |
|    n_updates        | 245374   |
----------------------------------
Eval num_timesteps=992000, episode_reward=197.75 +/- 0.58
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 992000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0658   |
|    n_updates        | 245499   |
----------------------------------
Eval num_timesteps=992500, episode_reward=197.38 +/- 0.75
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 992500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0347   |
|    n_updates        | 245624   |
----------------------------------
Eval num_timesteps=993000, episode_reward=197.81 +/- 0.52
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 993000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0646   |
|    n_updates        | 245749   |
----------------------------------
Eval num_timesteps=993500, episode_reward=183.13 +/- 32.61
Episode length: 478.64 +/- 139.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 993500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0566   |
|    n_updates        | 245874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3132     |
|    fps              | 46       |
|    time_elapsed     | 21475    |
|    total_timesteps  | 993561   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.093    |
|    n_updates        | 245890   |
----------------------------------
Eval num_timesteps=994000, episode_reward=197.85 +/- 0.51
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 994000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0257   |
|    n_updates        | 245999   |
----------------------------------
Eval num_timesteps=994500, episode_reward=197.62 +/- 1.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 994500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0594   |
|    n_updates        | 246124   |
----------------------------------
Eval num_timesteps=995000, episode_reward=96.17 +/- 29.80
Episode length: 216.56 +/- 101.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 96.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 995000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.141    |
|    n_updates        | 246249   |
----------------------------------
Eval num_timesteps=995500, episode_reward=197.68 +/- 1.11
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 995500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.851    |
|    n_updates        | 246374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3136     |
|    fps              | 46       |
|    time_elapsed     | 21527    |
|    total_timesteps  | 995661   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.176    |
|    n_updates        | 246415   |
----------------------------------
Eval num_timesteps=996000, episode_reward=181.48 +/- 38.43
Episode length: 458.74 +/- 164.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 459      |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 996000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.03     |
|    n_updates        | 246499   |
----------------------------------
Eval num_timesteps=996500, episode_reward=197.61 +/- 0.71
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 996500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0291   |
|    n_updates        | 246624   |
----------------------------------
Eval num_timesteps=997000, episode_reward=159.45 +/- 51.71
Episode length: 366.96 +/- 220.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 367      |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 997000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.139    |
|    n_updates        | 246749   |
----------------------------------
Eval num_timesteps=997500, episode_reward=197.94 +/- 0.31
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 997500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.051    |
|    n_updates        | 246874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 184      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3140     |
|    fps              | 46       |
|    time_elapsed     | 21582    |
|    total_timesteps  | 997761   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0434   |
|    n_updates        | 246940   |
----------------------------------
Eval num_timesteps=998000, episode_reward=172.88 +/- 45.40
Episode length: 452.10 +/- 140.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 452      |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 998000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0674   |
|    n_updates        | 246999   |
----------------------------------
Eval num_timesteps=998500, episode_reward=197.87 +/- 0.43
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 998500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.0625   |
|    n_updates        | 247124   |
----------------------------------
Eval num_timesteps=999000, episode_reward=197.88 +/- 0.37
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 999000   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.552    |
|    n_updates        | 247249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | 183      |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 3144     |
|    fps              | 46       |
|    time_elapsed     | 21625    |
|    total_timesteps  | 999363   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.238    |
|    n_updates        | 247340   |
----------------------------------
Eval num_timesteps=999500, episode_reward=197.96 +/- 0.20
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 999500   |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.222    |
|    n_updates        | 247374   |
----------------------------------
Eval num_timesteps=1000000, episode_reward=196.03 +/- 3.02
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 1000000  |
| train/              |          |
|    learning_rate    | 0.0005   |
|    loss             | 0.12     |
|    n_updates        | 247499   |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/dqn-stop-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
