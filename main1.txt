/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-4483.44 +/- 503.68
Episode length: 92.92 +/- 54.71
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.9      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 500       |
----------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-4544.16 +/- 505.37
Episode length: 96.00 +/- 76.45
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96        |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 1000      |
----------------------------------
Eval num_timesteps=1500, episode_reward=-4391.15 +/- 655.22
Episode length: 98.50 +/- 58.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.5      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 1500      |
----------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-4528.55 +/- 589.27
Episode length: 80.42 +/- 41.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 80.4      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 2000      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 67.2      |
|    ep_rew_mean     | -4.07e+03 |
| time/              |           |
|    fps             | 130       |
|    iterations      | 1         |
|    time_elapsed    | 15        |
|    total_timesteps | 2048      |
----------------------------------
Eval num_timesteps=2500, episode_reward=-4586.76 +/- 595.97
Episode length: 52.56 +/- 13.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.6      |
|    mean_reward          | -4.59e+03 |
| time/                   |           |
|    total_timesteps      | 2500      |
| train/                  |           |
|    approx_kl            | 18.177273 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.887    |
|    explained_variance   | 2.07e-05  |
|    learning_rate        | 0.0001    |
|    loss                 | 3.13e+05  |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.347     |
|    value_loss           | 7.91e+05  |
---------------------------------------
Eval num_timesteps=3000, episode_reward=-4371.14 +/- 833.52
Episode length: 50.80 +/- 18.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 3000      |
----------------------------------
New best mean reward!
Eval num_timesteps=3500, episode_reward=-4541.55 +/- 655.16
Episode length: 45.54 +/- 13.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.5      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 3500      |
----------------------------------
Eval num_timesteps=4000, episode_reward=-4621.14 +/- 653.07
Episode length: 53.28 +/- 20.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.62e+03 |
| time/              |           |
|    total_timesteps | 4000      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 63.5      |
|    ep_rew_mean     | -4.26e+03 |
| time/              |           |
|    fps             | 153       |
|    iterations      | 2         |
|    time_elapsed    | 26        |
|    total_timesteps | 4096      |
----------------------------------
Eval num_timesteps=4500, episode_reward=-4449.14 +/- 644.84
Episode length: 48.32 +/- 17.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -4.45e+03 |
| time/                   |           |
|    total_timesteps      | 4500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.63e-09 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.94e+05  |
|    n_updates            | 20        |
|    policy_gradient_loss | -1.2e-09  |
|    value_loss           | 9.18e+05  |
---------------------------------------
Eval num_timesteps=5000, episode_reward=-4610.35 +/- 586.72
Episode length: 50.22 +/- 15.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 5000      |
----------------------------------
Eval num_timesteps=5500, episode_reward=-4513.53 +/- 692.74
Episode length: 47.84 +/- 18.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 5500      |
----------------------------------
Eval num_timesteps=6000, episode_reward=-4464.75 +/- 648.43
Episode length: 46.96 +/- 13.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47        |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 6000      |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.8      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 164       |
|    iterations      | 3         |
|    time_elapsed    | 37        |
|    total_timesteps | 6144      |
----------------------------------
Eval num_timesteps=6500, episode_reward=-4389.94 +/- 652.12
Episode length: 48.68 +/- 14.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -4.39e+03 |
| time/                   |           |
|    total_timesteps      | 6500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.32e-15 |
|    explained_variance   | 0.000231  |
|    learning_rate        | 0.0001    |
|    loss                 | 5.4e+05   |
|    n_updates            | 30        |
|    policy_gradient_loss | -6.16e-10 |
|    value_loss           | 1.07e+06  |
---------------------------------------
Eval num_timesteps=7000, episode_reward=-4389.55 +/- 879.98
Episode length: 50.48 +/- 21.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 7000      |
----------------------------------
Eval num_timesteps=7500, episode_reward=-4266.75 +/- 891.48
Episode length: 50.00 +/- 17.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 7500      |
----------------------------------
New best mean reward!
Eval num_timesteps=8000, episode_reward=-4184.33 +/- 856.14
Episode length: 47.14 +/- 14.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.1      |
|    mean_reward     | -4.18e+03 |
| time/              |           |
|    total_timesteps | 8000      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.1      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 169       |
|    iterations      | 4         |
|    time_elapsed    | 48        |
|    total_timesteps | 8192      |
----------------------------------
Eval num_timesteps=8500, episode_reward=-4561.14 +/- 532.00
Episode length: 51.44 +/- 19.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -4.56e+03 |
| time/                   |           |
|    total_timesteps      | 8500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.97e-13 |
|    explained_variance   | 0.097     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.35e+05  |
|    n_updates            | 40        |
|    policy_gradient_loss | -5.43e-10 |
|    value_loss           | 1.03e+06  |
---------------------------------------
Eval num_timesteps=9000, episode_reward=-4441.15 +/- 629.54
Episode length: 55.46 +/- 22.14
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55.5      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 9000      |
----------------------------------
Eval num_timesteps=9500, episode_reward=-4414.75 +/- 803.31
Episode length: 50.68 +/- 19.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 9500      |
----------------------------------
Eval num_timesteps=10000, episode_reward=-4466.74 +/- 674.41
Episode length: 46.36 +/- 15.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.4      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 10000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.9      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 173       |
|    iterations      | 5         |
|    time_elapsed    | 59        |
|    total_timesteps | 10240     |
----------------------------------
Eval num_timesteps=10500, episode_reward=-4344.74 +/- 753.71
Episode length: 51.64 +/- 21.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -4.34e+03    |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 2.910383e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.48e-07    |
|    explained_variance   | 0.283        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.06e+05     |
|    n_updates            | 50           |
|    policy_gradient_loss | -1.83e-08    |
|    value_loss           | 9.38e+05     |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-4579.55 +/- 728.93
Episode length: 51.08 +/- 19.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 11000     |
----------------------------------
Eval num_timesteps=11500, episode_reward=-4476.35 +/- 692.92
Episode length: 52.38 +/- 19.13
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.4      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 11500     |
----------------------------------
Eval num_timesteps=12000, episode_reward=-4333.40 +/- 900.24
Episode length: 53.46 +/- 16.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.5      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 12000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.4      |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 174       |
|    iterations      | 6         |
|    time_elapsed    | 70        |
|    total_timesteps | 12288     |
----------------------------------
Eval num_timesteps=12500, episode_reward=-4438.75 +/- 690.99
Episode length: 47.64 +/- 15.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.6          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 12500         |
| train/                  |               |
|    approx_kl            | 1.4551915e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.49e-06     |
|    explained_variance   | 0.542         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.35e+05      |
|    n_updates            | 60            |
|    policy_gradient_loss | -5.19e-07     |
|    value_loss           | 7.21e+05      |
-------------------------------------------
Eval num_timesteps=13000, episode_reward=-4429.16 +/- 658.16
Episode length: 48.90 +/- 16.89
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.9      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 13000     |
----------------------------------
Eval num_timesteps=13500, episode_reward=-4563.94 +/- 622.90
Episode length: 50.82 +/- 14.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 13500     |
----------------------------------
Eval num_timesteps=14000, episode_reward=-4246.74 +/- 769.57
Episode length: 44.50 +/- 13.71
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44.5      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 14000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.5      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 176       |
|    iterations      | 7         |
|    time_elapsed    | 81        |
|    total_timesteps | 14336     |
----------------------------------
Eval num_timesteps=14500, episode_reward=-4397.55 +/- 774.89
Episode length: 52.80 +/- 18.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.8          |
|    mean_reward          | -4.4e+03      |
| time/                   |               |
|    total_timesteps      | 14500         |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.79e-06     |
|    explained_variance   | 0.556         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.92e+05      |
|    n_updates            | 70            |
|    policy_gradient_loss | -5.5e-07      |
|    value_loss           | 6.15e+05      |
-------------------------------------------
Eval num_timesteps=15000, episode_reward=-4233.53 +/- 734.12
Episode length: 51.74 +/- 17.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.7      |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 15000     |
----------------------------------
Eval num_timesteps=15500, episode_reward=-4307.53 +/- 856.23
Episode length: 49.92 +/- 18.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 15500     |
----------------------------------
Eval num_timesteps=16000, episode_reward=-4326.35 +/- 827.41
Episode length: 47.64 +/- 14.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 16000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.3      |
|    ep_rew_mean     | -4.31e+03 |
| time/              |           |
|    fps             | 176       |
|    iterations      | 8         |
|    time_elapsed    | 92        |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=16500, episode_reward=-4370.34 +/- 636.16
Episode length: 49.38 +/- 15.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.4         |
|    mean_reward          | -4.37e+03    |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 8.492498e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.15e-05    |
|    explained_variance   | 0.597        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.23e+05     |
|    n_updates            | 80           |
|    policy_gradient_loss | -1.88e-07    |
|    value_loss           | 6.18e+05     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=-4544.75 +/- 684.84
Episode length: 50.80 +/- 17.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 17000     |
----------------------------------
Eval num_timesteps=17500, episode_reward=-4371.94 +/- 681.74
Episode length: 46.20 +/- 14.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.2      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 17500     |
----------------------------------
Eval num_timesteps=18000, episode_reward=-4252.34 +/- 826.07
Episode length: 48.64 +/- 15.41
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 18000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.6      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 177       |
|    iterations      | 9         |
|    time_elapsed    | 103       |
|    total_timesteps | 18432     |
----------------------------------
Eval num_timesteps=18500, episode_reward=-4510.75 +/- 727.88
Episode length: 53.72 +/- 17.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.7          |
|    mean_reward          | -4.51e+03     |
| time/                   |               |
|    total_timesteps      | 18500         |
| train/                  |               |
|    approx_kl            | 6.7811925e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.78e-05     |
|    explained_variance   | 0.678         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.52e+05      |
|    n_updates            | 90            |
|    policy_gradient_loss | -1.24e-05     |
|    value_loss           | 5.56e+05      |
-------------------------------------------
Eval num_timesteps=19000, episode_reward=-4459.54 +/- 689.66
Episode length: 53.32 +/- 18.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 19000     |
----------------------------------
Eval num_timesteps=19500, episode_reward=-4471.94 +/- 728.74
Episode length: 48.08 +/- 14.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 19500     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-4337.54 +/- 831.94
Episode length: 46.82 +/- 14.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 20000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 10        |
|    time_elapsed    | 114       |
|    total_timesteps | 20480     |
----------------------------------
Eval num_timesteps=20500, episode_reward=-4186.34 +/- 788.77
Episode length: 48.34 +/- 15.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.3          |
|    mean_reward          | -4.19e+03     |
| time/                   |               |
|    total_timesteps      | 20500         |
| train/                  |               |
|    approx_kl            | 2.7142232e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000123     |
|    explained_variance   | 0.652         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.27e+05      |
|    n_updates            | 100           |
|    policy_gradient_loss | -2.2e-06      |
|    value_loss           | 5.35e+05      |
-------------------------------------------
Eval num_timesteps=21000, episode_reward=-4309.54 +/- 698.07
Episode length: 44.74 +/- 13.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44.7      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 21000     |
----------------------------------
Eval num_timesteps=21500, episode_reward=-4367.95 +/- 682.44
Episode length: 48.98 +/- 13.45
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 21500     |
----------------------------------
Eval num_timesteps=22000, episode_reward=-4405.15 +/- 715.35
Episode length: 53.16 +/- 17.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.2      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 22000     |
----------------------------------
Eval num_timesteps=22500, episode_reward=-4377.95 +/- 657.72
Episode length: 51.76 +/- 18.62
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 22500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.6      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 176       |
|    iterations      | 11        |
|    time_elapsed    | 127       |
|    total_timesteps | 22528     |
----------------------------------
Eval num_timesteps=23000, episode_reward=-4365.55 +/- 700.92
Episode length: 47.22 +/- 15.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.2         |
|    mean_reward          | -4.37e+03    |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 3.434252e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04e-06    |
|    explained_variance   | 0.653        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.43e+05     |
|    n_updates            | 110          |
|    policy_gradient_loss | 2.38e-06     |
|    value_loss           | 4.84e+05     |
------------------------------------------
Eval num_timesteps=23500, episode_reward=-4499.16 +/- 688.31
Episode length: 51.12 +/- 20.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-4375.95 +/- 595.02
Episode length: 49.12 +/- 16.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 24000     |
----------------------------------
Eval num_timesteps=24500, episode_reward=-4411.15 +/- 770.82
Episode length: 50.96 +/- 16.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 24500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.2      |
|    ep_rew_mean     | -4.51e+03 |
| time/              |           |
|    fps             | 177       |
|    iterations      | 12        |
|    time_elapsed    | 138       |
|    total_timesteps | 24576     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-4243.92 +/- 870.01
Episode length: 48.06 +/- 17.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.1          |
|    mean_reward          | -4.24e+03     |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | 5.1388633e-07 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000317     |
|    explained_variance   | 0.663         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.47e+05      |
|    n_updates            | 120           |
|    policy_gradient_loss | -7.43e-06     |
|    value_loss           | 5.55e+05      |
-------------------------------------------
Eval num_timesteps=25500, episode_reward=-4519.95 +/- 712.07
Episode length: 50.02 +/- 14.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 25500     |
----------------------------------
Eval num_timesteps=26000, episode_reward=-4441.55 +/- 789.06
Episode length: 51.24 +/- 16.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 26000     |
----------------------------------
Eval num_timesteps=26500, episode_reward=-4292.74 +/- 770.13
Episode length: 48.12 +/- 17.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 26500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.4      |
|    ep_rew_mean     | -4.53e+03 |
| time/              |           |
|    fps             | 177       |
|    iterations      | 13        |
|    time_elapsed    | 149       |
|    total_timesteps | 26624     |
----------------------------------
Eval num_timesteps=27000, episode_reward=-4328.34 +/- 785.90
Episode length: 50.54 +/- 14.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.5          |
|    mean_reward          | -4.33e+03     |
| time/                   |               |
|    total_timesteps      | 27000         |
| train/                  |               |
|    approx_kl            | 2.0372681e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.71e-05     |
|    explained_variance   | 0.666         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.11e+05      |
|    n_updates            | 130           |
|    policy_gradient_loss | -3.19e-07     |
|    value_loss           | 5.38e+05      |
-------------------------------------------
Eval num_timesteps=27500, episode_reward=-4595.95 +/- 528.73
Episode length: 49.82 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -4.6e+03 |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-4455.15 +/- 739.75
Episode length: 51.24 +/- 21.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 28000     |
----------------------------------
Eval num_timesteps=28500, episode_reward=-4405.15 +/- 615.00
Episode length: 50.02 +/- 17.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 28500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.2      |
|    ep_rew_mean     | -4.49e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 14        |
|    time_elapsed    | 160       |
|    total_timesteps | 28672     |
----------------------------------
Eval num_timesteps=29000, episode_reward=-4425.95 +/- 771.85
Episode length: 52.54 +/- 18.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.5          |
|    mean_reward          | -4.43e+03     |
| time/                   |               |
|    total_timesteps      | 29000         |
| train/                  |               |
|    approx_kl            | 6.4319465e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.6e-05      |
|    explained_variance   | 0.666         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.26e+05      |
|    n_updates            | 140           |
|    policy_gradient_loss | -6.04e-06     |
|    value_loss           | 5.05e+05      |
-------------------------------------------
Eval num_timesteps=29500, episode_reward=-4267.55 +/- 822.97
Episode length: 45.76 +/- 16.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.8      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 29500     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-4449.94 +/- 685.48
Episode length: 47.58 +/- 15.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 30000     |
----------------------------------
Eval num_timesteps=30500, episode_reward=-4447.15 +/- 683.29
Episode length: 49.28 +/- 16.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 30500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50        |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 15        |
|    time_elapsed    | 171       |
|    total_timesteps | 30720     |
----------------------------------
Eval num_timesteps=31000, episode_reward=-4471.55 +/- 676.74
Episode length: 53.36 +/- 18.81
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 53.4           |
|    mean_reward          | -4.47e+03      |
| time/                   |                |
|    total_timesteps      | 31000          |
| train/                  |                |
|    approx_kl            | 1.36496965e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.78e-05      |
|    explained_variance   | 0.684          |
|    learning_rate        | 0.0001         |
|    loss                 | 2.66e+05       |
|    n_updates            | 150            |
|    policy_gradient_loss | -5.41e-07      |
|    value_loss           | 5.07e+05       |
--------------------------------------------
Eval num_timesteps=31500, episode_reward=-4429.14 +/- 713.74
Episode length: 50.46 +/- 17.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 31500     |
----------------------------------
Eval num_timesteps=32000, episode_reward=-4153.55 +/- 782.93
Episode length: 46.82 +/- 12.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.15e+03 |
| time/              |           |
|    total_timesteps | 32000     |
----------------------------------
New best mean reward!
Eval num_timesteps=32500, episode_reward=-4271.14 +/- 756.57
Episode length: 48.14 +/- 19.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 32500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48        |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 16        |
|    time_elapsed    | 182       |
|    total_timesteps | 32768     |
----------------------------------
Eval num_timesteps=33000, episode_reward=-4557.53 +/- 592.38
Episode length: 51.90 +/- 20.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.9          |
|    mean_reward          | -4.56e+03     |
| time/                   |               |
|    total_timesteps      | 33000         |
| train/                  |               |
|    approx_kl            | 2.9732473e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000177     |
|    explained_variance   | 0.73          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.68e+05      |
|    n_updates            | 160           |
|    policy_gradient_loss | -5.6e-06      |
|    value_loss           | 4.9e+05       |
-------------------------------------------
Eval num_timesteps=33500, episode_reward=-4443.13 +/- 799.64
Episode length: 49.40 +/- 17.62
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 33500     |
----------------------------------
Eval num_timesteps=34000, episode_reward=-4280.74 +/- 869.74
Episode length: 49.58 +/- 19.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 34000     |
----------------------------------
Eval num_timesteps=34500, episode_reward=-4263.94 +/- 804.10
Episode length: 50.52 +/- 14.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 34500     |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 179      |
|    iterations      | 17       |
|    time_elapsed    | 194      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-4505.55 +/- 646.98
Episode length: 53.10 +/- 16.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.1          |
|    mean_reward          | -4.51e+03     |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | 2.5584304e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000209     |
|    explained_variance   | 0.753         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.32e+05      |
|    n_updates            | 170           |
|    policy_gradient_loss | -1.49e-05     |
|    value_loss           | 4.02e+05      |
-------------------------------------------
Eval num_timesteps=35500, episode_reward=-4348.35 +/- 800.39
Episode length: 52.00 +/- 19.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52        |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 35500     |
----------------------------------
Eval num_timesteps=36000, episode_reward=-4522.75 +/- 682.21
Episode length: 50.62 +/- 18.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 36000     |
----------------------------------
Eval num_timesteps=36500, episode_reward=-4577.95 +/- 643.19
Episode length: 47.82 +/- 17.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 36500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.8      |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 18        |
|    time_elapsed    | 205       |
|    total_timesteps | 36864     |
----------------------------------
Eval num_timesteps=37000, episode_reward=-4373.95 +/- 756.00
Episode length: 48.72 +/- 19.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.7          |
|    mean_reward          | -4.37e+03     |
| time/                   |               |
|    total_timesteps      | 37000         |
| train/                  |               |
|    approx_kl            | 1.5442522e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000772     |
|    explained_variance   | 0.707         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.02e+05      |
|    n_updates            | 180           |
|    policy_gradient_loss | -4.14e-05     |
|    value_loss           | 4.68e+05      |
-------------------------------------------
Eval num_timesteps=37500, episode_reward=-4276.74 +/- 713.86
Episode length: 46.56 +/- 15.89
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.6      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 37500     |
----------------------------------
Eval num_timesteps=38000, episode_reward=-4368.35 +/- 747.57
Episode length: 47.74 +/- 14.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.7      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 38000     |
----------------------------------
Eval num_timesteps=38500, episode_reward=-4243.69 +/- 1006.15
Episode length: 50.16 +/- 17.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.24e+03 |
| time/              |           |
|    total_timesteps | 38500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.7      |
|    ep_rew_mean     | -4.28e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 19        |
|    time_elapsed    | 216       |
|    total_timesteps | 38912     |
----------------------------------
Eval num_timesteps=39000, episode_reward=-4290.75 +/- 801.93
Episode length: 47.74 +/- 15.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.7          |
|    mean_reward          | -4.29e+03     |
| time/                   |               |
|    total_timesteps      | 39000         |
| train/                  |               |
|    approx_kl            | 3.7497724e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00285      |
|    explained_variance   | 0.804         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.47e+05      |
|    n_updates            | 190           |
|    policy_gradient_loss | -7.82e-05     |
|    value_loss           | 3.18e+05      |
-------------------------------------------
Eval num_timesteps=39500, episode_reward=-4637.16 +/- 514.24
Episode length: 47.56 +/- 12.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.64e+03 |
| time/              |           |
|    total_timesteps | 39500     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-4307.14 +/- 759.72
Episode length: 46.42 +/- 12.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.4      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 40000     |
----------------------------------
Eval num_timesteps=40500, episode_reward=-4057.55 +/- 786.50
Episode length: 43.86 +/- 14.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 43.9      |
|    mean_reward     | -4.06e+03 |
| time/              |           |
|    total_timesteps | 40500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.9      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 20        |
|    time_elapsed    | 226       |
|    total_timesteps | 40960     |
----------------------------------
Eval num_timesteps=41000, episode_reward=-4403.13 +/- 723.54
Episode length: 50.20 +/- 21.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.2          |
|    mean_reward          | -4.4e+03      |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 1.4648889e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00147      |
|    explained_variance   | 0.767         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.26e+05      |
|    n_updates            | 200           |
|    policy_gradient_loss | -1.98e-05     |
|    value_loss           | 3.52e+05      |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=-4492.34 +/- 765.59
Episode length: 49.58 +/- 16.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 41500     |
----------------------------------
Eval num_timesteps=42000, episode_reward=-4621.13 +/- 524.16
Episode length: 50.02 +/- 16.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.62e+03 |
| time/              |           |
|    total_timesteps | 42000     |
----------------------------------
Eval num_timesteps=42500, episode_reward=-4546.34 +/- 731.30
Episode length: 50.14 +/- 18.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.55e+03 |
| time/              |           |
|    total_timesteps | 42500     |
----------------------------------
Eval num_timesteps=43000, episode_reward=-4397.54 +/- 676.18
Episode length: 50.98 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.6      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 21        |
|    time_elapsed    | 239       |
|    total_timesteps | 43008     |
----------------------------------
Eval num_timesteps=43500, episode_reward=-4521.55 +/- 598.51
Episode length: 51.60 +/- 17.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.6          |
|    mean_reward          | -4.52e+03     |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 2.4780835e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00147      |
|    explained_variance   | 0.798         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.45e+05      |
|    n_updates            | 210           |
|    policy_gradient_loss | 0.000642      |
|    value_loss           | 3.31e+05      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=-4448.74 +/- 698.11
Episode length: 48.38 +/- 14.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 44000     |
----------------------------------
Eval num_timesteps=44500, episode_reward=-4513.14 +/- 661.73
Episode length: 50.08 +/- 15.05
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 44500     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-4260.35 +/- 750.17
Episode length: 52.30 +/- 16.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.3      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 45000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.8      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 22        |
|    time_elapsed    | 250       |
|    total_timesteps | 45056     |
----------------------------------
Eval num_timesteps=45500, episode_reward=-4444.74 +/- 755.63
Episode length: 46.22 +/- 12.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 46.2          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 45500         |
| train/                  |               |
|    approx_kl            | 2.8874987e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0043       |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.31e+05      |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.000214     |
|    value_loss           | 2.43e+05      |
-------------------------------------------
Eval num_timesteps=46000, episode_reward=-4337.14 +/- 773.27
Episode length: 45.48 +/- 13.60
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.5      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 46000     |
----------------------------------
Eval num_timesteps=46500, episode_reward=-4610.35 +/- 652.70
Episode length: 51.14 +/- 14.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 46500     |
----------------------------------
Eval num_timesteps=47000, episode_reward=-4540.34 +/- 679.51
Episode length: 51.60 +/- 15.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 47000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.6      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 23        |
|    time_elapsed    | 261       |
|    total_timesteps | 47104     |
----------------------------------
Eval num_timesteps=47500, episode_reward=-4363.63 +/- 960.69
Episode length: 47.18 +/- 15.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.2          |
|    mean_reward          | -4.36e+03     |
| time/                   |               |
|    total_timesteps      | 47500         |
| train/                  |               |
|    approx_kl            | 4.7916546e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00157      |
|    explained_variance   | 0.848         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2e+05       |
|    n_updates            | 230           |
|    policy_gradient_loss | -6.14e-05     |
|    value_loss           | 2.48e+05      |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=-4542.75 +/- 586.57
Episode length: 51.12 +/- 14.78
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 48000     |
----------------------------------
Eval num_timesteps=48500, episode_reward=-4302.75 +/- 875.21
Episode length: 46.76 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-4372.74 +/- 735.67
Episode length: 50.56 +/- 18.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 49000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 24        |
|    time_elapsed    | 272       |
|    total_timesteps | 49152     |
----------------------------------
Eval num_timesteps=49500, episode_reward=-4438.34 +/- 761.58
Episode length: 47.42 +/- 16.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.4          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 49500         |
| train/                  |               |
|    approx_kl            | 7.5035496e-06 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00442      |
|    explained_variance   | 0.846         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.98e+05      |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.000168     |
|    value_loss           | 2.85e+05      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-4265.94 +/- 861.77
Episode length: 46.82 +/- 16.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 50000     |
----------------------------------
Eval num_timesteps=50500, episode_reward=-4373.94 +/- 703.76
Episode length: 49.62 +/- 15.49
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 50500     |
----------------------------------
Eval num_timesteps=51000, episode_reward=-4532.35 +/- 789.10
Episode length: 49.34 +/- 14.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 51000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.3      |
|    ep_rew_mean     | -4.53e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 25        |
|    time_elapsed    | 283       |
|    total_timesteps | 51200     |
----------------------------------
Eval num_timesteps=51500, episode_reward=-4497.95 +/- 612.28
Episode length: 48.66 +/- 15.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.7         |
|    mean_reward          | -4.5e+03     |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 6.800692e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00143     |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.22e+04     |
|    n_updates            | 250          |
|    policy_gradient_loss | -1.51e-05    |
|    value_loss           | 2e+05        |
------------------------------------------
Eval num_timesteps=52000, episode_reward=-4550.34 +/- 620.44
Episode length: 49.32 +/- 14.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.55e+03 |
| time/              |           |
|    total_timesteps | 52000     |
----------------------------------
Eval num_timesteps=52500, episode_reward=-4291.16 +/- 817.09
Episode length: 50.14 +/- 16.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 52500     |
----------------------------------
Eval num_timesteps=53000, episode_reward=-4200.35 +/- 868.59
Episode length: 50.38 +/- 18.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -4.2e+03 |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.1      |
|    ep_rew_mean     | -4.55e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 26        |
|    time_elapsed    | 293       |
|    total_timesteps | 53248     |
----------------------------------
Eval num_timesteps=53500, episode_reward=-4452.74 +/- 720.88
Episode length: 46.68 +/- 14.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.7         |
|    mean_reward          | -4.45e+03    |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 4.408823e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00333     |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.61e+05     |
|    n_updates            | 260          |
|    policy_gradient_loss | -6.52e-05    |
|    value_loss           | 2.1e+05      |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-4429.94 +/- 542.35
Episode length: 46.82 +/- 14.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 54000     |
----------------------------------
Eval num_timesteps=54500, episode_reward=-4336.35 +/- 735.32
Episode length: 48.68 +/- 16.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 54500     |
----------------------------------
Eval num_timesteps=55000, episode_reward=-4351.15 +/- 694.81
Episode length: 48.98 +/- 17.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 55000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.8      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 27        |
|    time_elapsed    | 304       |
|    total_timesteps | 55296     |
----------------------------------
Eval num_timesteps=55500, episode_reward=-4354.75 +/- 962.76
Episode length: 50.16 +/- 18.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.2          |
|    mean_reward          | -4.35e+03     |
| time/                   |               |
|    total_timesteps      | 55500         |
| train/                  |               |
|    approx_kl            | 5.0044036e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00217      |
|    explained_variance   | 0.887         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.17e+05      |
|    n_updates            | 270           |
|    policy_gradient_loss | -8.3e-05      |
|    value_loss           | 2.28e+05      |
-------------------------------------------
Eval num_timesteps=56000, episode_reward=-4373.95 +/- 848.73
Episode length: 47.50 +/- 15.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.5      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 56000     |
----------------------------------
Eval num_timesteps=56500, episode_reward=-4373.53 +/- 810.92
Episode length: 48.40 +/- 19.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 56500     |
----------------------------------
Eval num_timesteps=57000, episode_reward=-4538.34 +/- 676.52
Episode length: 47.18 +/- 15.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.2      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 57000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 46.8      |
|    ep_rew_mean     | -4.31e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 28        |
|    time_elapsed    | 315       |
|    total_timesteps | 57344     |
----------------------------------
Eval num_timesteps=57500, episode_reward=-4527.55 +/- 565.27
Episode length: 50.64 +/- 13.98
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 50.6           |
|    mean_reward          | -4.53e+03      |
| time/                   |                |
|    total_timesteps      | 57500          |
| train/                  |                |
|    approx_kl            | 0.000119192875 |
|    clip_fraction        | 0.000977       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00819       |
|    explained_variance   | 0.893          |
|    learning_rate        | 0.0001         |
|    loss                 | 9.94e+04       |
|    n_updates            | 280            |
|    policy_gradient_loss | 2.45e-05       |
|    value_loss           | 2.05e+05       |
--------------------------------------------
Eval num_timesteps=58000, episode_reward=-4343.55 +/- 804.42
Episode length: 47.76 +/- 14.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 58000     |
----------------------------------
Eval num_timesteps=58500, episode_reward=-4454.35 +/- 747.36
Episode length: 49.32 +/- 16.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 58500     |
----------------------------------
Eval num_timesteps=59000, episode_reward=-4495.55 +/- 651.97
Episode length: 50.20 +/- 14.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 29        |
|    time_elapsed    | 326       |
|    total_timesteps | 59392     |
----------------------------------
Eval num_timesteps=59500, episode_reward=-4139.93 +/- 817.16
Episode length: 43.54 +/- 15.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 43.5         |
|    mean_reward          | -4.14e+03    |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0018503572 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00508     |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.09e+04     |
|    n_updates            | 290          |
|    policy_gradient_loss | 0.00122      |
|    value_loss           | 1.72e+05     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-4385.94 +/- 631.71
Episode length: 50.08 +/- 16.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 60000     |
----------------------------------
Eval num_timesteps=60500, episode_reward=-4413.54 +/- 810.34
Episode length: 48.30 +/- 13.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 60500     |
----------------------------------
Eval num_timesteps=61000, episode_reward=-4569.14 +/- 708.84
Episode length: 50.48 +/- 17.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 61000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.5      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 182       |
|    iterations      | 30        |
|    time_elapsed    | 337       |
|    total_timesteps | 61440     |
----------------------------------
Eval num_timesteps=61500, episode_reward=-4158.02 +/- 1024.75
Episode length: 47.06 +/- 18.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.1          |
|    mean_reward          | -4.16e+03     |
| time/                   |               |
|    total_timesteps      | 61500         |
| train/                  |               |
|    approx_kl            | 1.3959856e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00408      |
|    explained_variance   | 0.886         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.19e+04      |
|    n_updates            | 300           |
|    policy_gradient_loss | 3.18e-05      |
|    value_loss           | 1.8e+05       |
-------------------------------------------
Eval num_timesteps=62000, episode_reward=-4304.34 +/- 754.61
Episode length: 49.30 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-4463.95 +/- 744.02
Episode length: 50.30 +/- 17.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 62500     |
----------------------------------
Eval num_timesteps=63000, episode_reward=-4460.75 +/- 674.25
Episode length: 49.10 +/- 15.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 63000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.9      |
|    ep_rew_mean     | -4.44e+03 |
| time/              |           |
|    fps             | 182       |
|    iterations      | 31        |
|    time_elapsed    | 348       |
|    total_timesteps | 63488     |
----------------------------------
Eval num_timesteps=63500, episode_reward=-4628.34 +/- 609.17
Episode length: 49.18 +/- 14.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.2         |
|    mean_reward          | -4.63e+03    |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0003167932 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00595     |
|    explained_variance   | 0.889        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.08e+05     |
|    n_updates            | 310          |
|    policy_gradient_loss | 0.000207     |
|    value_loss           | 1.79e+05     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-4436.35 +/- 741.35
Episode length: 51.16 +/- 17.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 64000     |
----------------------------------
Eval num_timesteps=64500, episode_reward=-4370.34 +/- 760.77
Episode length: 54.82 +/- 19.60
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.8      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 64500     |
----------------------------------
Eval num_timesteps=65000, episode_reward=-4463.55 +/- 697.92
Episode length: 50.98 +/- 16.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 65000     |
----------------------------------
Eval num_timesteps=65500, episode_reward=-4401.55 +/- 850.57
Episode length: 51.66 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -4.5e+03 |
| time/              |          |
|    fps             | 181      |
|    iterations      | 32       |
|    time_elapsed    | 361      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-4271.96 +/- 839.14
Episode length: 52.94 +/- 15.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.9          |
|    mean_reward          | -4.27e+03     |
| time/                   |               |
|    total_timesteps      | 66000         |
| train/                  |               |
|    approx_kl            | 0.00077617425 |
|    clip_fraction        | 0.00435       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00595      |
|    explained_variance   | 0.881         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.71e+04      |
|    n_updates            | 320           |
|    policy_gradient_loss | 0.000267      |
|    value_loss           | 1.46e+05      |
-------------------------------------------
Eval num_timesteps=66500, episode_reward=-4547.53 +/- 677.50
Episode length: 51.78 +/- 15.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.55e+03 |
| time/              |           |
|    total_timesteps | 66500     |
----------------------------------
Eval num_timesteps=67000, episode_reward=-4413.94 +/- 772.31
Episode length: 46.68 +/- 15.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.7      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 67000     |
----------------------------------
Eval num_timesteps=67500, episode_reward=-4352.34 +/- 676.14
Episode length: 47.12 +/- 12.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.1      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 67500     |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -4.5e+03 |
| time/              |          |
|    fps             | 181      |
|    iterations      | 33       |
|    time_elapsed    | 372      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-4251.55 +/- 726.91
Episode length: 50.50 +/- 17.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.5          |
|    mean_reward          | -4.25e+03     |
| time/                   |               |
|    total_timesteps      | 68000         |
| train/                  |               |
|    approx_kl            | 4.7929934e-06 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00401      |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.37e+04      |
|    n_updates            | 330           |
|    policy_gradient_loss | 5.82e-05      |
|    value_loss           | 1.85e+05      |
-------------------------------------------
Eval num_timesteps=68500, episode_reward=-4527.15 +/- 657.95
Episode length: 53.38 +/- 16.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.4      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 68500     |
----------------------------------
Eval num_timesteps=69000, episode_reward=-4327.14 +/- 648.50
Episode length: 44.90 +/- 13.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44.9      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 69000     |
----------------------------------
Eval num_timesteps=69500, episode_reward=-4495.54 +/- 657.50
Episode length: 52.66 +/- 20.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53        |
|    ep_rew_mean     | -4.54e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 34        |
|    time_elapsed    | 383       |
|    total_timesteps | 69632     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-4406.74 +/- 814.80
Episode length: 52.44 +/- 16.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.4          |
|    mean_reward          | -4.41e+03     |
| time/                   |               |
|    total_timesteps      | 70000         |
| train/                  |               |
|    approx_kl            | 0.00013266513 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00327      |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.29e+04      |
|    n_updates            | 340           |
|    policy_gradient_loss | 0.000155      |
|    value_loss           | 1.35e+05      |
-------------------------------------------
Eval num_timesteps=70500, episode_reward=-4270.75 +/- 786.26
Episode length: 48.52 +/- 14.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 70500     |
----------------------------------
Eval num_timesteps=71000, episode_reward=-4404.33 +/- 723.24
Episode length: 47.76 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-4286.74 +/- 813.44
Episode length: 47.34 +/- 17.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.3      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 71500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.5      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 35        |
|    time_elapsed    | 394       |
|    total_timesteps | 71680     |
----------------------------------
Eval num_timesteps=72000, episode_reward=-4530.34 +/- 630.05
Episode length: 52.44 +/- 21.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.4          |
|    mean_reward          | -4.53e+03     |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 3.0297088e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000994     |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.07e+05      |
|    n_updates            | 350           |
|    policy_gradient_loss | -1.03e-05     |
|    value_loss           | 2.06e+05      |
-------------------------------------------
Eval num_timesteps=72500, episode_reward=-4443.14 +/- 772.04
Episode length: 51.46 +/- 15.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 72500     |
----------------------------------
Eval num_timesteps=73000, episode_reward=-4395.14 +/- 850.12
Episode length: 50.48 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-4505.14 +/- 566.88
Episode length: 48.92 +/- 17.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.9      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 73500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.8      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 36        |
|    time_elapsed    | 405       |
|    total_timesteps | 73728     |
----------------------------------
Eval num_timesteps=74000, episode_reward=-4354.35 +/- 713.66
Episode length: 47.42 +/- 15.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.4         |
|    mean_reward          | -4.35e+03    |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 6.254413e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00181     |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.0001       |
|    loss                 | 6.94e+04     |
|    n_updates            | 360          |
|    policy_gradient_loss | -1.29e-06    |
|    value_loss           | 1.5e+05      |
------------------------------------------
Eval num_timesteps=74500, episode_reward=-4583.14 +/- 614.44
Episode length: 50.48 +/- 14.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 74500     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-4490.34 +/- 654.95
Episode length: 49.12 +/- 14.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 75000     |
----------------------------------
Eval num_timesteps=75500, episode_reward=-4573.94 +/- 603.26
Episode length: 53.24 +/- 16.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.2      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 75500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 54        |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 37        |
|    time_elapsed    | 416       |
|    total_timesteps | 75776     |
----------------------------------
Eval num_timesteps=76000, episode_reward=-4432.35 +/- 748.06
Episode length: 51.62 +/- 15.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.6         |
|    mean_reward          | -4.43e+03    |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 7.473864e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00115     |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.0001       |
|    loss                 | 8.69e+04     |
|    n_updates            | 370          |
|    policy_gradient_loss | -1e-06       |
|    value_loss           | 1.64e+05     |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-4302.75 +/- 891.55
Episode length: 49.08 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-4527.94 +/- 720.98
Episode length: 49.08 +/- 19.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 77000     |
----------------------------------
Eval num_timesteps=77500, episode_reward=-4387.95 +/- 661.85
Episode length: 50.16 +/- 16.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 77500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52        |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 38        |
|    time_elapsed    | 428       |
|    total_timesteps | 77824     |
----------------------------------
Eval num_timesteps=78000, episode_reward=-4351.95 +/- 820.46
Episode length: 48.94 +/- 13.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.9          |
|    mean_reward          | -4.35e+03     |
| time/                   |               |
|    total_timesteps      | 78000         |
| train/                  |               |
|    approx_kl            | 2.4063047e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00209      |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.39e+05      |
|    n_updates            | 380           |
|    policy_gradient_loss | -5.33e-05     |
|    value_loss           | 2.52e+05      |
-------------------------------------------
Eval num_timesteps=78500, episode_reward=-4265.55 +/- 790.78
Episode length: 47.88 +/- 16.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 78500     |
----------------------------------
Eval num_timesteps=79000, episode_reward=-4263.94 +/- 832.41
Episode length: 46.44 +/- 17.89
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.4      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 79000     |
----------------------------------
Eval num_timesteps=79500, episode_reward=-4568.30 +/- 739.16
Episode length: 47.22 +/- 15.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.2      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 79500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 182       |
|    iterations      | 39        |
|    time_elapsed    | 438       |
|    total_timesteps | 79872     |
----------------------------------
Eval num_timesteps=80000, episode_reward=-4353.15 +/- 725.20
Episode length: 52.50 +/- 17.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.5          |
|    mean_reward          | -4.35e+03     |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 1.2823148e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00183      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.26e+04      |
|    n_updates            | 390           |
|    policy_gradient_loss | -1.64e-05     |
|    value_loss           | 1.47e+05      |
-------------------------------------------
Eval num_timesteps=80500, episode_reward=-4421.41 +/- 931.59
Episode length: 49.88 +/- 17.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 80500     |
----------------------------------
Eval num_timesteps=81000, episode_reward=-4546.35 +/- 527.24
Episode length: 50.72 +/- 15.39
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.55e+03 |
| time/              |           |
|    total_timesteps | 81000     |
----------------------------------
Eval num_timesteps=81500, episode_reward=-4384.34 +/- 737.31
Episode length: 50.82 +/- 17.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 81500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.2      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 40        |
|    time_elapsed    | 450       |
|    total_timesteps | 81920     |
----------------------------------
Eval num_timesteps=82000, episode_reward=-4521.14 +/- 629.78
Episode length: 47.32 +/- 14.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.3          |
|    mean_reward          | -4.52e+03     |
| time/                   |               |
|    total_timesteps      | 82000         |
| train/                  |               |
|    approx_kl            | 9.1968104e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00162      |
|    explained_variance   | 0.905         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.29e+04      |
|    n_updates            | 400           |
|    policy_gradient_loss | -4.51e-05     |
|    value_loss           | 1.57e+05      |
-------------------------------------------
Eval num_timesteps=82500, episode_reward=-4249.93 +/- 866.54
Episode length: 49.58 +/- 18.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 82500     |
----------------------------------
Eval num_timesteps=83000, episode_reward=-4403.14 +/- 857.42
Episode length: 49.90 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-4698.75 +/- 485.32
Episode length: 53.26 +/- 18.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -4.7e+03 |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.4      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 182       |
|    iterations      | 41        |
|    time_elapsed    | 461       |
|    total_timesteps | 83968     |
----------------------------------
Eval num_timesteps=84000, episode_reward=-4404.36 +/- 804.00
Episode length: 52.86 +/- 19.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.9          |
|    mean_reward          | -4.4e+03      |
| time/                   |               |
|    total_timesteps      | 84000         |
| train/                  |               |
|    approx_kl            | 8.5128704e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00144      |
|    explained_variance   | 0.899         |
|    learning_rate        | 0.0001        |
|    loss                 | 9.34e+04      |
|    n_updates            | 410           |
|    policy_gradient_loss | -9.98e-06     |
|    value_loss           | 1.81e+05      |
-------------------------------------------
Eval num_timesteps=84500, episode_reward=-4409.15 +/- 615.61
Episode length: 50.58 +/- 13.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 84500     |
----------------------------------
Eval num_timesteps=85000, episode_reward=-4573.14 +/- 584.31
Episode length: 48.58 +/- 16.38
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 85000     |
----------------------------------
Eval num_timesteps=85500, episode_reward=-4420.75 +/- 741.30
Episode length: 51.88 +/- 15.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 85500     |
----------------------------------
Eval num_timesteps=86000, episode_reward=-4534.74 +/- 734.61
Episode length: 50.96 +/- 17.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 86000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.7      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 42        |
|    time_elapsed    | 474       |
|    total_timesteps | 86016     |
----------------------------------
Eval num_timesteps=86500, episode_reward=-4503.55 +/- 596.00
Episode length: 48.36 +/- 14.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.4          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 86500         |
| train/                  |               |
|    approx_kl            | 0.00013963369 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00177      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.66e+04      |
|    n_updates            | 420           |
|    policy_gradient_loss | 0.000121      |
|    value_loss           | 1.55e+05      |
-------------------------------------------
Eval num_timesteps=87000, episode_reward=-4421.55 +/- 729.92
Episode length: 49.84 +/- 18.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 87000     |
----------------------------------
Eval num_timesteps=87500, episode_reward=-4388.74 +/- 843.59
Episode length: 53.48 +/- 17.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.5      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 87500     |
----------------------------------
Eval num_timesteps=88000, episode_reward=-4595.55 +/- 682.31
Episode length: 54.02 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -4.6e+03 |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 43        |
|    time_elapsed    | 486       |
|    total_timesteps | 88064     |
----------------------------------
Eval num_timesteps=88500, episode_reward=-4433.55 +/- 768.79
Episode length: 50.76 +/- 14.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.8          |
|    mean_reward          | -4.43e+03     |
| time/                   |               |
|    total_timesteps      | 88500         |
| train/                  |               |
|    approx_kl            | 2.2782478e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00124      |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.05e+05      |
|    n_updates            | 430           |
|    policy_gradient_loss | -4.27e-05     |
|    value_loss           | 1.61e+05      |
-------------------------------------------
Eval num_timesteps=89000, episode_reward=-4307.15 +/- 807.71
Episode length: 49.74 +/- 17.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 89000     |
----------------------------------
Eval num_timesteps=89500, episode_reward=-4463.95 +/- 689.97
Episode length: 54.24 +/- 19.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.2      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 89500     |
----------------------------------
Eval num_timesteps=90000, episode_reward=-4392.74 +/- 630.39
Episode length: 47.90 +/- 17.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 90000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 44        |
|    time_elapsed    | 497       |
|    total_timesteps | 90112     |
----------------------------------
Eval num_timesteps=90500, episode_reward=-4304.34 +/- 723.17
Episode length: 49.32 +/- 16.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.3          |
|    mean_reward          | -4.3e+03      |
| time/                   |               |
|    total_timesteps      | 90500         |
| train/                  |               |
|    approx_kl            | 6.1205355e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00102      |
|    explained_variance   | 0.905         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.49e+04      |
|    n_updates            | 440           |
|    policy_gradient_loss | 6.48e-07      |
|    value_loss           | 1.67e+05      |
-------------------------------------------
Eval num_timesteps=91000, episode_reward=-4535.14 +/- 617.05
Episode length: 50.66 +/- 14.51
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 91000     |
----------------------------------
Eval num_timesteps=91500, episode_reward=-4519.55 +/- 658.46
Episode length: 49.16 +/- 17.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 91500     |
----------------------------------
Eval num_timesteps=92000, episode_reward=-4381.55 +/- 785.27
Episode length: 50.24 +/- 18.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 92000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.1      |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 45        |
|    time_elapsed    | 508       |
|    total_timesteps | 92160     |
----------------------------------
Eval num_timesteps=92500, episode_reward=-4445.14 +/- 797.35
Episode length: 51.04 +/- 19.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51            |
|    mean_reward          | -4.45e+03     |
| time/                   |               |
|    total_timesteps      | 92500         |
| train/                  |               |
|    approx_kl            | 9.7672455e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00146      |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.73e+04      |
|    n_updates            | 450           |
|    policy_gradient_loss | -1.46e-05     |
|    value_loss           | 1.24e+05      |
-------------------------------------------
Eval num_timesteps=93000, episode_reward=-4435.15 +/- 578.60
Episode length: 52.48 +/- 19.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.5      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 93000     |
----------------------------------
Eval num_timesteps=93500, episode_reward=-4336.35 +/- 759.89
Episode length: 47.90 +/- 16.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 93500     |
----------------------------------
Eval num_timesteps=94000, episode_reward=-4333.95 +/- 785.36
Episode length: 49.04 +/- 15.07
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 94000     |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -4.5e+03 |
| time/              |          |
|    fps             | 181      |
|    iterations      | 46       |
|    time_elapsed    | 519      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-4401.55 +/- 622.87
Episode length: 48.60 +/- 15.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -4.4e+03      |
| time/                   |               |
|    total_timesteps      | 94500         |
| train/                  |               |
|    approx_kl            | 0.00011140981 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00429      |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.71e+04      |
|    n_updates            | 460           |
|    policy_gradient_loss | 6.65e-05      |
|    value_loss           | 1.25e+05      |
-------------------------------------------
Eval num_timesteps=95000, episode_reward=-4362.72 +/- 797.23
Episode length: 49.94 +/- 18.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 95000     |
----------------------------------
Eval num_timesteps=95500, episode_reward=-4390.35 +/- 715.43
Episode length: 49.52 +/- 16.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.5      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 95500     |
----------------------------------
Eval num_timesteps=96000, episode_reward=-4319.54 +/- 785.52
Episode length: 45.68 +/- 11.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.7      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 96000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.8      |
|    ep_rew_mean     | -4.47e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 47        |
|    time_elapsed    | 530       |
|    total_timesteps | 96256     |
----------------------------------
Eval num_timesteps=96500, episode_reward=-4389.15 +/- 629.00
Episode length: 49.58 +/- 17.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.6          |
|    mean_reward          | -4.39e+03     |
| time/                   |               |
|    total_timesteps      | 96500         |
| train/                  |               |
|    approx_kl            | 4.2648753e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00129      |
|    explained_variance   | 0.887         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.28e+05      |
|    n_updates            | 470           |
|    policy_gradient_loss | 2.51e-05      |
|    value_loss           | 1.95e+05      |
-------------------------------------------
Eval num_timesteps=97000, episode_reward=-4515.56 +/- 627.18
Episode length: 49.58 +/- 14.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 97000     |
----------------------------------
Eval num_timesteps=97500, episode_reward=-4332.36 +/- 748.84
Episode length: 48.74 +/- 15.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 97500     |
----------------------------------
Eval num_timesteps=98000, episode_reward=-4404.34 +/- 859.16
Episode length: 52.52 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.4      |
|    ep_rew_mean     | -4.56e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 48        |
|    time_elapsed    | 541       |
|    total_timesteps | 98304     |
----------------------------------
Eval num_timesteps=98500, episode_reward=-4543.54 +/- 672.30
Episode length: 52.52 +/- 19.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.5         |
|    mean_reward          | -4.54e+03    |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 9.138603e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.001       |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.81e+04     |
|    n_updates            | 480          |
|    policy_gradient_loss | -5.75e-07    |
|    value_loss           | 1.23e+05     |
------------------------------------------
Eval num_timesteps=99000, episode_reward=-4619.14 +/- 549.04
Episode length: 51.96 +/- 15.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52        |
|    mean_reward     | -4.62e+03 |
| time/              |           |
|    total_timesteps | 99000     |
----------------------------------
Eval num_timesteps=99500, episode_reward=-4406.34 +/- 733.13
Episode length: 51.28 +/- 18.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 99500     |
----------------------------------
Eval num_timesteps=100000, episode_reward=-4535.95 +/- 567.51
Episode length: 50.10 +/- 13.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 100000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.7      |
|    ep_rew_mean     | -4.44e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 49        |
|    time_elapsed    | 552       |
|    total_timesteps | 100352    |
----------------------------------
Eval num_timesteps=100500, episode_reward=-4496.74 +/- 675.09
Episode length: 47.10 +/- 14.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.1          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 100500        |
| train/                  |               |
|    approx_kl            | 1.6007107e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000542     |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.96e+04      |
|    n_updates            | 490           |
|    policy_gradient_loss | -1.2e-05      |
|    value_loss           | 1.78e+05      |
-------------------------------------------
Eval num_timesteps=101000, episode_reward=-4274.75 +/- 841.92
Episode length: 49.32 +/- 14.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 101000    |
----------------------------------
Eval num_timesteps=101500, episode_reward=-4449.13 +/- 730.24
Episode length: 49.44 +/- 17.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 101500    |
----------------------------------
Eval num_timesteps=102000, episode_reward=-4347.94 +/- 739.50
Episode length: 49.54 +/- 14.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.5      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 102000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 50        |
|    time_elapsed    | 563       |
|    total_timesteps | 102400    |
----------------------------------
Eval num_timesteps=102500, episode_reward=-4392.74 +/- 741.85
Episode length: 49.78 +/- 15.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -4.39e+03    |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 3.511933e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00105     |
|    explained_variance   | 0.886        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.43e+04     |
|    n_updates            | 500          |
|    policy_gradient_loss | 0.000129     |
|    value_loss           | 1.88e+05     |
------------------------------------------
Eval num_timesteps=103000, episode_reward=-4371.55 +/- 810.67
Episode length: 53.76 +/- 16.64
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.8      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 103000    |
----------------------------------
Eval num_timesteps=103500, episode_reward=-4399.56 +/- 755.62
Episode length: 53.82 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-4406.33 +/- 682.02
Episode length: 49.44 +/- 19.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 104000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -4.3e+03 |
| time/              |          |
|    fps             | 181      |
|    iterations      | 51       |
|    time_elapsed    | 575      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-4419.94 +/- 772.39
Episode length: 47.62 +/- 16.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.6         |
|    mean_reward          | -4.42e+03    |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 6.711343e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000874    |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.87e+04     |
|    n_updates            | 510          |
|    policy_gradient_loss | -2e-06       |
|    value_loss           | 1.77e+05     |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-4147.15 +/- 818.93
Episode length: 46.86 +/- 13.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.15e+03 |
| time/              |           |
|    total_timesteps | 105000    |
----------------------------------
Eval num_timesteps=105500, episode_reward=-4413.95 +/- 735.74
Episode length: 50.54 +/- 14.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 105500    |
----------------------------------
Eval num_timesteps=106000, episode_reward=-4381.15 +/- 637.65
Episode length: 49.14 +/- 14.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 106000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.9      |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 52        |
|    time_elapsed    | 586       |
|    total_timesteps | 106496    |
----------------------------------
Eval num_timesteps=106500, episode_reward=-4371.14 +/- 750.21
Episode length: 49.76 +/- 19.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.8          |
|    mean_reward          | -4.37e+03     |
| time/                   |               |
|    total_timesteps      | 106500        |
| train/                  |               |
|    approx_kl            | 0.00010461267 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00195      |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.25e+04      |
|    n_updates            | 520           |
|    policy_gradient_loss | -4.69e-05     |
|    value_loss           | 1.8e+05       |
-------------------------------------------
Eval num_timesteps=107000, episode_reward=-4353.15 +/- 876.54
Episode length: 47.28 +/- 18.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.3      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 107000    |
----------------------------------
Eval num_timesteps=107500, episode_reward=-4495.54 +/- 925.84
Episode length: 49.46 +/- 19.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-4517.56 +/- 608.91
Episode length: 49.76 +/- 14.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 108000    |
----------------------------------
Eval num_timesteps=108500, episode_reward=-4394.75 +/- 799.45
Episode length: 49.18 +/- 13.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 108500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.2      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 53        |
|    time_elapsed    | 599       |
|    total_timesteps | 108544    |
----------------------------------
Eval num_timesteps=109000, episode_reward=-4298.35 +/- 825.13
Episode length: 50.32 +/- 13.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.3          |
|    mean_reward          | -4.3e+03      |
| time/                   |               |
|    total_timesteps      | 109000        |
| train/                  |               |
|    approx_kl            | 8.2738145e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00144      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.05e+04      |
|    n_updates            | 530           |
|    policy_gradient_loss | 1.64e-05      |
|    value_loss           | 1.47e+05      |
-------------------------------------------
Eval num_timesteps=109500, episode_reward=-4469.55 +/- 711.84
Episode length: 51.76 +/- 17.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 109500    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-4410.35 +/- 664.76
Episode length: 51.30 +/- 17.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 110000    |
----------------------------------
Eval num_timesteps=110500, episode_reward=-4421.54 +/- 590.83
Episode length: 50.62 +/- 15.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 110500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.4      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 54        |
|    time_elapsed    | 610       |
|    total_timesteps | 110592    |
----------------------------------
Eval num_timesteps=111000, episode_reward=-4415.55 +/- 675.43
Episode length: 49.70 +/- 11.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.7          |
|    mean_reward          | -4.42e+03     |
| time/                   |               |
|    total_timesteps      | 111000        |
| train/                  |               |
|    approx_kl            | 1.4959369e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000838     |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.03e+05      |
|    n_updates            | 540           |
|    policy_gradient_loss | -1.22e-06     |
|    value_loss           | 1.46e+05      |
-------------------------------------------
Eval num_timesteps=111500, episode_reward=-4247.54 +/- 878.16
Episode length: 50.92 +/- 19.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 111500    |
----------------------------------
Eval num_timesteps=112000, episode_reward=-4355.55 +/- 847.93
Episode length: 47.52 +/- 15.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.5      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 112000    |
----------------------------------
Eval num_timesteps=112500, episode_reward=-4404.75 +/- 669.61
Episode length: 46.78 +/- 14.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.1      |
|    ep_rew_mean     | -4.54e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 55        |
|    time_elapsed    | 621       |
|    total_timesteps | 112640    |
----------------------------------
Eval num_timesteps=113000, episode_reward=-4371.55 +/- 812.85
Episode length: 52.54 +/- 15.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.5         |
|    mean_reward          | -4.37e+03    |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 7.485505e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000676    |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.04e+04     |
|    n_updates            | 550          |
|    policy_gradient_loss | 1.92e-05     |
|    value_loss           | 1.3e+05      |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-4356.34 +/- 935.43
Episode length: 47.76 +/- 17.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 113500    |
----------------------------------
Eval num_timesteps=114000, episode_reward=-4459.15 +/- 772.74
Episode length: 52.96 +/- 20.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53        |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 114000    |
----------------------------------
Eval num_timesteps=114500, episode_reward=-4252.74 +/- 799.31
Episode length: 50.66 +/- 16.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 114500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.4      |
|    ep_rew_mean     | -4.56e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 56        |
|    time_elapsed    | 632       |
|    total_timesteps | 114688    |
----------------------------------
Eval num_timesteps=115000, episode_reward=-4461.95 +/- 627.15
Episode length: 46.50 +/- 12.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 46.5          |
|    mean_reward          | -4.46e+03     |
| time/                   |               |
|    total_timesteps      | 115000        |
| train/                  |               |
|    approx_kl            | 2.0605512e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000776     |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.0001        |
|    loss                 | 9.59e+04      |
|    n_updates            | 560           |
|    policy_gradient_loss | -3.65e-06     |
|    value_loss           | 1.16e+05      |
-------------------------------------------
Eval num_timesteps=115500, episode_reward=-4376.74 +/- 719.22
Episode length: 51.64 +/- 23.14
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 115500    |
----------------------------------
Eval num_timesteps=116000, episode_reward=-4544.75 +/- 661.50
Episode length: 48.46 +/- 18.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 116000    |
----------------------------------
Eval num_timesteps=116500, episode_reward=-4408.74 +/- 689.56
Episode length: 51.44 +/- 15.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.4      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 116500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -4.6e+03 |
| time/              |          |
|    fps             | 181      |
|    iterations      | 57       |
|    time_elapsed    | 643      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-4556.35 +/- 665.79
Episode length: 52.60 +/- 14.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.6         |
|    mean_reward          | -4.56e+03    |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 4.985486e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00149     |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.77e+04     |
|    n_updates            | 570          |
|    policy_gradient_loss | 9.93e-06     |
|    value_loss           | 9.95e+04     |
------------------------------------------
Eval num_timesteps=117500, episode_reward=-4404.74 +/- 677.50
Episode length: 50.08 +/- 22.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-4294.73 +/- 673.98
Episode length: 47.92 +/- 19.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 118000    |
----------------------------------
Eval num_timesteps=118500, episode_reward=-4445.95 +/- 797.29
Episode length: 51.08 +/- 16.45
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 118500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.2      |
|    ep_rew_mean     | -4.49e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 58        |
|    time_elapsed    | 655       |
|    total_timesteps | 118784    |
----------------------------------
Eval num_timesteps=119000, episode_reward=-4574.34 +/- 630.96
Episode length: 54.50 +/- 16.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 54.5          |
|    mean_reward          | -4.57e+03     |
| time/                   |               |
|    total_timesteps      | 119000        |
| train/                  |               |
|    approx_kl            | 7.1013346e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00168      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.24e+04      |
|    n_updates            | 580           |
|    policy_gradient_loss | -2.45e-05     |
|    value_loss           | 1.37e+05      |
-------------------------------------------
Eval num_timesteps=119500, episode_reward=-4336.34 +/- 709.64
Episode length: 48.80 +/- 16.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 119500    |
----------------------------------
Eval num_timesteps=120000, episode_reward=-4279.95 +/- 849.04
Episode length: 46.94 +/- 12.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 120000    |
----------------------------------
Eval num_timesteps=120500, episode_reward=-4613.54 +/- 686.44
Episode length: 52.46 +/- 20.71
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.5      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 120500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 59        |
|    time_elapsed    | 666       |
|    total_timesteps | 120832    |
----------------------------------
Eval num_timesteps=121000, episode_reward=-4390.75 +/- 695.54
Episode length: 51.06 +/- 18.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.1          |
|    mean_reward          | -4.39e+03     |
| time/                   |               |
|    total_timesteps      | 121000        |
| train/                  |               |
|    approx_kl            | 4.7433423e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00153      |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.0001        |
|    loss                 | 9.5e+04       |
|    n_updates            | 590           |
|    policy_gradient_loss | -1.91e-05     |
|    value_loss           | 1.42e+05      |
-------------------------------------------
Eval num_timesteps=121500, episode_reward=-4466.74 +/- 620.19
Episode length: 50.30 +/- 16.41
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 121500    |
----------------------------------
Eval num_timesteps=122000, episode_reward=-4561.95 +/- 759.15
Episode length: 52.66 +/- 18.64
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.7      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 122000    |
----------------------------------
Eval num_timesteps=122500, episode_reward=-4400.35 +/- 801.68
Episode length: 51.56 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 181      |
|    iterations      | 60       |
|    time_elapsed    | 677      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-4284.34 +/- 764.56
Episode length: 48.94 +/- 17.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.9          |
|    mean_reward          | -4.28e+03     |
| time/                   |               |
|    total_timesteps      | 123000        |
| train/                  |               |
|    approx_kl            | 3.4223194e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00193      |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.47e+04      |
|    n_updates            | 600           |
|    policy_gradient_loss | 1.39e-05      |
|    value_loss           | 1.23e+05      |
-------------------------------------------
Eval num_timesteps=123500, episode_reward=-4400.35 +/- 862.75
Episode length: 49.38 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-4291.56 +/- 709.69
Episode length: 48.22 +/- 16.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 124000    |
----------------------------------
Eval num_timesteps=124500, episode_reward=-4492.35 +/- 634.25
Episode length: 51.88 +/- 18.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 124500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.4      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 61        |
|    time_elapsed    | 688       |
|    total_timesteps | 124928    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-4481.55 +/- 721.42
Episode length: 52.18 +/- 17.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -4.48e+03     |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 2.4214387e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00181      |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.0001        |
|    loss                 | 9.43e+04      |
|    n_updates            | 610           |
|    policy_gradient_loss | -7.67e-07     |
|    value_loss           | 1.55e+05      |
-------------------------------------------
Eval num_timesteps=125500, episode_reward=-4248.50 +/- 1051.81
Episode length: 46.82 +/- 12.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 125500    |
----------------------------------
Eval num_timesteps=126000, episode_reward=-4428.35 +/- 670.24
Episode length: 51.18 +/- 17.41
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 126000    |
----------------------------------
Eval num_timesteps=126500, episode_reward=-4421.16 +/- 690.87
Episode length: 53.22 +/- 16.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.2      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 126500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.3      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 62        |
|    time_elapsed    | 700       |
|    total_timesteps | 126976    |
----------------------------------
Eval num_timesteps=127000, episode_reward=-4432.35 +/- 973.69
Episode length: 53.90 +/- 20.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.9         |
|    mean_reward          | -4.43e+03    |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 7.794006e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00203     |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.57e+04     |
|    n_updates            | 620          |
|    policy_gradient_loss | 1.98e-06     |
|    value_loss           | 1.46e+05     |
------------------------------------------
Eval num_timesteps=127500, episode_reward=-4514.35 +/- 779.44
Episode length: 49.08 +/- 17.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 127500    |
----------------------------------
Eval num_timesteps=128000, episode_reward=-4564.35 +/- 608.51
Episode length: 52.80 +/- 17.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.8      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 128000    |
----------------------------------
Eval num_timesteps=128500, episode_reward=-4569.55 +/- 742.41
Episode length: 54.84 +/- 20.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.8      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 128500    |
----------------------------------
Eval num_timesteps=129000, episode_reward=-4304.34 +/- 684.82
Episode length: 52.66 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.5      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 63        |
|    time_elapsed    | 713       |
|    total_timesteps | 129024    |
----------------------------------
Eval num_timesteps=129500, episode_reward=-4322.74 +/- 658.98
Episode length: 45.64 +/- 18.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 45.6          |
|    mean_reward          | -4.32e+03     |
| time/                   |               |
|    total_timesteps      | 129500        |
| train/                  |               |
|    approx_kl            | 1.1369266e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00236      |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.34e+04      |
|    n_updates            | 630           |
|    policy_gradient_loss | 4.25e-05      |
|    value_loss           | 1.09e+05      |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-4419.54 +/- 784.61
Episode length: 54.78 +/- 23.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.8      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 130000    |
----------------------------------
Eval num_timesteps=130500, episode_reward=-4310.75 +/- 768.69
Episode length: 48.48 +/- 18.33
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 130500    |
----------------------------------
Eval num_timesteps=131000, episode_reward=-4439.15 +/- 598.30
Episode length: 50.76 +/- 13.49
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 131000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.4      |
|    ep_rew_mean     | -4.47e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 64        |
|    time_elapsed    | 725       |
|    total_timesteps | 131072    |
----------------------------------
Eval num_timesteps=131500, episode_reward=-4232.34 +/- 878.78
Episode length: 48.10 +/- 14.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.1          |
|    mean_reward          | -4.23e+03     |
| time/                   |               |
|    total_timesteps      | 131500        |
| train/                  |               |
|    approx_kl            | 3.9669976e-06 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00242      |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.51e+04      |
|    n_updates            | 640           |
|    policy_gradient_loss | 2.07e-06      |
|    value_loss           | 1.11e+05      |
-------------------------------------------
Eval num_timesteps=132000, episode_reward=-4482.75 +/- 662.96
Episode length: 49.80 +/- 17.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 132000    |
----------------------------------
Eval num_timesteps=132500, episode_reward=-4231.62 +/- 883.68
Episode length: 49.36 +/- 17.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 132500    |
----------------------------------
Eval num_timesteps=133000, episode_reward=-4608.75 +/- 503.79
Episode length: 52.06 +/- 17.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 133000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.7      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 65        |
|    time_elapsed    | 736       |
|    total_timesteps | 133120    |
----------------------------------
Eval num_timesteps=133500, episode_reward=-4421.54 +/- 710.06
Episode length: 53.38 +/- 20.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.4          |
|    mean_reward          | -4.42e+03     |
| time/                   |               |
|    total_timesteps      | 133500        |
| train/                  |               |
|    approx_kl            | 3.4837285e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00099      |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.71e+04      |
|    n_updates            | 650           |
|    policy_gradient_loss | 5.95e-06      |
|    value_loss           | 1.45e+05      |
-------------------------------------------
Eval num_timesteps=134000, episode_reward=-4518.35 +/- 622.80
Episode length: 54.48 +/- 20.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.5      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 134000    |
----------------------------------
Eval num_timesteps=134500, episode_reward=-4437.14 +/- 668.05
Episode length: 49.16 +/- 14.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 134500    |
----------------------------------
Eval num_timesteps=135000, episode_reward=-4323.14 +/- 725.76
Episode length: 45.46 +/- 16.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.5      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 135000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.4      |
|    ep_rew_mean     | -4.39e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 66        |
|    time_elapsed    | 747       |
|    total_timesteps | 135168    |
----------------------------------
Eval num_timesteps=135500, episode_reward=-4421.54 +/- 778.54
Episode length: 51.86 +/- 21.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.9         |
|    mean_reward          | -4.42e+03    |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 6.178743e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00121     |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11e+05     |
|    n_updates            | 660          |
|    policy_gradient_loss | -8.46e-06    |
|    value_loss           | 2.03e+05     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=-4346.35 +/- 717.31
Episode length: 49.40 +/- 14.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 136000    |
----------------------------------
Eval num_timesteps=136500, episode_reward=-4341.94 +/- 957.93
Episode length: 50.74 +/- 17.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 136500    |
----------------------------------
Eval num_timesteps=137000, episode_reward=-4465.56 +/- 708.86
Episode length: 57.28 +/- 20.74
----------------------------------
| eval/              |           |
|    mean_ep_length  | 57.3      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 137000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.5      |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 67        |
|    time_elapsed    | 759       |
|    total_timesteps | 137216    |
----------------------------------
Eval num_timesteps=137500, episode_reward=-4499.55 +/- 663.98
Episode length: 50.72 +/- 15.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.7          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 137500        |
| train/                  |               |
|    approx_kl            | 1.3387762e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00161      |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.29e+04      |
|    n_updates            | 670           |
|    policy_gradient_loss | 8.8e-07       |
|    value_loss           | 1.09e+05      |
-------------------------------------------
Eval num_timesteps=138000, episode_reward=-4499.53 +/- 664.68
Episode length: 51.18 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-4299.94 +/- 825.65
Episode length: 46.60 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-4415.12 +/- 703.05
Episode length: 45.74 +/- 15.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.7      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 139000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.7      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 68        |
|    time_elapsed    | 770       |
|    total_timesteps | 139264    |
----------------------------------
Eval num_timesteps=139500, episode_reward=-4464.35 +/- 741.09
Episode length: 44.86 +/- 14.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 44.9          |
|    mean_reward          | -4.46e+03     |
| time/                   |               |
|    total_timesteps      | 139500        |
| train/                  |               |
|    approx_kl            | 1.7584534e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00156      |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.83e+04      |
|    n_updates            | 680           |
|    policy_gradient_loss | 1.16e-06      |
|    value_loss           | 1.48e+05      |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-4327.54 +/- 769.18
Episode length: 46.88 +/- 15.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 140000    |
----------------------------------
Eval num_timesteps=140500, episode_reward=-4504.74 +/- 652.91
Episode length: 50.40 +/- 18.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-4621.14 +/- 687.54
Episode length: 54.54 +/- 22.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.5      |
|    mean_reward     | -4.62e+03 |
| time/              |           |
|    total_timesteps | 141000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.1      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 69        |
|    time_elapsed    | 781       |
|    total_timesteps | 141312    |
----------------------------------
Eval num_timesteps=141500, episode_reward=-4241.54 +/- 757.29
Episode length: 48.78 +/- 16.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.8          |
|    mean_reward          | -4.24e+03     |
| time/                   |               |
|    total_timesteps      | 141500        |
| train/                  |               |
|    approx_kl            | 9.1822585e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00158      |
|    explained_variance   | 0.895         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.12e+04      |
|    n_updates            | 690           |
|    policy_gradient_loss | 2.13e-07      |
|    value_loss           | 1.76e+05      |
-------------------------------------------
Eval num_timesteps=142000, episode_reward=-4549.94 +/- 701.91
Episode length: 51.46 +/- 16.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.55e+03 |
| time/              |           |
|    total_timesteps | 142000    |
----------------------------------
Eval num_timesteps=142500, episode_reward=-4410.61 +/- 704.49
Episode length: 48.10 +/- 16.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 142500    |
----------------------------------
Eval num_timesteps=143000, episode_reward=-4343.54 +/- 798.99
Episode length: 49.60 +/- 17.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 143000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.4      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 70        |
|    time_elapsed    | 793       |
|    total_timesteps | 143360    |
----------------------------------
Eval num_timesteps=143500, episode_reward=-4315.54 +/- 874.09
Episode length: 45.96 +/- 15.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 46            |
|    mean_reward          | -4.32e+03     |
| time/                   |               |
|    total_timesteps      | 143500        |
| train/                  |               |
|    approx_kl            | 1.1166849e-06 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00141      |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.23e+04      |
|    n_updates            | 700           |
|    policy_gradient_loss | -6.82e-06     |
|    value_loss           | 1.11e+05      |
-------------------------------------------
Eval num_timesteps=144000, episode_reward=-4447.94 +/- 910.31
Episode length: 49.28 +/- 16.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 144000    |
----------------------------------
Eval num_timesteps=144500, episode_reward=-4520.34 +/- 656.98
Episode length: 51.32 +/- 18.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 144500    |
----------------------------------
Eval num_timesteps=145000, episode_reward=-4511.15 +/- 729.01
Episode length: 50.10 +/- 17.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 145000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48        |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 71        |
|    time_elapsed    | 805       |
|    total_timesteps | 145408    |
----------------------------------
Eval num_timesteps=145500, episode_reward=-4529.15 +/- 619.23
Episode length: 49.68 +/- 12.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.7         |
|    mean_reward          | -4.53e+03    |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 3.623427e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0021      |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.69e+04     |
|    n_updates            | 710          |
|    policy_gradient_loss | 1.13e-06     |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=-4289.54 +/- 887.04
Episode length: 46.62 +/- 14.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.6      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 146000    |
----------------------------------
Eval num_timesteps=146500, episode_reward=-4412.74 +/- 604.46
Episode length: 49.24 +/- 14.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 146500    |
----------------------------------
Eval num_timesteps=147000, episode_reward=-4463.55 +/- 678.43
Episode length: 50.00 +/- 18.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 147000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.7      |
|    ep_rew_mean     | -4.39e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 72        |
|    time_elapsed    | 816       |
|    total_timesteps | 147456    |
----------------------------------
Eval num_timesteps=147500, episode_reward=-4408.89 +/- 842.83
Episode length: 48.74 +/- 15.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.7          |
|    mean_reward          | -4.41e+03     |
| time/                   |               |
|    total_timesteps      | 147500        |
| train/                  |               |
|    approx_kl            | 4.1886233e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00235      |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.6e+04       |
|    n_updates            | 720           |
|    policy_gradient_loss | -4.27e-07     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=-4428.73 +/- 675.69
Episode length: 45.88 +/- 15.77
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.9      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 148000    |
----------------------------------
Eval num_timesteps=148500, episode_reward=-4283.54 +/- 705.76
Episode length: 47.58 +/- 18.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 148500    |
----------------------------------
Eval num_timesteps=149000, episode_reward=-4465.14 +/- 658.78
Episode length: 51.62 +/- 18.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 149000    |
----------------------------------
Eval num_timesteps=149500, episode_reward=-4374.74 +/- 673.90
Episode length: 48.00 +/- 13.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 149500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 73        |
|    time_elapsed    | 829       |
|    total_timesteps | 149504    |
----------------------------------
Eval num_timesteps=150000, episode_reward=-4473.94 +/- 812.00
Episode length: 49.04 +/- 18.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49           |
|    mean_reward          | -4.47e+03    |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 8.530333e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00271     |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.94e+04     |
|    n_updates            | 730          |
|    policy_gradient_loss | -2.36e-05    |
|    value_loss           | 1.41e+05     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-4532.34 +/- 566.20
Episode length: 52.24 +/- 15.39
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.2      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 150500    |
----------------------------------
Eval num_timesteps=151000, episode_reward=-4391.15 +/- 1023.12
Episode length: 50.72 +/- 16.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 151000    |
----------------------------------
Eval num_timesteps=151500, episode_reward=-4391.54 +/- 794.84
Episode length: 48.56 +/- 15.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 151500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51        |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 74        |
|    time_elapsed    | 841       |
|    total_timesteps | 151552    |
----------------------------------
Eval num_timesteps=152000, episode_reward=-4574.35 +/- 643.36
Episode length: 49.28 +/- 13.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.3          |
|    mean_reward          | -4.57e+03     |
| time/                   |               |
|    total_timesteps      | 152000        |
| train/                  |               |
|    approx_kl            | 4.3268665e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00389      |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.33e+04      |
|    n_updates            | 740           |
|    policy_gradient_loss | -3e-06        |
|    value_loss           | 1.37e+05      |
-------------------------------------------
Eval num_timesteps=152500, episode_reward=-4445.14 +/- 754.89
Episode length: 51.82 +/- 16.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 152500    |
----------------------------------
Eval num_timesteps=153000, episode_reward=-4418.35 +/- 772.77
Episode length: 49.24 +/- 15.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 153000    |
----------------------------------
Eval num_timesteps=153500, episode_reward=-4425.15 +/- 670.39
Episode length: 49.32 +/- 14.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 153500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.8      |
|    ep_rew_mean     | -4.51e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 75        |
|    time_elapsed    | 852       |
|    total_timesteps | 153600    |
----------------------------------
Eval num_timesteps=154000, episode_reward=-4509.15 +/- 825.94
Episode length: 51.18 +/- 18.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.2          |
|    mean_reward          | -4.51e+03     |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 0.00015769797 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00417      |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.0001        |
|    loss                 | 5.43e+04      |
|    n_updates            | 750           |
|    policy_gradient_loss | 4.44e-06      |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=154500, episode_reward=-4214.34 +/- 837.14
Episode length: 46.24 +/- 12.62
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.2      |
|    mean_reward     | -4.21e+03 |
| time/              |           |
|    total_timesteps | 154500    |
----------------------------------
Eval num_timesteps=155000, episode_reward=-4643.95 +/- 553.57
Episode length: 57.06 +/- 20.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 57.1      |
|    mean_reward     | -4.64e+03 |
| time/              |           |
|    total_timesteps | 155000    |
----------------------------------
Eval num_timesteps=155500, episode_reward=-4284.34 +/- 659.14
Episode length: 51.26 +/- 19.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 155500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.51e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 76        |
|    time_elapsed    | 864       |
|    total_timesteps | 155648    |
----------------------------------
Eval num_timesteps=156000, episode_reward=-4378.35 +/- 851.72
Episode length: 47.50 +/- 15.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.5          |
|    mean_reward          | -4.38e+03     |
| time/                   |               |
|    total_timesteps      | 156000        |
| train/                  |               |
|    approx_kl            | 0.00020200852 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00247      |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.46e+04      |
|    n_updates            | 760           |
|    policy_gradient_loss | 5.33e-05      |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=156500, episode_reward=-4433.55 +/- 818.42
Episode length: 52.88 +/- 16.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.9      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 156500    |
----------------------------------
Eval num_timesteps=157000, episode_reward=-4401.94 +/- 711.42
Episode length: 46.32 +/- 14.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-4397.54 +/- 752.82
Episode length: 43.76 +/- 12.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.8     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.6      |
|    ep_rew_mean     | -4.52e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 77        |
|    time_elapsed    | 874       |
|    total_timesteps | 157696    |
----------------------------------
Eval num_timesteps=158000, episode_reward=-4355.53 +/- 761.30
Episode length: 50.10 +/- 20.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.1          |
|    mean_reward          | -4.36e+03     |
| time/                   |               |
|    total_timesteps      | 158000        |
| train/                  |               |
|    approx_kl            | 2.6689726e-05 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00203      |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.31e+04      |
|    n_updates            | 770           |
|    policy_gradient_loss | -2.72e-07     |
|    value_loss           | 1.11e+05      |
-------------------------------------------
Eval num_timesteps=158500, episode_reward=-4467.15 +/- 788.06
Episode length: 52.50 +/- 17.14
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.5      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 158500    |
----------------------------------
Eval num_timesteps=159000, episode_reward=-4279.55 +/- 792.92
Episode length: 50.64 +/- 17.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 159000    |
----------------------------------
Eval num_timesteps=159500, episode_reward=-4423.91 +/- 866.34
Episode length: 49.38 +/- 17.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 159500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.3      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 78        |
|    time_elapsed    | 886       |
|    total_timesteps | 159744    |
----------------------------------
Eval num_timesteps=160000, episode_reward=-4563.55 +/- 605.11
Episode length: 50.06 +/- 18.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.1         |
|    mean_reward          | -4.56e+03    |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 5.264324e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00147     |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0001       |
|    loss                 | 5.81e+04     |
|    n_updates            | 780          |
|    policy_gradient_loss | -1.23e-06    |
|    value_loss           | 1.21e+05     |
------------------------------------------
Eval num_timesteps=160500, episode_reward=-4365.46 +/- 733.81
Episode length: 54.46 +/- 18.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.5      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 160500    |
----------------------------------
Eval num_timesteps=161000, episode_reward=-4557.94 +/- 654.35
Episode length: 49.58 +/- 16.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 161000    |
----------------------------------
Eval num_timesteps=161500, episode_reward=-4447.55 +/- 802.89
Episode length: 52.22 +/- 16.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.2      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 161500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.3      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 79        |
|    time_elapsed    | 898       |
|    total_timesteps | 161792    |
----------------------------------
Eval num_timesteps=162000, episode_reward=-4345.55 +/- 698.85
Episode length: 50.14 +/- 17.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.1         |
|    mean_reward          | -4.35e+03    |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 3.620109e-06 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00172     |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.94e+04     |
|    n_updates            | 790          |
|    policy_gradient_loss | 1.09e-05     |
|    value_loss           | 1.13e+05     |
------------------------------------------
Eval num_timesteps=162500, episode_reward=-4481.95 +/- 695.47
Episode length: 56.08 +/- 19.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.1      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 162500    |
----------------------------------
Eval num_timesteps=163000, episode_reward=-4326.75 +/- 868.52
Episode length: 49.00 +/- 14.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 163000    |
----------------------------------
Eval num_timesteps=163500, episode_reward=-4501.95 +/- 659.73
Episode length: 52.44 +/- 15.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47        |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 80        |
|    time_elapsed    | 909       |
|    total_timesteps | 163840    |
----------------------------------
Eval num_timesteps=164000, episode_reward=-4235.94 +/- 1026.21
Episode length: 47.78 +/- 16.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.8          |
|    mean_reward          | -4.24e+03     |
| time/                   |               |
|    total_timesteps      | 164000        |
| train/                  |               |
|    approx_kl            | 1.9892468e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00201      |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.62e+04      |
|    n_updates            | 800           |
|    policy_gradient_loss | -1.23e-05     |
|    value_loss           | 1.29e+05      |
-------------------------------------------
Eval num_timesteps=164500, episode_reward=-4128.75 +/- 850.24
Episode length: 47.84 +/- 15.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.13e+03 |
| time/              |           |
|    total_timesteps | 164500    |
----------------------------------
Eval num_timesteps=165000, episode_reward=-4364.35 +/- 782.96
Episode length: 48.02 +/- 17.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 165000    |
----------------------------------
Eval num_timesteps=165500, episode_reward=-4493.94 +/- 593.09
Episode length: 49.94 +/- 17.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 165500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.1      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 81        |
|    time_elapsed    | 921       |
|    total_timesteps | 165888    |
----------------------------------
Eval num_timesteps=166000, episode_reward=-4354.34 +/- 701.79
Episode length: 48.66 +/- 16.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.7          |
|    mean_reward          | -4.35e+03     |
| time/                   |               |
|    total_timesteps      | 166000        |
| train/                  |               |
|    approx_kl            | 2.6537338e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00245      |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.02e+04      |
|    n_updates            | 810           |
|    policy_gradient_loss | 4.54e-06      |
|    value_loss           | 8.12e+04      |
-------------------------------------------
Eval num_timesteps=166500, episode_reward=-4267.94 +/- 729.44
Episode length: 48.34 +/- 18.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 166500    |
----------------------------------
Eval num_timesteps=167000, episode_reward=-4307.94 +/- 805.66
Episode length: 50.30 +/- 18.49
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 167000    |
----------------------------------
Eval num_timesteps=167500, episode_reward=-4427.54 +/- 859.32
Episode length: 51.98 +/- 16.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52        |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 167500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -4.5e+03 |
| time/              |          |
|    fps             | 180      |
|    iterations      | 82       |
|    time_elapsed    | 932      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-4419.26 +/- 948.73
Episode length: 52.78 +/- 19.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.8          |
|    mean_reward          | -4.42e+03     |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 0.00032722394 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00118      |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.92e+04      |
|    n_updates            | 820           |
|    policy_gradient_loss | -5.06e-05     |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=168500, episode_reward=-4190.34 +/- 846.35
Episode length: 46.34 +/- 19.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.3      |
|    mean_reward     | -4.19e+03 |
| time/              |           |
|    total_timesteps | 168500    |
----------------------------------
Eval num_timesteps=169000, episode_reward=-4231.14 +/- 812.48
Episode length: 45.06 +/- 15.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.1      |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 169000    |
----------------------------------
Eval num_timesteps=169500, episode_reward=-4385.94 +/- 741.10
Episode length: 50.70 +/- 19.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 169500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.6      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 83        |
|    time_elapsed    | 943       |
|    total_timesteps | 169984    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-4473.56 +/- 806.98
Episode length: 53.76 +/- 16.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.8          |
|    mean_reward          | -4.47e+03     |
| time/                   |               |
|    total_timesteps      | 170000        |
| train/                  |               |
|    approx_kl            | 3.2712705e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00137      |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.75e+04      |
|    n_updates            | 830           |
|    policy_gradient_loss | -8.84e-06     |
|    value_loss           | 1.49e+05      |
-------------------------------------------
Eval num_timesteps=170500, episode_reward=-4628.74 +/- 487.45
Episode length: 48.40 +/- 15.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.63e+03 |
| time/              |           |
|    total_timesteps | 170500    |
----------------------------------
Eval num_timesteps=171000, episode_reward=-4194.33 +/- 749.67
Episode length: 48.70 +/- 16.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.19e+03 |
| time/              |           |
|    total_timesteps | 171000    |
----------------------------------
Eval num_timesteps=171500, episode_reward=-4335.95 +/- 787.49
Episode length: 52.66 +/- 18.64
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.7      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 171500    |
----------------------------------
Eval num_timesteps=172000, episode_reward=-4411.87 +/- 844.38
Episode length: 49.62 +/- 19.41
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 172000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.7      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 84        |
|    time_elapsed    | 957       |
|    total_timesteps | 172032    |
----------------------------------
Eval num_timesteps=172500, episode_reward=-4263.22 +/- 1060.31
Episode length: 50.32 +/- 22.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.3         |
|    mean_reward          | -4.26e+03    |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 9.758514e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00228     |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.9e+04      |
|    n_updates            | 840          |
|    policy_gradient_loss | 1.3e-05      |
|    value_loss           | 9.17e+04     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=-4409.95 +/- 774.59
Episode length: 48.66 +/- 15.61
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 173000    |
----------------------------------
Eval num_timesteps=173500, episode_reward=-4341.63 +/- 926.17
Episode length: 49.78 +/- 16.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 173500    |
----------------------------------
Eval num_timesteps=174000, episode_reward=-4348.73 +/- 860.59
Episode length: 48.20 +/- 14.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 174000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.5      |
|    ep_rew_mean     | -4.47e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 85        |
|    time_elapsed    | 968       |
|    total_timesteps | 174080    |
----------------------------------
Eval num_timesteps=174500, episode_reward=-4594.74 +/- 723.13
Episode length: 53.24 +/- 19.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.2          |
|    mean_reward          | -4.59e+03     |
| time/                   |               |
|    total_timesteps      | 174500        |
| train/                  |               |
|    approx_kl            | 2.9905874e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00289      |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.33e+04      |
|    n_updates            | 850           |
|    policy_gradient_loss | 5.62e-06      |
|    value_loss           | 1.2e+05       |
-------------------------------------------
Eval num_timesteps=175000, episode_reward=-4543.95 +/- 678.56
Episode length: 52.12 +/- 16.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 175000    |
----------------------------------
Eval num_timesteps=175500, episode_reward=-4578.75 +/- 581.37
Episode length: 48.74 +/- 14.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 175500    |
----------------------------------
Eval num_timesteps=176000, episode_reward=-4251.15 +/- 794.37
Episode length: 50.86 +/- 18.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 176000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.9      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 86        |
|    time_elapsed    | 979       |
|    total_timesteps | 176128    |
----------------------------------
Eval num_timesteps=176500, episode_reward=-4360.34 +/- 724.94
Episode length: 46.16 +/- 14.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 46.2          |
|    mean_reward          | -4.36e+03     |
| time/                   |               |
|    total_timesteps      | 176500        |
| train/                  |               |
|    approx_kl            | 0.00013808641 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00396      |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.67e+04      |
|    n_updates            | 860           |
|    policy_gradient_loss | -7.74e-06     |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=177000, episode_reward=-4373.53 +/- 715.97
Episode length: 44.70 +/- 13.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44.7      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 177000    |
----------------------------------
Eval num_timesteps=177500, episode_reward=-4309.54 +/- 855.19
Episode length: 53.06 +/- 21.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.1      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 177500    |
----------------------------------
Eval num_timesteps=178000, episode_reward=-4483.95 +/- 567.77
Episode length: 50.04 +/- 21.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 178000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.1      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 87        |
|    time_elapsed    | 990       |
|    total_timesteps | 178176    |
----------------------------------
Eval num_timesteps=178500, episode_reward=-4422.34 +/- 717.69
Episode length: 49.12 +/- 13.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.1         |
|    mean_reward          | -4.42e+03    |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 1.516426e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00441     |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0001       |
|    loss                 | 4.02e+04     |
|    n_updates            | 870          |
|    policy_gradient_loss | 1.39e-05     |
|    value_loss           | 1.2e+05      |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-4515.15 +/- 821.91
Episode length: 57.96 +/- 17.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 58        |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 179000    |
----------------------------------
Eval num_timesteps=179500, episode_reward=-4452.75 +/- 709.92
Episode length: 47.82 +/- 12.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 179500    |
----------------------------------
Eval num_timesteps=180000, episode_reward=-4264.34 +/- 799.18
Episode length: 46.32 +/- 14.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.3      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 180000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.4      |
|    ep_rew_mean     | -4.44e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 88        |
|    time_elapsed    | 1002      |
|    total_timesteps | 180224    |
----------------------------------
Eval num_timesteps=180500, episode_reward=-4296.35 +/- 843.09
Episode length: 49.68 +/- 16.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.7         |
|    mean_reward          | -4.3e+03     |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 7.971306e-06 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0034      |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.93e+04     |
|    n_updates            | 880          |
|    policy_gradient_loss | 2.68e-05     |
|    value_loss           | 1.25e+05     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-4352.35 +/- 745.53
Episode length: 47.58 +/- 17.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 181000    |
----------------------------------
Eval num_timesteps=181500, episode_reward=-4322.75 +/- 767.26
Episode length: 51.72 +/- 16.07
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.7      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 181500    |
----------------------------------
Eval num_timesteps=182000, episode_reward=-4349.53 +/- 790.28
Episode length: 48.92 +/- 19.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.9      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 182000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.1      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 89        |
|    time_elapsed    | 1013      |
|    total_timesteps | 182272    |
----------------------------------
Eval num_timesteps=182500, episode_reward=-4557.15 +/- 590.68
Episode length: 52.70 +/- 17.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.7          |
|    mean_reward          | -4.56e+03     |
| time/                   |               |
|    total_timesteps      | 182500        |
| train/                  |               |
|    approx_kl            | 7.5054384e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00354      |
|    explained_variance   | 0.934         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.01e+04      |
|    n_updates            | 890           |
|    policy_gradient_loss | -2.01e-05     |
|    value_loss           | 9.46e+04      |
-------------------------------------------
Eval num_timesteps=183000, episode_reward=-4395.94 +/- 780.86
Episode length: 52.34 +/- 14.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-4447.55 +/- 598.72
Episode length: 50.14 +/- 15.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 183500    |
----------------------------------
Eval num_timesteps=184000, episode_reward=-4401.96 +/- 824.55
Episode length: 48.54 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.6      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 90        |
|    time_elapsed    | 1025      |
|    total_timesteps | 184320    |
----------------------------------
Eval num_timesteps=184500, episode_reward=-4319.53 +/- 801.49
Episode length: 48.16 +/- 20.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.2          |
|    mean_reward          | -4.32e+03     |
| time/                   |               |
|    total_timesteps      | 184500        |
| train/                  |               |
|    approx_kl            | 1.2446981e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00366      |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.67e+04      |
|    n_updates            | 900           |
|    policy_gradient_loss | 2.03e-05      |
|    value_loss           | 1.2e+05       |
-------------------------------------------
Eval num_timesteps=185000, episode_reward=-4164.73 +/- 804.53
Episode length: 44.76 +/- 11.33
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44.8      |
|    mean_reward     | -4.16e+03 |
| time/              |           |
|    total_timesteps | 185000    |
----------------------------------
Eval num_timesteps=185500, episode_reward=-4375.94 +/- 581.51
Episode length: 49.04 +/- 19.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 185500    |
----------------------------------
Eval num_timesteps=186000, episode_reward=-4349.15 +/- 962.55
Episode length: 49.66 +/- 16.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 186000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 179      |
|    iterations      | 91       |
|    time_elapsed    | 1036     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-4456.75 +/- 692.23
Episode length: 53.90 +/- 17.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.9         |
|    mean_reward          | -4.46e+03    |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 5.104122e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00178     |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.7e+04      |
|    n_updates            | 910          |
|    policy_gradient_loss | -1.75e-05    |
|    value_loss           | 1.36e+05     |
------------------------------------------
Eval num_timesteps=187000, episode_reward=-4394.34 +/- 677.42
Episode length: 51.34 +/- 14.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 187000    |
----------------------------------
Eval num_timesteps=187500, episode_reward=-4176.58 +/- 863.52
Episode length: 46.72 +/- 13.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.7      |
|    mean_reward     | -4.18e+03 |
| time/              |           |
|    total_timesteps | 187500    |
----------------------------------
Eval num_timesteps=188000, episode_reward=-4442.35 +/- 625.64
Episode length: 50.04 +/- 16.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 188000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.8      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 92        |
|    time_elapsed    | 1047      |
|    total_timesteps | 188416    |
----------------------------------
Eval num_timesteps=188500, episode_reward=-4539.55 +/- 655.53
Episode length: 49.48 +/- 15.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.5          |
|    mean_reward          | -4.54e+03     |
| time/                   |               |
|    total_timesteps      | 188500        |
| train/                  |               |
|    approx_kl            | 5.7939586e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00205      |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.61e+04      |
|    n_updates            | 920           |
|    policy_gradient_loss | -2.76e-05     |
|    value_loss           | 1.14e+05      |
-------------------------------------------
Eval num_timesteps=189000, episode_reward=-4371.13 +/- 613.33
Episode length: 46.16 +/- 16.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.2      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 189000    |
----------------------------------
Eval num_timesteps=189500, episode_reward=-4317.82 +/- 913.08
Episode length: 48.62 +/- 19.38
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 189500    |
----------------------------------
Eval num_timesteps=190000, episode_reward=-4471.15 +/- 772.87
Episode length: 52.10 +/- 16.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 190000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 93        |
|    time_elapsed    | 1059      |
|    total_timesteps | 190464    |
----------------------------------
Eval num_timesteps=190500, episode_reward=-4372.74 +/- 757.73
Episode length: 48.56 +/- 15.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.6         |
|    mean_reward          | -4.37e+03    |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 1.387237e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00185     |
|    explained_variance   | 0.909        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.05e+05     |
|    n_updates            | 930          |
|    policy_gradient_loss | -5.11e-05    |
|    value_loss           | 1.48e+05     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-4561.55 +/- 552.96
Episode length: 54.38 +/- 16.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.4      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 191000    |
----------------------------------
Eval num_timesteps=191500, episode_reward=-4431.94 +/- 813.62
Episode length: 50.52 +/- 18.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 191500    |
----------------------------------
Eval num_timesteps=192000, episode_reward=-4446.74 +/- 781.68
Episode length: 52.22 +/- 15.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.2      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 192000    |
----------------------------------
Eval num_timesteps=192500, episode_reward=-4530.36 +/- 678.38
Episode length: 55.62 +/- 17.39
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55.6      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 192500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 179      |
|    iterations      | 94       |
|    time_elapsed    | 1072     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-4404.35 +/- 724.94
Episode length: 48.02 +/- 17.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48            |
|    mean_reward          | -4.4e+03      |
| time/                   |               |
|    total_timesteps      | 193000        |
| train/                  |               |
|    approx_kl            | 2.0110747e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00234      |
|    explained_variance   | 0.936         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.39e+04      |
|    n_updates            | 940           |
|    policy_gradient_loss | 9.97e-07      |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=193500, episode_reward=-4356.35 +/- 677.48
Episode length: 47.74 +/- 15.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.7      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 193500    |
----------------------------------
Eval num_timesteps=194000, episode_reward=-4510.34 +/- 762.56
Episode length: 52.86 +/- 18.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.9      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 194000    |
----------------------------------
Eval num_timesteps=194500, episode_reward=-4297.94 +/- 777.26
Episode length: 48.12 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.44e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 95        |
|    time_elapsed    | 1084      |
|    total_timesteps | 194560    |
----------------------------------
Eval num_timesteps=195000, episode_reward=-4285.14 +/- 684.60
Episode length: 50.46 +/- 19.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.5          |
|    mean_reward          | -4.29e+03     |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 0.00041699523 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00156      |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.28e+04      |
|    n_updates            | 950           |
|    policy_gradient_loss | -4.73e-05     |
|    value_loss           | 9.37e+04      |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=-4460.75 +/- 677.26
Episode length: 53.54 +/- 18.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.5      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 195500    |
----------------------------------
Eval num_timesteps=196000, episode_reward=-4280.34 +/- 828.05
Episode length: 45.98 +/- 15.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46        |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 196000    |
----------------------------------
Eval num_timesteps=196500, episode_reward=-4530.75 +/- 697.90
Episode length: 49.06 +/- 17.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 196500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.4      |
|    ep_rew_mean     | -4.51e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 96        |
|    time_elapsed    | 1095      |
|    total_timesteps | 196608    |
----------------------------------
Eval num_timesteps=197000, episode_reward=-4275.94 +/- 796.61
Episode length: 46.32 +/- 14.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 46.3          |
|    mean_reward          | -4.28e+03     |
| time/                   |               |
|    total_timesteps      | 197000        |
| train/                  |               |
|    approx_kl            | 4.4811168e-07 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00157      |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.34e+04      |
|    n_updates            | 960           |
|    policy_gradient_loss | -2.91e-06     |
|    value_loss           | 1.29e+05      |
-------------------------------------------
Eval num_timesteps=197500, episode_reward=-4273.55 +/- 884.93
Episode length: 48.04 +/- 16.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 197500    |
----------------------------------
Eval num_timesteps=198000, episode_reward=-4478.73 +/- 707.68
Episode length: 48.42 +/- 16.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 198000    |
----------------------------------
Eval num_timesteps=198500, episode_reward=-4359.95 +/- 694.79
Episode length: 49.40 +/- 14.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 198500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.2      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 97        |
|    time_elapsed    | 1107      |
|    total_timesteps | 198656    |
----------------------------------
Eval num_timesteps=199000, episode_reward=-4371.95 +/- 709.97
Episode length: 46.34 +/- 16.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 46.3          |
|    mean_reward          | -4.37e+03     |
| time/                   |               |
|    total_timesteps      | 199000        |
| train/                  |               |
|    approx_kl            | 2.7813454e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00197      |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.61e+04      |
|    n_updates            | 970           |
|    policy_gradient_loss | -4.98e-05     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=199500, episode_reward=-4423.94 +/- 817.78
Episode length: 50.22 +/- 17.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 199500    |
----------------------------------
Eval num_timesteps=200000, episode_reward=-4413.95 +/- 696.93
Episode length: 49.72 +/- 18.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 200000    |
----------------------------------
Eval num_timesteps=200500, episode_reward=-4607.55 +/- 701.20
Episode length: 51.10 +/- 20.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 200500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.4      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 98        |
|    time_elapsed    | 1118      |
|    total_timesteps | 200704    |
----------------------------------
Eval num_timesteps=201000, episode_reward=-4325.55 +/- 716.10
Episode length: 53.20 +/- 18.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.2          |
|    mean_reward          | -4.33e+03     |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 0.00013295907 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00312      |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.66e+04      |
|    n_updates            | 980           |
|    policy_gradient_loss | -4.63e-05     |
|    value_loss           | 1.02e+05      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=-4590.35 +/- 579.47
Episode length: 52.90 +/- 17.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.9      |
|    mean_reward     | -4.59e+03 |
| time/              |           |
|    total_timesteps | 201500    |
----------------------------------
Eval num_timesteps=202000, episode_reward=-4518.35 +/- 630.55
Episode length: 50.12 +/- 17.27
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 202000    |
----------------------------------
Eval num_timesteps=202500, episode_reward=-4417.54 +/- 628.69
Episode length: 46.90 +/- 17.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 202500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.8      |
|    ep_rew_mean     | -4.39e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 99        |
|    time_elapsed    | 1129      |
|    total_timesteps | 202752    |
----------------------------------
Eval num_timesteps=203000, episode_reward=-4440.35 +/- 711.09
Episode length: 52.52 +/- 17.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.5          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 203000        |
| train/                  |               |
|    approx_kl            | 0.00010071468 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00454      |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.7e+04       |
|    n_updates            | 990           |
|    policy_gradient_loss | -1.98e-05     |
|    value_loss           | 1.11e+05      |
-------------------------------------------
Eval num_timesteps=203500, episode_reward=-4597.95 +/- 651.64
Episode length: 54.68 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -4.6e+03 |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-4359.84 +/- 825.99
Episode length: 46.30 +/- 14.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.3      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 204000    |
----------------------------------
Eval num_timesteps=204500, episode_reward=-4433.14 +/- 613.99
Episode length: 47.46 +/- 12.43
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.5      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 204500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.3      |
|    ep_rew_mean     | -4.49e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 100       |
|    time_elapsed    | 1141      |
|    total_timesteps | 204800    |
----------------------------------
Eval num_timesteps=205000, episode_reward=-4437.15 +/- 576.37
Episode length: 51.62 +/- 16.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.6          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 205000        |
| train/                  |               |
|    approx_kl            | 0.00010969766 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00365      |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.42e+04      |
|    n_updates            | 1000          |
|    policy_gradient_loss | 8.55e-05      |
|    value_loss           | 8.76e+04      |
-------------------------------------------
Eval num_timesteps=205500, episode_reward=-4295.95 +/- 847.43
Episode length: 52.18 +/- 18.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-4318.74 +/- 860.99
Episode length: 49.72 +/- 16.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 206000    |
----------------------------------
Eval num_timesteps=206500, episode_reward=-4396.34 +/- 873.51
Episode length: 52.72 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.6      |
|    ep_rew_mean     | -4.56e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 101       |
|    time_elapsed    | 1153      |
|    total_timesteps | 206848    |
----------------------------------
Eval num_timesteps=207000, episode_reward=-4349.55 +/- 810.29
Episode length: 49.72 +/- 15.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.7          |
|    mean_reward          | -4.35e+03     |
| time/                   |               |
|    total_timesteps      | 207000        |
| train/                  |               |
|    approx_kl            | 4.9447408e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0042       |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.52e+04      |
|    n_updates            | 1010          |
|    policy_gradient_loss | 4.58e-06      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=207500, episode_reward=-4259.94 +/- 795.60
Episode length: 46.96 +/- 18.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47        |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 207500    |
----------------------------------
Eval num_timesteps=208000, episode_reward=-4588.75 +/- 502.48
Episode length: 51.74 +/- 16.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.7      |
|    mean_reward     | -4.59e+03 |
| time/              |           |
|    total_timesteps | 208000    |
----------------------------------
Eval num_timesteps=208500, episode_reward=-4559.94 +/- 681.53
Episode length: 54.22 +/- 14.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.2      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 208500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.3      |
|    ep_rew_mean     | -4.56e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 102       |
|    time_elapsed    | 1164      |
|    total_timesteps | 208896    |
----------------------------------
Eval num_timesteps=209000, episode_reward=-4308.75 +/- 676.68
Episode length: 50.00 +/- 16.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50            |
|    mean_reward          | -4.31e+03     |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 1.2351986e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00521      |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.68e+04      |
|    n_updates            | 1020          |
|    policy_gradient_loss | -7.09e-06     |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=-4424.35 +/- 693.79
Episode length: 51.46 +/- 16.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 209500    |
----------------------------------
Eval num_timesteps=210000, episode_reward=-4439.54 +/- 701.52
Episode length: 48.88 +/- 19.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.9      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 210000    |
----------------------------------
Eval num_timesteps=210500, episode_reward=-4187.14 +/- 768.24
Episode length: 47.96 +/- 18.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.19e+03 |
| time/              |           |
|    total_timesteps | 210500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.1      |
|    ep_rew_mean     | -4.54e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 103       |
|    time_elapsed    | 1175      |
|    total_timesteps | 210944    |
----------------------------------
Eval num_timesteps=211000, episode_reward=-4297.94 +/- 907.58
Episode length: 50.24 +/- 18.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.2          |
|    mean_reward          | -4.3e+03      |
| time/                   |               |
|    total_timesteps      | 211000        |
| train/                  |               |
|    approx_kl            | 0.00010047178 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00432      |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.12e+04      |
|    n_updates            | 1030          |
|    policy_gradient_loss | -1.33e-05     |
|    value_loss           | 1.19e+05      |
-------------------------------------------
Eval num_timesteps=211500, episode_reward=-4349.15 +/- 689.88
Episode length: 48.88 +/- 17.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.9      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 211500    |
----------------------------------
Eval num_timesteps=212000, episode_reward=-4249.14 +/- 859.52
Episode length: 47.62 +/- 15.74
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 212000    |
----------------------------------
Eval num_timesteps=212500, episode_reward=-4569.54 +/- 707.84
Episode length: 53.46 +/- 18.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.5      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 212500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.4      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 104       |
|    time_elapsed    | 1187      |
|    total_timesteps | 212992    |
----------------------------------
Eval num_timesteps=213000, episode_reward=-4459.55 +/- 689.73
Episode length: 51.40 +/- 17.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.4         |
|    mean_reward          | -4.46e+03    |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 8.843982e-05 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00441     |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.93e+04     |
|    n_updates            | 1040         |
|    policy_gradient_loss | 8.44e-05     |
|    value_loss           | 1.27e+05     |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-4371.95 +/- 813.92
Episode length: 52.78 +/- 17.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.8      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 213500    |
----------------------------------
Eval num_timesteps=214000, episode_reward=-4367.95 +/- 777.58
Episode length: 52.32 +/- 19.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.3      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 214000    |
----------------------------------
Eval num_timesteps=214500, episode_reward=-4449.95 +/- 708.10
Episode length: 50.38 +/- 17.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 214500    |
----------------------------------
Eval num_timesteps=215000, episode_reward=-4264.73 +/- 747.00
Episode length: 45.70 +/- 17.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.7      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 215000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.5      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 105       |
|    time_elapsed    | 1200      |
|    total_timesteps | 215040    |
----------------------------------
Eval num_timesteps=215500, episode_reward=-4373.54 +/- 793.71
Episode length: 48.58 +/- 17.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -4.37e+03     |
| time/                   |               |
|    total_timesteps      | 215500        |
| train/                  |               |
|    approx_kl            | 3.2142561e-06 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0042       |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.8e+04       |
|    n_updates            | 1050          |
|    policy_gradient_loss | 6.57e-05      |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=216000, episode_reward=-4368.34 +/- 754.45
Episode length: 45.80 +/- 17.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.8      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 216000    |
----------------------------------
Eval num_timesteps=216500, episode_reward=-4371.94 +/- 770.04
Episode length: 48.72 +/- 18.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 216500    |
----------------------------------
Eval num_timesteps=217000, episode_reward=-4390.35 +/- 703.19
Episode length: 48.72 +/- 14.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 217000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.4      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 106       |
|    time_elapsed    | 1211      |
|    total_timesteps | 217088    |
----------------------------------
Eval num_timesteps=217500, episode_reward=-4281.54 +/- 787.24
Episode length: 47.36 +/- 17.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.4          |
|    mean_reward          | -4.28e+03     |
| time/                   |               |
|    total_timesteps      | 217500        |
| train/                  |               |
|    approx_kl            | 2.4127075e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00417      |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.21e+04      |
|    n_updates            | 1060          |
|    policy_gradient_loss | 7.2e-05       |
|    value_loss           | 1.18e+05      |
-------------------------------------------
Eval num_timesteps=218000, episode_reward=-4402.35 +/- 770.91
Episode length: 52.66 +/- 18.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-4393.55 +/- 714.41
Episode length: 51.16 +/- 15.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 218500    |
----------------------------------
Eval num_timesteps=219000, episode_reward=-4500.35 +/- 792.42
Episode length: 48.74 +/- 13.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.3      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 107       |
|    time_elapsed    | 1223      |
|    total_timesteps | 219136    |
----------------------------------
Eval num_timesteps=219500, episode_reward=-4400.74 +/- 764.41
Episode length: 49.24 +/- 16.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.2          |
|    mean_reward          | -4.4e+03      |
| time/                   |               |
|    total_timesteps      | 219500        |
| train/                  |               |
|    approx_kl            | 6.3108164e-06 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00407      |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.55e+04      |
|    n_updates            | 1070          |
|    policy_gradient_loss | -9.65e-05     |
|    value_loss           | 1.25e+05      |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=-4260.34 +/- 771.87
Episode length: 49.86 +/- 16.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 220000    |
----------------------------------
Eval num_timesteps=220500, episode_reward=-4387.94 +/- 731.26
Episode length: 50.58 +/- 19.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 220500    |
----------------------------------
Eval num_timesteps=221000, episode_reward=-4477.16 +/- 680.66
Episode length: 52.28 +/- 15.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.3      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 221000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.8      |
|    ep_rew_mean     | -4.29e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 108       |
|    time_elapsed    | 1234      |
|    total_timesteps | 221184    |
----------------------------------
Eval num_timesteps=221500, episode_reward=-4356.35 +/- 802.85
Episode length: 52.30 +/- 17.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.3         |
|    mean_reward          | -4.36e+03    |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 8.242446e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00629     |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.61e+04     |
|    n_updates            | 1080         |
|    policy_gradient_loss | -2.11e-06    |
|    value_loss           | 9.67e+04     |
------------------------------------------
Eval num_timesteps=222000, episode_reward=-4432.35 +/- 661.94
Episode length: 49.64 +/- 16.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 222000    |
----------------------------------
Eval num_timesteps=222500, episode_reward=-4458.75 +/- 739.96
Episode length: 55.16 +/- 16.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55.2      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 222500    |
----------------------------------
Eval num_timesteps=223000, episode_reward=-4315.55 +/- 778.01
Episode length: 49.22 +/- 15.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 223000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.8      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 109       |
|    time_elapsed    | 1246      |
|    total_timesteps | 223232    |
----------------------------------
Eval num_timesteps=223500, episode_reward=-4481.14 +/- 628.27
Episode length: 48.24 +/- 18.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.2          |
|    mean_reward          | -4.48e+03     |
| time/                   |               |
|    total_timesteps      | 223500        |
| train/                  |               |
|    approx_kl            | 1.1450029e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00581      |
|    explained_variance   | 0.933         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.58e+04      |
|    n_updates            | 1090          |
|    policy_gradient_loss | 2.91e-05      |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=224000, episode_reward=-4415.14 +/- 693.52
Episode length: 48.34 +/- 13.78
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 224000    |
----------------------------------
Eval num_timesteps=224500, episode_reward=-4445.55 +/- 666.60
Episode length: 54.00 +/- 19.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54        |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 224500    |
----------------------------------
Eval num_timesteps=225000, episode_reward=-4198.35 +/- 749.77
Episode length: 48.38 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -4.2e+03 |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.5      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 110       |
|    time_elapsed    | 1257      |
|    total_timesteps | 225280    |
----------------------------------
Eval num_timesteps=225500, episode_reward=-4506.74 +/- 698.99
Episode length: 49.92 +/- 20.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.9         |
|    mean_reward          | -4.51e+03    |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 9.931828e-05 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00592     |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.8e+04      |
|    n_updates            | 1100         |
|    policy_gradient_loss | -2.96e-06    |
|    value_loss           | 1.28e+05     |
------------------------------------------
Eval num_timesteps=226000, episode_reward=-4448.34 +/- 691.14
Episode length: 49.42 +/- 19.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 226000    |
----------------------------------
Eval num_timesteps=226500, episode_reward=-4332.74 +/- 648.36
Episode length: 50.18 +/- 16.89
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 226500    |
----------------------------------
Eval num_timesteps=227000, episode_reward=-4381.15 +/- 859.64
Episode length: 50.60 +/- 19.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 227000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.5      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 111       |
|    time_elapsed    | 1268      |
|    total_timesteps | 227328    |
----------------------------------
Eval num_timesteps=227500, episode_reward=-4591.15 +/- 561.64
Episode length: 52.08 +/- 14.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.1          |
|    mean_reward          | -4.59e+03     |
| time/                   |               |
|    total_timesteps      | 227500        |
| train/                  |               |
|    approx_kl            | 0.00011047002 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00664      |
|    explained_variance   | 0.919         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.57e+04      |
|    n_updates            | 1110          |
|    policy_gradient_loss | -7.93e-06     |
|    value_loss           | 1.4e+05       |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=-4509.94 +/- 713.64
Episode length: 51.84 +/- 19.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 228000    |
----------------------------------
Eval num_timesteps=228500, episode_reward=-4371.95 +/- 726.69
Episode length: 48.28 +/- 16.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 228500    |
----------------------------------
Eval num_timesteps=229000, episode_reward=-4627.95 +/- 569.34
Episode length: 54.74 +/- 19.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.7      |
|    mean_reward     | -4.63e+03 |
| time/              |           |
|    total_timesteps | 229000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.52e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 112       |
|    time_elapsed    | 1280      |
|    total_timesteps | 229376    |
----------------------------------
Eval num_timesteps=229500, episode_reward=-4451.54 +/- 701.68
Episode length: 50.52 +/- 20.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.5          |
|    mean_reward          | -4.45e+03     |
| time/                   |               |
|    total_timesteps      | 229500        |
| train/                  |               |
|    approx_kl            | 1.4644145e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00513      |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.77e+04      |
|    n_updates            | 1120          |
|    policy_gradient_loss | 2.75e-05      |
|    value_loss           | 8.46e+04      |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=-4232.74 +/- 753.65
Episode length: 48.32 +/- 17.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 230000    |
----------------------------------
Eval num_timesteps=230500, episode_reward=-4498.34 +/- 723.23
Episode length: 50.58 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-4520.34 +/- 714.85
Episode length: 49.72 +/- 20.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 231000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.4      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 113       |
|    time_elapsed    | 1291      |
|    total_timesteps | 231424    |
----------------------------------
Eval num_timesteps=231500, episode_reward=-4495.14 +/- 692.36
Episode length: 52.30 +/- 16.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.3          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 231500        |
| train/                  |               |
|    approx_kl            | 6.6967914e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00301      |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.61e+04      |
|    n_updates            | 1130          |
|    policy_gradient_loss | -1.56e-05     |
|    value_loss           | 1.35e+05      |
-------------------------------------------
Eval num_timesteps=232000, episode_reward=-4303.94 +/- 765.03
Episode length: 50.22 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-4421.95 +/- 714.50
Episode length: 52.18 +/- 15.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.2      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 232500    |
----------------------------------
Eval num_timesteps=233000, episode_reward=-4348.89 +/- 908.87
Episode length: 49.44 +/- 17.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 233000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.8      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 114       |
|    time_elapsed    | 1302      |
|    total_timesteps | 233472    |
----------------------------------
Eval num_timesteps=233500, episode_reward=-4439.55 +/- 656.56
Episode length: 49.88 +/- 19.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.9          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 233500        |
| train/                  |               |
|    approx_kl            | 1.4525722e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00391      |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.0001        |
|    loss                 | 6.91e+04      |
|    n_updates            | 1140          |
|    policy_gradient_loss | -2.31e-05     |
|    value_loss           | 1.29e+05      |
-------------------------------------------
Eval num_timesteps=234000, episode_reward=-4422.33 +/- 629.48
Episode length: 46.82 +/- 16.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 234000    |
----------------------------------
Eval num_timesteps=234500, episode_reward=-4627.94 +/- 659.01
Episode length: 53.34 +/- 18.45
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.63e+03 |
| time/              |           |
|    total_timesteps | 234500    |
----------------------------------
Eval num_timesteps=235000, episode_reward=-4271.95 +/- 773.96
Episode length: 45.94 +/- 14.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.9      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 235000    |
----------------------------------
Eval num_timesteps=235500, episode_reward=-4221.54 +/- 757.13
Episode length: 46.10 +/- 15.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.1      |
|    mean_reward     | -4.22e+03 |
| time/              |           |
|    total_timesteps | 235500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.8      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 115       |
|    time_elapsed    | 1315      |
|    total_timesteps | 235520    |
----------------------------------
Eval num_timesteps=236000, episode_reward=-4494.35 +/- 603.44
Episode length: 54.24 +/- 18.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 54.2          |
|    mean_reward          | -4.49e+03     |
| time/                   |               |
|    total_timesteps      | 236000        |
| train/                  |               |
|    approx_kl            | 4.8341462e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00377      |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.0001        |
|    loss                 | 3.19e+04      |
|    n_updates            | 1150          |
|    policy_gradient_loss | 7.22e-06      |
|    value_loss           | 9.55e+04      |
-------------------------------------------
Eval num_timesteps=236500, episode_reward=-4281.95 +/- 684.57
Episode length: 48.64 +/- 14.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 236500    |
----------------------------------
Eval num_timesteps=237000, episode_reward=-4260.75 +/- 895.66
Episode length: 51.92 +/- 18.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 237000    |
----------------------------------
Eval num_timesteps=237500, episode_reward=-4229.95 +/- 871.99
Episode length: 49.84 +/- 16.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 237500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.6      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 116       |
|    time_elapsed    | 1327      |
|    total_timesteps | 237568    |
----------------------------------
Eval num_timesteps=238000, episode_reward=-4337.16 +/- 735.86
Episode length: 50.60 +/- 16.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.6         |
|    mean_reward          | -4.34e+03    |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 4.934499e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00371     |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.46e+04     |
|    n_updates            | 1160         |
|    policy_gradient_loss | 8.16e-05     |
|    value_loss           | 1.17e+05     |
------------------------------------------
Eval num_timesteps=238500, episode_reward=-4458.74 +/- 622.21
Episode length: 47.76 +/- 15.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 238500    |
----------------------------------
Eval num_timesteps=239000, episode_reward=-4576.35 +/- 706.79
Episode length: 52.76 +/- 15.43
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.8      |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 239000    |
----------------------------------
Eval num_timesteps=239500, episode_reward=-4347.14 +/- 743.21
Episode length: 49.10 +/- 20.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 239500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.9      |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 117       |
|    time_elapsed    | 1338      |
|    total_timesteps | 239616    |
----------------------------------
Eval num_timesteps=240000, episode_reward=-4303.55 +/- 895.36
Episode length: 46.96 +/- 15.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47          |
|    mean_reward          | -4.3e+03    |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 7.30489e-05 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00585    |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.53e+04    |
|    n_updates            | 1170        |
|    policy_gradient_loss | 3.2e-06     |
|    value_loss           | 1.41e+05    |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=-4363.14 +/- 753.51
Episode length: 48.98 +/- 13.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 240500    |
----------------------------------
Eval num_timesteps=241000, episode_reward=-4365.13 +/- 789.84
Episode length: 49.44 +/- 20.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 241000    |
----------------------------------
Eval num_timesteps=241500, episode_reward=-4381.94 +/- 853.74
Episode length: 48.06 +/- 15.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 241500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.3      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 118       |
|    time_elapsed    | 1349      |
|    total_timesteps | 241664    |
----------------------------------
Eval num_timesteps=242000, episode_reward=-4243.55 +/- 807.28
Episode length: 49.16 +/- 16.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.2          |
|    mean_reward          | -4.24e+03     |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 0.00012527066 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00517      |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.34e+04      |
|    n_updates            | 1180          |
|    policy_gradient_loss | -3.87e-05     |
|    value_loss           | 8.6e+04       |
-------------------------------------------
Eval num_timesteps=242500, episode_reward=-4419.94 +/- 782.76
Episode length: 51.88 +/- 17.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 242500    |
----------------------------------
Eval num_timesteps=243000, episode_reward=-4469.94 +/- 766.47
Episode length: 49.20 +/- 17.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 243000    |
----------------------------------
Eval num_timesteps=243500, episode_reward=-4382.35 +/- 760.83
Episode length: 48.54 +/- 18.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 243500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.9      |
|    ep_rew_mean     | -4.44e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 119       |
|    time_elapsed    | 1361      |
|    total_timesteps | 243712    |
----------------------------------
Eval num_timesteps=244000, episode_reward=-4411.54 +/- 838.72
Episode length: 49.16 +/- 17.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.2          |
|    mean_reward          | -4.41e+03     |
| time/                   |               |
|    total_timesteps      | 244000        |
| train/                  |               |
|    approx_kl            | 1.2267643e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00282      |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.54e+04      |
|    n_updates            | 1190          |
|    policy_gradient_loss | -1.14e-05     |
|    value_loss           | 1.37e+05      |
-------------------------------------------
Eval num_timesteps=244500, episode_reward=-4205.15 +/- 790.11
Episode length: 48.58 +/- 15.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.21e+03 |
| time/              |           |
|    total_timesteps | 244500    |
----------------------------------
Eval num_timesteps=245000, episode_reward=-4432.75 +/- 628.04
Episode length: 48.60 +/- 15.07
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 245000    |
----------------------------------
Eval num_timesteps=245500, episode_reward=-4558.76 +/- 595.04
Episode length: 55.04 +/- 16.92
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55        |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 245500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.1      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 120       |
|    time_elapsed    | 1372      |
|    total_timesteps | 245760    |
----------------------------------
Eval num_timesteps=246000, episode_reward=-4344.75 +/- 605.70
Episode length: 47.78 +/- 15.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.8          |
|    mean_reward          | -4.34e+03     |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 1.1203578e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00402      |
|    explained_variance   | 0.938         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.52e+04      |
|    n_updates            | 1200          |
|    policy_gradient_loss | -2.48e-05     |
|    value_loss           | 9.65e+04      |
-------------------------------------------
Eval num_timesteps=246500, episode_reward=-4535.95 +/- 607.69
Episode length: 51.56 +/- 18.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 246500    |
----------------------------------
Eval num_timesteps=247000, episode_reward=-4373.94 +/- 762.58
Episode length: 51.20 +/- 19.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 247000    |
----------------------------------
Eval num_timesteps=247500, episode_reward=-4341.14 +/- 805.03
Episode length: 48.32 +/- 13.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 247500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.8      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 121       |
|    time_elapsed    | 1383      |
|    total_timesteps | 247808    |
----------------------------------
Eval num_timesteps=248000, episode_reward=-4521.55 +/- 707.29
Episode length: 52.22 +/- 19.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -4.52e+03     |
| time/                   |               |
|    total_timesteps      | 248000        |
| train/                  |               |
|    approx_kl            | 5.1717507e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00451      |
|    explained_variance   | 0.902         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.22e+04      |
|    n_updates            | 1210          |
|    policy_gradient_loss | -2.35e-06     |
|    value_loss           | 1.74e+05      |
-------------------------------------------
Eval num_timesteps=248500, episode_reward=-4297.14 +/- 807.79
Episode length: 45.86 +/- 12.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-4446.74 +/- 827.42
Episode length: 51.12 +/- 18.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 249000    |
----------------------------------
Eval num_timesteps=249500, episode_reward=-4353.53 +/- 740.40
Episode length: 42.94 +/- 14.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 42.9      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 249500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.8      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 122       |
|    time_elapsed    | 1394      |
|    total_timesteps | 249856    |
----------------------------------
Eval num_timesteps=250000, episode_reward=-4332.75 +/- 709.48
Episode length: 51.10 +/- 17.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.1          |
|    mean_reward          | -4.33e+03     |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 1.0509393e-06 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00408      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.47e+04      |
|    n_updates            | 1220          |
|    policy_gradient_loss | 6.77e-06      |
|    value_loss           | 1.61e+05      |
-------------------------------------------
Eval num_timesteps=250500, episode_reward=-4368.35 +/- 802.10
Episode length: 51.14 +/- 18.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 250500    |
----------------------------------
Eval num_timesteps=251000, episode_reward=-4482.75 +/- 694.68
Episode length: 50.12 +/- 18.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 251000    |
----------------------------------
Eval num_timesteps=251500, episode_reward=-4348.75 +/- 766.23
Episode length: 48.70 +/- 16.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 251500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 179      |
|    iterations      | 123      |
|    time_elapsed    | 1406     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-4496.74 +/- 616.84
Episode length: 48.36 +/- 15.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.4          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 252000        |
| train/                  |               |
|    approx_kl            | 5.1941606e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00354      |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.1e+04       |
|    n_updates            | 1230          |
|    policy_gradient_loss | 4.64e-07      |
|    value_loss           | 1.22e+05      |
-------------------------------------------
Eval num_timesteps=252500, episode_reward=-4484.75 +/- 720.86
Episode length: 53.76 +/- 18.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.8      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 252500    |
----------------------------------
Eval num_timesteps=253000, episode_reward=-4494.74 +/- 679.89
Episode length: 46.78 +/- 16.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 253000    |
----------------------------------
Eval num_timesteps=253500, episode_reward=-4529.15 +/- 590.71
Episode length: 54.74 +/- 18.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.7      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 253500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 124       |
|    time_elapsed    | 1417      |
|    total_timesteps | 253952    |
----------------------------------
Eval num_timesteps=254000, episode_reward=-4380.34 +/- 754.50
Episode length: 44.90 +/- 14.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 44.9          |
|    mean_reward          | -4.38e+03     |
| time/                   |               |
|    total_timesteps      | 254000        |
| train/                  |               |
|    approx_kl            | 0.00014187046 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00419      |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.9e+04       |
|    n_updates            | 1240          |
|    policy_gradient_loss | -4.57e-06     |
|    value_loss           | 1.39e+05      |
-------------------------------------------
Eval num_timesteps=254500, episode_reward=-4481.93 +/- 596.47
Episode length: 47.18 +/- 17.51
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.2      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 254500    |
----------------------------------
Eval num_timesteps=255000, episode_reward=-4383.14 +/- 731.87
Episode length: 51.66 +/- 16.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.7      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 255000    |
----------------------------------
Eval num_timesteps=255500, episode_reward=-4401.55 +/- 771.33
Episode length: 53.60 +/- 16.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-4354.34 +/- 705.89
Episode length: 47.24 +/- 17.33
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.2      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 256000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.8      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 125       |
|    time_elapsed    | 1430      |
|    total_timesteps | 256000    |
----------------------------------
Eval num_timesteps=256500, episode_reward=-4170.35 +/- 809.84
Episode length: 47.70 +/- 15.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.7          |
|    mean_reward          | -4.17e+03     |
| time/                   |               |
|    total_timesteps      | 256500        |
| train/                  |               |
|    approx_kl            | 2.7765054e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00437      |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.32e+04      |
|    n_updates            | 1250          |
|    policy_gradient_loss | -1.38e-05     |
|    value_loss           | 1.1e+05       |
-------------------------------------------
Eval num_timesteps=257000, episode_reward=-4406.34 +/- 620.74
Episode length: 48.22 +/- 14.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 257000    |
----------------------------------
Eval num_timesteps=257500, episode_reward=-4512.74 +/- 697.09
Episode length: 51.34 +/- 18.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 257500    |
----------------------------------
Eval num_timesteps=258000, episode_reward=-4363.95 +/- 816.55
Episode length: 48.12 +/- 15.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 258000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.8      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 126       |
|    time_elapsed    | 1442      |
|    total_timesteps | 258048    |
----------------------------------
Eval num_timesteps=258500, episode_reward=-4476.35 +/- 754.49
Episode length: 53.04 +/- 20.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53            |
|    mean_reward          | -4.48e+03     |
| time/                   |               |
|    total_timesteps      | 258500        |
| train/                  |               |
|    approx_kl            | 3.0529918e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.004        |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.15e+04      |
|    n_updates            | 1260          |
|    policy_gradient_loss | 3.65e-06      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=259000, episode_reward=-4475.54 +/- 623.50
Episode length: 49.74 +/- 15.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 259000    |
----------------------------------
Eval num_timesteps=259500, episode_reward=-4407.55 +/- 772.97
Episode length: 48.72 +/- 16.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 259500    |
----------------------------------
Eval num_timesteps=260000, episode_reward=-4482.34 +/- 742.75
Episode length: 46.88 +/- 13.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 260000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52        |
|    ep_rew_mean     | -4.47e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 127       |
|    time_elapsed    | 1453      |
|    total_timesteps | 260096    |
----------------------------------
Eval num_timesteps=260500, episode_reward=-4440.35 +/- 836.67
Episode length: 47.58 +/- 14.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.6         |
|    mean_reward          | -4.44e+03    |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 7.259293e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00524     |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.19e+04     |
|    n_updates            | 1270         |
|    policy_gradient_loss | -2.2e-05     |
|    value_loss           | 8.98e+04     |
------------------------------------------
Eval num_timesteps=261000, episode_reward=-4317.54 +/- 707.69
Episode length: 48.84 +/- 18.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 261000    |
----------------------------------
Eval num_timesteps=261500, episode_reward=-4440.74 +/- 748.17
Episode length: 52.08 +/- 19.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 261500    |
----------------------------------
Eval num_timesteps=262000, episode_reward=-4417.93 +/- 647.35
Episode length: 49.32 +/- 16.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 262000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.5      |
|    ep_rew_mean     | -4.54e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 128       |
|    time_elapsed    | 1464      |
|    total_timesteps | 262144    |
----------------------------------
Eval num_timesteps=262500, episode_reward=-4357.55 +/- 656.25
Episode length: 51.46 +/- 19.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.5          |
|    mean_reward          | -4.36e+03     |
| time/                   |               |
|    total_timesteps      | 262500        |
| train/                  |               |
|    approx_kl            | 0.00016843877 |
|    clip_fraction        | 0.00112       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00571      |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.86e+04      |
|    n_updates            | 1280          |
|    policy_gradient_loss | -5.94e-05     |
|    value_loss           | 7.65e+04      |
-------------------------------------------
Eval num_timesteps=263000, episode_reward=-4425.95 +/- 703.37
Episode length: 50.94 +/- 19.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 263000    |
----------------------------------
Eval num_timesteps=263500, episode_reward=-4468.75 +/- 685.07
Episode length: 53.18 +/- 18.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.2      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 263500    |
----------------------------------
Eval num_timesteps=264000, episode_reward=-4622.35 +/- 589.02
Episode length: 50.38 +/- 14.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.62e+03 |
| time/              |           |
|    total_timesteps | 264000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53        |
|    ep_rew_mean     | -4.57e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 129       |
|    time_elapsed    | 1476      |
|    total_timesteps | 264192    |
----------------------------------
Eval num_timesteps=264500, episode_reward=-4264.35 +/- 887.02
Episode length: 47.40 +/- 16.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.4         |
|    mean_reward          | -4.26e+03    |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 7.711211e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00431     |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.45e+04     |
|    n_updates            | 1290         |
|    policy_gradient_loss | 4.35e-05     |
|    value_loss           | 7.11e+04     |
------------------------------------------
Eval num_timesteps=265000, episode_reward=-4305.15 +/- 763.84
Episode length: 48.12 +/- 15.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 265000    |
----------------------------------
Eval num_timesteps=265500, episode_reward=-4455.95 +/- 563.95
Episode length: 47.74 +/- 17.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.7      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 265500    |
----------------------------------
Eval num_timesteps=266000, episode_reward=-4434.35 +/- 816.02
Episode length: 50.94 +/- 17.64
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 266000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.2      |
|    ep_rew_mean     | -4.58e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 130       |
|    time_elapsed    | 1487      |
|    total_timesteps | 266240    |
----------------------------------
Eval num_timesteps=266500, episode_reward=-4354.73 +/- 913.70
Episode length: 52.16 +/- 20.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -4.35e+03     |
| time/                   |               |
|    total_timesteps      | 266500        |
| train/                  |               |
|    approx_kl            | 1.8080435e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00495      |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.69e+04      |
|    n_updates            | 1300          |
|    policy_gradient_loss | -1.7e-05      |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=267000, episode_reward=-4306.35 +/- 608.47
Episode length: 51.60 +/- 15.91
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 267000    |
----------------------------------
Eval num_timesteps=267500, episode_reward=-4471.54 +/- 639.02
Episode length: 52.56 +/- 16.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.6      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 267500    |
----------------------------------
Eval num_timesteps=268000, episode_reward=-4315.15 +/- 787.05
Episode length: 43.72 +/- 15.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 43.7      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 268000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51        |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 131       |
|    time_elapsed    | 1498      |
|    total_timesteps | 268288    |
----------------------------------
Eval num_timesteps=268500, episode_reward=-4507.14 +/- 658.90
Episode length: 51.54 +/- 18.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.5          |
|    mean_reward          | -4.51e+03     |
| time/                   |               |
|    total_timesteps      | 268500        |
| train/                  |               |
|    approx_kl            | 1.6119739e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00591      |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.21e+04      |
|    n_updates            | 1310          |
|    policy_gradient_loss | 4.47e-05      |
|    value_loss           | 1.24e+05      |
-------------------------------------------
Eval num_timesteps=269000, episode_reward=-4455.14 +/- 695.35
Episode length: 50.48 +/- 17.91
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 269000    |
----------------------------------
Eval num_timesteps=269500, episode_reward=-4331.55 +/- 776.38
Episode length: 50.86 +/- 16.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 269500    |
----------------------------------
Eval num_timesteps=270000, episode_reward=-4390.34 +/- 696.33
Episode length: 48.64 +/- 16.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 270000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -4.5e+03 |
| time/              |          |
|    fps             | 179      |
|    iterations      | 132      |
|    time_elapsed    | 1510     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-4400.33 +/- 770.82
Episode length: 49.12 +/- 16.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.1         |
|    mean_reward          | -4.4e+03     |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 3.656739e-05 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00591     |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0001       |
|    loss                 | 5.02e+04     |
|    n_updates            | 1320         |
|    policy_gradient_loss | -3.65e-05    |
|    value_loss           | 8.82e+04     |
------------------------------------------
Eval num_timesteps=271000, episode_reward=-4560.32 +/- 749.94
Episode length: 49.96 +/- 16.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 271000    |
----------------------------------
Eval num_timesteps=271500, episode_reward=-4406.75 +/- 705.17
Episode length: 48.40 +/- 16.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 271500    |
----------------------------------
Eval num_timesteps=272000, episode_reward=-4476.35 +/- 719.36
Episode length: 56.00 +/- 18.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56        |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 272000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.8      |
|    ep_rew_mean     | -4.47e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 133       |
|    time_elapsed    | 1521      |
|    total_timesteps | 272384    |
----------------------------------
Eval num_timesteps=272500, episode_reward=-4274.35 +/- 815.87
Episode length: 50.92 +/- 21.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.9         |
|    mean_reward          | -4.27e+03    |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 1.360639e-05 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0043      |
|    explained_variance   | 0.919        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.18e+04     |
|    n_updates            | 1330         |
|    policy_gradient_loss | -1.65e-06    |
|    value_loss           | 1.26e+05     |
------------------------------------------
Eval num_timesteps=273000, episode_reward=-4505.96 +/- 635.37
Episode length: 51.06 +/- 17.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 273000    |
----------------------------------
Eval num_timesteps=273500, episode_reward=-4520.35 +/- 700.63
Episode length: 51.54 +/- 15.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 273500    |
----------------------------------
Eval num_timesteps=274000, episode_reward=-4364.35 +/- 761.70
Episode length: 48.60 +/- 18.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 274000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.9      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 134       |
|    time_elapsed    | 1533      |
|    total_timesteps | 274432    |
----------------------------------
Eval num_timesteps=274500, episode_reward=-4555.95 +/- 614.62
Episode length: 51.60 +/- 17.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.6          |
|    mean_reward          | -4.56e+03     |
| time/                   |               |
|    total_timesteps      | 274500        |
| train/                  |               |
|    approx_kl            | 4.8782094e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00458      |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.78e+04      |
|    n_updates            | 1340          |
|    policy_gradient_loss | -1.68e-05     |
|    value_loss           | 1.47e+05      |
-------------------------------------------
Eval num_timesteps=275000, episode_reward=-4461.54 +/- 693.51
Episode length: 49.42 +/- 15.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 275000    |
----------------------------------
Eval num_timesteps=275500, episode_reward=-4366.75 +/- 748.33
Episode length: 53.20 +/- 22.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.2      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 275500    |
----------------------------------
Eval num_timesteps=276000, episode_reward=-4414.35 +/- 733.03
Episode length: 49.64 +/- 17.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 276000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.8      |
|    ep_rew_mean     | -4.27e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 135       |
|    time_elapsed    | 1544      |
|    total_timesteps | 276480    |
----------------------------------
Eval num_timesteps=276500, episode_reward=-4448.36 +/- 731.06
Episode length: 53.50 +/- 18.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.5          |
|    mean_reward          | -4.45e+03     |
| time/                   |               |
|    total_timesteps      | 276500        |
| train/                  |               |
|    approx_kl            | 0.00022318162 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00739      |
|    explained_variance   | 0.937         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.29e+04      |
|    n_updates            | 1350          |
|    policy_gradient_loss | -0.000154     |
|    value_loss           | 1.1e+05       |
-------------------------------------------
Eval num_timesteps=277000, episode_reward=-4309.54 +/- 787.62
Episode length: 49.30 +/- 18.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 277000    |
----------------------------------
Eval num_timesteps=277500, episode_reward=-4403.94 +/- 793.86
Episode length: 52.98 +/- 14.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-4349.53 +/- 750.15
Episode length: 45.72 +/- 18.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.7      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 278000    |
----------------------------------
Eval num_timesteps=278500, episode_reward=-4281.14 +/- 698.95
Episode length: 46.30 +/- 13.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.3      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 278500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.6      |
|    ep_rew_mean     | -4.23e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 136       |
|    time_elapsed    | 1557      |
|    total_timesteps | 278528    |
----------------------------------
Eval num_timesteps=279000, episode_reward=-4449.95 +/- 824.14
Episode length: 49.52 +/- 17.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -4.45e+03   |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 8.86925e-05 |
|    clip_fraction        | 0.000537    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00893    |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0001      |
|    loss                 | 5.3e+04     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -9.93e-05   |
|    value_loss           | 1.35e+05    |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=-4522.76 +/- 676.80
Episode length: 54.24 +/- 18.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.2      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 279500    |
----------------------------------
Eval num_timesteps=280000, episode_reward=-4297.15 +/- 620.24
Episode length: 44.92 +/- 12.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-4523.55 +/- 751.31
Episode length: 51.82 +/- 15.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 280500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.3      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 137       |
|    time_elapsed    | 1569      |
|    total_timesteps | 280576    |
----------------------------------
Eval num_timesteps=281000, episode_reward=-4459.15 +/- 784.10
Episode length: 52.60 +/- 17.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.6          |
|    mean_reward          | -4.46e+03     |
| time/                   |               |
|    total_timesteps      | 281000        |
| train/                  |               |
|    approx_kl            | 4.1162595e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.16e+04      |
|    n_updates            | 1370          |
|    policy_gradient_loss | -6.9e-07      |
|    value_loss           | 1.36e+05      |
-------------------------------------------
Eval num_timesteps=281500, episode_reward=-4471.56 +/- 654.08
Episode length: 56.64 +/- 18.89
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.6      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 281500    |
----------------------------------
Eval num_timesteps=282000, episode_reward=-4357.95 +/- 756.95
Episode length: 51.34 +/- 14.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 282000    |
----------------------------------
Eval num_timesteps=282500, episode_reward=-4351.54 +/- 811.76
Episode length: 50.16 +/- 17.74
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 282500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50        |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 138       |
|    time_elapsed    | 1580      |
|    total_timesteps | 282624    |
----------------------------------
Eval num_timesteps=283000, episode_reward=-4339.54 +/- 816.50
Episode length: 46.88 +/- 18.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.9         |
|    mean_reward          | -4.34e+03    |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 2.458549e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0153      |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0001       |
|    loss                 | 5.99e+04     |
|    n_updates            | 1380         |
|    policy_gradient_loss | -1.75e-05    |
|    value_loss           | 1.19e+05     |
------------------------------------------
Eval num_timesteps=283500, episode_reward=-4417.15 +/- 638.73
Episode length: 50.14 +/- 16.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 283500    |
----------------------------------
Eval num_timesteps=284000, episode_reward=-4425.14 +/- 645.53
Episode length: 49.00 +/- 15.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 284000    |
----------------------------------
Eval num_timesteps=284500, episode_reward=-4347.81 +/- 774.11
Episode length: 45.34 +/- 14.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.3      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 284500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -4.5e+03 |
| time/              |          |
|    fps             | 178      |
|    iterations      | 139      |
|    time_elapsed    | 1592     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-4459.95 +/- 658.99
Episode length: 49.18 +/- 16.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.2          |
|    mean_reward          | -4.46e+03     |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 0.00012505677 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0156       |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.34e+04      |
|    n_updates            | 1390          |
|    policy_gradient_loss | 3.22e-06      |
|    value_loss           | 7.74e+04      |
-------------------------------------------
Eval num_timesteps=285500, episode_reward=-4416.74 +/- 705.23
Episode length: 52.34 +/- 20.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.3      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 285500    |
----------------------------------
Eval num_timesteps=286000, episode_reward=-4149.54 +/- 852.10
Episode length: 48.18 +/- 16.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.15e+03 |
| time/              |           |
|    total_timesteps | 286000    |
----------------------------------
Eval num_timesteps=286500, episode_reward=-4332.34 +/- 748.98
Episode length: 51.20 +/- 16.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 286500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.6      |
|    ep_rew_mean     | -4.53e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 140       |
|    time_elapsed    | 1603      |
|    total_timesteps | 286720    |
----------------------------------
Eval num_timesteps=287000, episode_reward=-4412.75 +/- 745.30
Episode length: 51.82 +/- 16.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.8         |
|    mean_reward          | -4.41e+03    |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 9.331346e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.88e+04     |
|    n_updates            | 1400         |
|    policy_gradient_loss | -7.89e-05    |
|    value_loss           | 7.65e+04     |
------------------------------------------
Eval num_timesteps=287500, episode_reward=-4292.73 +/- 763.61
Episode length: 45.22 +/- 16.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.2      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 287500    |
----------------------------------
Eval num_timesteps=288000, episode_reward=-4301.95 +/- 834.96
Episode length: 54.88 +/- 22.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-4497.15 +/- 709.83
Episode length: 48.34 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.6      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 141       |
|    time_elapsed    | 1614      |
|    total_timesteps | 288768    |
----------------------------------
Eval num_timesteps=289000, episode_reward=-4600.74 +/- 615.74
Episode length: 47.32 +/- 16.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.3          |
|    mean_reward          | -4.6e+03      |
| time/                   |               |
|    total_timesteps      | 289000        |
| train/                  |               |
|    approx_kl            | 3.6124635e-05 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.13e+04      |
|    n_updates            | 1410          |
|    policy_gradient_loss | 6.69e-06      |
|    value_loss           | 1.19e+05      |
-------------------------------------------
Eval num_timesteps=289500, episode_reward=-4389.14 +/- 810.89
Episode length: 48.10 +/- 13.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 289500    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-4502.34 +/- 615.46
Episode length: 47.98 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-4560.75 +/- 578.79
Episode length: 50.06 +/- 16.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 290500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.5      |
|    ep_rew_mean     | -4.23e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 142       |
|    time_elapsed    | 1626      |
|    total_timesteps | 290816    |
----------------------------------
Eval num_timesteps=291000, episode_reward=-4590.36 +/- 655.15
Episode length: 51.92 +/- 16.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.9          |
|    mean_reward          | -4.59e+03     |
| time/                   |               |
|    total_timesteps      | 291000        |
| train/                  |               |
|    approx_kl            | 4.4602697e-05 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.018        |
|    explained_variance   | 0.9           |
|    learning_rate        | 0.0001        |
|    loss                 | 4.74e+04      |
|    n_updates            | 1420          |
|    policy_gradient_loss | 1.11e-05      |
|    value_loss           | 1.62e+05      |
-------------------------------------------
Eval num_timesteps=291500, episode_reward=-4285.95 +/- 830.81
Episode length: 48.30 +/- 18.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 291500    |
----------------------------------
Eval num_timesteps=292000, episode_reward=-4261.95 +/- 882.50
Episode length: 47.08 +/- 17.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.1      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 292000    |
----------------------------------
Eval num_timesteps=292500, episode_reward=-4413.74 +/- 735.47
Episode length: 52.22 +/- 17.89
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.2      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 292500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49        |
|    ep_rew_mean     | -4.27e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 143       |
|    time_elapsed    | 1637      |
|    total_timesteps | 292864    |
----------------------------------
Eval num_timesteps=293000, episode_reward=-4491.54 +/- 706.98
Episode length: 49.28 +/- 15.11
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 49.3           |
|    mean_reward          | -4.49e+03      |
| time/                   |                |
|    total_timesteps      | 293000         |
| train/                  |                |
|    approx_kl            | 0.000118735246 |
|    clip_fraction        | 0.000879       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0251        |
|    explained_variance   | 0.918          |
|    learning_rate        | 0.0001         |
|    loss                 | 6.15e+04       |
|    n_updates            | 1430           |
|    policy_gradient_loss | 4.87e-05       |
|    value_loss           | 1.2e+05        |
--------------------------------------------
Eval num_timesteps=293500, episode_reward=-4318.34 +/- 824.28
Episode length: 49.58 +/- 21.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 293500    |
----------------------------------
Eval num_timesteps=294000, episode_reward=-4325.54 +/- 832.29
Episode length: 49.78 +/- 17.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 294000    |
----------------------------------
Eval num_timesteps=294500, episode_reward=-4558.74 +/- 689.03
Episode length: 45.40 +/- 14.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.4      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 294500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.3      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 178       |
|    iterations      | 144       |
|    time_elapsed    | 1648      |
|    total_timesteps | 294912    |
----------------------------------
Eval num_timesteps=295000, episode_reward=-4364.34 +/- 815.51
Episode length: 53.60 +/- 18.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.6          |
|    mean_reward          | -4.36e+03     |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 0.00029114998 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0218       |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.3e+04       |
|    n_updates            | 1440          |
|    policy_gradient_loss | -4.96e-05     |
|    value_loss           | 1.3e+05       |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=-4287.13 +/- 732.90
Episode length: 47.42 +/- 18.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.4      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 295500    |
----------------------------------
Eval num_timesteps=296000, episode_reward=-4608.34 +/- 583.24
Episode length: 50.18 +/- 19.74
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 296000    |
----------------------------------
Eval num_timesteps=296500, episode_reward=-4437.95 +/- 651.84
Episode length: 50.96 +/- 17.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 296500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.8      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 145       |
|    time_elapsed    | 1657      |
|    total_timesteps | 296960    |
----------------------------------
Eval num_timesteps=297000, episode_reward=-4299.54 +/- 839.88
Episode length: 45.02 +/- 14.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 45            |
|    mean_reward          | -4.3e+03      |
| time/                   |               |
|    total_timesteps      | 297000        |
| train/                  |               |
|    approx_kl            | 0.00040523274 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0169       |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.04e+04      |
|    n_updates            | 1450          |
|    policy_gradient_loss | 7.42e-05      |
|    value_loss           | 9.76e+04      |
-------------------------------------------
Eval num_timesteps=297500, episode_reward=-4530.72 +/- 839.05
Episode length: 50.14 +/- 15.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 297500    |
----------------------------------
Eval num_timesteps=298000, episode_reward=-4232.34 +/- 872.58
Episode length: 46.96 +/- 15.33
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47        |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 298000    |
----------------------------------
Eval num_timesteps=298500, episode_reward=-4379.94 +/- 688.44
Episode length: 44.76 +/- 12.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44.8      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 298500    |
----------------------------------
Eval num_timesteps=299000, episode_reward=-4624.33 +/- 518.82
Episode length: 49.98 +/- 20.13
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.62e+03 |
| time/              |           |
|    total_timesteps | 299000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.1      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 146       |
|    time_elapsed    | 1667      |
|    total_timesteps | 299008    |
----------------------------------
Eval num_timesteps=299500, episode_reward=-4495.94 +/- 705.66
Episode length: 51.56 +/- 20.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.6          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 299500        |
| train/                  |               |
|    approx_kl            | 0.00010442268 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.01e+04      |
|    n_updates            | 1460          |
|    policy_gradient_loss | 4.22e-05      |
|    value_loss           | 1.39e+05      |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=-4335.55 +/- 806.57
Episode length: 47.50 +/- 17.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.5      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 300000    |
----------------------------------
Eval num_timesteps=300500, episode_reward=-4409.14 +/- 693.70
Episode length: 50.46 +/- 17.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 300500    |
----------------------------------
Eval num_timesteps=301000, episode_reward=-4413.94 +/- 757.30
Episode length: 51.62 +/- 17.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 301000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.8      |
|    ep_rew_mean     | -4.39e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 147       |
|    time_elapsed    | 1677      |
|    total_timesteps | 301056    |
----------------------------------
Eval num_timesteps=301500, episode_reward=-4560.34 +/- 716.47
Episode length: 49.26 +/- 16.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.3         |
|    mean_reward          | -4.56e+03    |
| time/                   |              |
|    total_timesteps      | 301500       |
| train/                  |              |
|    approx_kl            | 5.517059e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0173      |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.7e+04      |
|    n_updates            | 1470         |
|    policy_gradient_loss | 5.86e-05     |
|    value_loss           | 8.39e+04     |
------------------------------------------
Eval num_timesteps=302000, episode_reward=-4345.15 +/- 756.76
Episode length: 49.14 +/- 14.86
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 302000    |
----------------------------------
Eval num_timesteps=302500, episode_reward=-4496.75 +/- 712.42
Episode length: 51.62 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-4471.95 +/- 767.40
Episode length: 50.46 +/- 17.77
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 303000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.8      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 148       |
|    time_elapsed    | 1686      |
|    total_timesteps | 303104    |
----------------------------------
Eval num_timesteps=303500, episode_reward=-4441.55 +/- 760.23
Episode length: 52.48 +/- 17.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.5          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 303500        |
| train/                  |               |
|    approx_kl            | 0.00010453025 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.72e+04      |
|    n_updates            | 1480          |
|    policy_gradient_loss | 2.82e-05      |
|    value_loss           | 1.35e+05      |
-------------------------------------------
Eval num_timesteps=304000, episode_reward=-4327.95 +/- 861.50
Episode length: 48.20 +/- 14.60
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 304000    |
----------------------------------
Eval num_timesteps=304500, episode_reward=-4283.56 +/- 693.67
Episode length: 51.88 +/- 13.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 304500    |
----------------------------------
Eval num_timesteps=305000, episode_reward=-4397.94 +/- 619.05
Episode length: 45.72 +/- 13.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.6      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 179       |
|    iterations      | 149       |
|    time_elapsed    | 1695      |
|    total_timesteps | 305152    |
----------------------------------
Eval num_timesteps=305500, episode_reward=-4423.54 +/- 837.93
Episode length: 53.30 +/- 18.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.3          |
|    mean_reward          | -4.42e+03     |
| time/                   |               |
|    total_timesteps      | 305500        |
| train/                  |               |
|    approx_kl            | 1.2511911e-05 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0123       |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.05e+04      |
|    n_updates            | 1490          |
|    policy_gradient_loss | 5.37e-05      |
|    value_loss           | 8.81e+04      |
-------------------------------------------
Eval num_timesteps=306000, episode_reward=-4538.35 +/- 622.69
Episode length: 50.40 +/- 14.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 306000    |
----------------------------------
Eval num_timesteps=306500, episode_reward=-4292.34 +/- 738.35
Episode length: 49.28 +/- 16.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 306500    |
----------------------------------
Eval num_timesteps=307000, episode_reward=-3896.74 +/- 884.43
Episode length: 43.14 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.1     |
|    mean_reward     | -3.9e+03 |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53        |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 150       |
|    time_elapsed    | 1704      |
|    total_timesteps | 307200    |
----------------------------------
Eval num_timesteps=307500, episode_reward=-4463.54 +/- 796.07
Episode length: 49.24 +/- 20.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.2          |
|    mean_reward          | -4.46e+03     |
| time/                   |               |
|    total_timesteps      | 307500        |
| train/                  |               |
|    approx_kl            | 4.1267078e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.927         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.85e+04      |
|    n_updates            | 1500          |
|    policy_gradient_loss | 4.38e-06      |
|    value_loss           | 1.28e+05      |
-------------------------------------------
Eval num_timesteps=308000, episode_reward=-4411.14 +/- 635.75
Episode length: 47.92 +/- 13.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 308000    |
----------------------------------
Eval num_timesteps=308500, episode_reward=-4323.15 +/- 796.55
Episode length: 50.68 +/- 20.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.7      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 308500    |
----------------------------------
Eval num_timesteps=309000, episode_reward=-4367.95 +/- 718.55
Episode length: 49.76 +/- 17.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 309000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 151       |
|    time_elapsed    | 1713      |
|    total_timesteps | 309248    |
----------------------------------
Eval num_timesteps=309500, episode_reward=-4496.74 +/- 703.13
Episode length: 49.40 +/- 15.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.4          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 309500        |
| train/                  |               |
|    approx_kl            | 0.00017380106 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0108       |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.98e+04      |
|    n_updates            | 1510          |
|    policy_gradient_loss | -3.41e-05     |
|    value_loss           | 9.99e+04      |
-------------------------------------------
Eval num_timesteps=310000, episode_reward=-4372.74 +/- 684.62
Episode length: 46.56 +/- 14.77
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.6      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 310000    |
----------------------------------
Eval num_timesteps=310500, episode_reward=-4379.55 +/- 798.76
Episode length: 49.82 +/- 12.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 310500    |
----------------------------------
Eval num_timesteps=311000, episode_reward=-4354.33 +/- 816.07
Episode length: 52.00 +/- 17.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52        |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 311000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.47e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 152       |
|    time_elapsed    | 1722      |
|    total_timesteps | 311296    |
----------------------------------
Eval num_timesteps=311500, episode_reward=-4425.94 +/- 740.02
Episode length: 49.40 +/- 14.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.4          |
|    mean_reward          | -4.43e+03     |
| time/                   |               |
|    total_timesteps      | 311500        |
| train/                  |               |
|    approx_kl            | 0.00018343472 |
|    clip_fraction        | 0.00161       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0092       |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.07e+04      |
|    n_updates            | 1520          |
|    policy_gradient_loss | 4.48e-05      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=312000, episode_reward=-4484.73 +/- 672.02
Episode length: 49.16 +/- 16.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 312000    |
----------------------------------
Eval num_timesteps=312500, episode_reward=-4452.53 +/- 799.25
Episode length: 49.88 +/- 17.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 312500    |
----------------------------------
Eval num_timesteps=313000, episode_reward=-4465.14 +/- 789.52
Episode length: 51.38 +/- 18.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.4      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 313000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.8      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 180       |
|    iterations      | 153       |
|    time_elapsed    | 1731      |
|    total_timesteps | 313344    |
----------------------------------
Eval num_timesteps=313500, episode_reward=-4484.74 +/- 670.96
Episode length: 50.42 +/- 15.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.4         |
|    mean_reward          | -4.48e+03    |
| time/                   |              |
|    total_timesteps      | 313500       |
| train/                  |              |
|    approx_kl            | 4.192069e-05 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00798     |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.82e+04     |
|    n_updates            | 1530         |
|    policy_gradient_loss | 3.4e-05      |
|    value_loss           | 1.14e+05     |
------------------------------------------
Eval num_timesteps=314000, episode_reward=-4291.06 +/- 844.68
Episode length: 47.88 +/- 17.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 314000    |
----------------------------------
Eval num_timesteps=314500, episode_reward=-4532.34 +/- 570.17
Episode length: 48.18 +/- 15.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 314500    |
----------------------------------
Eval num_timesteps=315000, episode_reward=-4508.35 +/- 694.49
Episode length: 51.06 +/- 20.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 315000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.8      |
|    ep_rew_mean     | -4.51e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 154       |
|    time_elapsed    | 1740      |
|    total_timesteps | 315392    |
----------------------------------
Eval num_timesteps=315500, episode_reward=-4391.54 +/- 754.96
Episode length: 48.64 +/- 15.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -4.39e+03     |
| time/                   |               |
|    total_timesteps      | 315500        |
| train/                  |               |
|    approx_kl            | 1.0567601e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00567      |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.43e+04      |
|    n_updates            | 1540          |
|    policy_gradient_loss | 1.38e-05      |
|    value_loss           | 9.53e+04      |
-------------------------------------------
Eval num_timesteps=316000, episode_reward=-4255.94 +/- 901.21
Episode length: 50.42 +/- 14.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 316000    |
----------------------------------
Eval num_timesteps=316500, episode_reward=-4466.75 +/- 773.96
Episode length: 51.48 +/- 16.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 316500    |
----------------------------------
Eval num_timesteps=317000, episode_reward=-4164.74 +/- 948.81
Episode length: 49.38 +/- 18.49
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.16e+03 |
| time/              |           |
|    total_timesteps | 317000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.3      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 155       |
|    time_elapsed    | 1749      |
|    total_timesteps | 317440    |
----------------------------------
Eval num_timesteps=317500, episode_reward=-4430.34 +/- 676.93
Episode length: 48.80 +/- 17.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.8          |
|    mean_reward          | -4.43e+03     |
| time/                   |               |
|    total_timesteps      | 317500        |
| train/                  |               |
|    approx_kl            | 0.00012006619 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00646      |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.19e+04      |
|    n_updates            | 1550          |
|    policy_gradient_loss | 2.4e-05       |
|    value_loss           | 1.45e+05      |
-------------------------------------------
Eval num_timesteps=318000, episode_reward=-4433.94 +/- 658.10
Episode length: 49.74 +/- 17.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 318000    |
----------------------------------
Eval num_timesteps=318500, episode_reward=-4350.74 +/- 844.31
Episode length: 52.14 +/- 18.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 318500    |
----------------------------------
Eval num_timesteps=319000, episode_reward=-4432.75 +/- 808.80
Episode length: 49.50 +/- 16.64
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.5      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 319000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.2      |
|    ep_rew_mean     | -4.39e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 156       |
|    time_elapsed    | 1758      |
|    total_timesteps | 319488    |
----------------------------------
Eval num_timesteps=319500, episode_reward=-4413.55 +/- 725.39
Episode length: 48.52 +/- 18.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.5          |
|    mean_reward          | -4.41e+03     |
| time/                   |               |
|    total_timesteps      | 319500        |
| train/                  |               |
|    approx_kl            | 2.2126827e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00609      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.38e+04      |
|    n_updates            | 1560          |
|    policy_gradient_loss | 4.18e-05      |
|    value_loss           | 1.45e+05      |
-------------------------------------------
Eval num_timesteps=320000, episode_reward=-4389.15 +/- 826.30
Episode length: 49.60 +/- 18.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 320000    |
----------------------------------
Eval num_timesteps=320500, episode_reward=-4235.15 +/- 862.99
Episode length: 49.66 +/- 18.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.24e+03 |
| time/              |           |
|    total_timesteps | 320500    |
----------------------------------
Eval num_timesteps=321000, episode_reward=-4342.75 +/- 597.52
Episode length: 47.38 +/- 13.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.4      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 321000    |
----------------------------------
Eval num_timesteps=321500, episode_reward=-4524.34 +/- 678.80
Episode length: 49.98 +/- 16.41
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 321500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.1      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 157       |
|    time_elapsed    | 1769      |
|    total_timesteps | 321536    |
----------------------------------
Eval num_timesteps=322000, episode_reward=-4506.74 +/- 702.20
Episode length: 48.84 +/- 17.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.8          |
|    mean_reward          | -4.51e+03     |
| time/                   |               |
|    total_timesteps      | 322000        |
| train/                  |               |
|    approx_kl            | 2.0748994e-06 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.009        |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.0001        |
|    loss                 | 8.49e+04      |
|    n_updates            | 1570          |
|    policy_gradient_loss | 6.24e-05      |
|    value_loss           | 1.24e+05      |
-------------------------------------------
Eval num_timesteps=322500, episode_reward=-4464.35 +/- 714.09
Episode length: 53.32 +/- 18.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 322500    |
----------------------------------
Eval num_timesteps=323000, episode_reward=-4348.90 +/- 992.24
Episode length: 48.82 +/- 15.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 323000    |
----------------------------------
Eval num_timesteps=323500, episode_reward=-4666.35 +/- 546.38
Episode length: 51.62 +/- 15.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.67e+03 |
| time/              |           |
|    total_timesteps | 323500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 181       |
|    iterations      | 158       |
|    time_elapsed    | 1778      |
|    total_timesteps | 323584    |
----------------------------------
Eval num_timesteps=324000, episode_reward=-4465.15 +/- 625.29
Episode length: 51.22 +/- 16.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.2          |
|    mean_reward          | -4.47e+03     |
| time/                   |               |
|    total_timesteps      | 324000        |
| train/                  |               |
|    approx_kl            | 4.2894564e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00755      |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.81e+04      |
|    n_updates            | 1580          |
|    policy_gradient_loss | -8.66e-05     |
|    value_loss           | 1.23e+05      |
-------------------------------------------
Eval num_timesteps=324500, episode_reward=-4305.13 +/- 759.35
Episode length: 45.36 +/- 15.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.4      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 324500    |
----------------------------------
Eval num_timesteps=325000, episode_reward=-4361.95 +/- 831.20
Episode length: 49.00 +/- 16.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 325000    |
----------------------------------
Eval num_timesteps=325500, episode_reward=-4400.35 +/- 674.26
Episode length: 50.46 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 182      |
|    iterations      | 159      |
|    time_elapsed    | 1787     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=-4370.34 +/- 748.21
Episode length: 49.08 +/- 18.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.1          |
|    mean_reward          | -4.37e+03     |
| time/                   |               |
|    total_timesteps      | 326000        |
| train/                  |               |
|    approx_kl            | 3.6092242e-06 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0074       |
|    explained_variance   | 0.907         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.29e+04      |
|    n_updates            | 1590          |
|    policy_gradient_loss | -1.23e-05     |
|    value_loss           | 1.48e+05      |
-------------------------------------------
Eval num_timesteps=326500, episode_reward=-4336.75 +/- 717.48
Episode length: 51.96 +/- 17.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52        |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 326500    |
----------------------------------
Eval num_timesteps=327000, episode_reward=-4381.14 +/- 780.03
Episode length: 47.96 +/- 16.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 327000    |
----------------------------------
Eval num_timesteps=327500, episode_reward=-4271.14 +/- 725.74
Episode length: 45.50 +/- 15.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.5      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 327500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.4      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 182       |
|    iterations      | 160       |
|    time_elapsed    | 1796      |
|    total_timesteps | 327680    |
----------------------------------
Eval num_timesteps=328000, episode_reward=-4605.15 +/- 568.29
Episode length: 55.96 +/- 16.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 56            |
|    mean_reward          | -4.61e+03     |
| time/                   |               |
|    total_timesteps      | 328000        |
| train/                  |               |
|    approx_kl            | 1.9717147e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00798      |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.76e+04      |
|    n_updates            | 1600          |
|    policy_gradient_loss | 2.09e-05      |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=328500, episode_reward=-4368.34 +/- 771.16
Episode length: 51.54 +/- 20.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 328500    |
----------------------------------
Eval num_timesteps=329000, episode_reward=-4431.94 +/- 777.44
Episode length: 50.60 +/- 15.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 329000    |
----------------------------------
Eval num_timesteps=329500, episode_reward=-4524.35 +/- 644.73
Episode length: 53.26 +/- 16.21
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 329500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.4      |
|    ep_rew_mean     | -4.35e+03 |
| time/              |           |
|    fps             | 182       |
|    iterations      | 161       |
|    time_elapsed    | 1806      |
|    total_timesteps | 329728    |
----------------------------------
Eval num_timesteps=330000, episode_reward=-4382.32 +/- 512.13
Episode length: 50.30 +/- 16.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.3          |
|    mean_reward          | -4.38e+03     |
| time/                   |               |
|    total_timesteps      | 330000        |
| train/                  |               |
|    approx_kl            | 2.2028689e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00734      |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.89e+04      |
|    n_updates            | 1610          |
|    policy_gradient_loss | -6.06e-06     |
|    value_loss           | 1.32e+05      |
-------------------------------------------
Eval num_timesteps=330500, episode_reward=-4379.95 +/- 797.08
Episode length: 50.28 +/- 17.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 330500    |
----------------------------------
Eval num_timesteps=331000, episode_reward=-4522.75 +/- 523.12
Episode length: 55.26 +/- 18.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55.3      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 331000    |
----------------------------------
Eval num_timesteps=331500, episode_reward=-4647.94 +/- 616.08
Episode length: 49.38 +/- 15.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.65e+03 |
| time/              |           |
|    total_timesteps | 331500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.7      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 182       |
|    iterations      | 162       |
|    time_elapsed    | 1815      |
|    total_timesteps | 331776    |
----------------------------------
Eval num_timesteps=332000, episode_reward=-4358.34 +/- 688.19
Episode length: 54.48 +/- 14.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 54.5          |
|    mean_reward          | -4.36e+03     |
| time/                   |               |
|    total_timesteps      | 332000        |
| train/                  |               |
|    approx_kl            | 9.2380156e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0.937         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.71e+04      |
|    n_updates            | 1620          |
|    policy_gradient_loss | 0.000103      |
|    value_loss           | 9.85e+04      |
-------------------------------------------
Eval num_timesteps=332500, episode_reward=-4288.74 +/- 721.19
Episode length: 49.92 +/- 13.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 332500    |
----------------------------------
Eval num_timesteps=333000, episode_reward=-4465.93 +/- 692.16
Episode length: 51.52 +/- 20.43
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 333000    |
----------------------------------
Eval num_timesteps=333500, episode_reward=-4381.53 +/- 796.99
Episode length: 47.18 +/- 14.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.2      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 333500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 182      |
|    iterations      | 163      |
|    time_elapsed    | 1824     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-4190.74 +/- 845.05
Episode length: 48.72 +/- 19.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.7         |
|    mean_reward          | -4.19e+03    |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0003082935 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00633     |
|    explained_variance   | 0.926        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.38e+04     |
|    n_updates            | 1630         |
|    policy_gradient_loss | -2.1e-05     |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=334500, episode_reward=-4288.74 +/- 819.45
Episode length: 46.84 +/- 17.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 334500    |
----------------------------------
Eval num_timesteps=335000, episode_reward=-4409.15 +/- 718.05
Episode length: 49.46 +/- 14.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.5      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 335000    |
----------------------------------
Eval num_timesteps=335500, episode_reward=-4397.95 +/- 814.89
Episode length: 48.88 +/- 18.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.5      |
|    ep_rew_mean     | -4.42e+03 |
| time/              |           |
|    fps             | 183       |
|    iterations      | 164       |
|    time_elapsed    | 1833      |
|    total_timesteps | 335872    |
----------------------------------
Eval num_timesteps=336000, episode_reward=-4601.14 +/- 691.15
Episode length: 53.10 +/- 18.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.1          |
|    mean_reward          | -4.6e+03      |
| time/                   |               |
|    total_timesteps      | 336000        |
| train/                  |               |
|    approx_kl            | 2.6534486e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00602      |
|    explained_variance   | 0.929         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.95e+04      |
|    n_updates            | 1640          |
|    policy_gradient_loss | 9.88e-07      |
|    value_loss           | 1.1e+05       |
-------------------------------------------
Eval num_timesteps=336500, episode_reward=-4515.54 +/- 691.49
Episode length: 53.20 +/- 18.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.2      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 336500    |
----------------------------------
Eval num_timesteps=337000, episode_reward=-4308.35 +/- 849.59
Episode length: 52.44 +/- 19.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.4      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 337000    |
----------------------------------
Eval num_timesteps=337500, episode_reward=-4514.34 +/- 644.32
Episode length: 51.50 +/- 17.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 337500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49        |
|    ep_rew_mean     | -4.28e+03 |
| time/              |           |
|    fps             | 183       |
|    iterations      | 165       |
|    time_elapsed    | 1843      |
|    total_timesteps | 337920    |
----------------------------------
Eval num_timesteps=338000, episode_reward=-4581.54 +/- 616.13
Episode length: 52.92 +/- 18.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.9          |
|    mean_reward          | -4.58e+03     |
| time/                   |               |
|    total_timesteps      | 338000        |
| train/                  |               |
|    approx_kl            | 2.3681787e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00515      |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.48e+04      |
|    n_updates            | 1650          |
|    policy_gradient_loss | -3.25e-06     |
|    value_loss           | 1.26e+05      |
-------------------------------------------
Eval num_timesteps=338500, episode_reward=-4414.34 +/- 760.22
Episode length: 48.82 +/- 18.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 338500    |
----------------------------------
Eval num_timesteps=339000, episode_reward=-4347.76 +/- 964.94
Episode length: 50.62 +/- 17.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 339000    |
----------------------------------
Eval num_timesteps=339500, episode_reward=-4509.94 +/- 626.13
Episode length: 50.82 +/- 17.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 339500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 183       |
|    iterations      | 166       |
|    time_elapsed    | 1852      |
|    total_timesteps | 339968    |
----------------------------------
Eval num_timesteps=340000, episode_reward=-4469.54 +/- 686.05
Episode length: 47.66 +/- 14.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.7          |
|    mean_reward          | -4.47e+03     |
| time/                   |               |
|    total_timesteps      | 340000        |
| train/                  |               |
|    approx_kl            | 1.5317346e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00718      |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.78e+04      |
|    n_updates            | 1660          |
|    policy_gradient_loss | 3.56e-05      |
|    value_loss           | 6.71e+04      |
-------------------------------------------
Eval num_timesteps=340500, episode_reward=-4478.34 +/- 645.15
Episode length: 51.52 +/- 17.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 340500    |
----------------------------------
Eval num_timesteps=341000, episode_reward=-4264.35 +/- 839.20
Episode length: 49.96 +/- 19.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 341000    |
----------------------------------
Eval num_timesteps=341500, episode_reward=-4646.35 +/- 456.48
Episode length: 53.60 +/- 16.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.6      |
|    mean_reward     | -4.65e+03 |
| time/              |           |
|    total_timesteps | 341500    |
----------------------------------
Eval num_timesteps=342000, episode_reward=-4427.14 +/- 818.30
Episode length: 47.56 +/- 15.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 342000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.5      |
|    ep_rew_mean     | -4.29e+03 |
| time/              |           |
|    fps             | 183       |
|    iterations      | 167       |
|    time_elapsed    | 1863      |
|    total_timesteps | 342016    |
----------------------------------
Eval num_timesteps=342500, episode_reward=-4577.15 +/- 745.98
Episode length: 53.32 +/- 19.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.3          |
|    mean_reward          | -4.58e+03     |
| time/                   |               |
|    total_timesteps      | 342500        |
| train/                  |               |
|    approx_kl            | 0.00020870019 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00541      |
|    explained_variance   | 0.914         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.9e+04       |
|    n_updates            | 1670          |
|    policy_gradient_loss | 1.26e-05      |
|    value_loss           | 1.43e+05      |
-------------------------------------------
Eval num_timesteps=343000, episode_reward=-4321.55 +/- 784.78
Episode length: 50.04 +/- 15.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 343000    |
----------------------------------
Eval num_timesteps=343500, episode_reward=-4246.75 +/- 757.41
Episode length: 45.40 +/- 15.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.4      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 343500    |
----------------------------------
Eval num_timesteps=344000, episode_reward=-4586.34 +/- 622.22
Episode length: 46.00 +/- 16.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46        |
|    mean_reward     | -4.59e+03 |
| time/              |           |
|    total_timesteps | 344000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.9      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 183       |
|    iterations      | 168       |
|    time_elapsed    | 1872      |
|    total_timesteps | 344064    |
----------------------------------
Eval num_timesteps=344500, episode_reward=-4625.55 +/- 612.34
Episode length: 52.08 +/- 17.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.1          |
|    mean_reward          | -4.63e+03     |
| time/                   |               |
|    total_timesteps      | 344500        |
| train/                  |               |
|    approx_kl            | 3.0492723e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00624      |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.13e+04      |
|    n_updates            | 1680          |
|    policy_gradient_loss | -4.28e-05     |
|    value_loss           | 7.54e+04      |
-------------------------------------------
Eval num_timesteps=345000, episode_reward=-4144.75 +/- 1012.56
Episode length: 51.44 +/- 18.07
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.4      |
|    mean_reward     | -4.14e+03 |
| time/              |           |
|    total_timesteps | 345000    |
----------------------------------
Eval num_timesteps=345500, episode_reward=-4273.54 +/- 727.70
Episode length: 46.90 +/- 14.75
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 345500    |
----------------------------------
Eval num_timesteps=346000, episode_reward=-4370.34 +/- 827.88
Episode length: 48.32 +/- 16.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 346000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.4      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 183       |
|    iterations      | 169       |
|    time_elapsed    | 1881      |
|    total_timesteps | 346112    |
----------------------------------
Eval num_timesteps=346500, episode_reward=-4391.54 +/- 723.72
Episode length: 48.62 +/- 15.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.6         |
|    mean_reward          | -4.39e+03    |
| time/                   |              |
|    total_timesteps      | 346500       |
| train/                  |              |
|    approx_kl            | 6.088958e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00713     |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.53e+04     |
|    n_updates            | 1690         |
|    policy_gradient_loss | 3.8e-06      |
|    value_loss           | 1.21e+05     |
------------------------------------------
Eval num_timesteps=347000, episode_reward=-4479.54 +/- 640.99
Episode length: 50.48 +/- 16.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 347000    |
----------------------------------
Eval num_timesteps=347500, episode_reward=-4434.34 +/- 691.74
Episode length: 50.90 +/- 15.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 347500    |
----------------------------------
Eval num_timesteps=348000, episode_reward=-4441.95 +/- 736.70
Episode length: 56.90 +/- 19.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.9      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 348000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.7      |
|    ep_rew_mean     | -4.44e+03 |
| time/              |           |
|    fps             | 184       |
|    iterations      | 170       |
|    time_elapsed    | 1890      |
|    total_timesteps | 348160    |
----------------------------------
Eval num_timesteps=348500, episode_reward=-4331.94 +/- 827.95
Episode length: 47.54 +/- 16.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.5          |
|    mean_reward          | -4.33e+03     |
| time/                   |               |
|    total_timesteps      | 348500        |
| train/                  |               |
|    approx_kl            | 4.2076514e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00791      |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.35e+04      |
|    n_updates            | 1700          |
|    policy_gradient_loss | -4.17e-05     |
|    value_loss           | 7.26e+04      |
-------------------------------------------
Eval num_timesteps=349000, episode_reward=-4336.76 +/- 899.21
Episode length: 50.78 +/- 16.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 349000    |
----------------------------------
Eval num_timesteps=349500, episode_reward=-4224.34 +/- 777.52
Episode length: 46.88 +/- 14.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.22e+03 |
| time/              |           |
|    total_timesteps | 349500    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-4584.35 +/- 648.41
Episode length: 51.00 +/- 17.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 350000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.1      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 184       |
|    iterations      | 171       |
|    time_elapsed    | 1899      |
|    total_timesteps | 350208    |
----------------------------------
Eval num_timesteps=350500, episode_reward=-4178.83 +/- 969.72
Episode length: 47.16 +/- 15.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.2         |
|    mean_reward          | -4.18e+03    |
| time/                   |              |
|    total_timesteps      | 350500       |
| train/                  |              |
|    approx_kl            | 4.395956e-05 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0073      |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.42e+04     |
|    n_updates            | 1710         |
|    policy_gradient_loss | 0.000197     |
|    value_loss           | 1.09e+05     |
------------------------------------------
Eval num_timesteps=351000, episode_reward=-4499.15 +/- 649.87
Episode length: 51.04 +/- 13.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-4403.94 +/- 714.95
Episode length: 51.42 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-4391.94 +/- 657.76
Episode length: 46.62 +/- 15.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.6      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 352000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.3      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 184       |
|    iterations      | 172       |
|    time_elapsed    | 1908      |
|    total_timesteps | 352256    |
----------------------------------
Eval num_timesteps=352500, episode_reward=-4479.15 +/- 871.76
Episode length: 53.14 +/- 19.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.1          |
|    mean_reward          | -4.48e+03     |
| time/                   |               |
|    total_timesteps      | 352500        |
| train/                  |               |
|    approx_kl            | 2.9813964e-05 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.57e+04      |
|    n_updates            | 1720          |
|    policy_gradient_loss | -2.53e-05     |
|    value_loss           | 8.02e+04      |
-------------------------------------------
Eval num_timesteps=353000, episode_reward=-4483.94 +/- 769.56
Episode length: 53.12 +/- 17.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.1      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 353000    |
----------------------------------
Eval num_timesteps=353500, episode_reward=-4404.34 +/- 708.89
Episode length: 49.08 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-4565.95 +/- 760.15
Episode length: 53.08 +/- 16.61
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.1      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 354000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.7      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 184       |
|    iterations      | 173       |
|    time_elapsed    | 1918      |
|    total_timesteps | 354304    |
----------------------------------
Eval num_timesteps=354500, episode_reward=-4465.55 +/- 863.34
Episode length: 52.22 +/- 18.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.2          |
|    mean_reward          | -4.47e+03     |
| time/                   |               |
|    total_timesteps      | 354500        |
| train/                  |               |
|    approx_kl            | 0.00011634795 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00565      |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.0001        |
|    loss                 | 3.5e+04       |
|    n_updates            | 1730          |
|    policy_gradient_loss | -0.000111     |
|    value_loss           | 7.77e+04      |
-------------------------------------------
Eval num_timesteps=355000, episode_reward=-4471.55 +/- 610.20
Episode length: 51.14 +/- 15.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 355000    |
----------------------------------
Eval num_timesteps=355500, episode_reward=-4447.56 +/- 667.98
Episode length: 51.12 +/- 14.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 355500    |
----------------------------------
Eval num_timesteps=356000, episode_reward=-4579.55 +/- 610.70
Episode length: 53.34 +/- 17.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 356000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.6      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 184       |
|    iterations      | 174       |
|    time_elapsed    | 1927      |
|    total_timesteps | 356352    |
----------------------------------
Eval num_timesteps=356500, episode_reward=-4589.15 +/- 651.66
Episode length: 49.72 +/- 14.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.7          |
|    mean_reward          | -4.59e+03     |
| time/                   |               |
|    total_timesteps      | 356500        |
| train/                  |               |
|    approx_kl            | 8.4376545e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00665      |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.94e+04      |
|    n_updates            | 1740          |
|    policy_gradient_loss | 1.66e-05      |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=357000, episode_reward=-4339.94 +/- 743.85
Episode length: 48.64 +/- 17.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 357000    |
----------------------------------
Eval num_timesteps=357500, episode_reward=-4408.35 +/- 740.92
Episode length: 49.06 +/- 18.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 357500    |
----------------------------------
Eval num_timesteps=358000, episode_reward=-4465.16 +/- 823.97
Episode length: 56.46 +/- 18.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.5      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 358000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 185      |
|    iterations      | 175      |
|    time_elapsed    | 1937     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=-4591.95 +/- 680.27
Episode length: 52.50 +/- 15.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.5         |
|    mean_reward          | -4.59e+03    |
| time/                   |              |
|    total_timesteps      | 358500       |
| train/                  |              |
|    approx_kl            | 7.954077e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00667     |
|    explained_variance   | 0.924        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.84e+04     |
|    n_updates            | 1750         |
|    policy_gradient_loss | -3.91e-06    |
|    value_loss           | 1.34e+05     |
------------------------------------------
Eval num_timesteps=359000, episode_reward=-4577.15 +/- 572.01
Episode length: 48.84 +/- 15.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -4.58e+03 |
| time/              |           |
|    total_timesteps | 359000    |
----------------------------------
Eval num_timesteps=359500, episode_reward=-4469.55 +/- 689.55
Episode length: 52.14 +/- 18.27
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 359500    |
----------------------------------
Eval num_timesteps=360000, episode_reward=-4279.95 +/- 776.13
Episode length: 48.34 +/- 17.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 360000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.4      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 185       |
|    iterations      | 176       |
|    time_elapsed    | 1946      |
|    total_timesteps | 360448    |
----------------------------------
Eval num_timesteps=360500, episode_reward=-4250.75 +/- 786.28
Episode length: 47.64 +/- 19.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.6         |
|    mean_reward          | -4.25e+03    |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 5.754159e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00741     |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.1e+04      |
|    n_updates            | 1760         |
|    policy_gradient_loss | 5.02e-05     |
|    value_loss           | 1.22e+05     |
------------------------------------------
Eval num_timesteps=361000, episode_reward=-4444.75 +/- 681.11
Episode length: 52.58 +/- 16.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.6      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 361000    |
----------------------------------
Eval num_timesteps=361500, episode_reward=-4334.74 +/- 841.96
Episode length: 49.62 +/- 18.92
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 361500    |
----------------------------------
Eval num_timesteps=362000, episode_reward=-4497.13 +/- 637.43
Episode length: 50.56 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.9      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 185       |
|    iterations      | 177       |
|    time_elapsed    | 1955      |
|    total_timesteps | 362496    |
----------------------------------
Eval num_timesteps=362500, episode_reward=-4439.94 +/- 707.41
Episode length: 50.18 +/- 18.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.2          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 362500        |
| train/                  |               |
|    approx_kl            | 7.3594565e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00612      |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.0001        |
|    loss                 | 3.73e+04      |
|    n_updates            | 1770          |
|    policy_gradient_loss | 1.96e-05      |
|    value_loss           | 7.22e+04      |
-------------------------------------------
Eval num_timesteps=363000, episode_reward=-4396.75 +/- 835.96
Episode length: 50.68 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-4347.55 +/- 704.94
Episode length: 49.26 +/- 16.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 363500    |
----------------------------------
Eval num_timesteps=364000, episode_reward=-4488.74 +/- 562.26
Episode length: 50.12 +/- 16.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 364000    |
----------------------------------
Eval num_timesteps=364500, episode_reward=-4338.74 +/- 892.88
Episode length: 51.24 +/- 19.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 364500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 185       |
|    iterations      | 178       |
|    time_elapsed    | 1966      |
|    total_timesteps | 364544    |
----------------------------------
Eval num_timesteps=365000, episode_reward=-4412.35 +/- 695.59
Episode length: 47.60 +/- 19.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.6          |
|    mean_reward          | -4.41e+03     |
| time/                   |               |
|    total_timesteps      | 365000        |
| train/                  |               |
|    approx_kl            | 2.5489135e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00389      |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.96e+04      |
|    n_updates            | 1780          |
|    policy_gradient_loss | -4.16e-06     |
|    value_loss           | 8.73e+04      |
-------------------------------------------
Eval num_timesteps=365500, episode_reward=-4150.34 +/- 994.23
Episode length: 48.46 +/- 17.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.15e+03 |
| time/              |           |
|    total_timesteps | 365500    |
----------------------------------
Eval num_timesteps=366000, episode_reward=-4186.35 +/- 787.09
Episode length: 48.22 +/- 17.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.19e+03 |
| time/              |           |
|    total_timesteps | 366000    |
----------------------------------
Eval num_timesteps=366500, episode_reward=-4524.75 +/- 562.70
Episode length: 48.02 +/- 14.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 366500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.1      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 185       |
|    iterations      | 179       |
|    time_elapsed    | 1975      |
|    total_timesteps | 366592    |
----------------------------------
Eval num_timesteps=367000, episode_reward=-4307.13 +/- 915.53
Episode length: 51.68 +/- 16.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.7          |
|    mean_reward          | -4.31e+03     |
| time/                   |               |
|    total_timesteps      | 367000        |
| train/                  |               |
|    approx_kl            | 1.9935018e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00392      |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.87e+04      |
|    n_updates            | 1790          |
|    policy_gradient_loss | -2.33e-05     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=367500, episode_reward=-4401.94 +/- 746.68
Episode length: 51.74 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-4445.14 +/- 736.58
Episode length: 51.82 +/- 13.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 368000    |
----------------------------------
Eval num_timesteps=368500, episode_reward=-4514.75 +/- 666.44
Episode length: 50.40 +/- 18.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 368500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 185      |
|    iterations      | 180      |
|    time_elapsed    | 1984     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=-4335.94 +/- 823.28
Episode length: 51.04 +/- 16.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51            |
|    mean_reward          | -4.34e+03     |
| time/                   |               |
|    total_timesteps      | 369000        |
| train/                  |               |
|    approx_kl            | 1.4124089e-07 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00351      |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.9e+04       |
|    n_updates            | 1800          |
|    policy_gradient_loss | 1.3e-05       |
|    value_loss           | 7.18e+04      |
-------------------------------------------
Eval num_timesteps=369500, episode_reward=-4425.14 +/- 729.11
Episode length: 50.44 +/- 15.74
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 369500    |
----------------------------------
Eval num_timesteps=370000, episode_reward=-4364.74 +/- 794.95
Episode length: 48.30 +/- 13.43
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 370000    |
----------------------------------
Eval num_timesteps=370500, episode_reward=-4317.14 +/- 820.92
Episode length: 47.42 +/- 16.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.4      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 370500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 185       |
|    iterations      | 181       |
|    time_elapsed    | 1993      |
|    total_timesteps | 370688    |
----------------------------------
Eval num_timesteps=371000, episode_reward=-4518.75 +/- 670.28
Episode length: 53.64 +/- 18.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.6         |
|    mean_reward          | -4.52e+03    |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 0.0005033633 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.007       |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.66e+04     |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.000113    |
|    value_loss           | 7.94e+04     |
------------------------------------------
Eval num_timesteps=371500, episode_reward=-4194.34 +/- 867.77
Episode length: 45.74 +/- 17.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.7      |
|    mean_reward     | -4.19e+03 |
| time/              |           |
|    total_timesteps | 371500    |
----------------------------------
Eval num_timesteps=372000, episode_reward=-4368.35 +/- 774.46
Episode length: 49.76 +/- 18.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 372000    |
----------------------------------
Eval num_timesteps=372500, episode_reward=-4165.93 +/- 802.52
Episode length: 48.46 +/- 19.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.17e+03 |
| time/              |           |
|    total_timesteps | 372500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.5      |
|    ep_rew_mean     | -4.57e+03 |
| time/              |           |
|    fps             | 186       |
|    iterations      | 182       |
|    time_elapsed    | 2003      |
|    total_timesteps | 372736    |
----------------------------------
Eval num_timesteps=373000, episode_reward=-4350.35 +/- 714.83
Episode length: 48.02 +/- 15.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48            |
|    mean_reward          | -4.35e+03     |
| time/                   |               |
|    total_timesteps      | 373000        |
| train/                  |               |
|    approx_kl            | 0.00046987788 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00594      |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.82e+04      |
|    n_updates            | 1820          |
|    policy_gradient_loss | -8.57e-05     |
|    value_loss           | 6.16e+04      |
-------------------------------------------
Eval num_timesteps=373500, episode_reward=-4386.35 +/- 729.99
Episode length: 49.58 +/- 19.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 373500    |
----------------------------------
Eval num_timesteps=374000, episode_reward=-4333.94 +/- 712.85
Episode length: 53.32 +/- 19.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 374000    |
----------------------------------
Eval num_timesteps=374500, episode_reward=-4388.75 +/- 733.10
Episode length: 51.68 +/- 14.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.7      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 374500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.8      |
|    ep_rew_mean     | -4.52e+03 |
| time/              |           |
|    fps             | 186       |
|    iterations      | 183       |
|    time_elapsed    | 2012      |
|    total_timesteps | 374784    |
----------------------------------
Eval num_timesteps=375000, episode_reward=-4598.75 +/- 609.28
Episode length: 51.70 +/- 18.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.7          |
|    mean_reward          | -4.6e+03      |
| time/                   |               |
|    total_timesteps      | 375000        |
| train/                  |               |
|    approx_kl            | 1.2543751e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00285      |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.86e+04      |
|    n_updates            | 1830          |
|    policy_gradient_loss | -1.09e-06     |
|    value_loss           | 8.97e+04      |
-------------------------------------------
Eval num_timesteps=375500, episode_reward=-4251.55 +/- 945.58
Episode length: 47.40 +/- 17.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.4      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 375500    |
----------------------------------
Eval num_timesteps=376000, episode_reward=-4330.74 +/- 831.78
Episode length: 49.02 +/- 17.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 376000    |
----------------------------------
Eval num_timesteps=376500, episode_reward=-4615.15 +/- 595.05
Episode length: 55.48 +/- 17.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55.5      |
|    mean_reward     | -4.62e+03 |
| time/              |           |
|    total_timesteps | 376500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.4      |
|    ep_rew_mean     | -4.49e+03 |
| time/              |           |
|    fps             | 186       |
|    iterations      | 184       |
|    time_elapsed    | 2021      |
|    total_timesteps | 376832    |
----------------------------------
Eval num_timesteps=377000, episode_reward=-4317.14 +/- 703.07
Episode length: 51.74 +/- 18.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.7          |
|    mean_reward          | -4.32e+03     |
| time/                   |               |
|    total_timesteps      | 377000        |
| train/                  |               |
|    approx_kl            | 1.0900432e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00336      |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.51e+04      |
|    n_updates            | 1840          |
|    policy_gradient_loss | -3.78e-05     |
|    value_loss           | 1.16e+05      |
-------------------------------------------
Eval num_timesteps=377500, episode_reward=-4410.35 +/- 790.82
Episode length: 53.02 +/- 17.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53        |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 377500    |
----------------------------------
Eval num_timesteps=378000, episode_reward=-4563.54 +/- 706.93
Episode length: 50.62 +/- 17.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 378000    |
----------------------------------
Eval num_timesteps=378500, episode_reward=-4284.74 +/- 782.88
Episode length: 50.00 +/- 13.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 378500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 186       |
|    iterations      | 185       |
|    time_elapsed    | 2030      |
|    total_timesteps | 378880    |
----------------------------------
Eval num_timesteps=379000, episode_reward=-4570.35 +/- 619.97
Episode length: 54.88 +/- 15.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 54.9          |
|    mean_reward          | -4.57e+03     |
| time/                   |               |
|    total_timesteps      | 379000        |
| train/                  |               |
|    approx_kl            | 1.2613047e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00313      |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.12e+04      |
|    n_updates            | 1850          |
|    policy_gradient_loss | 1.6e-05       |
|    value_loss           | 9.51e+04      |
-------------------------------------------
Eval num_timesteps=379500, episode_reward=-4483.15 +/- 797.99
Episode length: 54.34 +/- 16.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.3      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 379500    |
----------------------------------
Eval num_timesteps=380000, episode_reward=-4155.95 +/- 813.48
Episode length: 50.46 +/- 16.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.5      |
|    mean_reward     | -4.16e+03 |
| time/              |           |
|    total_timesteps | 380000    |
----------------------------------
Eval num_timesteps=380500, episode_reward=-4388.75 +/- 733.91
Episode length: 49.36 +/- 14.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 380500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 186      |
|    iterations      | 186      |
|    time_elapsed    | 2040     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=-4257.95 +/- 797.30
Episode length: 48.44 +/- 14.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.4          |
|    mean_reward          | -4.26e+03     |
| time/                   |               |
|    total_timesteps      | 381000        |
| train/                  |               |
|    approx_kl            | 2.1613698e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00446      |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.23e+04      |
|    n_updates            | 1860          |
|    policy_gradient_loss | -1.08e-05     |
|    value_loss           | 6.69e+04      |
-------------------------------------------
Eval num_timesteps=381500, episode_reward=-4348.74 +/- 835.21
Episode length: 52.72 +/- 16.38
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.7      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 381500    |
----------------------------------
Eval num_timesteps=382000, episode_reward=-4499.94 +/- 660.99
Episode length: 49.22 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-4469.15 +/- 727.97
Episode length: 51.04 +/- 17.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 382500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.2      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 186       |
|    iterations      | 187       |
|    time_elapsed    | 2049      |
|    total_timesteps | 382976    |
----------------------------------
Eval num_timesteps=383000, episode_reward=-4433.95 +/- 568.80
Episode length: 47.92 +/- 15.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 47.9          |
|    mean_reward          | -4.43e+03     |
| time/                   |               |
|    total_timesteps      | 383000        |
| train/                  |               |
|    approx_kl            | 1.7503044e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00481      |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.62e+04      |
|    n_updates            | 1870          |
|    policy_gradient_loss | 1.9e-05       |
|    value_loss           | 9.73e+04      |
-------------------------------------------
Eval num_timesteps=383500, episode_reward=-4296.34 +/- 872.47
Episode length: 46.90 +/- 14.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-4298.35 +/- 658.11
Episode length: 47.14 +/- 13.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -4.3e+03 |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-4229.14 +/- 914.00
Episode length: 48.56 +/- 16.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 384500    |
----------------------------------
Eval num_timesteps=385000, episode_reward=-4438.75 +/- 642.83
Episode length: 49.60 +/- 17.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 385000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.6      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 186       |
|    iterations      | 188       |
|    time_elapsed    | 2060      |
|    total_timesteps | 385024    |
----------------------------------
Eval num_timesteps=385500, episode_reward=-4373.11 +/- 870.78
Episode length: 52.00 +/- 16.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52           |
|    mean_reward          | -4.37e+03    |
| time/                   |              |
|    total_timesteps      | 385500       |
| train/                  |              |
|    approx_kl            | 8.798379e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00524     |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.3e+04      |
|    n_updates            | 1880         |
|    policy_gradient_loss | -8.56e-06    |
|    value_loss           | 8.67e+04     |
------------------------------------------
Eval num_timesteps=386000, episode_reward=-4477.95 +/- 740.25
Episode length: 51.22 +/- 16.13
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 386000    |
----------------------------------
Eval num_timesteps=386500, episode_reward=-4487.95 +/- 759.98
Episode length: 51.42 +/- 20.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.4      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 386500    |
----------------------------------
Eval num_timesteps=387000, episode_reward=-4278.74 +/- 931.13
Episode length: 48.14 +/- 17.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 387000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.3      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 187       |
|    iterations      | 189       |
|    time_elapsed    | 2069      |
|    total_timesteps | 387072    |
----------------------------------
Eval num_timesteps=387500, episode_reward=-4472.33 +/- 656.63
Episode length: 47.34 +/- 16.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.3         |
|    mean_reward          | -4.47e+03    |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 3.844133e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00651     |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.48e+04     |
|    n_updates            | 1890         |
|    policy_gradient_loss | 1.55e-05     |
|    value_loss           | 8.06e+04     |
------------------------------------------
Eval num_timesteps=388000, episode_reward=-4327.54 +/- 689.83
Episode length: 46.10 +/- 15.41
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.1      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 388000    |
----------------------------------
Eval num_timesteps=388500, episode_reward=-4386.35 +/- 844.52
Episode length: 50.00 +/- 15.45
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 388500    |
----------------------------------
Eval num_timesteps=389000, episode_reward=-4278.75 +/- 753.64
Episode length: 47.02 +/- 15.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47        |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 389000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 54.9      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 187       |
|    iterations      | 190       |
|    time_elapsed    | 2078      |
|    total_timesteps | 389120    |
----------------------------------
Eval num_timesteps=389500, episode_reward=-4333.94 +/- 805.73
Episode length: 47.96 +/- 13.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48            |
|    mean_reward          | -4.33e+03     |
| time/                   |               |
|    total_timesteps      | 389500        |
| train/                  |               |
|    approx_kl            | 0.00023815868 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.93          |
|    learning_rate        | 0.0001        |
|    loss                 | 8.84e+04      |
|    n_updates            | 1900          |
|    policy_gradient_loss | 2.5e-05       |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=390000, episode_reward=-4421.55 +/- 764.98
Episode length: 51.84 +/- 18.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 390000    |
----------------------------------
Eval num_timesteps=390500, episode_reward=-4198.34 +/- 799.13
Episode length: 45.40 +/- 13.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -4.2e+03 |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-4398.34 +/- 767.70
Episode length: 47.84 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 55.6      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 187       |
|    iterations      | 191       |
|    time_elapsed    | 2087      |
|    total_timesteps | 391168    |
----------------------------------
Eval num_timesteps=391500, episode_reward=-4501.15 +/- 657.37
Episode length: 55.30 +/- 19.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 55.3          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 391500        |
| train/                  |               |
|    approx_kl            | 0.00020217482 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00913      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.37e+04      |
|    n_updates            | 1910          |
|    policy_gradient_loss | 2.77e-05      |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=392000, episode_reward=-4551.70 +/- 852.81
Episode length: 51.00 +/- 19.74
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.55e+03 |
| time/              |           |
|    total_timesteps | 392000    |
----------------------------------
Eval num_timesteps=392500, episode_reward=-4472.74 +/- 670.93
Episode length: 49.34 +/- 15.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 392500    |
----------------------------------
Eval num_timesteps=393000, episode_reward=-4413.53 +/- 841.10
Episode length: 46.44 +/- 13.21
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.4      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 393000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53        |
|    ep_rew_mean     | -4.34e+03 |
| time/              |           |
|    fps             | 187       |
|    iterations      | 192       |
|    time_elapsed    | 2096      |
|    total_timesteps | 393216    |
----------------------------------
Eval num_timesteps=393500, episode_reward=-4363.54 +/- 754.10
Episode length: 48.20 +/- 17.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.2          |
|    mean_reward          | -4.36e+03     |
| time/                   |               |
|    total_timesteps      | 393500        |
| train/                  |               |
|    approx_kl            | 1.6312988e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00604      |
|    explained_variance   | 0.919         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.96e+04      |
|    n_updates            | 1920          |
|    policy_gradient_loss | 4.2e-05       |
|    value_loss           | 1.16e+05      |
-------------------------------------------
Eval num_timesteps=394000, episode_reward=-4379.55 +/- 680.85
Episode length: 50.76 +/- 14.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 394000    |
----------------------------------
Eval num_timesteps=394500, episode_reward=-4657.53 +/- 618.22
Episode length: 52.80 +/- 18.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.8      |
|    mean_reward     | -4.66e+03 |
| time/              |           |
|    total_timesteps | 394500    |
----------------------------------
Eval num_timesteps=395000, episode_reward=-4538.34 +/- 608.00
Episode length: 51.62 +/- 18.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 395000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.6      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 187       |
|    iterations      | 193       |
|    time_elapsed    | 2105      |
|    total_timesteps | 395264    |
----------------------------------
Eval num_timesteps=395500, episode_reward=-4405.70 +/- 913.93
Episode length: 54.52 +/- 17.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 54.5          |
|    mean_reward          | -4.41e+03     |
| time/                   |               |
|    total_timesteps      | 395500        |
| train/                  |               |
|    approx_kl            | 0.00036748487 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0152       |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.0001        |
|    loss                 | 9.82e+04      |
|    n_updates            | 1930          |
|    policy_gradient_loss | 8.92e-05      |
|    value_loss           | 1.24e+05      |
-------------------------------------------
Eval num_timesteps=396000, episode_reward=-4598.34 +/- 618.83
Episode length: 54.74 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -4.6e+03 |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-4562.35 +/- 721.27
Episode length: 52.14 +/- 21.13
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.56e+03 |
| time/              |           |
|    total_timesteps | 396500    |
----------------------------------
Eval num_timesteps=397000, episode_reward=-4349.54 +/- 739.90
Episode length: 45.78 +/- 14.05
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.8      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 397000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.7      |
|    ep_rew_mean     | -4.26e+03 |
| time/              |           |
|    fps             | 187       |
|    iterations      | 194       |
|    time_elapsed    | 2115      |
|    total_timesteps | 397312    |
----------------------------------
Eval num_timesteps=397500, episode_reward=-4284.35 +/- 793.95
Episode length: 51.14 +/- 15.77
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 51.1           |
|    mean_reward          | -4.28e+03      |
| time/                   |                |
|    total_timesteps      | 397500         |
| train/                  |                |
|    approx_kl            | 1.32886635e-05 |
|    clip_fraction        | 0.000732       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0138        |
|    explained_variance   | 0.919          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.93e+04       |
|    n_updates            | 1940           |
|    policy_gradient_loss | -0.00014       |
|    value_loss           | 1.22e+05       |
--------------------------------------------
Eval num_timesteps=398000, episode_reward=-4493.55 +/- 654.95
Episode length: 56.14 +/- 22.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.1      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 398000    |
----------------------------------
Eval num_timesteps=398500, episode_reward=-4363.54 +/- 692.35
Episode length: 46.00 +/- 17.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46        |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 398500    |
----------------------------------
Eval num_timesteps=399000, episode_reward=-4433.14 +/- 695.75
Episode length: 48.54 +/- 14.71
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 399000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.1      |
|    ep_rew_mean     | -4.31e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 195       |
|    time_elapsed    | 2124      |
|    total_timesteps | 399360    |
----------------------------------
Eval num_timesteps=399500, episode_reward=-4468.75 +/- 592.43
Episode length: 52.06 +/- 18.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.1          |
|    mean_reward          | -4.47e+03     |
| time/                   |               |
|    total_timesteps      | 399500        |
| train/                  |               |
|    approx_kl            | 0.00016188624 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.87e+04      |
|    n_updates            | 1950          |
|    policy_gradient_loss | -0.000156     |
|    value_loss           | 9.55e+04      |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=-4417.14 +/- 796.20
Episode length: 45.02 +/- 15.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45        |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 400000    |
----------------------------------
Eval num_timesteps=400500, episode_reward=-4478.34 +/- 759.81
Episode length: 49.20 +/- 15.27
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 400500    |
----------------------------------
Eval num_timesteps=401000, episode_reward=-4450.35 +/- 843.65
Episode length: 52.52 +/- 15.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.5      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 401000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.1      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 196       |
|    time_elapsed    | 2133      |
|    total_timesteps | 401408    |
----------------------------------
Eval num_timesteps=401500, episode_reward=-4514.35 +/- 616.89
Episode length: 51.66 +/- 16.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51.7          |
|    mean_reward          | -4.51e+03     |
| time/                   |               |
|    total_timesteps      | 401500        |
| train/                  |               |
|    approx_kl            | 3.4694094e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00795      |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.75e+04      |
|    n_updates            | 1960          |
|    policy_gradient_loss | 3.92e-06      |
|    value_loss           | 9.54e+04      |
-------------------------------------------
Eval num_timesteps=402000, episode_reward=-4325.94 +/- 725.87
Episode length: 48.74 +/- 15.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 402000    |
----------------------------------
Eval num_timesteps=402500, episode_reward=-4325.54 +/- 867.32
Episode length: 51.10 +/- 16.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 402500    |
----------------------------------
Eval num_timesteps=403000, episode_reward=-4457.55 +/- 676.09
Episode length: 47.62 +/- 15.27
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.6      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 403000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.9      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 197       |
|    time_elapsed    | 2142      |
|    total_timesteps | 403456    |
----------------------------------
Eval num_timesteps=403500, episode_reward=-4501.95 +/- 756.06
Episode length: 49.20 +/- 13.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.2         |
|    mean_reward          | -4.5e+03     |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 9.793439e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0064      |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.91e+04     |
|    n_updates            | 1970         |
|    policy_gradient_loss | -1.83e-06    |
|    value_loss           | 9.6e+04      |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-4423.94 +/- 657.66
Episode length: 47.88 +/- 16.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 404000    |
----------------------------------
Eval num_timesteps=404500, episode_reward=-4328.35 +/- 835.37
Episode length: 51.46 +/- 17.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.5      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 404500    |
----------------------------------
Eval num_timesteps=405000, episode_reward=-4317.54 +/- 729.84
Episode length: 47.00 +/- 16.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47        |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 405000    |
----------------------------------
Eval num_timesteps=405500, episode_reward=-4425.54 +/- 683.86
Episode length: 56.72 +/- 17.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.7      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 405500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.1      |
|    ep_rew_mean     | -4.25e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 198       |
|    time_elapsed    | 2153      |
|    total_timesteps | 405504    |
----------------------------------
Eval num_timesteps=406000, episode_reward=-4400.35 +/- 781.97
Episode length: 48.98 +/- 15.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49            |
|    mean_reward          | -4.4e+03      |
| time/                   |               |
|    total_timesteps      | 406000        |
| train/                  |               |
|    approx_kl            | 2.5686779e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00782      |
|    explained_variance   | 0.886         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.34e+04      |
|    n_updates            | 1980          |
|    policy_gradient_loss | 1.09e-05      |
|    value_loss           | 1.8e+05       |
-------------------------------------------
Eval num_timesteps=406500, episode_reward=-4515.95 +/- 668.73
Episode length: 52.62 +/- 15.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.6      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 406500    |
----------------------------------
Eval num_timesteps=407000, episode_reward=-4388.74 +/- 895.10
Episode length: 48.38 +/- 16.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 407000    |
----------------------------------
Eval num_timesteps=407500, episode_reward=-4472.34 +/- 728.43
Episode length: 47.12 +/- 16.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.1      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 407500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.2      |
|    ep_rew_mean     | -4.29e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 199       |
|    time_elapsed    | 2162      |
|    total_timesteps | 407552    |
----------------------------------
Eval num_timesteps=408000, episode_reward=-4424.75 +/- 630.27
Episode length: 49.26 +/- 13.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.3          |
|    mean_reward          | -4.42e+03     |
| time/                   |               |
|    total_timesteps      | 408000        |
| train/                  |               |
|    approx_kl            | 1.1452357e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00812      |
|    explained_variance   | 0.918         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.02e+04      |
|    n_updates            | 1990          |
|    policy_gradient_loss | 3.05e-05      |
|    value_loss           | 1.14e+05      |
-------------------------------------------
Eval num_timesteps=408500, episode_reward=-4209.95 +/- 952.71
Episode length: 48.22 +/- 18.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.21e+03 |
| time/              |           |
|    total_timesteps | 408500    |
----------------------------------
Eval num_timesteps=409000, episode_reward=-4432.35 +/- 644.06
Episode length: 46.34 +/- 13.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.3      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 409000    |
----------------------------------
Eval num_timesteps=409500, episode_reward=-4463.96 +/- 844.68
Episode length: 51.02 +/- 19.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 409500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.3      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 200       |
|    time_elapsed    | 2171      |
|    total_timesteps | 409600    |
----------------------------------
Eval num_timesteps=410000, episode_reward=-4443.14 +/- 738.56
Episode length: 48.54 +/- 15.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.5          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 410000        |
| train/                  |               |
|    approx_kl            | 2.8696726e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00695      |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.37e+04      |
|    n_updates            | 2000          |
|    policy_gradient_loss | 2.37e-06      |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=410500, episode_reward=-4471.96 +/- 689.22
Episode length: 52.38 +/- 14.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.4      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 410500    |
----------------------------------
Eval num_timesteps=411000, episode_reward=-4377.55 +/- 782.93
Episode length: 49.42 +/- 16.33
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 411000    |
----------------------------------
Eval num_timesteps=411500, episode_reward=-4469.95 +/- 724.02
Episode length: 55.70 +/- 18.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55.7      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 411500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 46.3      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 201       |
|    time_elapsed    | 2180      |
|    total_timesteps | 411648    |
----------------------------------
Eval num_timesteps=412000, episode_reward=-4465.00 +/- 933.59
Episode length: 49.90 +/- 18.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.9          |
|    mean_reward          | -4.46e+03     |
| time/                   |               |
|    total_timesteps      | 412000        |
| train/                  |               |
|    approx_kl            | 4.0259067e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00601      |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.27e+04      |
|    n_updates            | 2010          |
|    policy_gradient_loss | 4.86e-05      |
|    value_loss           | 1.21e+05      |
-------------------------------------------
Eval num_timesteps=412500, episode_reward=-4423.14 +/- 738.32
Episode length: 48.96 +/- 16.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 412500    |
----------------------------------
Eval num_timesteps=413000, episode_reward=-4353.55 +/- 968.42
Episode length: 50.06 +/- 19.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.1      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 413000    |
----------------------------------
Eval num_timesteps=413500, episode_reward=-4377.15 +/- 798.12
Episode length: 48.58 +/- 18.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 413500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.5      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 188       |
|    iterations      | 202       |
|    time_elapsed    | 2190      |
|    total_timesteps | 413696    |
----------------------------------
Eval num_timesteps=414000, episode_reward=-4455.55 +/- 667.09
Episode length: 48.60 +/- 18.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.6         |
|    mean_reward          | -4.46e+03    |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 9.024749e-06 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00699     |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.49e+04     |
|    n_updates            | 2020         |
|    policy_gradient_loss | 4.01e-05     |
|    value_loss           | 1.07e+05     |
------------------------------------------
Eval num_timesteps=414500, episode_reward=-4415.94 +/- 706.65
Episode length: 52.12 +/- 17.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 414500    |
----------------------------------
Eval num_timesteps=415000, episode_reward=-4516.75 +/- 699.27
Episode length: 46.44 +/- 15.38
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.4      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 415000    |
----------------------------------
Eval num_timesteps=415500, episode_reward=-4257.94 +/- 842.22
Episode length: 48.58 +/- 17.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.26e+03 |
| time/              |           |
|    total_timesteps | 415500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.5      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 189       |
|    iterations      | 203       |
|    time_elapsed    | 2199      |
|    total_timesteps | 415744    |
----------------------------------
Eval num_timesteps=416000, episode_reward=-4485.94 +/- 736.96
Episode length: 50.48 +/- 17.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.5          |
|    mean_reward          | -4.49e+03     |
| time/                   |               |
|    total_timesteps      | 416000        |
| train/                  |               |
|    approx_kl            | 5.1534735e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00488      |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.65e+04      |
|    n_updates            | 2030          |
|    policy_gradient_loss | 1.25e-05      |
|    value_loss           | 8.69e+04      |
-------------------------------------------
Eval num_timesteps=416500, episode_reward=-4374.34 +/- 701.80
Episode length: 52.08 +/- 16.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 416500    |
----------------------------------
Eval num_timesteps=417000, episode_reward=-4426.34 +/- 874.02
Episode length: 52.88 +/- 19.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.9      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 417000    |
----------------------------------
Eval num_timesteps=417500, episode_reward=-4446.35 +/- 886.86
Episode length: 52.54 +/- 18.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.5      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 417500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 189      |
|    iterations      | 204      |
|    time_elapsed    | 2208     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=-4496.35 +/- 693.39
Episode length: 52.32 +/- 18.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.3          |
|    mean_reward          | -4.5e+03      |
| time/                   |               |
|    total_timesteps      | 418000        |
| train/                  |               |
|    approx_kl            | 0.00015899807 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00447      |
|    explained_variance   | 0.941         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.26e+04      |
|    n_updates            | 2040          |
|    policy_gradient_loss | -7.93e-05     |
|    value_loss           | 9.58e+04      |
-------------------------------------------
Eval num_timesteps=418500, episode_reward=-4360.34 +/- 737.71
Episode length: 52.84 +/- 20.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.8      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 418500    |
----------------------------------
Eval num_timesteps=419000, episode_reward=-4306.75 +/- 784.76
Episode length: 49.86 +/- 18.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 419000    |
----------------------------------
Eval num_timesteps=419500, episode_reward=-4474.32 +/- 776.90
Episode length: 49.92 +/- 16.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 419500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.3      |
|    ep_rew_mean     | -4.52e+03 |
| time/              |           |
|    fps             | 189       |
|    iterations      | 205       |
|    time_elapsed    | 2217      |
|    total_timesteps | 419840    |
----------------------------------
Eval num_timesteps=420000, episode_reward=-4420.35 +/- 690.43
Episode length: 49.72 +/- 14.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.7          |
|    mean_reward          | -4.42e+03     |
| time/                   |               |
|    total_timesteps      | 420000        |
| train/                  |               |
|    approx_kl            | 1.7377897e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00447      |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.66e+04      |
|    n_updates            | 2050          |
|    policy_gradient_loss | 2.36e-06      |
|    value_loss           | 7.2e+04       |
-------------------------------------------
Eval num_timesteps=420500, episode_reward=-4426.75 +/- 607.18
Episode length: 49.62 +/- 15.51
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 420500    |
----------------------------------
Eval num_timesteps=421000, episode_reward=-4453.14 +/- 556.53
Episode length: 50.40 +/- 16.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 421000    |
----------------------------------
Eval num_timesteps=421500, episode_reward=-4343.55 +/- 906.11
Episode length: 47.92 +/- 15.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.9      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 421500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 54.3      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 189       |
|    iterations      | 206       |
|    time_elapsed    | 2227      |
|    total_timesteps | 421888    |
----------------------------------
Eval num_timesteps=422000, episode_reward=-4425.95 +/- 745.75
Episode length: 49.64 +/- 15.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.6          |
|    mean_reward          | -4.43e+03     |
| time/                   |               |
|    total_timesteps      | 422000        |
| train/                  |               |
|    approx_kl            | 0.00066788064 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00659      |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.13e+04      |
|    n_updates            | 2060          |
|    policy_gradient_loss | 3.25e-06      |
|    value_loss           | 7.48e+04      |
-------------------------------------------
Eval num_timesteps=422500, episode_reward=-4571.54 +/- 577.34
Episode length: 46.64 +/- 14.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.6      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 422500    |
----------------------------------
Eval num_timesteps=423000, episode_reward=-4457.14 +/- 695.58
Episode length: 51.74 +/- 17.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.7      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 423000    |
----------------------------------
Eval num_timesteps=423500, episode_reward=-4238.75 +/- 847.40
Episode length: 48.30 +/- 16.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.24e+03 |
| time/              |           |
|    total_timesteps | 423500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.3      |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 189       |
|    iterations      | 207       |
|    time_elapsed    | 2236      |
|    total_timesteps | 423936    |
----------------------------------
Eval num_timesteps=424000, episode_reward=-4499.95 +/- 689.74
Episode length: 47.88 +/- 14.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.9         |
|    mean_reward          | -4.5e+03     |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 5.654001e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00427     |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.0001       |
|    loss                 | 1.72e+05     |
|    n_updates            | 2070         |
|    policy_gradient_loss | 2.1e-05      |
|    value_loss           | 1.58e+05     |
------------------------------------------
Eval num_timesteps=424500, episode_reward=-4323.14 +/- 755.79
Episode length: 50.62 +/- 18.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 424500    |
----------------------------------
Eval num_timesteps=425000, episode_reward=-4449.95 +/- 668.80
Episode length: 48.40 +/- 13.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 425000    |
----------------------------------
Eval num_timesteps=425500, episode_reward=-4358.35 +/- 667.30
Episode length: 50.32 +/- 14.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 425500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.8      |
|    ep_rew_mean     | -4.45e+03 |
| time/              |           |
|    fps             | 189       |
|    iterations      | 208       |
|    time_elapsed    | 2245      |
|    total_timesteps | 425984    |
----------------------------------
Eval num_timesteps=426000, episode_reward=-4323.54 +/- 893.52
Episode length: 53.24 +/- 24.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.2          |
|    mean_reward          | -4.32e+03     |
| time/                   |               |
|    total_timesteps      | 426000        |
| train/                  |               |
|    approx_kl            | 5.3142692e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00712      |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.17e+04      |
|    n_updates            | 2080          |
|    policy_gradient_loss | 3.63e-05      |
|    value_loss           | 8.67e+04      |
-------------------------------------------
Eval num_timesteps=426500, episode_reward=-4183.15 +/- 928.50
Episode length: 46.80 +/- 17.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.8      |
|    mean_reward     | -4.18e+03 |
| time/              |           |
|    total_timesteps | 426500    |
----------------------------------
Eval num_timesteps=427000, episode_reward=-4484.33 +/- 485.53
Episode length: 48.02 +/- 14.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 427000    |
----------------------------------
Eval num_timesteps=427500, episode_reward=-4477.15 +/- 563.54
Episode length: 51.64 +/- 18.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 427500    |
----------------------------------
Eval num_timesteps=428000, episode_reward=-4507.53 +/- 709.15
Episode length: 47.36 +/- 18.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.4      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 428000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.4      |
|    ep_rew_mean     | -4.38e+03 |
| time/              |           |
|    fps             | 189       |
|    iterations      | 209       |
|    time_elapsed    | 2255      |
|    total_timesteps | 428032    |
----------------------------------
Eval num_timesteps=428500, episode_reward=-4461.54 +/- 688.17
Episode length: 46.08 +/- 13.87
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 46.1           |
|    mean_reward          | -4.46e+03      |
| time/                   |                |
|    total_timesteps      | 428500         |
| train/                  |                |
|    approx_kl            | 1.39459735e-05 |
|    clip_fraction        | 0.000342       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00596       |
|    explained_variance   | 0.911          |
|    learning_rate        | 0.0001         |
|    loss                 | 5.9e+04        |
|    n_updates            | 2090           |
|    policy_gradient_loss | 9.21e-06       |
|    value_loss           | 1.46e+05       |
--------------------------------------------
Eval num_timesteps=429000, episode_reward=-4127.54 +/- 905.01
Episode length: 46.72 +/- 15.38
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.7      |
|    mean_reward     | -4.13e+03 |
| time/              |           |
|    total_timesteps | 429000    |
----------------------------------
Eval num_timesteps=429500, episode_reward=-4346.69 +/- 820.96
Episode length: 48.48 +/- 17.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.5      |
|    mean_reward     | -4.35e+03 |
| time/              |           |
|    total_timesteps | 429500    |
----------------------------------
Eval num_timesteps=430000, episode_reward=-4588.75 +/- 610.36
Episode length: 51.74 +/- 16.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.7      |
|    mean_reward     | -4.59e+03 |
| time/              |           |
|    total_timesteps | 430000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -4.4e+03 |
| time/              |          |
|    fps             | 189      |
|    iterations      | 210      |
|    time_elapsed    | 2264     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=-4369.94 +/- 688.67
Episode length: 47.14 +/- 16.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.1         |
|    mean_reward          | -4.37e+03    |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 3.817113e-06 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00599     |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.58e+04     |
|    n_updates            | 2100         |
|    policy_gradient_loss | -2.56e-05    |
|    value_loss           | 1.16e+05     |
------------------------------------------
Eval num_timesteps=431000, episode_reward=-4516.35 +/- 590.85
Episode length: 56.30 +/- 17.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.3      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 431000    |
----------------------------------
Eval num_timesteps=431500, episode_reward=-4362.36 +/- 792.15
Episode length: 53.04 +/- 14.47
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53        |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 431500    |
----------------------------------
Eval num_timesteps=432000, episode_reward=-4420.34 +/- 650.58
Episode length: 47.52 +/- 17.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.5      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 432000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.4      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 211       |
|    time_elapsed    | 2274      |
|    total_timesteps | 432128    |
----------------------------------
Eval num_timesteps=432500, episode_reward=-4561.15 +/- 536.82
Episode length: 51.04 +/- 14.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51            |
|    mean_reward          | -4.56e+03     |
| time/                   |               |
|    total_timesteps      | 432500        |
| train/                  |               |
|    approx_kl            | 1.6118516e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00765      |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.23e+04      |
|    n_updates            | 2110          |
|    policy_gradient_loss | 3.84e-05      |
|    value_loss           | 7.43e+04      |
-------------------------------------------
Eval num_timesteps=433000, episode_reward=-4466.75 +/- 643.34
Episode length: 48.36 +/- 15.16
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 433000    |
----------------------------------
Eval num_timesteps=433500, episode_reward=-4470.76 +/- 776.17
Episode length: 53.34 +/- 17.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.3      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 433500    |
----------------------------------
Eval num_timesteps=434000, episode_reward=-4391.95 +/- 748.69
Episode length: 52.26 +/- 18.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.3      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 434000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.1      |
|    ep_rew_mean     | -4.46e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 212       |
|    time_elapsed    | 2283      |
|    total_timesteps | 434176    |
----------------------------------
Eval num_timesteps=434500, episode_reward=-4596.74 +/- 599.83
Episode length: 48.90 +/- 17.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 48.9          |
|    mean_reward          | -4.6e+03      |
| time/                   |               |
|    total_timesteps      | 434500        |
| train/                  |               |
|    approx_kl            | 0.00012247678 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00815      |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.56e+04      |
|    n_updates            | 2120          |
|    policy_gradient_loss | -3.47e-05     |
|    value_loss           | 1.18e+05      |
-------------------------------------------
Eval num_timesteps=435000, episode_reward=-4499.93 +/- 695.01
Episode length: 49.82 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -4.5e+03 |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=-4507.95 +/- 741.96
Episode length: 51.10 +/- 15.47
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 435500    |
----------------------------------
Eval num_timesteps=436000, episode_reward=-4417.16 +/- 735.75
Episode length: 46.56 +/- 19.27
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.6      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 436000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.3      |
|    ep_rew_mean     | -4.39e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 213       |
|    time_elapsed    | 2292      |
|    total_timesteps | 436224    |
----------------------------------
Eval num_timesteps=436500, episode_reward=-4439.54 +/- 698.86
Episode length: 46.70 +/- 14.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 46.7          |
|    mean_reward          | -4.44e+03     |
| time/                   |               |
|    total_timesteps      | 436500        |
| train/                  |               |
|    approx_kl            | 2.0026724e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00944      |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.69e+04      |
|    n_updates            | 2130          |
|    policy_gradient_loss | 4.57e-06      |
|    value_loss           | 1.14e+05      |
-------------------------------------------
Eval num_timesteps=437000, episode_reward=-4305.55 +/- 775.85
Episode length: 49.52 +/- 16.91
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.5      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 437000    |
----------------------------------
Eval num_timesteps=437500, episode_reward=-4422.34 +/- 687.98
Episode length: 47.74 +/- 17.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.7      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 437500    |
----------------------------------
Eval num_timesteps=438000, episode_reward=-4319.95 +/- 764.64
Episode length: 49.12 +/- 14.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.1      |
|    mean_reward     | -4.32e+03 |
| time/              |           |
|    total_timesteps | 438000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50        |
|    ep_rew_mean     | -4.31e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 214       |
|    time_elapsed    | 2301      |
|    total_timesteps | 438272    |
----------------------------------
Eval num_timesteps=438500, episode_reward=-4427.55 +/- 824.51
Episode length: 47.64 +/- 14.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.6         |
|    mean_reward          | -4.43e+03    |
| time/                   |              |
|    total_timesteps      | 438500       |
| train/                  |              |
|    approx_kl            | 9.675021e-05 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0122      |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.18e+04     |
|    n_updates            | 2140         |
|    policy_gradient_loss | 3.07e-05     |
|    value_loss           | 1.11e+05     |
------------------------------------------
Eval num_timesteps=439000, episode_reward=-4383.94 +/- 775.15
Episode length: 44.00 +/- 12.07
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44        |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 439000    |
----------------------------------
Eval num_timesteps=439500, episode_reward=-4399.94 +/- 577.48
Episode length: 48.10 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-4462.34 +/- 850.81
Episode length: 53.52 +/- 17.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.5      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 440000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 48.3      |
|    ep_rew_mean     | -4.37e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 215       |
|    time_elapsed    | 2310      |
|    total_timesteps | 440320    |
----------------------------------
Eval num_timesteps=440500, episode_reward=-4387.54 +/- 644.46
Episode length: 49.04 +/- 15.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49            |
|    mean_reward          | -4.39e+03     |
| time/                   |               |
|    total_timesteps      | 440500        |
| train/                  |               |
|    approx_kl            | 0.00025915806 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.909         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.82e+04      |
|    n_updates            | 2150          |
|    policy_gradient_loss | -0.00014      |
|    value_loss           | 1.57e+05      |
-------------------------------------------
Eval num_timesteps=441000, episode_reward=-4508.75 +/- 657.09
Episode length: 49.30 +/- 16.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 441000    |
----------------------------------
Eval num_timesteps=441500, episode_reward=-4534.75 +/- 637.31
Episode length: 52.80 +/- 16.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.8      |
|    mean_reward     | -4.53e+03 |
| time/              |           |
|    total_timesteps | 441500    |
----------------------------------
Eval num_timesteps=442000, episode_reward=-4311.53 +/- 720.92
Episode length: 44.26 +/- 14.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 44.3      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 442000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 49.6      |
|    ep_rew_mean     | -4.43e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 216       |
|    time_elapsed    | 2319      |
|    total_timesteps | 442368    |
----------------------------------
Eval num_timesteps=442500, episode_reward=-4505.94 +/- 603.28
Episode length: 48.92 +/- 17.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.9        |
|    mean_reward          | -4.51e+03   |
| time/                   |             |
|    total_timesteps      | 442500      |
| train/                  |             |
|    approx_kl            | 0.000195161 |
|    clip_fraction        | 0.000439    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.021      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.52e+04    |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.000176   |
|    value_loss           | 1.01e+05    |
-----------------------------------------
Eval num_timesteps=443000, episode_reward=-4573.15 +/- 630.55
Episode length: 54.38 +/- 19.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.4      |
|    mean_reward     | -4.57e+03 |
| time/              |           |
|    total_timesteps | 443000    |
----------------------------------
Eval num_timesteps=443500, episode_reward=-4412.74 +/- 646.31
Episode length: 48.26 +/- 16.39
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 443500    |
----------------------------------
Eval num_timesteps=444000, episode_reward=-4486.35 +/- 785.90
Episode length: 50.62 +/- 16.89
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 444000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.1      |
|    ep_rew_mean     | -4.47e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 217       |
|    time_elapsed    | 2328      |
|    total_timesteps | 444416    |
----------------------------------
Eval num_timesteps=444500, episode_reward=-4349.53 +/- 718.60
Episode length: 46.48 +/- 12.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.5         |
|    mean_reward          | -4.35e+03    |
| time/                   |              |
|    total_timesteps      | 444500       |
| train/                  |              |
|    approx_kl            | 0.0002192127 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0385      |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.84e+04     |
|    n_updates            | 2170         |
|    policy_gradient_loss | -6.11e-05    |
|    value_loss           | 7.1e+04      |
------------------------------------------
Eval num_timesteps=445000, episode_reward=-4336.75 +/- 775.82
Episode length: 56.18 +/- 23.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.2      |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 445000    |
----------------------------------
Eval num_timesteps=445500, episode_reward=-4467.13 +/- 605.29
Episode length: 49.16 +/- 16.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 445500    |
----------------------------------
Eval num_timesteps=446000, episode_reward=-4331.55 +/- 817.87
Episode length: 47.70 +/- 14.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.7      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 446000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52        |
|    ep_rew_mean     | -4.54e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 218       |
|    time_elapsed    | 2337      |
|    total_timesteps | 446464    |
----------------------------------
Eval num_timesteps=446500, episode_reward=-4291.54 +/- 698.28
Episode length: 49.14 +/- 13.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.1          |
|    mean_reward          | -4.29e+03     |
| time/                   |               |
|    total_timesteps      | 446500        |
| train/                  |               |
|    approx_kl            | 0.00020777789 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0375       |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.71e+04      |
|    n_updates            | 2180          |
|    policy_gradient_loss | -0.00016      |
|    value_loss           | 7.12e+04      |
-------------------------------------------
Eval num_timesteps=447000, episode_reward=-4757.56 +/- 496.90
Episode length: 52.22 +/- 20.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.2      |
|    mean_reward     | -4.76e+03 |
| time/              |           |
|    total_timesteps | 447000    |
----------------------------------
Eval num_timesteps=447500, episode_reward=-4333.94 +/- 745.93
Episode length: 48.20 +/- 14.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.2      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 447500    |
----------------------------------
Eval num_timesteps=448000, episode_reward=-4611.14 +/- 556.57
Episode length: 52.14 +/- 19.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 448000    |
----------------------------------
Eval num_timesteps=448500, episode_reward=-4367.15 +/- 731.90
Episode length: 51.56 +/- 19.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 448500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 47.9      |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 190       |
|    iterations      | 219       |
|    time_elapsed    | 2348      |
|    total_timesteps | 448512    |
----------------------------------
Eval num_timesteps=449000, episode_reward=-4332.74 +/- 718.89
Episode length: 51.72 +/- 16.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.7       |
|    mean_reward          | -4.33e+03  |
| time/                   |            |
|    total_timesteps      | 449000     |
| train/                  |            |
|    approx_kl            | 0.04653192 |
|    clip_fraction        | 0.0609     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.23e+04   |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0047    |
|    value_loss           | 8.53e+04   |
----------------------------------------
Eval num_timesteps=449500, episode_reward=-4394.75 +/- 866.24
Episode length: 48.14 +/- 14.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.39e+03 |
| time/              |           |
|    total_timesteps | 449500    |
----------------------------------
Eval num_timesteps=450000, episode_reward=-4589.54 +/- 620.37
Episode length: 53.02 +/- 18.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53        |
|    mean_reward     | -4.59e+03 |
| time/              |           |
|    total_timesteps | 450000    |
----------------------------------
Eval num_timesteps=450500, episode_reward=-4475.15 +/- 753.59
Episode length: 52.14 +/- 19.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.1      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 450500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.6      |
|    ep_rew_mean     | -4.41e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 220       |
|    time_elapsed    | 2357      |
|    total_timesteps | 450560    |
----------------------------------
Eval num_timesteps=451000, episode_reward=-4339.14 +/- 699.41
Episode length: 50.10 +/- 17.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.1        |
|    mean_reward          | -4.34e+03   |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.001710788 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.45e+04    |
|    n_updates            | 2200        |
|    policy_gradient_loss | 0.00039     |
|    value_loss           | 8.87e+04    |
-----------------------------------------
Eval num_timesteps=451500, episode_reward=-4481.55 +/- 737.13
Episode length: 51.86 +/- 16.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 451500    |
----------------------------------
Eval num_timesteps=452000, episode_reward=-4441.15 +/- 812.63
Episode length: 50.98 +/- 19.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 452000    |
----------------------------------
Eval num_timesteps=452500, episode_reward=-4418.35 +/- 787.68
Episode length: 51.44 +/- 17.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.4      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 452500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51        |
|    ep_rew_mean     | -4.36e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 221       |
|    time_elapsed    | 2366      |
|    total_timesteps | 452608    |
----------------------------------
Eval num_timesteps=453000, episode_reward=-4449.15 +/- 637.81
Episode length: 52.00 +/- 19.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52           |
|    mean_reward          | -4.45e+03    |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0016125796 |
|    clip_fraction        | 0.039        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.48e+04     |
|    n_updates            | 2210         |
|    policy_gradient_loss | 0.00188      |
|    value_loss           | 9.17e+04     |
------------------------------------------
Eval num_timesteps=453500, episode_reward=-4334.75 +/- 741.28
Episode length: 51.14 +/- 13.41
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.1      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 453500    |
----------------------------------
Eval num_timesteps=454000, episode_reward=-4231.94 +/- 771.98
Episode length: 50.32 +/- 16.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.23e+03 |
| time/              |           |
|    total_timesteps | 454000    |
----------------------------------
Eval num_timesteps=454500, episode_reward=-4362.74 +/- 818.44
Episode length: 49.86 +/- 19.54
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 454500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.6      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 222       |
|    time_elapsed    | 2375      |
|    total_timesteps | 454656    |
----------------------------------
Eval num_timesteps=455000, episode_reward=-4413.54 +/- 593.30
Episode length: 49.78 +/- 18.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -4.41e+03    |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0016151301 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.04e+04     |
|    n_updates            | 2220         |
|    policy_gradient_loss | 0.00183      |
|    value_loss           | 7.44e+04     |
------------------------------------------
Eval num_timesteps=455500, episode_reward=-4434.75 +/- 663.73
Episode length: 56.78 +/- 18.58
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.8      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 455500    |
----------------------------------
Eval num_timesteps=456000, episode_reward=-4401.94 +/- 721.06
Episode length: 51.12 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=-4409.14 +/- 718.32
Episode length: 49.88 +/- 19.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 456500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.1      |
|    ep_rew_mean     | -4.24e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 223       |
|    time_elapsed    | 2385      |
|    total_timesteps | 456704    |
----------------------------------
Eval num_timesteps=457000, episode_reward=-4463.15 +/- 675.04
Episode length: 51.76 +/- 15.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.8       |
|    mean_reward          | -4.46e+03  |
| time/                   |            |
|    total_timesteps      | 457000     |
| train/                  |            |
|    approx_kl            | 0.01509389 |
|    clip_fraction        | 0.075      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.235     |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.03e+04   |
|    n_updates            | 2230       |
|    policy_gradient_loss | 0.00437    |
|    value_loss           | 7.73e+04   |
----------------------------------------
Eval num_timesteps=457500, episode_reward=-4444.74 +/- 610.01
Episode length: 50.90 +/- 17.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 457500    |
----------------------------------
Eval num_timesteps=458000, episode_reward=-4441.94 +/- 802.22
Episode length: 48.14 +/- 17.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.1      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 458000    |
----------------------------------
Eval num_timesteps=458500, episode_reward=-4306.34 +/- 806.82
Episode length: 51.76 +/- 17.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 458500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.5      |
|    ep_rew_mean     | -4.15e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 224       |
|    time_elapsed    | 2394      |
|    total_timesteps | 458752    |
----------------------------------
Eval num_timesteps=459000, episode_reward=-4487.55 +/- 543.17
Episode length: 50.10 +/- 16.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.1         |
|    mean_reward          | -4.49e+03    |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0042010564 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.933        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.43e+04     |
|    n_updates            | 2240         |
|    policy_gradient_loss | 0.00446      |
|    value_loss           | 9.47e+04     |
------------------------------------------
Eval num_timesteps=459500, episode_reward=-4363.93 +/- 779.87
Episode length: 49.80 +/- 20.71
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.8      |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 459500    |
----------------------------------
Eval num_timesteps=460000, episode_reward=-4384.75 +/- 684.57
Episode length: 53.06 +/- 15.67
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53.1      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 460000    |
----------------------------------
Eval num_timesteps=460500, episode_reward=-4453.95 +/- 703.22
Episode length: 49.48 +/- 13.10
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.5      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 460500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 54.4      |
|    ep_rew_mean     | -4.25e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 225       |
|    time_elapsed    | 2403      |
|    total_timesteps | 460800    |
----------------------------------
Eval num_timesteps=461000, episode_reward=-4549.93 +/- 633.71
Episode length: 49.34 +/- 14.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.3         |
|    mean_reward          | -4.55e+03    |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0035805055 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.76e+04     |
|    n_updates            | 2250         |
|    policy_gradient_loss | 0.00174      |
|    value_loss           | 5.95e+04     |
------------------------------------------
Eval num_timesteps=461500, episode_reward=-4480.75 +/- 560.82
Episode length: 50.30 +/- 15.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 461500    |
----------------------------------
Eval num_timesteps=462000, episode_reward=-4249.15 +/- 854.73
Episode length: 47.28 +/- 18.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.3      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 462000    |
----------------------------------
Eval num_timesteps=462500, episode_reward=-4270.34 +/- 633.54
Episode length: 45.36 +/- 12.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 45.4      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 462500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.4      |
|    ep_rew_mean     | -4.26e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 226       |
|    time_elapsed    | 2412      |
|    total_timesteps | 462848    |
----------------------------------
Eval num_timesteps=463000, episode_reward=-4551.94 +/- 617.25
Episode length: 52.08 +/- 20.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.1         |
|    mean_reward          | -4.55e+03    |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0016336796 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.13e+04     |
|    n_updates            | 2260         |
|    policy_gradient_loss | 0.0028       |
|    value_loss           | 7.74e+04     |
------------------------------------------
Eval num_timesteps=463500, episode_reward=-4380.35 +/- 744.59
Episode length: 49.94 +/- 16.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 463500    |
----------------------------------
Eval num_timesteps=464000, episode_reward=-4447.55 +/- 721.03
Episode length: 52.30 +/- 18.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.3      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 464000    |
----------------------------------
Eval num_timesteps=464500, episode_reward=-4436.74 +/- 804.34
Episode length: 51.88 +/- 19.47
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 464500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52        |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 191       |
|    iterations      | 227       |
|    time_elapsed    | 2422      |
|    total_timesteps | 464896    |
----------------------------------
Eval num_timesteps=465000, episode_reward=-4471.95 +/- 740.44
Episode length: 50.32 +/- 16.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.3         |
|    mean_reward          | -4.47e+03    |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0039110924 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.97e+04     |
|    n_updates            | 2270         |
|    policy_gradient_loss | 0.00109      |
|    value_loss           | 1.11e+05     |
------------------------------------------
Eval num_timesteps=465500, episode_reward=-4491.15 +/- 779.76
Episode length: 51.60 +/- 18.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 465500    |
----------------------------------
Eval num_timesteps=466000, episode_reward=-4515.06 +/- 881.98
Episode length: 54.64 +/- 18.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.6      |
|    mean_reward     | -4.52e+03 |
| time/              |           |
|    total_timesteps | 466000    |
----------------------------------
Eval num_timesteps=466500, episode_reward=-4360.36 +/- 789.48
Episode length: 50.96 +/- 18.06
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.36e+03 |
| time/              |           |
|    total_timesteps | 466500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -4.3e+03 |
| time/              |          |
|    fps             | 192      |
|    iterations      | 228      |
|    time_elapsed    | 2431     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=-4401.14 +/- 699.08
Episode length: 50.40 +/- 15.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.4         |
|    mean_reward          | -4.4e+03     |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0014277378 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.94e+04     |
|    n_updates            | 2280         |
|    policy_gradient_loss | 0.000973     |
|    value_loss           | 9.64e+04     |
------------------------------------------
Eval num_timesteps=467500, episode_reward=-4471.15 +/- 634.00
Episode length: 50.22 +/- 15.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.2      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 467500    |
----------------------------------
Eval num_timesteps=468000, episode_reward=-4165.95 +/- 760.90
Episode length: 49.38 +/- 17.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.17e+03 |
| time/              |           |
|    total_timesteps | 468000    |
----------------------------------
Eval num_timesteps=468500, episode_reward=-4537.54 +/- 592.39
Episode length: 49.02 +/- 17.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49        |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 468500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.5      |
|    ep_rew_mean     | -4.17e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 229       |
|    time_elapsed    | 2440      |
|    total_timesteps | 468992    |
----------------------------------
Eval num_timesteps=469000, episode_reward=-4386.74 +/- 719.89
Episode length: 50.02 +/- 17.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -4.39e+03    |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0013025282 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.79e+04     |
|    n_updates            | 2290         |
|    policy_gradient_loss | 0.00268      |
|    value_loss           | 8.76e+04     |
------------------------------------------
Eval num_timesteps=469500, episode_reward=-4424.75 +/- 706.56
Episode length: 48.32 +/- 14.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.3      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 469500    |
----------------------------------
Eval num_timesteps=470000, episode_reward=-4432.34 +/- 601.74
Episode length: 50.64 +/- 16.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 470000    |
----------------------------------
Eval num_timesteps=470500, episode_reward=-4281.15 +/- 834.72
Episode length: 53.00 +/- 18.62
----------------------------------
| eval/              |           |
|    mean_ep_length  | 53        |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 470500    |
----------------------------------
Eval num_timesteps=471000, episode_reward=-4325.55 +/- 700.44
Episode length: 47.66 +/- 15.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.7      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 471000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.5      |
|    ep_rew_mean     | -4.21e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 230       |
|    time_elapsed    | 2451      |
|    total_timesteps | 471040    |
----------------------------------
Eval num_timesteps=471500, episode_reward=-4315.94 +/- 715.06
Episode length: 48.18 +/- 15.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.2         |
|    mean_reward          | -4.32e+03    |
| time/                   |              |
|    total_timesteps      | 471500       |
| train/                  |              |
|    approx_kl            | 0.0010932571 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.144       |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.97e+04     |
|    n_updates            | 2300         |
|    policy_gradient_loss | 0.00274      |
|    value_loss           | 7.3e+04      |
------------------------------------------
Eval num_timesteps=472000, episode_reward=-4185.94 +/- 852.94
Episode length: 47.46 +/- 13.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.5      |
|    mean_reward     | -4.19e+03 |
| time/              |           |
|    total_timesteps | 472000    |
----------------------------------
Eval num_timesteps=472500, episode_reward=-4656.75 +/- 640.12
Episode length: 55.82 +/- 17.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 55.8      |
|    mean_reward     | -4.66e+03 |
| time/              |           |
|    total_timesteps | 472500    |
----------------------------------
Eval num_timesteps=473000, episode_reward=-4404.75 +/- 719.15
Episode length: 50.02 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -4.4e+03 |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.3      |
|    ep_rew_mean     | -4.26e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 231       |
|    time_elapsed    | 2460      |
|    total_timesteps | 473088    |
----------------------------------
Eval num_timesteps=473500, episode_reward=-4227.94 +/- 811.14
Episode length: 46.22 +/- 15.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.2         |
|    mean_reward          | -4.23e+03    |
| time/                   |              |
|    total_timesteps      | 473500       |
| train/                  |              |
|    approx_kl            | 0.0013642859 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.36e+04     |
|    n_updates            | 2310         |
|    policy_gradient_loss | 0.00119      |
|    value_loss           | 8.75e+04     |
------------------------------------------
Eval num_timesteps=474000, episode_reward=-4585.55 +/- 657.52
Episode length: 50.34 +/- 15.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.59e+03 |
| time/              |           |
|    total_timesteps | 474000    |
----------------------------------
Eval num_timesteps=474500, episode_reward=-4484.35 +/- 732.04
Episode length: 51.28 +/- 14.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 474500    |
----------------------------------
Eval num_timesteps=475000, episode_reward=-4431.95 +/- 688.94
Episode length: 51.62 +/- 16.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 475000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.3      |
|    ep_rew_mean     | -4.26e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 232       |
|    time_elapsed    | 2470      |
|    total_timesteps | 475136    |
----------------------------------
Eval num_timesteps=475500, episode_reward=-4436.75 +/- 602.55
Episode length: 55.94 +/- 15.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.9         |
|    mean_reward          | -4.44e+03    |
| time/                   |              |
|    total_timesteps      | 475500       |
| train/                  |              |
|    approx_kl            | 0.0043255417 |
|    clip_fraction        | 0.0311       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.4e+04      |
|    n_updates            | 2320         |
|    policy_gradient_loss | 0.00304      |
|    value_loss           | 8.63e+04     |
------------------------------------------
Eval num_timesteps=476000, episode_reward=-4553.55 +/- 605.76
Episode length: 50.90 +/- 17.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.9      |
|    mean_reward     | -4.55e+03 |
| time/              |           |
|    total_timesteps | 476000    |
----------------------------------
Eval num_timesteps=476500, episode_reward=-4268.75 +/- 612.09
Episode length: 49.20 +/- 16.38
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 476500    |
----------------------------------
Eval num_timesteps=477000, episode_reward=-4379.15 +/- 675.16
Episode length: 51.86 +/- 15.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.9      |
|    mean_reward     | -4.38e+03 |
| time/              |           |
|    total_timesteps | 477000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 54.8      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 233       |
|    time_elapsed    | 2479      |
|    total_timesteps | 477184    |
----------------------------------
Eval num_timesteps=477500, episode_reward=-4492.74 +/- 607.70
Episode length: 47.82 +/- 17.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.8         |
|    mean_reward          | -4.49e+03    |
| time/                   |              |
|    total_timesteps      | 477500       |
| train/                  |              |
|    approx_kl            | 0.0064027878 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.69e+04     |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.000244    |
|    value_loss           | 8.32e+04     |
------------------------------------------
Eval num_timesteps=478000, episode_reward=-4459.95 +/- 773.29
Episode length: 46.90 +/- 14.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.9      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 478000    |
----------------------------------
Eval num_timesteps=478500, episode_reward=-4467.14 +/- 697.70
Episode length: 51.64 +/- 18.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.6      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 478500    |
----------------------------------
Eval num_timesteps=479000, episode_reward=-4493.15 +/- 697.26
Episode length: 49.66 +/- 14.45
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.49e+03 |
| time/              |           |
|    total_timesteps | 479000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 55.2      |
|    ep_rew_mean     | -4.33e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 234       |
|    time_elapsed    | 2488      |
|    total_timesteps | 479232    |
----------------------------------
Eval num_timesteps=479500, episode_reward=-4350.35 +/- 851.83
Episode length: 50.66 +/- 12.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.7         |
|    mean_reward          | -4.35e+03    |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0017384482 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.166       |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.28e+04     |
|    n_updates            | 2340         |
|    policy_gradient_loss | 0.000947     |
|    value_loss           | 7.63e+04     |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-4433.55 +/- 722.55
Episode length: 48.58 +/- 16.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 480000    |
----------------------------------
Eval num_timesteps=480500, episode_reward=-4306.35 +/- 916.19
Episode length: 48.68 +/- 16.64
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.7      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 480500    |
----------------------------------
Eval num_timesteps=481000, episode_reward=-4607.94 +/- 690.22
Episode length: 56.18 +/- 23.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 56.2      |
|    mean_reward     | -4.61e+03 |
| time/              |           |
|    total_timesteps | 481000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.3      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 235       |
|    time_elapsed    | 2497      |
|    total_timesteps | 481280    |
----------------------------------
Eval num_timesteps=481500, episode_reward=-4233.95 +/- 841.81
Episode length: 47.58 +/- 14.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.6         |
|    mean_reward          | -4.23e+03    |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 0.0033129528 |
|    clip_fraction        | 0.0357       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.15e+04     |
|    n_updates            | 2350         |
|    policy_gradient_loss | 0.00094      |
|    value_loss           | 8.28e+04     |
------------------------------------------
Eval num_timesteps=482000, episode_reward=-4236.74 +/- 947.45
Episode length: 48.40 +/- 15.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.24e+03 |
| time/              |           |
|    total_timesteps | 482000    |
----------------------------------
Eval num_timesteps=482500, episode_reward=-4310.74 +/- 744.90
Episode length: 49.32 +/- 18.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.3      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 482500    |
----------------------------------
Eval num_timesteps=483000, episode_reward=-4436.74 +/- 633.23
Episode length: 49.68 +/- 18.23
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.44e+03 |
| time/              |           |
|    total_timesteps | 483000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -4.3e+03 |
| time/              |          |
|    fps             | 192      |
|    iterations      | 236      |
|    time_elapsed    | 2506     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=-4345.54 +/- 820.85
Episode length: 48.12 +/- 15.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -4.35e+03   |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.002465275 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.89e+04    |
|    n_updates            | 2360        |
|    policy_gradient_loss | 0.00159     |
|    value_loss           | 7.32e+04    |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=-4331.55 +/- 602.33
Episode length: 48.64 +/- 11.38
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.6      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 484000    |
----------------------------------
Eval num_timesteps=484500, episode_reward=-4457.54 +/- 701.98
Episode length: 51.00 +/- 19.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51        |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 484500    |
----------------------------------
Eval num_timesteps=485000, episode_reward=-4242.73 +/- 778.28
Episode length: 47.98 +/- 19.13
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.24e+03 |
| time/              |           |
|    total_timesteps | 485000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.9      |
|    ep_rew_mean     | -4.32e+03 |
| time/              |           |
|    fps             | 192       |
|    iterations      | 237       |
|    time_elapsed    | 2515      |
|    total_timesteps | 485376    |
----------------------------------
Eval num_timesteps=485500, episode_reward=-4631.55 +/- 608.76
Episode length: 49.58 +/- 16.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.6         |
|    mean_reward          | -4.63e+03    |
| time/                   |              |
|    total_timesteps      | 485500       |
| train/                  |              |
|    approx_kl            | 0.0032839552 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.59e+04     |
|    n_updates            | 2370         |
|    policy_gradient_loss | 0.000464     |
|    value_loss           | 1.12e+05     |
------------------------------------------
Eval num_timesteps=486000, episode_reward=-4428.75 +/- 830.82
Episode length: 51.82 +/- 20.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.8      |
|    mean_reward     | -4.43e+03 |
| time/              |           |
|    total_timesteps | 486000    |
----------------------------------
Eval num_timesteps=486500, episode_reward=-4294.73 +/- 756.78
Episode length: 48.82 +/- 18.60
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.8      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 486500    |
----------------------------------
Eval num_timesteps=487000, episode_reward=-4284.34 +/- 810.87
Episode length: 51.24 +/- 17.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.28e+03 |
| time/              |           |
|    total_timesteps | 487000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.3      |
|    ep_rew_mean     | -4.26e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 238       |
|    time_elapsed    | 2525      |
|    total_timesteps | 487424    |
----------------------------------
Eval num_timesteps=487500, episode_reward=-4409.55 +/- 622.11
Episode length: 51.96 +/- 19.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | -4.41e+03   |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.005031407 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.88e+04    |
|    n_updates            | 2380        |
|    policy_gradient_loss | 0.000613    |
|    value_loss           | 1.1e+05     |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=-4460.74 +/- 783.71
Episode length: 52.92 +/- 20.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.9      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 488000    |
----------------------------------
Eval num_timesteps=488500, episode_reward=-4305.15 +/- 792.48
Episode length: 47.06 +/- 15.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.1      |
|    mean_reward     | -4.31e+03 |
| time/              |           |
|    total_timesteps | 488500    |
----------------------------------
Eval num_timesteps=489000, episode_reward=-4541.55 +/- 720.10
Episode length: 49.96 +/- 17.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.54e+03 |
| time/              |           |
|    total_timesteps | 489000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.9      |
|    ep_rew_mean     | -4.23e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 239       |
|    time_elapsed    | 2534      |
|    total_timesteps | 489472    |
----------------------------------
Eval num_timesteps=489500, episode_reward=-4419.94 +/- 829.19
Episode length: 48.42 +/- 15.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.4         |
|    mean_reward          | -4.42e+03    |
| time/                   |              |
|    total_timesteps      | 489500       |
| train/                  |              |
|    approx_kl            | 0.0019335131 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | 0.934        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.16e+04     |
|    n_updates            | 2390         |
|    policy_gradient_loss | 0.00171      |
|    value_loss           | 9.53e+04     |
------------------------------------------
Eval num_timesteps=490000, episode_reward=-4465.95 +/- 777.54
Episode length: 54.32 +/- 14.94
----------------------------------
| eval/              |           |
|    mean_ep_length  | 54.3      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 490000    |
----------------------------------
Eval num_timesteps=490500, episode_reward=-4253.54 +/- 812.27
Episode length: 42.62 +/- 15.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 42.6      |
|    mean_reward     | -4.25e+03 |
| time/              |           |
|    total_timesteps | 490500    |
----------------------------------
Eval num_timesteps=491000, episode_reward=-4457.95 +/- 672.02
Episode length: 50.30 +/- 16.27
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.3      |
|    mean_reward     | -4.46e+03 |
| time/              |           |
|    total_timesteps | 491000    |
----------------------------------
Eval num_timesteps=491500, episode_reward=-4291.95 +/- 777.99
Episode length: 47.38 +/- 17.62
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.4      |
|    mean_reward     | -4.29e+03 |
| time/              |           |
|    total_timesteps | 491500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 51.5      |
|    ep_rew_mean     | -4.23e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 240       |
|    time_elapsed    | 2545      |
|    total_timesteps | 491520    |
----------------------------------
Eval num_timesteps=492000, episode_reward=-4575.95 +/- 698.54
Episode length: 51.18 +/- 20.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.2         |
|    mean_reward          | -4.58e+03    |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0024296278 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.94e+04     |
|    n_updates            | 2400         |
|    policy_gradient_loss | 0.00131      |
|    value_loss           | 1.2e+05      |
------------------------------------------
Eval num_timesteps=492500, episode_reward=-4408.73 +/- 932.21
Episode length: 50.78 +/- 16.60
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.8      |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 492500    |
----------------------------------
Eval num_timesteps=493000, episode_reward=-4417.55 +/- 636.42
Episode length: 51.34 +/- 15.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.3      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 493000    |
----------------------------------
Eval num_timesteps=493500, episode_reward=-4505.15 +/- 582.01
Episode length: 49.72 +/- 16.47
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.7      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 493500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 50.7      |
|    ep_rew_mean     | -4.17e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 241       |
|    time_elapsed    | 2554      |
|    total_timesteps | 493568    |
----------------------------------
Eval num_timesteps=494000, episode_reward=-4383.15 +/- 660.83
Episode length: 49.64 +/- 16.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -4.38e+03   |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.004736637 |
|    clip_fraction        | 0.0539      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.87e+04    |
|    n_updates            | 2410        |
|    policy_gradient_loss | 0.000764    |
|    value_loss           | 1.65e+05    |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=-4479.95 +/- 662.02
Episode length: 52.88 +/- 17.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.9      |
|    mean_reward     | -4.48e+03 |
| time/              |           |
|    total_timesteps | 494500    |
----------------------------------
Eval num_timesteps=495000, episode_reward=-4474.35 +/- 627.56
Episode length: 51.16 +/- 21.18
----------------------------------
| eval/              |           |
|    mean_ep_length  | 51.2      |
|    mean_reward     | -4.47e+03 |
| time/              |           |
|    total_timesteps | 495000    |
----------------------------------
Eval num_timesteps=495500, episode_reward=-4214.34 +/- 683.49
Episode length: 49.40 +/- 16.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.4      |
|    mean_reward     | -4.21e+03 |
| time/              |           |
|    total_timesteps | 495500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.5      |
|    ep_rew_mean     | -4.14e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 242       |
|    time_elapsed    | 2563      |
|    total_timesteps | 495616    |
----------------------------------
Eval num_timesteps=496000, episode_reward=-4341.54 +/- 656.99
Episode length: 52.78 +/- 15.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.8        |
|    mean_reward          | -4.34e+03   |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.004050104 |
|    clip_fraction        | 0.0421      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0001      |
|    loss                 | 6.49e+04    |
|    n_updates            | 2420        |
|    policy_gradient_loss | 3.98e-05    |
|    value_loss           | 1.18e+05    |
-----------------------------------------
Eval num_timesteps=496500, episode_reward=-4365.15 +/- 728.20
Episode length: 49.24 +/- 14.92
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.2      |
|    mean_reward     | -4.37e+03 |
| time/              |           |
|    total_timesteps | 496500    |
----------------------------------
Eval num_timesteps=497000, episode_reward=-4417.14 +/- 677.82
Episode length: 47.84 +/- 14.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 47.8      |
|    mean_reward     | -4.42e+03 |
| time/              |           |
|    total_timesteps | 497000    |
----------------------------------
Eval num_timesteps=497500, episode_reward=-4339.13 +/- 737.84
Episode length: 47.96 +/- 18.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48        |
|    mean_reward     | -4.34e+03 |
| time/              |           |
|    total_timesteps | 497500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 53.2      |
|    ep_rew_mean     | -4.25e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 243       |
|    time_elapsed    | 2573      |
|    total_timesteps | 497664    |
----------------------------------
Eval num_timesteps=498000, episode_reward=-4431.94 +/- 661.04
Episode length: 46.28 +/- 14.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.3         |
|    mean_reward          | -4.43e+03    |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 0.0019119467 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.18e+04     |
|    n_updates            | 2430         |
|    policy_gradient_loss | 0.00249      |
|    value_loss           | 7.33e+04     |
------------------------------------------
Eval num_timesteps=498500, episode_reward=-4328.33 +/- 696.79
Episode length: 48.40 +/- 14.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 48.4      |
|    mean_reward     | -4.33e+03 |
| time/              |           |
|    total_timesteps | 498500    |
----------------------------------
Eval num_timesteps=499000, episode_reward=-4274.75 +/- 880.93
Episode length: 50.36 +/- 16.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50.4      |
|    mean_reward     | -4.27e+03 |
| time/              |           |
|    total_timesteps | 499000    |
----------------------------------
Eval num_timesteps=499500, episode_reward=-4121.95 +/- 915.33
Episode length: 46.70 +/- 14.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 46.7      |
|    mean_reward     | -4.12e+03 |
| time/              |           |
|    total_timesteps | 499500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.1      |
|    ep_rew_mean     | -4.22e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 244       |
|    time_elapsed    | 2581      |
|    total_timesteps | 499712    |
----------------------------------
Eval num_timesteps=500000, episode_reward=-4504.36 +/- 723.04
Episode length: 52.44 +/- 16.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.4         |
|    mean_reward          | -4.5e+03     |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0007538833 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.139       |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.44e+04     |
|    n_updates            | 2440         |
|    policy_gradient_loss | 0.00138      |
|    value_loss           | 1.47e+05     |
------------------------------------------
Eval num_timesteps=500500, episode_reward=-4507.94 +/- 618.94
Episode length: 49.94 +/- 17.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 49.9      |
|    mean_reward     | -4.51e+03 |
| time/              |           |
|    total_timesteps | 500500    |
----------------------------------
Eval num_timesteps=501000, episode_reward=-4409.94 +/- 779.47
Episode length: 50.02 +/- 18.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 50        |
|    mean_reward     | -4.41e+03 |
| time/              |           |
|    total_timesteps | 501000    |
----------------------------------
Eval num_timesteps=501500, episode_reward=-4445.95 +/- 735.73
Episode length: 52.60 +/- 16.36
----------------------------------
| eval/              |           |
|    mean_ep_length  | 52.6      |
|    mean_reward     | -4.45e+03 |
| time/              |           |
|    total_timesteps | 501500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 52.1      |
|    ep_rew_mean     | -4.14e+03 |
| time/              |           |
|    fps             | 193       |
|    iterations      | 245       |
|    time_elapsed    | 2591      |
|    total_timesteps | 501760    |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-11/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 500000
Frame skip: 4
