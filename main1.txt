/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-249.72 +/- 43.00
Episode length: 47.30 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-242.47 +/- 49.76
Episode length: 43.30 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.3     |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 253      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-248.51 +/- 41.57
Episode length: 47.40 +/- 16.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.4        |
|    mean_reward          | -249        |
| time/                   |             |
|    total_timesteps      | 1500        |
| train/                  |             |
|    approx_kl            | 0.009151204 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -2.13e-05   |
|    learning_rate        | 0.0005      |
|    loss                 | 713         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 1.97e+03    |
-----------------------------------------
Eval num_timesteps=2000, episode_reward=-249.57 +/- 45.45
Episode length: 46.00 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-170.50 +/- 72.12
Episode length: 58.04 +/- 23.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58          |
|    mean_reward          | -171        |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.009113502 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.00151     |
|    learning_rate        | 0.0005      |
|    loss                 | 467         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 926         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=-157.76 +/- 64.89
Episode length: 51.30 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-236.82 +/- 56.38
Episode length: 46.92 +/- 13.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.9        |
|    mean_reward          | -237        |
| time/                   |             |
|    total_timesteps      | 3500        |
| train/                  |             |
|    approx_kl            | 0.015165521 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.00237     |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 2.62e+03    |
-----------------------------------------
Eval num_timesteps=4000, episode_reward=-234.71 +/- 49.88
Episode length: 45.62 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-202.95 +/- 68.42
Episode length: 55.54 +/- 21.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.5        |
|    mean_reward          | -203        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.020801652 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.003       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.65e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0477     |
|    value_loss           | 3.72e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-214.06 +/- 67.62
Episode length: 56.66 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.7     |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-188.08 +/- 80.40
Episode length: 66.16 +/- 67.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.2        |
|    mean_reward          | -188        |
| time/                   |             |
|    total_timesteps      | 5500        |
| train/                  |             |
|    approx_kl            | 0.020778429 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.00437     |
|    learning_rate        | 0.0005      |
|    loss                 | 1.06e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0494     |
|    value_loss           | 2.1e+03     |
-----------------------------------------
Eval num_timesteps=6000, episode_reward=-185.35 +/- 63.79
Episode length: 53.66 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 225      |
|    iterations      | 6        |
|    time_elapsed    | 27       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-160.25 +/- 48.72
Episode length: 63.02 +/- 20.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 63         |
|    mean_reward          | -160       |
| time/                   |            |
|    total_timesteps      | 6500       |
| train/                  |            |
|    approx_kl            | 0.01970297 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -2.02      |
|    explained_variance   | -0.000691  |
|    learning_rate        | 0.0005     |
|    loss                 | 745        |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0285    |
|    value_loss           | 1.33e+03   |
----------------------------------------
Eval num_timesteps=7000, episode_reward=-156.81 +/- 48.84
Episode length: 64.84 +/- 26.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.8     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 7        |
|    time_elapsed    | 32       |
|    total_timesteps | 7168     |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=7500, episode_reward=-36.44 +/- 148.19
Episode length: 305.94 +/- 229.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 306        |
|    mean_reward          | -36.4      |
| time/                   |            |
|    total_timesteps      | 7500       |
| train/                  |            |
|    approx_kl            | 0.03223882 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -2.01      |
|    explained_variance   | 0.0114     |
|    learning_rate        | 0.0005     |
|    loss                 | 644        |
|    n_updates            | 62         |
|    policy_gradient_loss | 0.00969    |
|    value_loss           | 993        |
----------------------------------------
New best mean reward!
Eval num_timesteps=8000, episode_reward=-55.18 +/- 133.08
Episode length: 250.54 +/- 225.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | -55.2    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 8        |
|    time_elapsed    | 51       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-207.81 +/- 75.92
Episode length: 99.30 +/- 143.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.3        |
|    mean_reward          | -208        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.017485598 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.00826     |
|    learning_rate        | 0.0005      |
|    loss                 | 654         |
|    n_updates            | 72          |
|    policy_gradient_loss | -0.0325     |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-225.64 +/- 85.99
Episode length: 106.90 +/- 156.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 9        |
|    time_elapsed    | 59       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-60.10 +/- 120.98
Episode length: 338.72 +/- 211.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 339         |
|    mean_reward          | -60.1       |
| time/                   |             |
|    total_timesteps      | 9500        |
| train/                  |             |
|    approx_kl            | 0.024162285 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.006       |
|    learning_rate        | 0.0005      |
|    loss                 | 655         |
|    n_updates            | 82          |
|    policy_gradient_loss | -0.0477     |
|    value_loss           | 1.73e+03    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=-40.36 +/- 120.63
Episode length: 358.30 +/- 210.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -40.4    |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 10       |
|    time_elapsed    | 83       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-122.87 +/- 74.81
Episode length: 159.36 +/- 185.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | -123        |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.025654335 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.00594     |
|    learning_rate        | 0.0005      |
|    loss                 | 1.58e+03    |
|    n_updates            | 92          |
|    policy_gradient_loss | -0.0488     |
|    value_loss           | 2.31e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-133.31 +/- 83.80
Episode length: 121.32 +/- 154.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 120      |
|    iterations      | 11       |
|    time_elapsed    | 93       |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-149.79 +/- 47.63
Episode length: 67.48 +/- 29.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 67.5       |
|    mean_reward          | -150       |
| time/                   |            |
|    total_timesteps      | 11500      |
| train/                  |            |
|    approx_kl            | 0.03299863 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.97      |
|    explained_variance   | 0.0201     |
|    learning_rate        | 0.0005     |
|    loss                 | 315        |
|    n_updates            | 102        |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 741        |
----------------------------------------
Eval num_timesteps=12000, episode_reward=-148.61 +/- 50.80
Episode length: 69.60 +/- 69.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.6     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 123      |
|    iterations      | 12       |
|    time_elapsed    | 99       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=-144.50 +/- 53.10
Episode length: 62.62 +/- 35.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62.6        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.029062804 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.00507     |
|    learning_rate        | 0.0005      |
|    loss                 | 1.57e+03    |
|    n_updates            | 112         |
|    policy_gradient_loss | -0.0608     |
|    value_loss           | 2.75e+03    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-144.08 +/- 45.87
Episode length: 59.08 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 13       |
|    time_elapsed    | 104      |
|    total_timesteps | 13312    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-135.87 +/- 47.66
Episode length: 76.42 +/- 71.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.4        |
|    mean_reward          | -136        |
| time/                   |             |
|    total_timesteps      | 13500       |
| train/                  |             |
|    approx_kl            | 0.029097136 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.000252    |
|    learning_rate        | 0.0005      |
|    loss                 | 1.09e+03    |
|    n_updates            | 122         |
|    policy_gradient_loss | -0.0574     |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=14000, episode_reward=-145.73 +/- 46.47
Episode length: 85.86 +/- 94.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.9     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 14       |
|    time_elapsed    | 110      |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=14500, episode_reward=-71.87 +/- 92.42
Episode length: 249.58 +/- 210.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | -71.9      |
| time/                   |            |
|    total_timesteps      | 14500      |
| train/                  |            |
|    approx_kl            | 0.05958596 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.91      |
|    explained_variance   | 0.0193     |
|    learning_rate        | 0.0005     |
|    loss                 | 392        |
|    n_updates            | 126        |
|    policy_gradient_loss | -0.00384   |
|    value_loss           | 954        |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-70.56 +/- 79.94
Episode length: 249.74 +/- 190.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | -70.6    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 15       |
|    time_elapsed    | 128      |
|    total_timesteps | 15360    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=15500, episode_reward=-48.33 +/- 98.11
Episode length: 322.30 +/- 209.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | -48.3       |
| time/                   |             |
|    total_timesteps      | 15500       |
| train/                  |             |
|    approx_kl            | 0.024560874 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.00617     |
|    learning_rate        | 0.0005      |
|    loss                 | 382         |
|    n_updates            | 127         |
|    policy_gradient_loss | 0.00531     |
|    value_loss           | 782         |
-----------------------------------------
Eval num_timesteps=16000, episode_reward=-28.97 +/- 107.66
Episode length: 363.30 +/- 192.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 363      |
|    mean_reward     | -29      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 107      |
|    iterations      | 16       |
|    time_elapsed    | 151      |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=16500, episode_reward=-147.53 +/- 37.11
Episode length: 90.38 +/- 65.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.4        |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.070607826 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0202      |
|    learning_rate        | 0.0005      |
|    loss                 | 293         |
|    n_updates            | 129         |
|    policy_gradient_loss | 0.0156      |
|    value_loss           | 915         |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=-144.12 +/- 28.98
Episode length: 76.76 +/- 30.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 17       |
|    time_elapsed    | 158      |
|    total_timesteps | 17408    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=17500, episode_reward=-165.54 +/- 57.24
Episode length: 71.46 +/- 51.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.5        |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 17500       |
| train/                  |             |
|    approx_kl            | 0.027545776 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.0136     |
|    learning_rate        | 0.0005      |
|    loss                 | 424         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.00737     |
|    value_loss           | 699         |
-----------------------------------------
Eval num_timesteps=18000, episode_reward=-168.41 +/- 34.59
Episode length: 91.00 +/- 53.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91       |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 18       |
|    time_elapsed    | 164      |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=18500, episode_reward=-227.94 +/- 62.88
Episode length: 54.54 +/- 21.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.5        |
|    mean_reward          | -228        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.035378486 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.0223      |
|    learning_rate        | 0.0005      |
|    loss                 | 978         |
|    n_updates            | 132         |
|    policy_gradient_loss | 0.0186      |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=-239.01 +/- 49.07
Episode length: 54.52 +/- 23.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 19       |
|    time_elapsed    | 169      |
|    total_timesteps | 19456    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=19500, episode_reward=-248.21 +/- 51.19
Episode length: 55.82 +/- 18.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.8        |
|    mean_reward          | -248        |
| time/                   |             |
|    total_timesteps      | 19500       |
| train/                  |             |
|    approx_kl            | 0.048321974 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | -0.019      |
|    learning_rate        | 0.0005      |
|    loss                 | 975         |
|    n_updates            | 134         |
|    policy_gradient_loss | 0.00638     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=-257.30 +/- 45.48
Episode length: 51.52 +/- 14.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 20       |
|    time_elapsed    | 173      |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=20500, episode_reward=-248.30 +/- 48.77
Episode length: 54.10 +/- 17.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.1        |
|    mean_reward          | -248        |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.031700682 |
|    clip_fraction        | 0.37        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.0257      |
|    learning_rate        | 0.0005      |
|    loss                 | 855         |
|    n_updates            | 139         |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=-247.52 +/- 49.45
Episode length: 52.60 +/- 19.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-251.99 +/- 47.43
Episode length: 50.94 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 21       |
|    time_elapsed    | 179      |
|    total_timesteps | 21504    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=22000, episode_reward=-245.19 +/- 50.72
Episode length: 50.64 +/- 15.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.6        |
|    mean_reward          | -245        |
| time/                   |             |
|    total_timesteps      | 22000       |
| train/                  |             |
|    approx_kl            | 0.042146504 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.0055      |
|    learning_rate        | 0.0005      |
|    loss                 | 847         |
|    n_updates            | 141         |
|    policy_gradient_loss | 0.0148      |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=22500, episode_reward=-246.07 +/- 51.94
Episode length: 47.70 +/- 15.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 22       |
|    time_elapsed    | 184      |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=23000, episode_reward=-147.53 +/- 84.63
Episode length: 45.02 +/- 14.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45          |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.041093502 |
|    clip_fraction        | 0.356       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.0174      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.5e+03     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 2.71e+03    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-168.22 +/- 75.71
Episode length: 46.74 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 23       |
|    time_elapsed    | 188      |
|    total_timesteps | 23552    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=24000, episode_reward=-87.39 +/- 80.29
Episode length: 43.02 +/- 9.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 43         |
|    mean_reward          | -87.4      |
| time/                   |            |
|    total_timesteps      | 24000      |
| train/                  |            |
|    approx_kl            | 0.07935929 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | 0.00796    |
|    learning_rate        | 0.0005     |
|    loss                 | 1.3e+03    |
|    n_updates            | 153        |
|    policy_gradient_loss | 0.0186     |
|    value_loss           | 3e+03      |
----------------------------------------
Eval num_timesteps=24500, episode_reward=-77.00 +/- 73.65
Episode length: 42.92 +/- 10.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.9     |
|    mean_reward     | -77      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.3     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 127      |
|    iterations      | 24       |
|    time_elapsed    | 192      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=25000, episode_reward=-118.73 +/- 65.73
Episode length: 44.30 +/- 9.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.3        |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.056986522 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.0251      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.25e+03    |
|    n_updates            | 155         |
|    policy_gradient_loss | 0.0128      |
|    value_loss           | 2.89e+03    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=-129.40 +/- 78.48
Episode length: 42.38 +/- 10.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 25       |
|    time_elapsed    | 196      |
|    total_timesteps | 25600    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=26000, episode_reward=-141.38 +/- 47.00
Episode length: 130.48 +/- 112.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 130        |
|    mean_reward          | -141       |
| time/                   |            |
|    total_timesteps      | 26000      |
| train/                  |            |
|    approx_kl            | 0.02979633 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.46      |
|    explained_variance   | -0.0104    |
|    learning_rate        | 0.0005     |
|    loss                 | 1.49e+03   |
|    n_updates            | 156        |
|    policy_gradient_loss | 0.013      |
|    value_loss           | 3.29e+03   |
----------------------------------------
Eval num_timesteps=26500, episode_reward=-148.66 +/- 33.20
Episode length: 120.34 +/- 94.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.3     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 26       |
|    time_elapsed    | 205      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=27000, episode_reward=-231.25 +/- 63.40
Episode length: 83.74 +/- 84.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.7        |
|    mean_reward          | -231        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.032370884 |
|    clip_fraction        | 0.411       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.0006      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.05e+03    |
|    n_updates            | 157         |
|    policy_gradient_loss | 0.00785     |
|    value_loss           | 2.21e+03    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-204.80 +/- 86.55
Episode length: 121.00 +/- 134.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.2     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 27       |
|    time_elapsed    | 213      |
|    total_timesteps | 27648    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=28000, episode_reward=-158.17 +/- 53.18
Episode length: 139.38 +/- 97.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 139         |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 28000       |
| train/                  |             |
|    approx_kl            | 0.027555734 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.0181      |
|    learning_rate        | 0.0005      |
|    loss                 | 782         |
|    n_updates            | 158         |
|    policy_gradient_loss | 0.00629     |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=28500, episode_reward=-140.82 +/- 68.13
Episode length: 115.70 +/- 111.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.6     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 28       |
|    time_elapsed    | 222      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=29000, episode_reward=-107.72 +/- 119.48
Episode length: 299.82 +/- 178.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | -108       |
| time/                   |            |
|    total_timesteps      | 29000      |
| train/                  |            |
|    approx_kl            | 0.04032733 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | 0.0118     |
|    learning_rate        | 0.0005     |
|    loss                 | 760        |
|    n_updates            | 159        |
|    policy_gradient_loss | 0.0346     |
|    value_loss           | 1.31e+03   |
----------------------------------------
Eval num_timesteps=29500, episode_reward=-137.12 +/- 104.20
Episode length: 284.70 +/- 155.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.3     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 29       |
|    time_elapsed    | 242      |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-166.22 +/- 64.58
Episode length: 212.74 +/- 131.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 213       |
|    mean_reward          | -166      |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0369024 |
|    clip_fraction        | 0.325     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.58     |
|    explained_variance   | 0.0477    |
|    learning_rate        | 0.0005    |
|    loss                 | 510       |
|    n_updates            | 169       |
|    policy_gradient_loss | -0.027    |
|    value_loss           | 1.12e+03  |
---------------------------------------
Eval num_timesteps=30500, episode_reward=-160.52 +/- 75.07
Episode length: 222.98 +/- 137.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.1     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 30       |
|    time_elapsed    | 257      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=31000, episode_reward=8.63 +/- 114.39
Episode length: 459.30 +/- 139.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 459         |
|    mean_reward          | 8.63        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.060920052 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.0136      |
|    learning_rate        | 0.0005      |
|    loss                 | 336         |
|    n_updates            | 173         |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 609         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=31500, episode_reward=-28.27 +/- 133.14
Episode length: 392.54 +/- 179.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | -28.3    |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 31       |
|    time_elapsed    | 286      |
|    total_timesteps | 31744    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=32000, episode_reward=-78.97 +/- 129.07
Episode length: 363.72 +/- 172.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 364        |
|    mean_reward          | -79        |
| time/                   |            |
|    total_timesteps      | 32000      |
| train/                  |            |
|    approx_kl            | 0.05766186 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | 0.00883    |
|    learning_rate        | 0.0005     |
|    loss                 | 347        |
|    n_updates            | 178        |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 972        |
----------------------------------------
Eval num_timesteps=32500, episode_reward=-84.09 +/- 132.47
Episode length: 362.88 +/- 170.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 363      |
|    mean_reward     | -84.1    |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 32       |
|    time_elapsed    | 311      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=33000, episode_reward=-10.18 +/- 125.57
Episode length: 437.14 +/- 160.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 437        |
|    mean_reward          | -10.2      |
| time/                   |            |
|    total_timesteps      | 33000      |
| train/                  |            |
|    approx_kl            | 0.06818218 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.53      |
|    explained_variance   | 0.0185     |
|    learning_rate        | 0.0005     |
|    loss                 | 480        |
|    n_updates            | 181        |
|    policy_gradient_loss | 0.00465    |
|    value_loss           | 695        |
----------------------------------------
Eval num_timesteps=33500, episode_reward=-18.98 +/- 132.95
Episode length: 398.76 +/- 183.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 399      |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 33       |
|    time_elapsed    | 339      |
|    total_timesteps | 33792    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
Eval num_timesteps=34000, episode_reward=-116.70 +/- 113.94
Episode length: 325.24 +/- 159.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 325        |
|    mean_reward          | -117       |
| time/                   |            |
|    total_timesteps      | 34000      |
| train/                  |            |
|    approx_kl            | 0.03854457 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.47      |
|    explained_variance   | 0.0177     |
|    learning_rate        | 0.0005     |
|    loss                 | 420        |
|    n_updates            | 189        |
|    policy_gradient_loss | -0.0362    |
|    value_loss           | 755        |
----------------------------------------
Eval num_timesteps=34500, episode_reward=-139.86 +/- 94.41
Episode length: 253.96 +/- 150.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 34       |
|    time_elapsed    | 359      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.07
Eval num_timesteps=35000, episode_reward=-179.52 +/- 60.99
Episode length: 211.20 +/- 113.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | -180        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.052068308 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.0127      |
|    learning_rate        | 0.0005      |
|    loss                 | 708         |
|    n_updates            | 197         |
|    policy_gradient_loss | -0.0388     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-173.49 +/- 61.44
Episode length: 216.14 +/- 97.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 35       |
|    time_elapsed    | 374      |
|    total_timesteps | 35840    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=36000, episode_reward=-140.51 +/- 92.04
Episode length: 242.42 +/- 125.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 242         |
|    mean_reward          | -141        |
| time/                   |             |
|    total_timesteps      | 36000       |
| train/                  |             |
|    approx_kl            | 0.052885562 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.054       |
|    learning_rate        | 0.0005      |
|    loss                 | 267         |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.00285     |
|    value_loss           | 572         |
-----------------------------------------
Eval num_timesteps=36500, episode_reward=-142.32 +/- 76.15
Episode length: 263.78 +/- 137.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 94       |
|    iterations      | 36       |
|    time_elapsed    | 392      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-178.39 +/- 31.60
Episode length: 184.10 +/- 72.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | -178       |
| time/                   |            |
|    total_timesteps      | 37000      |
| train/                  |            |
|    approx_kl            | 0.03265862 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | 0.0075     |
|    learning_rate        | 0.0005     |
|    loss                 | 703        |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.04      |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=37500, episode_reward=-171.75 +/- 48.25
Episode length: 187.20 +/- 77.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 37       |
|    time_elapsed    | 405      |
|    total_timesteps | 37888    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=38000, episode_reward=-173.76 +/- 72.17
Episode length: 220.00 +/- 101.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | -174        |
| time/                   |             |
|    total_timesteps      | 38000       |
| train/                  |             |
|    approx_kl            | 0.051952556 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.0381      |
|    learning_rate        | 0.0005      |
|    loss                 | 728         |
|    n_updates            | 212         |
|    policy_gradient_loss | 0.00339     |
|    value_loss           | 1.48e+03    |
-----------------------------------------
Eval num_timesteps=38500, episode_reward=-181.17 +/- 36.38
Episode length: 192.62 +/- 66.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 38       |
|    time_elapsed    | 419      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=39000, episode_reward=-186.82 +/- 22.77
Episode length: 211.92 +/- 84.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 212         |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.061943356 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.0256      |
|    learning_rate        | 0.0005      |
|    loss                 | 642         |
|    n_updates            | 214         |
|    policy_gradient_loss | 0.0171      |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=-173.64 +/- 41.72
Episode length: 211.06 +/- 93.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 39       |
|    time_elapsed    | 434      |
|    total_timesteps | 39936    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=40000, episode_reward=-172.60 +/- 21.70
Episode length: 212.06 +/- 85.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 212         |
|    mean_reward          | -173        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.042192236 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.0434      |
|    learning_rate        | 0.0005      |
|    loss                 | 507         |
|    n_updates            | 218         |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=40500, episode_reward=-174.88 +/- 43.84
Episode length: 216.70 +/- 102.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 40       |
|    time_elapsed    | 449      |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=41000, episode_reward=-179.04 +/- 24.10
Episode length: 199.18 +/- 67.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.031830184 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.0207      |
|    learning_rate        | 0.0005      |
|    loss                 | 580         |
|    n_updates            | 222         |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 1.12e+03    |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=-175.76 +/- 25.37
Episode length: 207.50 +/- 78.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 41       |
|    time_elapsed    | 464      |
|    total_timesteps | 41984    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=42000, episode_reward=-177.55 +/- 21.09
Episode length: 187.52 +/- 63.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 42000       |
| train/                  |             |
|    approx_kl            | 0.024949918 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.0753      |
|    learning_rate        | 0.0005      |
|    loss                 | 498         |
|    n_updates            | 223         |
|    policy_gradient_loss | 0.0139      |
|    value_loss           | 969         |
-----------------------------------------
Eval num_timesteps=42500, episode_reward=-177.43 +/- 23.86
Episode length: 186.80 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-173.79 +/- 25.55
Episode length: 186.82 +/- 50.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 42       |
|    time_elapsed    | 483      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=43500, episode_reward=-180.13 +/- 20.40
Episode length: 199.50 +/- 75.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 200         |
|    mean_reward          | -180        |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.032143764 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0.0604      |
|    learning_rate        | 0.0005      |
|    loss                 | 424         |
|    n_updates            | 226         |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 901         |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-174.88 +/- 22.27
Episode length: 193.08 +/- 71.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 43       |
|    time_elapsed    | 497      |
|    total_timesteps | 44032    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=44500, episode_reward=-175.60 +/- 25.22
Episode length: 223.12 +/- 93.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | -176        |
| time/                   |             |
|    total_timesteps      | 44500       |
| train/                  |             |
|    approx_kl            | 0.027331298 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.0562      |
|    learning_rate        | 0.0005      |
|    loss                 | 470         |
|    n_updates            | 227         |
|    policy_gradient_loss | 0.00412     |
|    value_loss           | 789         |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=-182.38 +/- 23.59
Episode length: 198.04 +/- 68.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 44       |
|    time_elapsed    | 512      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=45500, episode_reward=-181.88 +/- 26.34
Episode length: 159.78 +/- 70.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.039398182 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.0791      |
|    learning_rate        | 0.0005      |
|    loss                 | 577         |
|    n_updates            | 229         |
|    policy_gradient_loss | 0.0143      |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=-176.40 +/- 27.75
Episode length: 168.24 +/- 62.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 45       |
|    time_elapsed    | 524      |
|    total_timesteps | 46080    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=46500, episode_reward=-179.84 +/- 23.42
Episode length: 212.64 +/- 69.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | -180        |
| time/                   |             |
|    total_timesteps      | 46500       |
| train/                  |             |
|    approx_kl            | 0.048575394 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.0396      |
|    learning_rate        | 0.0005      |
|    loss                 | 672         |
|    n_updates            | 231         |
|    policy_gradient_loss | 0.0104      |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=47000, episode_reward=-182.79 +/- 22.88
Episode length: 190.30 +/- 62.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 46       |
|    time_elapsed    | 538      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=47500, episode_reward=-179.20 +/- 24.31
Episode length: 196.30 +/- 76.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.031158762 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.0947      |
|    learning_rate        | 0.0005      |
|    loss                 | 904         |
|    n_updates            | 232         |
|    policy_gradient_loss | 0.0199      |
|    value_loss           | 1.82e+03    |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=-179.79 +/- 22.78
Episode length: 208.40 +/- 73.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 47       |
|    time_elapsed    | 553      |
|    total_timesteps | 48128    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=48500, episode_reward=-184.41 +/- 30.77
Episode length: 150.46 +/- 69.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | -184        |
| time/                   |             |
|    total_timesteps      | 48500       |
| train/                  |             |
|    approx_kl            | 0.025361177 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.0952      |
|    learning_rate        | 0.0005      |
|    loss                 | 961         |
|    n_updates            | 233         |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=49000, episode_reward=-178.21 +/- 51.03
Episode length: 166.38 +/- 69.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 48       |
|    time_elapsed    | 564      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=49500, episode_reward=-178.02 +/- 61.97
Episode length: 131.52 +/- 49.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.036168408 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.127       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.09e+03    |
|    n_updates            | 234         |
|    policy_gradient_loss | 0.02        |
|    value_loss           | 2.3e+03     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-178.41 +/- 62.16
Episode length: 142.52 +/- 65.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 143      |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 49       |
|    time_elapsed    | 574      |
|    total_timesteps | 50176    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=50500, episode_reward=-176.96 +/- 52.85
Episode length: 141.64 +/- 70.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 142        |
|    mean_reward          | -177       |
| time/                   |            |
|    total_timesteps      | 50500      |
| train/                  |            |
|    approx_kl            | 0.03805879 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.0773     |
|    learning_rate        | 0.0005     |
|    loss                 | 1.15e+03   |
|    n_updates            | 236        |
|    policy_gradient_loss | 0.00888    |
|    value_loss           | 2.64e+03   |
----------------------------------------
Eval num_timesteps=51000, episode_reward=-154.58 +/- 99.68
Episode length: 145.02 +/- 91.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 50       |
|    time_elapsed    | 585      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=51500, episode_reward=-200.40 +/- 26.69
Episode length: 182.74 +/- 75.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | -200        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.046239447 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.0457      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.35e+03    |
|    n_updates            | 238         |
|    policy_gradient_loss | 0.00904     |
|    value_loss           | 2.16e+03    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-199.17 +/- 22.63
Episode length: 201.50 +/- 86.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 51       |
|    time_elapsed    | 599      |
|    total_timesteps | 52224    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=52500, episode_reward=-118.01 +/- 96.98
Episode length: 174.96 +/- 129.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | -118        |
| time/                   |             |
|    total_timesteps      | 52500       |
| train/                  |             |
|    approx_kl            | 0.048810046 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0005      |
|    loss                 | 617         |
|    n_updates            | 242         |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=53000, episode_reward=-143.39 +/- 85.16
Episode length: 133.72 +/- 88.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 52       |
|    time_elapsed    | 610      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=53500, episode_reward=-50.51 +/- 159.82
Episode length: 336.40 +/- 166.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 336         |
|    mean_reward          | -50.5       |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.028063368 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.0675      |
|    learning_rate        | 0.0005      |
|    loss                 | 571         |
|    n_updates            | 243         |
|    policy_gradient_loss | 0.029       |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=-114.97 +/- 131.08
Episode length: 281.58 +/- 149.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 53       |
|    time_elapsed    | 631      |
|    total_timesteps | 54272    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=54500, episode_reward=-159.89 +/- 88.50
Episode length: 218.30 +/- 112.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | -160        |
| time/                   |             |
|    total_timesteps      | 54500       |
| train/                  |             |
|    approx_kl            | 0.030304044 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.0494      |
|    learning_rate        | 0.0005      |
|    loss                 | 279         |
|    n_updates            | 244         |
|    policy_gradient_loss | 0.00907     |
|    value_loss           | 792         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-163.52 +/- 78.37
Episode length: 236.56 +/- 101.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 54       |
|    time_elapsed    | 647      |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=55500, episode_reward=-199.02 +/- 20.78
Episode length: 204.86 +/- 70.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | -199       |
| time/                   |            |
|    total_timesteps      | 55500      |
| train/                  |            |
|    approx_kl            | 0.06761276 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.39      |
|    explained_variance   | 0.0687     |
|    learning_rate        | 0.0005     |
|    loss                 | 560        |
|    n_updates            | 247        |
|    policy_gradient_loss | -0.00428   |
|    value_loss           | 942        |
----------------------------------------
Eval num_timesteps=56000, episode_reward=-185.73 +/- 78.19
Episode length: 231.62 +/- 91.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 55       |
|    time_elapsed    | 663      |
|    total_timesteps | 56320    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=56500, episode_reward=-102.00 +/- 136.48
Episode length: 301.14 +/- 160.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 301        |
|    mean_reward          | -102       |
| time/                   |            |
|    total_timesteps      | 56500      |
| train/                  |            |
|    approx_kl            | 0.07433166 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.0607     |
|    learning_rate        | 0.0005     |
|    loss                 | 675        |
|    n_updates            | 249        |
|    policy_gradient_loss | 0.00293    |
|    value_loss           | 1.47e+03   |
----------------------------------------
Eval num_timesteps=57000, episode_reward=-52.88 +/- 171.30
Episode length: 323.16 +/- 176.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | -52.9    |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 56       |
|    time_elapsed    | 684      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=57500, episode_reward=135.63 +/- 98.35
Episode length: 476.74 +/- 117.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 477        |
|    mean_reward          | 136        |
| time/                   |            |
|    total_timesteps      | 57500      |
| train/                  |            |
|    approx_kl            | 0.05033713 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.0152     |
|    learning_rate        | 0.0005     |
|    loss                 | 855        |
|    n_updates            | 251        |
|    policy_gradient_loss | 0.0047     |
|    value_loss           | 1.3e+03    |
----------------------------------------
New best mean reward!
Eval num_timesteps=58000, episode_reward=133.30 +/- 105.56
Episode length: 456.20 +/- 148.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 456      |
|    mean_reward     | 133      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 57       |
|    time_elapsed    | 716      |
|    total_timesteps | 58368    |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.07
Eval num_timesteps=58500, episode_reward=87.41 +/- 130.63
Episode length: 457.66 +/- 138.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 458        |
|    mean_reward          | 87.4       |
| time/                   |            |
|    total_timesteps      | 58500      |
| train/                  |            |
|    approx_kl            | 0.06568621 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | 0.01       |
|    learning_rate        | 0.0005     |
|    loss                 | 102        |
|    n_updates            | 258        |
|    policy_gradient_loss | -0.0284    |
|    value_loss           | 373        |
----------------------------------------
Eval num_timesteps=59000, episode_reward=78.75 +/- 138.42
Episode length: 440.22 +/- 153.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 78.7     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | -118     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 58       |
|    time_elapsed    | 746      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=59500, episode_reward=168.99 +/- 84.85
Episode length: 503.12 +/- 87.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 503         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.026978305 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.000189    |
|    learning_rate        | 0.0005      |
|    loss                 | 131         |
|    n_updates            | 259         |
|    policy_gradient_loss | 0.00758     |
|    value_loss           | 803         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=60000, episode_reward=126.02 +/- 123.92
Episode length: 476.76 +/- 120.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | 126      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 59       |
|    time_elapsed    | 779      |
|    total_timesteps | 60416    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=60500, episode_reward=191.59 +/- 16.61
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 60500       |
| train/                  |             |
|    approx_kl            | 0.045728903 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.0303      |
|    learning_rate        | 0.0005      |
|    loss                 | 1e+03       |
|    n_updates            | 262         |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 1.18e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=61000, episode_reward=189.78 +/- 21.23
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 60       |
|    time_elapsed    | 815      |
|    total_timesteps | 61440    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=61500, episode_reward=104.70 +/- 120.24
Episode length: 475.68 +/- 123.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 105         |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.058850206 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.0457      |
|    learning_rate        | 0.0005      |
|    loss                 | 340         |
|    n_updates            | 264         |
|    policy_gradient_loss | 0.00897     |
|    value_loss           | 754         |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=141.71 +/- 91.99
Episode length: 503.68 +/- 85.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | 142      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 61       |
|    time_elapsed    | 849      |
|    total_timesteps | 62464    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
Eval num_timesteps=62500, episode_reward=119.31 +/- 70.44
Episode length: 517.26 +/- 54.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 517        |
|    mean_reward          | 119        |
| time/                   |            |
|    total_timesteps      | 62500      |
| train/                  |            |
|    approx_kl            | 0.03653554 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.0645     |
|    learning_rate        | 0.0005     |
|    loss                 | 139        |
|    n_updates            | 272        |
|    policy_gradient_loss | -0.0242    |
|    value_loss           | 323        |
----------------------------------------
Eval num_timesteps=63000, episode_reward=107.04 +/- 96.69
Episode length: 504.78 +/- 82.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | -96.1    |
| time/              |          |
|    fps             | 71       |
|    iterations      | 62       |
|    time_elapsed    | 883      |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=63500, episode_reward=137.18 +/- 73.34
Episode length: 515.98 +/- 50.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 137         |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.029507618 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.0441      |
|    learning_rate        | 0.0005      |
|    loss                 | 297         |
|    n_updates            | 273         |
|    policy_gradient_loss | 0.0042      |
|    value_loss           | 634         |
-----------------------------------------
Eval num_timesteps=64000, episode_reward=142.74 +/- 65.80
Episode length: 517.54 +/- 52.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=159.91 +/- 40.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | -89.6    |
| time/              |          |
|    fps             | 68       |
|    iterations      | 63       |
|    time_elapsed    | 935      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=178.68 +/- 52.94
Episode length: 515.50 +/- 66.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 65000       |
| train/                  |             |
|    approx_kl            | 0.032741204 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.0204      |
|    learning_rate        | 0.0005      |
|    loss                 | 258         |
|    n_updates            | 283         |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 326         |
-----------------------------------------
Eval num_timesteps=65500, episode_reward=175.69 +/- 67.04
Episode length: 516.06 +/- 62.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | -80.6    |
| time/              |          |
|    fps             | 67       |
|    iterations      | 64       |
|    time_elapsed    | 970      |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.07
Eval num_timesteps=66000, episode_reward=196.51 +/- 14.15
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.046135176 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.0163      |
|    learning_rate        | 0.0005      |
|    loss                 | 335         |
|    n_updates            | 291         |
|    policy_gradient_loss | -0.028      |
|    value_loss           | 748         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=66500, episode_reward=181.91 +/- 59.25
Episode length: 516.62 +/- 58.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | -77.9    |
| time/              |          |
|    fps             | 66       |
|    iterations      | 65       |
|    time_elapsed    | 1005     |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=196.56 +/- 12.82
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 67000       |
| train/                  |             |
|    approx_kl            | 0.031245107 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.0114      |
|    learning_rate        | 0.0005      |
|    loss                 | 312         |
|    n_updates            | 301         |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 450         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=67500, episode_reward=198.63 +/- 1.68
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | -71.5    |
| time/              |          |
|    fps             | 64       |
|    iterations      | 66       |
|    time_elapsed    | 1041     |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=68000, episode_reward=161.92 +/- 73.30
Episode length: 517.02 +/- 55.86
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 517      |
|    mean_reward          | 162      |
| time/                   |          |
|    total_timesteps      | 68000    |
| train/                  |          |
|    approx_kl            | 0.065855 |
|    clip_fraction        | 0.272    |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.56    |
|    explained_variance   | 0.0295   |
|    learning_rate        | 0.0005   |
|    loss                 | 214      |
|    n_updates            | 303      |
|    policy_gradient_loss | 0.00786  |
|    value_loss           | 523      |
--------------------------------------
Eval num_timesteps=68500, episode_reward=183.30 +/- 42.34
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | -66.9    |
| time/              |          |
|    fps             | 63       |
|    iterations      | 67       |
|    time_elapsed    | 1076     |
|    total_timesteps | 68608    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=69000, episode_reward=149.76 +/- 63.97
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 69000       |
| train/                  |             |
|    approx_kl            | 0.059667308 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.0291      |
|    learning_rate        | 0.0005      |
|    loss                 | 504         |
|    n_updates            | 306         |
|    policy_gradient_loss | 0.00559     |
|    value_loss           | 1.43e+03    |
-----------------------------------------
Eval num_timesteps=69500, episode_reward=150.06 +/- 70.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | -62.7    |
| time/              |          |
|    fps             | 62       |
|    iterations      | 68       |
|    time_elapsed    | 1111     |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=70000, episode_reward=200.38 +/- 5.11
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 200        |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.05906081 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.0715     |
|    learning_rate        | 0.0005     |
|    loss                 | 181        |
|    n_updates            | 308        |
|    policy_gradient_loss | 0.000962   |
|    value_loss           | 477        |
----------------------------------------
New best mean reward!
Eval num_timesteps=70500, episode_reward=199.81 +/- 3.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | -60.4    |
| time/              |          |
|    fps             | 61       |
|    iterations      | 69       |
|    time_elapsed    | 1147     |
|    total_timesteps | 70656    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=71000, episode_reward=194.76 +/- 14.11
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 71000       |
| train/                  |             |
|    approx_kl            | 0.044104505 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.035       |
|    learning_rate        | 0.0005      |
|    loss                 | 235         |
|    n_updates            | 310         |
|    policy_gradient_loss | 0.00057     |
|    value_loss           | 981         |
-----------------------------------------
Eval num_timesteps=71500, episode_reward=195.95 +/- 10.76
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | -54.4    |
| time/              |          |
|    fps             | 60       |
|    iterations      | 70       |
|    time_elapsed    | 1182     |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=72000, episode_reward=203.53 +/- 13.02
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 204        |
| time/                   |            |
|    total_timesteps      | 72000      |
| train/                  |            |
|    approx_kl            | 0.05255543 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.0747     |
|    learning_rate        | 0.0005     |
|    loss                 | 342        |
|    n_updates            | 312        |
|    policy_gradient_loss | 0.0106     |
|    value_loss           | 489        |
----------------------------------------
New best mean reward!
Eval num_timesteps=72500, episode_reward=202.38 +/- 9.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | -47.7    |
| time/              |          |
|    fps             | 59       |
|    iterations      | 71       |
|    time_elapsed    | 1217     |
|    total_timesteps | 72704    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=73000, episode_reward=160.50 +/- 70.81
Episode length: 489.14 +/- 104.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 489         |
|    mean_reward          | 161         |
| time/                   |             |
|    total_timesteps      | 73000       |
| train/                  |             |
|    approx_kl            | 0.074367106 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.0892      |
|    learning_rate        | 0.0005      |
|    loss                 | 265         |
|    n_updates            | 315         |
|    policy_gradient_loss | 0.00188     |
|    value_loss           | 732         |
-----------------------------------------
Eval num_timesteps=73500, episode_reward=146.80 +/- 77.83
Episode length: 428.86 +/- 182.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 147      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 252      |
|    ep_rew_mean     | -43.3    |
| time/              |          |
|    fps             | 59       |
|    iterations      | 72       |
|    time_elapsed    | 1248     |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=74000, episode_reward=191.87 +/- 26.53
Episode length: 521.34 +/- 25.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 521        |
|    mean_reward          | 192        |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.06519742 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.0968     |
|    learning_rate        | 0.0005     |
|    loss                 | 531        |
|    n_updates            | 317        |
|    policy_gradient_loss | 0.0204     |
|    value_loss           | 1.14e+03   |
----------------------------------------
Eval num_timesteps=74500, episode_reward=196.68 +/- 15.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 258      |
|    ep_rew_mean     | -35      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 73       |
|    time_elapsed    | 1284     |
|    total_timesteps | 74752    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=75000, episode_reward=189.52 +/- 30.46
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.024840523 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.0729      |
|    learning_rate        | 0.0005      |
|    loss                 | 533         |
|    n_updates            | 318         |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 1.32e+03    |
-----------------------------------------
Eval num_timesteps=75500, episode_reward=189.43 +/- 29.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 266      |
|    ep_rew_mean     | -30.8    |
| time/              |          |
|    fps             | 57       |
|    iterations      | 74       |
|    time_elapsed    | 1320     |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=76000, episode_reward=157.75 +/- 56.18
Episode length: 517.50 +/- 52.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.018134667 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.195       |
|    learning_rate        | 0.0005      |
|    loss                 | 212         |
|    n_updates            | 319         |
|    policy_gradient_loss | 0.0127      |
|    value_loss           | 414         |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=162.42 +/- 52.87
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | -29      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 75       |
|    time_elapsed    | 1355     |
|    total_timesteps | 76800    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=77000, episode_reward=146.89 +/- 70.65
Episode length: 478.80 +/- 128.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 479        |
|    mean_reward          | 147        |
| time/                   |            |
|    total_timesteps      | 77000      |
| train/                  |            |
|    approx_kl            | 0.03140553 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.00224    |
|    learning_rate        | 0.0005     |
|    loss                 | 1.41e+03   |
|    n_updates            | 320        |
|    policy_gradient_loss | 0.00356    |
|    value_loss           | 2.25e+03   |
----------------------------------------
Eval num_timesteps=77500, episode_reward=157.06 +/- 59.67
Episode length: 484.96 +/- 114.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 283      |
|    ep_rew_mean     | -20.8    |
| time/              |          |
|    fps             | 56       |
|    iterations      | 76       |
|    time_elapsed    | 1388     |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=78000, episode_reward=175.24 +/- 59.13
Episode length: 512.88 +/- 62.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.029153785 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.0522      |
|    learning_rate        | 0.0005      |
|    loss                 | 508         |
|    n_updates            | 321         |
|    policy_gradient_loss | 0.00966     |
|    value_loss           | 1.38e+03    |
-----------------------------------------
Eval num_timesteps=78500, episode_reward=160.38 +/- 67.50
Episode length: 496.22 +/- 99.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 77       |
|    time_elapsed    | 1421     |
|    total_timesteps | 78848    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=79000, episode_reward=203.69 +/- 7.65
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 204        |
| time/                   |            |
|    total_timesteps      | 79000      |
| train/                  |            |
|    approx_kl            | 0.02935505 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.118      |
|    learning_rate        | 0.0005     |
|    loss                 | 646        |
|    n_updates            | 322        |
|    policy_gradient_loss | 0.0144     |
|    value_loss           | 1.65e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=79500, episode_reward=201.43 +/- 14.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | -16.8    |
| time/              |          |
|    fps             | 54       |
|    iterations      | 78       |
|    time_elapsed    | 1457     |
|    total_timesteps | 79872    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=80000, episode_reward=200.47 +/- 18.04
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.023317803 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.963      |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.0005      |
|    loss                 | 324         |
|    n_updates            | 323         |
|    policy_gradient_loss | 0.00649     |
|    value_loss           | 998         |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=197.33 +/- 23.67
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | -7.98    |
| time/              |          |
|    fps             | 54       |
|    iterations      | 79       |
|    time_elapsed    | 1493     |
|    total_timesteps | 80896    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=81000, episode_reward=195.70 +/- 48.39
Episode length: 519.70 +/- 37.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 520         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 81000       |
| train/                  |             |
|    approx_kl            | 0.038344137 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.222       |
|    learning_rate        | 0.0005      |
|    loss                 | 102         |
|    n_updates            | 325         |
|    policy_gradient_loss | 0.00143     |
|    value_loss           | 406         |
-----------------------------------------
Eval num_timesteps=81500, episode_reward=197.67 +/- 33.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | -2.93    |
| time/              |          |
|    fps             | 53       |
|    iterations      | 80       |
|    time_elapsed    | 1528     |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=82000, episode_reward=195.79 +/- 40.17
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 196        |
| time/                   |            |
|    total_timesteps      | 82000      |
| train/                  |            |
|    approx_kl            | 0.05952968 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.986     |
|    explained_variance   | 0.162      |
|    learning_rate        | 0.0005     |
|    loss                 | 256        |
|    n_updates            | 327        |
|    policy_gradient_loss | 0.0106     |
|    value_loss           | 749        |
----------------------------------------
Eval num_timesteps=82500, episode_reward=207.24 +/- 12.01
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 322      |
|    ep_rew_mean     | 1.55     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 81       |
|    time_elapsed    | 1564     |
|    total_timesteps | 82944    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=83000, episode_reward=190.78 +/- 27.68
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 83000       |
| train/                  |             |
|    approx_kl            | 0.026160816 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.0005      |
|    loss                 | 227         |
|    n_updates            | 328         |
|    policy_gradient_loss | 0.0183      |
|    value_loss           | 557         |
-----------------------------------------
Eval num_timesteps=83500, episode_reward=195.17 +/- 24.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 331      |
|    ep_rew_mean     | 8.2      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 82       |
|    time_elapsed    | 1599     |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.23
Eval num_timesteps=84000, episode_reward=184.36 +/- 29.66
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 184        |
| time/                   |            |
|    total_timesteps      | 84000      |
| train/                  |            |
|    approx_kl            | 0.13912298 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.663     |
|    explained_variance   | 0.178      |
|    learning_rate        | 0.0005     |
|    loss                 | 204        |
|    n_updates            | 330        |
|    policy_gradient_loss | 0.0102     |
|    value_loss           | 750        |
----------------------------------------
Eval num_timesteps=84500, episode_reward=180.43 +/- 34.36
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 51       |
|    iterations      | 83       |
|    time_elapsed    | 1634     |
|    total_timesteps | 84992    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=85000, episode_reward=193.94 +/- 21.25
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 85000       |
| train/                  |             |
|    approx_kl            | 0.030795418 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.0005      |
|    loss                 | 124         |
|    n_updates            | 331         |
|    policy_gradient_loss | 0.0138      |
|    value_loss           | 194         |
-----------------------------------------
Eval num_timesteps=85500, episode_reward=188.11 +/- 31.23
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=196.00 +/- 18.51
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 344      |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 84       |
|    time_elapsed    | 1687     |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=86500, episode_reward=-80.76 +/- 146.70
Episode length: 285.40 +/- 178.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 285        |
|    mean_reward          | -80.8      |
| time/                   |            |
|    total_timesteps      | 86500      |
| train/                  |            |
|    approx_kl            | 0.04867041 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.742     |
|    explained_variance   | 0.0167     |
|    learning_rate        | 0.0005     |
|    loss                 | 289        |
|    n_updates            | 332        |
|    policy_gradient_loss | 0.0196     |
|    value_loss           | 359        |
----------------------------------------
Eval num_timesteps=87000, episode_reward=-52.18 +/- 151.27
Episode length: 330.36 +/- 181.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | -52.2    |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 353      |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 85       |
|    time_elapsed    | 1708     |
|    total_timesteps | 87040    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=87500, episode_reward=179.64 +/- 34.82
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 87500       |
| train/                  |             |
|    approx_kl            | 0.030414667 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.0824      |
|    learning_rate        | 0.0005      |
|    loss                 | 371         |
|    n_updates            | 335         |
|    policy_gradient_loss | 0.000974    |
|    value_loss           | 640         |
-----------------------------------------
Eval num_timesteps=88000, episode_reward=167.48 +/- 80.08
Episode length: 511.28 +/- 67.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 511      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 356      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 86       |
|    time_elapsed    | 1744     |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=88500, episode_reward=192.84 +/- 17.17
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 193        |
| time/                   |            |
|    total_timesteps      | 88500      |
| train/                  |            |
|    approx_kl            | 0.03118813 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.866     |
|    explained_variance   | 0.201      |
|    learning_rate        | 0.0005     |
|    loss                 | 136        |
|    n_updates            | 336        |
|    policy_gradient_loss | 0.0117     |
|    value_loss           | 844        |
----------------------------------------
Eval num_timesteps=89000, episode_reward=195.12 +/- 13.75
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 50       |
|    iterations      | 87       |
|    time_elapsed    | 1779     |
|    total_timesteps | 89088    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=89500, episode_reward=197.38 +/- 7.42
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 89500       |
| train/                  |             |
|    approx_kl            | 0.040471118 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.0005      |
|    loss                 | 225         |
|    n_updates            | 337         |
|    policy_gradient_loss | 0.0202      |
|    value_loss           | 456         |
-----------------------------------------
Eval num_timesteps=90000, episode_reward=197.52 +/- 7.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 372      |
|    ep_rew_mean     | 29.3     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 88       |
|    time_elapsed    | 1814     |
|    total_timesteps | 90112    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=90500, episode_reward=197.46 +/- 5.33
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 90500       |
| train/                  |             |
|    approx_kl            | 0.039385963 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.0005      |
|    loss                 | 79.1        |
|    n_updates            | 339         |
|    policy_gradient_loss | -0.000463   |
|    value_loss           | 237         |
-----------------------------------------
Eval num_timesteps=91000, episode_reward=193.88 +/- 16.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 380      |
|    ep_rew_mean     | 34.7     |
| time/              |          |
|    fps             | 49       |
|    iterations      | 89       |
|    time_elapsed    | 1849     |
|    total_timesteps | 91136    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=91500, episode_reward=200.24 +/- 2.85
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 200        |
| time/                   |            |
|    total_timesteps      | 91500      |
| train/                  |            |
|    approx_kl            | 0.05051743 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.926     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0005     |
|    loss                 | 218        |
|    n_updates            | 341        |
|    policy_gradient_loss | 0.00419    |
|    value_loss           | 541        |
----------------------------------------
Eval num_timesteps=92000, episode_reward=197.62 +/- 7.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 385      |
|    ep_rew_mean     | 38.3     |
| time/              |          |
|    fps             | 48       |
|    iterations      | 90       |
|    time_elapsed    | 1884     |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=197.72 +/- 5.92
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | 198       |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0369549 |
|    clip_fraction        | 0.269     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07     |
|    explained_variance   | 0.235     |
|    learning_rate        | 0.0005    |
|    loss                 | 104       |
|    n_updates            | 351       |
|    policy_gradient_loss | -0.024    |
|    value_loss           | 424       |
---------------------------------------
Eval num_timesteps=93000, episode_reward=197.00 +/- 10.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 390      |
|    ep_rew_mean     | 44       |
| time/              |          |
|    fps             | 48       |
|    iterations      | 91       |
|    time_elapsed    | 1920     |
|    total_timesteps | 93184    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=93500, episode_reward=199.53 +/- 2.57
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 93500       |
| train/                  |             |
|    approx_kl            | 0.046510912 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0005      |
|    loss                 | 251         |
|    n_updates            | 355         |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 267         |
-----------------------------------------
Eval num_timesteps=94000, episode_reward=197.04 +/- 15.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 392      |
|    ep_rew_mean     | 46.7     |
| time/              |          |
|    fps             | 48       |
|    iterations      | 92       |
|    time_elapsed    | 1955     |
|    total_timesteps | 94208    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=94500, episode_reward=196.58 +/- 11.88
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.028538568 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0005      |
|    loss                 | 182         |
|    n_updates            | 357         |
|    policy_gradient_loss | -0.0005     |
|    value_loss           | 247         |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=198.49 +/- 1.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 50.9     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 93       |
|    time_elapsed    | 1991     |
|    total_timesteps | 95232    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=95500, episode_reward=198.56 +/- 17.22
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 95500       |
| train/                  |             |
|    approx_kl            | 0.062179707 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0005      |
|    loss                 | 135         |
|    n_updates            | 360         |
|    policy_gradient_loss | 0.00474     |
|    value_loss           | 234         |
-----------------------------------------
Eval num_timesteps=96000, episode_reward=198.88 +/- 14.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 408      |
|    ep_rew_mean     | 58.4     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 94       |
|    time_elapsed    | 2027     |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=96500, episode_reward=184.52 +/- 33.66
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 185        |
| time/                   |            |
|    total_timesteps      | 96500      |
| train/                  |            |
|    approx_kl            | 0.02504324 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.0005     |
|    loss                 | 33.6       |
|    n_updates            | 361        |
|    policy_gradient_loss | 0.00695    |
|    value_loss           | 239        |
----------------------------------------
Eval num_timesteps=97000, episode_reward=184.55 +/- 28.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | 65.5     |
| time/              |          |
|    fps             | 47       |
|    iterations      | 95       |
|    time_elapsed    | 2062     |
|    total_timesteps | 97280    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=97500, episode_reward=172.13 +/- 46.26
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 97500       |
| train/                  |             |
|    approx_kl            | 0.042125065 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.209       |
|    learning_rate        | 0.0005      |
|    loss                 | 181         |
|    n_updates            | 365         |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 454         |
-----------------------------------------
Eval num_timesteps=98000, episode_reward=166.92 +/- 56.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 420      |
|    ep_rew_mean     | 70.4     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 96       |
|    time_elapsed    | 2098     |
|    total_timesteps | 98304    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=98500, episode_reward=100.05 +/- 79.66
Episode length: 507.88 +/- 74.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 508         |
|    mean_reward          | 100         |
| time/                   |             |
|    total_timesteps      | 98500       |
| train/                  |             |
|    approx_kl            | 0.040823914 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.282       |
|    learning_rate        | 0.0005      |
|    loss                 | 82.8        |
|    n_updates            | 369         |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 254         |
-----------------------------------------
Eval num_timesteps=99000, episode_reward=104.25 +/- 108.03
Episode length: 506.14 +/- 74.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 104      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 424      |
|    ep_rew_mean     | 72.6     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 97       |
|    time_elapsed    | 2132     |
|    total_timesteps | 99328    |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.07
Eval num_timesteps=99500, episode_reward=91.83 +/- 109.92
Episode length: 492.80 +/- 98.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 493        |
|    mean_reward          | 91.8       |
| time/                   |            |
|    total_timesteps      | 99500      |
| train/                  |            |
|    approx_kl            | 0.04840631 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.266      |
|    learning_rate        | 0.0005     |
|    loss                 | 115        |
|    n_updates            | 376        |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 206        |
----------------------------------------
Eval num_timesteps=100000, episode_reward=101.43 +/- 95.42
Episode length: 506.70 +/- 76.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | 101      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 428      |
|    ep_rew_mean     | 75       |
| time/              |          |
|    fps             | 46       |
|    iterations      | 98       |
|    time_elapsed    | 2166     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=65.02 +/- 137.69
Episode length: 476.10 +/- 106.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 476        |
|    mean_reward          | 65         |
| time/                   |            |
|    total_timesteps      | 100500     |
| train/                  |            |
|    approx_kl            | 0.03195933 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.46      |
|    explained_variance   | 0.176      |
|    learning_rate        | 0.0005     |
|    loss                 | 105        |
|    n_updates            | 386        |
|    policy_gradient_loss | -0.0298    |
|    value_loss           | 459        |
----------------------------------------
Eval num_timesteps=101000, episode_reward=91.35 +/- 88.23
Episode length: 513.10 +/- 58.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 513      |
|    mean_reward     | 91.3     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 76.5     |
| time/              |          |
|    fps             | 46       |
|    iterations      | 99       |
|    time_elapsed    | 2199     |
|    total_timesteps | 101376   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=101500, episode_reward=22.61 +/- 130.79
Episode length: 423.12 +/- 150.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | 22.6        |
| time/                   |             |
|    total_timesteps      | 101500      |
| train/                  |             |
|    approx_kl            | 0.039860893 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0005      |
|    loss                 | 304         |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.000964    |
|    value_loss           | 490         |
-----------------------------------------
Eval num_timesteps=102000, episode_reward=23.71 +/- 98.18
Episode length: 470.36 +/- 110.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 23.7     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 77.7     |
| time/              |          |
|    fps             | 45       |
|    iterations      | 100      |
|    time_elapsed    | 2230     |
|    total_timesteps | 102400   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=102500, episode_reward=-27.04 +/- 106.65
Episode length: 370.02 +/- 157.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 370        |
|    mean_reward          | -27        |
| time/                   |            |
|    total_timesteps      | 102500     |
| train/                  |            |
|    approx_kl            | 0.05872488 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | 0.148      |
|    learning_rate        | 0.0005     |
|    loss                 | 175        |
|    n_updates            | 393        |
|    policy_gradient_loss | 0.00483    |
|    value_loss           | 636        |
----------------------------------------
Eval num_timesteps=103000, episode_reward=-31.02 +/- 111.84
Episode length: 372.30 +/- 155.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 372      |
|    mean_reward     | -31      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 440      |
|    ep_rew_mean     | 78.5     |
| time/              |          |
|    fps             | 45       |
|    iterations      | 101      |
|    time_elapsed    | 2255     |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=46.57 +/- 89.46
Episode length: 478.36 +/- 113.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 46.6        |
| time/                   |             |
|    total_timesteps      | 103500      |
| train/                  |             |
|    approx_kl            | 0.032571007 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0005      |
|    loss                 | 186         |
|    n_updates            | 403         |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 247         |
-----------------------------------------
Eval num_timesteps=104000, episode_reward=32.72 +/- 104.74
Episode length: 489.44 +/- 83.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | 32.7     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 444      |
|    ep_rew_mean     | 86.1     |
| time/              |          |
|    fps             | 45       |
|    iterations      | 102      |
|    time_elapsed    | 2289     |
|    total_timesteps | 104448   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.09
Eval num_timesteps=104500, episode_reward=23.28 +/- 117.94
Episode length: 455.12 +/- 123.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 455         |
|    mean_reward          | 23.3        |
| time/                   |             |
|    total_timesteps      | 104500      |
| train/                  |             |
|    approx_kl            | 0.049347907 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.0844      |
|    learning_rate        | 0.0005      |
|    loss                 | 782         |
|    n_updates            | 408         |
|    policy_gradient_loss | 0.0184      |
|    value_loss           | 1.57e+03    |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=0.68 +/- 131.66
Episode length: 416.62 +/- 155.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 417      |
|    mean_reward     | 0.685    |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 442      |
|    ep_rew_mean     | 87.6     |
| time/              |          |
|    fps             | 45       |
|    iterations      | 103      |
|    time_elapsed    | 2319     |
|    total_timesteps | 105472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=105500, episode_reward=-25.73 +/- 121.83
Episode length: 430.00 +/- 149.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 430        |
|    mean_reward          | -25.7      |
| time/                   |            |
|    total_timesteps      | 105500     |
| train/                  |            |
|    approx_kl            | 0.06773919 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.261      |
|    learning_rate        | 0.0005     |
|    loss                 | 578        |
|    n_updates            | 410        |
|    policy_gradient_loss | 0.00416    |
|    value_loss           | 976        |
----------------------------------------
Eval num_timesteps=106000, episode_reward=-55.70 +/- 125.95
Episode length: 380.62 +/- 158.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | -55.7    |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 438      |
|    ep_rew_mean     | 84.4     |
| time/              |          |
|    fps             | 45       |
|    iterations      | 104      |
|    time_elapsed    | 2347     |
|    total_timesteps | 106496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=106500, episode_reward=-138.95 +/- 105.50
Episode length: 212.88 +/- 128.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | -139        |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.029190352 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0005      |
|    loss                 | 201         |
|    n_updates            | 411         |
|    policy_gradient_loss | 0.0204      |
|    value_loss           | 420         |
-----------------------------------------
Eval num_timesteps=107000, episode_reward=-150.79 +/- 98.35
Episode length: 264.04 +/- 127.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-139.32 +/- 90.47
Episode length: 218.18 +/- 126.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 428      |
|    ep_rew_mean     | 77       |
| time/              |          |
|    fps             | 45       |
|    iterations      | 105      |
|    time_elapsed    | 2372     |
|    total_timesteps | 107520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=108000, episode_reward=-23.76 +/- 106.39
Episode length: 312.62 +/- 185.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 313         |
|    mean_reward          | -23.8       |
| time/                   |             |
|    total_timesteps      | 108000      |
| train/                  |             |
|    approx_kl            | 0.032678507 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.24        |
|    learning_rate        | 0.0005      |
|    loss                 | 789         |
|    n_updates            | 412         |
|    policy_gradient_loss | 0.00414     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=108500, episode_reward=-58.42 +/- 98.06
Episode length: 256.32 +/- 157.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | -58.4    |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 71       |
| time/              |          |
|    fps             | 45       |
|    iterations      | 106      |
|    time_elapsed    | 2392     |
|    total_timesteps | 108544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=109000, episode_reward=40.61 +/- 103.62
Episode length: 488.28 +/- 77.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 40.6        |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.031260747 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.23e+03    |
|    n_updates            | 413         |
|    policy_gradient_loss | 0.0128      |
|    value_loss           | 2.32e+03    |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=16.92 +/- 95.45
Episode length: 462.10 +/- 126.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 462      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 74       |
| time/              |          |
|    fps             | 45       |
|    iterations      | 107      |
|    time_elapsed    | 2425     |
|    total_timesteps | 109568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=110000, episode_reward=74.89 +/- 90.04
Episode length: 508.74 +/- 56.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 74.9        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.033607803 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0005      |
|    loss                 | 310         |
|    n_updates            | 414         |
|    policy_gradient_loss | 0.0121      |
|    value_loss           | 553         |
-----------------------------------------
Eval num_timesteps=110500, episode_reward=74.31 +/- 104.07
Episode length: 495.48 +/- 91.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 74.3     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 76.2     |
| time/              |          |
|    fps             | 44       |
|    iterations      | 108      |
|    time_elapsed    | 2460     |
|    total_timesteps | 110592   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=111000, episode_reward=134.75 +/- 67.73
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.050075762 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.918      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0005      |
|    loss                 | 466         |
|    n_updates            | 417         |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 745         |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=121.20 +/- 84.37
Episode length: 523.66 +/- 9.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 524      |
|    mean_reward     | 121      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 76.1     |
| time/              |          |
|    fps             | 44       |
|    iterations      | 109      |
|    time_elapsed    | 2495     |
|    total_timesteps | 111616   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=112000, episode_reward=101.46 +/- 96.75
Episode length: 506.74 +/- 65.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 101         |
| time/                   |             |
|    total_timesteps      | 112000      |
| train/                  |             |
|    approx_kl            | 0.047112916 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.421       |
|    learning_rate        | 0.0005      |
|    loss                 | 253         |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.0041      |
|    value_loss           | 383         |
-----------------------------------------
Eval num_timesteps=112500, episode_reward=108.39 +/- 89.41
Episode length: 509.60 +/- 52.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 108      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 73.1     |
| time/              |          |
|    fps             | 44       |
|    iterations      | 110      |
|    time_elapsed    | 2530     |
|    total_timesteps | 112640   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=113000, episode_reward=97.75 +/- 98.82
Episode length: 502.86 +/- 73.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 503         |
|    mean_reward          | 97.7        |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.032700367 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.0005      |
|    loss                 | 383         |
|    n_updates            | 423         |
|    policy_gradient_loss | 0.00253     |
|    value_loss           | 674         |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=76.13 +/- 110.01
Episode length: 494.68 +/- 85.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 76.1     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 411      |
|    ep_rew_mean     | 72.3     |
| time/              |          |
|    fps             | 44       |
|    iterations      | 111      |
|    time_elapsed    | 2564     |
|    total_timesteps | 113664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=114000, episode_reward=57.73 +/- 94.89
Episode length: 483.40 +/- 92.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 483         |
|    mean_reward          | 57.7        |
| time/                   |             |
|    total_timesteps      | 114000      |
| train/                  |             |
|    approx_kl            | 0.058945443 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0005      |
|    loss                 | 254         |
|    n_updates            | 425         |
|    policy_gradient_loss | 0.00674     |
|    value_loss           | 742         |
-----------------------------------------
Eval num_timesteps=114500, episode_reward=48.71 +/- 102.77
Episode length: 460.76 +/- 120.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 461      |
|    mean_reward     | 48.7     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 411      |
|    ep_rew_mean     | 72.1     |
| time/              |          |
|    fps             | 44       |
|    iterations      | 112      |
|    time_elapsed    | 2596     |
|    total_timesteps | 114688   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=115000, episode_reward=73.15 +/- 95.94
Episode length: 511.10 +/- 58.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 511         |
|    mean_reward          | 73.1        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.046320546 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0005      |
|    loss                 | 452         |
|    n_updates            | 427         |
|    policy_gradient_loss | 0.0208      |
|    value_loss           | 726         |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=55.98 +/- 86.64
Episode length: 522.80 +/- 12.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 523      |
|    mean_reward     | 56       |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 71       |
| time/              |          |
|    fps             | 43       |
|    iterations      | 113      |
|    time_elapsed    | 2631     |
|    total_timesteps | 115712   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=116000, episode_reward=122.83 +/- 84.98
Episode length: 521.72 +/- 22.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 522         |
|    mean_reward          | 123         |
| time/                   |             |
|    total_timesteps      | 116000      |
| train/                  |             |
|    approx_kl            | 0.051917657 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0005      |
|    loss                 | 232         |
|    n_updates            | 430         |
|    policy_gradient_loss | 0.0041      |
|    value_loss           | 591         |
-----------------------------------------
Eval num_timesteps=116500, episode_reward=111.32 +/- 92.29
Episode length: 514.04 +/- 56.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 66.3     |
| time/              |          |
|    fps             | 43       |
|    iterations      | 114      |
|    time_elapsed    | 2667     |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.08
Eval num_timesteps=117000, episode_reward=83.08 +/- 85.24
Episode length: 476.50 +/- 109.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 476        |
|    mean_reward          | 83.1       |
| time/                   |            |
|    total_timesteps      | 117000     |
| train/                  |            |
|    approx_kl            | 0.04182143 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.243      |
|    learning_rate        | 0.0005     |
|    loss                 | 723        |
|    n_updates            | 435        |
|    policy_gradient_loss | -0.00943   |
|    value_loss           | 829        |
----------------------------------------
Eval num_timesteps=117500, episode_reward=93.59 +/- 96.54
Episode length: 484.26 +/- 117.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | 93.6     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 68.4     |
| time/              |          |
|    fps             | 43       |
|    iterations      | 115      |
|    time_elapsed    | 2700     |
|    total_timesteps | 117760   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=118000, episode_reward=110.93 +/- 92.01
Episode length: 518.76 +/- 32.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 519       |
|    mean_reward          | 111       |
| time/                   |           |
|    total_timesteps      | 118000    |
| train/                  |           |
|    approx_kl            | 0.0362831 |
|    clip_fraction        | 0.194     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12     |
|    explained_variance   | 0.268     |
|    learning_rate        | 0.0005    |
|    loss                 | 486       |
|    n_updates            | 437       |
|    policy_gradient_loss | 0.00872   |
|    value_loss           | 935       |
---------------------------------------
Eval num_timesteps=118500, episode_reward=109.89 +/- 87.50
Episode length: 495.38 +/- 74.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 110      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 416      |
|    ep_rew_mean     | 71       |
| time/              |          |
|    fps             | 43       |
|    iterations      | 116      |
|    time_elapsed    | 2734     |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=119000, episode_reward=158.05 +/- 76.11
Episode length: 520.08 +/- 34.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 520         |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.043949697 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.0005      |
|    loss                 | 121         |
|    n_updates            | 446         |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 245         |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=131.17 +/- 89.92
Episode length: 518.82 +/- 25.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 419      |
|    ep_rew_mean     | 70.6     |
| time/              |          |
|    fps             | 43       |
|    iterations      | 117      |
|    time_elapsed    | 2770     |
|    total_timesteps | 119808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=120000, episode_reward=144.62 +/- 70.17
Episode length: 516.52 +/- 59.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.030137759 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.391       |
|    learning_rate        | 0.0005      |
|    loss                 | 161         |
|    n_updates            | 447         |
|    policy_gradient_loss | 0.00266     |
|    value_loss           | 407         |
-----------------------------------------
Eval num_timesteps=120500, episode_reward=133.33 +/- 83.59
Episode length: 518.46 +/- 45.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 133      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 427      |
|    ep_rew_mean     | 72.4     |
| time/              |          |
|    fps             | 43       |
|    iterations      | 118      |
|    time_elapsed    | 2805     |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.10
Eval num_timesteps=121000, episode_reward=179.14 +/- 57.68
Episode length: 518.38 +/- 46.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 518        |
|    mean_reward          | 179        |
| time/                   |            |
|    total_timesteps      | 121000     |
| train/                  |            |
|    approx_kl            | 0.09853691 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.531      |
|    learning_rate        | 0.0005     |
|    loss                 | 76.5       |
|    n_updates            | 454        |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 250        |
----------------------------------------
Eval num_timesteps=121500, episode_reward=178.91 +/- 49.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 430      |
|    ep_rew_mean     | 72.9     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 119      |
|    time_elapsed    | 2841     |
|    total_timesteps | 121856   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.07
Eval num_timesteps=122000, episode_reward=167.09 +/- 65.04
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 167        |
| time/                   |            |
|    total_timesteps      | 122000     |
| train/                  |            |
|    approx_kl            | 0.05785349 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.441      |
|    learning_rate        | 0.0005     |
|    loss                 | 202        |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 337        |
----------------------------------------
Eval num_timesteps=122500, episode_reward=179.85 +/- 54.65
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 433      |
|    ep_rew_mean     | 75.2     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 120      |
|    time_elapsed    | 2877     |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
Eval num_timesteps=123000, episode_reward=133.96 +/- 109.69
Episode length: 481.34 +/- 105.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 481        |
|    mean_reward          | 134        |
| time/                   |            |
|    total_timesteps      | 123000     |
| train/                  |            |
|    approx_kl            | 0.05218644 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.39      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.0005     |
|    loss                 | 140        |
|    n_updates            | 467        |
|    policy_gradient_loss | 0.0016     |
|    value_loss           | 154        |
----------------------------------------
Eval num_timesteps=123500, episode_reward=111.14 +/- 127.37
Episode length: 467.16 +/- 123.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 431      |
|    ep_rew_mean     | 72.1     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 121      |
|    time_elapsed    | 2909     |
|    total_timesteps | 123904   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.12
Eval num_timesteps=124000, episode_reward=97.61 +/- 102.44
Episode length: 455.74 +/- 123.76
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 456      |
|    mean_reward          | 97.6     |
| time/                   |          |
|    total_timesteps      | 124000   |
| train/                  |          |
|    approx_kl            | 0.057307 |
|    clip_fraction        | 0.265    |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.55    |
|    explained_variance   | 0.378    |
|    learning_rate        | 0.0005   |
|    loss                 | 114      |
|    n_updates            | 471      |
|    policy_gradient_loss | 0.000631 |
|    value_loss           | 422      |
--------------------------------------
Eval num_timesteps=124500, episode_reward=69.75 +/- 113.52
Episode length: 440.34 +/- 128.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 69.7     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 74.5     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 122      |
|    time_elapsed    | 2940     |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=125000, episode_reward=113.19 +/- 101.49
Episode length: 477.62 +/- 101.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | 113         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.025805999 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0005      |
|    loss                 | 90.8        |
|    n_updates            | 473         |
|    policy_gradient_loss | 0.00613     |
|    value_loss           | 329         |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=127.09 +/- 119.33
Episode length: 457.38 +/- 119.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 127      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 76.7     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 123      |
|    time_elapsed    | 2972     |
|    total_timesteps | 125952   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=126000, episode_reward=87.66 +/- 120.45
Episode length: 460.68 +/- 116.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | 87.7        |
| time/                   |             |
|    total_timesteps      | 126000      |
| train/                  |             |
|    approx_kl            | 0.044774063 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.0005      |
|    loss                 | 181         |
|    n_updates            | 476         |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 407         |
-----------------------------------------
Eval num_timesteps=126500, episode_reward=109.79 +/- 131.19
Episode length: 470.58 +/- 114.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 110      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 79.7     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 124      |
|    time_elapsed    | 3003     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=109.30 +/- 129.98
Episode length: 454.26 +/- 119.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 454         |
|    mean_reward          | 109         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.033926666 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0005      |
|    loss                 | 189         |
|    n_updates            | 486         |
|    policy_gradient_loss | 0.00309     |
|    value_loss           | 357         |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=91.10 +/- 128.15
Episode length: 425.26 +/- 145.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 425      |
|    mean_reward     | 91.1     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=83.97 +/- 130.74
Episode length: 430.52 +/- 138.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 84       |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 77.1     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 125      |
|    time_elapsed    | 3048     |
|    total_timesteps | 128000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=128500, episode_reward=101.19 +/- 120.24
Episode length: 449.42 +/- 133.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 449        |
|    mean_reward          | 101        |
| time/                   |            |
|    total_timesteps      | 128500     |
| train/                  |            |
|    approx_kl            | 0.02628933 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.0005     |
|    loss                 | 291        |
|    n_updates            | 487        |
|    policy_gradient_loss | 0.00365    |
|    value_loss           | 390        |
----------------------------------------
Eval num_timesteps=129000, episode_reward=97.17 +/- 112.03
Episode length: 459.04 +/- 113.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 97.2     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 435      |
|    ep_rew_mean     | 77.9     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 126      |
|    time_elapsed    | 3079     |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=129500, episode_reward=23.43 +/- 101.30
Episode length: 386.16 +/- 150.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 386         |
|    mean_reward          | 23.4        |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.055459384 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0005      |
|    loss                 | 158         |
|    n_updates            | 490         |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 334         |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=32.65 +/- 95.29
Episode length: 373.08 +/- 170.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 373      |
|    mean_reward     | 32.7     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 78       |
| time/              |          |
|    fps             | 41       |
|    iterations      | 127      |
|    time_elapsed    | 3105     |
|    total_timesteps | 130048   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=130500, episode_reward=-16.92 +/- 54.48
Episode length: 160.24 +/- 171.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | -16.9       |
| time/                   |             |
|    total_timesteps      | 130500      |
| train/                  |             |
|    approx_kl            | 0.074945465 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0005      |
|    loss                 | 296         |
|    n_updates            | 494         |
|    policy_gradient_loss | 0.00704     |
|    value_loss           | 633         |
-----------------------------------------
Eval num_timesteps=131000, episode_reward=-17.11 +/- 53.91
Episode length: 166.26 +/- 169.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | -17.1    |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 440      |
|    ep_rew_mean     | 79.9     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 128      |
|    time_elapsed    | 3117     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-12.72 +/- 72.76
Episode length: 154.68 +/- 150.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | -12.7       |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.030798268 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0005      |
|    loss                 | 124         |
|    n_updates            | 504         |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 205         |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=-21.57 +/- 53.91
Episode length: 144.98 +/- 142.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | -21.6    |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 441      |
|    ep_rew_mean     | 85.8     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 129      |
|    time_elapsed    | 3128     |
|    total_timesteps | 132096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=132500, episode_reward=74.92 +/- 97.57
Episode length: 455.74 +/- 119.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 456         |
|    mean_reward          | 74.9        |
| time/                   |             |
|    total_timesteps      | 132500      |
| train/                  |             |
|    approx_kl            | 0.044914402 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0005      |
|    loss                 | 489         |
|    n_updates            | 506         |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 536         |
-----------------------------------------
Eval num_timesteps=133000, episode_reward=42.94 +/- 101.69
Episode length: 379.02 +/- 157.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | 42.9     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 441      |
|    ep_rew_mean     | 84.5     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 130      |
|    time_elapsed    | 3157     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=134.70 +/- 107.04
Episode length: 438.36 +/- 133.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 438         |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.029716898 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0005      |
|    loss                 | 105         |
|    n_updates            | 516         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 514         |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=111.54 +/- 113.65
Episode length: 411.48 +/- 151.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | 112      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 441      |
|    ep_rew_mean     | 83       |
| time/              |          |
|    fps             | 42       |
|    iterations      | 131      |
|    time_elapsed    | 3186     |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=105.20 +/- 91.18
Episode length: 381.24 +/- 140.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 381        |
|    mean_reward          | 105        |
| time/                   |            |
|    total_timesteps      | 134500     |
| train/                  |            |
|    approx_kl            | 0.02743239 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.83      |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0005     |
|    loss                 | 250        |
|    n_updates            | 526        |
|    policy_gradient_loss | -0.00784   |
|    value_loss           | 207        |
----------------------------------------
Eval num_timesteps=135000, episode_reward=106.54 +/- 106.37
Episode length: 389.98 +/- 147.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 441      |
|    ep_rew_mean     | 85.5     |
| time/              |          |
|    fps             | 42       |
|    iterations      | 132      |
|    time_elapsed    | 3212     |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
Eval num_timesteps=135500, episode_reward=148.77 +/- 112.58
Episode length: 457.56 +/- 132.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | 149         |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.060568605 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0005      |
|    loss                 | 378         |
|    n_updates            | 533         |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 837         |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=123.55 +/- 112.49
Episode length: 479.94 +/- 104.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 438      |
|    ep_rew_mean     | 85.7     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 133      |
|    time_elapsed    | 3244     |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=97.93 +/- 113.15
Episode length: 439.64 +/- 139.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 97.9        |
| time/                   |             |
|    total_timesteps      | 136500      |
| train/                  |             |
|    approx_kl            | 0.027304413 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0005      |
|    loss                 | 122         |
|    n_updates            | 543         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 133         |
-----------------------------------------
Eval num_timesteps=137000, episode_reward=107.68 +/- 116.28
Episode length: 462.14 +/- 122.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 462      |
|    mean_reward     | 108      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 80.6     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 134      |
|    time_elapsed    | 3275     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=206.41 +/- 35.45
Episode length: 517.42 +/- 53.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.018266894 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0005      |
|    loss                 | 746         |
|    n_updates            | 553         |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=187.77 +/- 75.86
Episode length: 509.74 +/- 74.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 77.7     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 135      |
|    time_elapsed    | 3310     |
|    total_timesteps | 138240   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=138500, episode_reward=133.48 +/- 157.65
Episode length: 369.96 +/- 184.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 370       |
|    mean_reward          | 133       |
| time/                   |           |
|    total_timesteps      | 138500    |
| train/                  |           |
|    approx_kl            | 0.0851608 |
|    clip_fraction        | 0.309     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.63     |
|    explained_variance   | 0.265     |
|    learning_rate        | 0.0005    |
|    loss                 | 407       |
|    n_updates            | 556       |
|    policy_gradient_loss | 0.0147    |
|    value_loss           | 675       |
---------------------------------------
Eval num_timesteps=139000, episode_reward=166.25 +/- 153.33
Episode length: 434.50 +/- 159.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 434      |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | 75.5     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 136      |
|    time_elapsed    | 3337     |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=139500, episode_reward=177.99 +/- 98.95
Episode length: 482.36 +/- 102.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 482         |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.029209368 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0005      |
|    loss                 | 855         |
|    n_updates            | 559         |
|    policy_gradient_loss | 0.00548     |
|    value_loss           | 901         |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=192.63 +/- 80.08
Episode length: 508.60 +/- 66.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 433      |
|    ep_rew_mean     | 74       |
| time/              |          |
|    fps             | 41       |
|    iterations      | 137      |
|    time_elapsed    | 3370     |
|    total_timesteps | 140288   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=140500, episode_reward=188.69 +/- 45.50
Episode length: 515.50 +/- 46.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 140500      |
| train/                  |             |
|    approx_kl            | 0.055707414 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0005      |
|    loss                 | 515         |
|    n_updates            | 562         |
|    policy_gradient_loss | 0.011       |
|    value_loss           | 845         |
-----------------------------------------
Eval num_timesteps=141000, episode_reward=190.78 +/- 30.06
Episode length: 516.74 +/- 57.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | 66.9     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 138      |
|    time_elapsed    | 3405     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=196.70 +/- 7.80
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.020606782 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0005      |
|    loss                 | 282         |
|    n_updates            | 572         |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 897         |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=197.50 +/- 2.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | 66.2     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 139      |
|    time_elapsed    | 3440     |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=197.98 +/- 0.12
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 142500     |
| train/                  |            |
|    approx_kl            | 0.03139423 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.289      |
|    learning_rate        | 0.0005     |
|    loss                 | 128        |
|    n_updates            | 582        |
|    policy_gradient_loss | -0.00686   |
|    value_loss           | 513        |
----------------------------------------
Eval num_timesteps=143000, episode_reward=198.00 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | 65.3     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 140      |
|    time_elapsed    | 3475     |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.07
Eval num_timesteps=143500, episode_reward=198.05 +/- 0.73
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.040239643 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0005      |
|    loss                 | 495         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 597         |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=193.24 +/- 32.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | 69.5     |
| time/              |          |
|    fps             | 41       |
|    iterations      | 141      |
|    time_elapsed    | 3511     |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=197.70 +/- 0.70
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 144500      |
| train/                  |             |
|    approx_kl            | 0.033833556 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0005      |
|    loss                 | 88          |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 257         |
-----------------------------------------
Eval num_timesteps=145000, episode_reward=197.71 +/- 0.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | 72.3     |
| time/              |          |
|    fps             | 40       |
|    iterations      | 142      |
|    time_elapsed    | 3548     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=193.52 +/- 22.60
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.030475387 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0005      |
|    loss                 | 76.9        |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00933    |
|    value_loss           | 110         |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=197.88 +/- 0.43
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | 71.3     |
| time/              |          |
|    fps             | 40       |
|    iterations      | 143      |
|    time_elapsed    | 3585     |
|    total_timesteps | 146432   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=146500, episode_reward=198.81 +/- 2.70
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 199        |
| time/                   |            |
|    total_timesteps      | 146500     |
| train/                  |            |
|    approx_kl            | 0.04895789 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | 0.132      |
|    learning_rate        | 0.0005     |
|    loss                 | 538        |
|    n_updates            | 615        |
|    policy_gradient_loss | 0.00627    |
|    value_loss           | 1.13e+03   |
----------------------------------------
Eval num_timesteps=147000, episode_reward=197.24 +/- 9.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 430      |
|    ep_rew_mean     | 72.9     |
| time/              |          |
|    fps             | 40       |
|    iterations      | 144      |
|    time_elapsed    | 3622     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=197.51 +/- 0.79
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.027229749 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0005      |
|    loss                 | 43.3        |
|    n_updates            | 625         |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 304         |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=197.81 +/- 0.59
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 432      |
|    ep_rew_mean     | 80.1     |
| time/              |          |
|    fps             | 40       |
|    iterations      | 145      |
|    time_elapsed    | 3659     |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=203.17 +/- 13.76
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 148500      |
| train/                  |             |
|    approx_kl            | 0.043054216 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0005      |
|    loss                 | 841         |
|    n_updates            | 635         |
|    policy_gradient_loss | -0.0463     |
|    value_loss           | 983         |
-----------------------------------------
Eval num_timesteps=149000, episode_reward=182.02 +/- 75.34
Episode length: 514.74 +/- 63.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=194.97 +/- 44.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 430      |
|    ep_rew_mean     | 77       |
| time/              |          |
|    fps             | 40       |
|    iterations      | 146      |
|    time_elapsed    | 3712     |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=150000, episode_reward=198.53 +/- 35.60
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 199        |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.03668844 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0005     |
|    loss                 | 841        |
|    n_updates            | 637        |
|    policy_gradient_loss | -0.00831   |
|    value_loss           | 1.11e+03   |
----------------------------------------
Eval num_timesteps=150500, episode_reward=189.00 +/- 42.86
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 433      |
|    ep_rew_mean     | 81.1     |
| time/              |          |
|    fps             | 40       |
|    iterations      | 147      |
|    time_elapsed    | 3748     |
|    total_timesteps | 150528   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=151000, episode_reward=192.33 +/- 37.74
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 151000      |
| train/                  |             |
|    approx_kl            | 0.055423588 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.0005      |
|    loss                 | 527         |
|    n_updates            | 641         |
|    policy_gradient_loss | 0.00891     |
|    value_loss           | 943         |
-----------------------------------------
Eval num_timesteps=151500, episode_reward=196.18 +/- 10.78
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 438      |
|    ep_rew_mean     | 83.8     |
| time/              |          |
|    fps             | 40       |
|    iterations      | 148      |
|    time_elapsed    | 3784     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=183.82 +/- 49.58
Episode length: 514.94 +/- 70.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 515        |
|    mean_reward          | 184        |
| time/                   |            |
|    total_timesteps      | 152000     |
| train/                  |            |
|    approx_kl            | 0.03125567 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | 0.19       |
|    learning_rate        | 0.0005     |
|    loss                 | 179        |
|    n_updates            | 651        |
|    policy_gradient_loss | -0.0444    |
|    value_loss           | 458        |
----------------------------------------
Eval num_timesteps=152500, episode_reward=185.52 +/- 53.35
Episode length: 515.88 +/- 63.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 444      |
|    ep_rew_mean     | 89.5     |
| time/              |          |
|    fps             | 39       |
|    iterations      | 149      |
|    time_elapsed    | 3819     |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=181.39 +/- 58.68
Episode length: 515.36 +/- 67.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 153000      |
| train/                  |             |
|    approx_kl            | 0.014903035 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0005      |
|    loss                 | 752         |
|    n_updates            | 661         |
|    policy_gradient_loss | 0.000828    |
|    value_loss           | 1.63e+03    |
-----------------------------------------
Eval num_timesteps=153500, episode_reward=179.92 +/- 57.82
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 450      |
|    ep_rew_mean     | 96.7     |
| time/              |          |
|    fps             | 39       |
|    iterations      | 150      |
|    time_elapsed    | 3855     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=188.99 +/- 34.28
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.027420878 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0005      |
|    loss                 | 281         |
|    n_updates            | 671         |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 807         |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=197.59 +/- 8.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 449      |
|    ep_rew_mean     | 101      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 151      |
|    time_elapsed    | 3891     |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=197.72 +/- 2.26
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 155000     |
| train/                  |            |
|    approx_kl            | 0.02011368 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | 0.375      |
|    learning_rate        | 0.0005     |
|    loss                 | 522        |
|    n_updates            | 681        |
|    policy_gradient_loss | -0.00522   |
|    value_loss           | 1.22e+03   |
----------------------------------------
Eval num_timesteps=155500, episode_reward=199.02 +/- 24.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 446      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 152      |
|    time_elapsed    | 3927     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=188.80 +/- 10.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.032948323 |
|    clip_fraction        | 0.371       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 691         |
|    policy_gradient_loss | 0.00445     |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=189.23 +/- 15.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 445      |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 153      |
|    time_elapsed    | 3963     |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=209.01 +/- 44.49
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 157000      |
| train/                  |             |
|    approx_kl            | 0.034014978 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.0005      |
|    loss                 | 406         |
|    n_updates            | 701         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 909         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=157500, episode_reward=196.91 +/- 32.84
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 442      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 154      |
|    time_elapsed    | 3999     |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=158000, episode_reward=209.20 +/- 138.47
Episode length: 440.88 +/- 169.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 441         |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.033620875 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0005      |
|    loss                 | 616         |
|    n_updates            | 704         |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 1.34e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=158500, episode_reward=216.99 +/- 120.60
Episode length: 468.94 +/- 138.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 437      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 155      |
|    time_elapsed    | 4030     |
|    total_timesteps | 158720   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
Eval num_timesteps=159000, episode_reward=246.80 +/- 114.31
Episode length: 507.52 +/- 85.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 508       |
|    mean_reward          | 247       |
| time/                   |           |
|    total_timesteps      | 159000    |
| train/                  |           |
|    approx_kl            | 0.0402516 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55     |
|    explained_variance   | 0.456     |
|    learning_rate        | 0.0005    |
|    loss                 | 649       |
|    n_updates            | 711       |
|    policy_gradient_loss | -0.0151   |
|    value_loss           | 1.27e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=159500, episode_reward=236.19 +/- 130.25
Episode length: 483.38 +/- 125.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 483      |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 428      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 156      |
|    time_elapsed    | 4064     |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
Eval num_timesteps=160000, episode_reward=236.61 +/- 110.15
Episode length: 508.30 +/- 81.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 508         |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.047683623 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.0005      |
|    loss                 | 1.24e+03    |
|    n_updates            | 719         |
|    policy_gradient_loss | -0.0432     |
|    value_loss           | 2.59e+03    |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=256.78 +/- 115.49
Episode length: 489.44 +/- 120.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 489      |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 427      |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 157      |
|    time_elapsed    | 4098     |
|    total_timesteps | 160768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=161000, episode_reward=192.66 +/- 61.33
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 161000      |
| train/                  |             |
|    approx_kl            | 0.023325756 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.0005      |
|    loss                 | 732         |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=161500, episode_reward=195.73 +/- 61.18
Episode length: 517.58 +/- 51.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 423      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 158      |
|    time_elapsed    | 4133     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=166.46 +/- 51.77
Episode length: 516.36 +/- 60.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 166         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.040507734 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.0005      |
|    loss                 | 679         |
|    n_updates            | 730         |
|    policy_gradient_loss | 0.00105     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=153.22 +/- 76.33
Episode length: 507.98 +/- 83.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 153      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 419      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 159      |
|    time_elapsed    | 4168     |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=198.33 +/- 15.18
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 163000      |
| train/                  |             |
|    approx_kl            | 0.027639493 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0005      |
|    loss                 | 182         |
|    n_updates            | 740         |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 377         |
-----------------------------------------
Eval num_timesteps=163500, episode_reward=199.11 +/- 15.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 160      |
|    time_elapsed    | 4203     |
|    total_timesteps | 163840   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=164000, episode_reward=240.36 +/- 61.82
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.049374223 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.0005      |
|    loss                 | 652         |
|    n_updates            | 743         |
|    policy_gradient_loss | 0.00768     |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=164500, episode_reward=213.35 +/- 49.29
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 161      |
|    time_elapsed    | 4239     |
|    total_timesteps | 164864   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=165000, episode_reward=188.37 +/- 17.23
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 165000      |
| train/                  |             |
|    approx_kl            | 0.041419476 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0005      |
|    loss                 | 773         |
|    n_updates            | 747         |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=165500, episode_reward=197.96 +/- 41.90
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 162      |
|    time_elapsed    | 4274     |
|    total_timesteps | 165888   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=166000, episode_reward=202.87 +/- 78.01
Episode length: 516.80 +/- 57.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.053630363 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0005      |
|    loss                 | 454         |
|    n_updates            | 750         |
|    policy_gradient_loss | 0.024       |
|    value_loss           | 985         |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=194.11 +/- 82.93
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 404      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 163      |
|    time_elapsed    | 4309     |
|    total_timesteps | 166912   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=167000, episode_reward=201.31 +/- 26.55
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 201        |
| time/                   |            |
|    total_timesteps      | 167000     |
| train/                  |            |
|    approx_kl            | 0.04964458 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.0005     |
|    loss                 | 727        |
|    n_updates            | 754        |
|    policy_gradient_loss | 0.00776    |
|    value_loss           | 1.41e+03   |
----------------------------------------
Eval num_timesteps=167500, episode_reward=188.40 +/- 79.08
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 404      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 164      |
|    time_elapsed    | 4344     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=196.83 +/- 48.86
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.019017056 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0005      |
|    loss                 | 93.8        |
|    n_updates            | 764         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 148         |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=210.98 +/- 54.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 401      |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 165      |
|    time_elapsed    | 4380     |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=196.78 +/- 47.18
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 169000      |
| train/                  |             |
|    approx_kl            | 0.029083434 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0005      |
|    loss                 | 288         |
|    n_updates            | 774         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 457         |
-----------------------------------------
Eval num_timesteps=169500, episode_reward=211.08 +/- 40.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 402      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 166      |
|    time_elapsed    | 4416     |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=170000, episode_reward=232.19 +/- 92.37
Episode length: 499.56 +/- 100.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.025202272 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0005      |
|    loss                 | 373         |
|    n_updates            | 775         |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 929         |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=232.57 +/- 112.25
Episode length: 498.24 +/- 105.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=218.99 +/- 96.07
Episode length: 516.24 +/- 61.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 393      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 167      |
|    time_elapsed    | 4467     |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=268.34 +/- 95.90
Episode length: 515.56 +/- 66.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 268         |
| time/                   |             |
|    total_timesteps      | 171500      |
| train/                  |             |
|    approx_kl            | 0.038802624 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.16e+03    |
|    n_updates            | 785         |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 1.79e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=172000, episode_reward=214.36 +/- 128.19
Episode length: 497.06 +/- 108.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 388      |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 168      |
|    time_elapsed    | 4502     |
|    total_timesteps | 172032   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=172500, episode_reward=207.85 +/- 112.32
Episode length: 486.42 +/- 122.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 486        |
|    mean_reward          | 208        |
| time/                   |            |
|    total_timesteps      | 172500     |
| train/                  |            |
|    approx_kl            | 0.06718203 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.59      |
|    explained_variance   | 0.374      |
|    learning_rate        | 0.0005     |
|    loss                 | 814        |
|    n_updates            | 790        |
|    policy_gradient_loss | 0.00451    |
|    value_loss           | 1.95e+03   |
----------------------------------------
Eval num_timesteps=173000, episode_reward=217.57 +/- 105.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 388      |
|    ep_rew_mean     | 136      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 169      |
|    time_elapsed    | 4536     |
|    total_timesteps | 173056   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=173500, episode_reward=35.52 +/- 129.63
Episode length: 282.24 +/- 204.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 35.5        |
| time/                   |             |
|    total_timesteps      | 173500      |
| train/                  |             |
|    approx_kl            | 0.040218063 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0005      |
|    loss                 | 356         |
|    n_updates            | 793         |
|    policy_gradient_loss | -0.00479    |
|    value_loss           | 671         |
-----------------------------------------
Eval num_timesteps=174000, episode_reward=31.76 +/- 129.79
Episode length: 286.68 +/- 221.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 31.8     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 376      |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 170      |
|    time_elapsed    | 4555     |
|    total_timesteps | 174080   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=174500, episode_reward=66.85 +/- 98.48
Episode length: 500.96 +/- 95.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | 66.9        |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.057040006 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0005      |
|    loss                 | 720         |
|    n_updates            | 795         |
|    policy_gradient_loss | 0.0236      |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=73.50 +/- 68.61
Episode length: 517.10 +/- 55.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 73.5     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 368      |
|    ep_rew_mean     | 139      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 171      |
|    time_elapsed    | 4590     |
|    total_timesteps | 175104   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
Eval num_timesteps=175500, episode_reward=-90.48 +/- 110.76
Episode length: 263.64 +/- 191.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 264        |
|    mean_reward          | -90.5      |
| time/                   |            |
|    total_timesteps      | 175500     |
| train/                  |            |
|    approx_kl            | 0.04963313 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | 0.243      |
|    learning_rate        | 0.0005     |
|    loss                 | 673        |
|    n_updates            | 802        |
|    policy_gradient_loss | 0.0104     |
|    value_loss           | 1.32e+03   |
----------------------------------------
Eval num_timesteps=176000, episode_reward=-112.46 +/- 115.78
Episode length: 235.68 +/- 175.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 172      |
|    time_elapsed    | 4607     |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.13
Eval num_timesteps=176500, episode_reward=0.58 +/- 116.62
Episode length: 411.74 +/- 175.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 412         |
|    mean_reward          | 0.577       |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.061094426 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | -0.242      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.09e+03    |
|    n_updates            | 806         |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=5.47 +/- 126.45
Episode length: 372.08 +/- 198.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 372      |
|    mean_reward     | 5.47     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 356      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 173      |
|    time_elapsed    | 4635     |
|    total_timesteps | 177152   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=177500, episode_reward=176.34 +/- 21.49
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 177500      |
| train/                  |             |
|    approx_kl            | 0.035463095 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0005      |
|    loss                 | 235         |
|    n_updates            | 811         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 445         |
-----------------------------------------
Eval num_timesteps=178000, episode_reward=175.32 +/- 6.09
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 356      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 174      |
|    time_elapsed    | 4671     |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=178500, episode_reward=179.84 +/- 42.69
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.043341268 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.0005      |
|    loss                 | 302         |
|    n_updates            | 815         |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 559         |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=183.85 +/- 31.13
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 356      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 175      |
|    time_elapsed    | 4707     |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=193.65 +/- 25.85
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 194        |
| time/                   |            |
|    total_timesteps      | 179500     |
| train/                  |            |
|    approx_kl            | 0.03627269 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.427      |
|    learning_rate        | 0.0005     |
|    loss                 | 119        |
|    n_updates            | 825        |
|    policy_gradient_loss | -0.0338    |
|    value_loss           | 486        |
----------------------------------------
Eval num_timesteps=180000, episode_reward=190.65 +/- 38.40
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 352      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 176      |
|    time_elapsed    | 4745     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=210.28 +/- 48.80
Episode length: 517.50 +/- 52.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | 210         |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.033461317 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0005      |
|    loss                 | 623         |
|    n_updates            | 835         |
|    policy_gradient_loss | 0.00307     |
|    value_loss           | 977         |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=204.77 +/- 44.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 177      |
|    time_elapsed    | 4782     |
|    total_timesteps | 181248   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.06
Eval num_timesteps=181500, episode_reward=184.04 +/- 111.40
Episode length: 385.74 +/- 187.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 386         |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 181500      |
| train/                  |             |
|    approx_kl            | 0.048419308 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0005      |
|    loss                 | 936         |
|    n_updates            | 841         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=182000, episode_reward=195.76 +/- 132.47
Episode length: 380.26 +/- 187.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 380      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 341      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 178      |
|    time_elapsed    | 4808     |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.06
Eval num_timesteps=182500, episode_reward=271.47 +/- 153.00
Episode length: 476.98 +/- 121.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 477        |
|    mean_reward          | 271        |
| time/                   |            |
|    total_timesteps      | 182500     |
| train/                  |            |
|    approx_kl            | 0.06263709 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.424      |
|    learning_rate        | 0.0005     |
|    loss                 | 536        |
|    n_updates            | 847        |
|    policy_gradient_loss | 0.0102     |
|    value_loss           | 1.66e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=183000, episode_reward=271.80 +/- 103.56
Episode length: 516.82 +/- 57.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 335      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 179      |
|    time_elapsed    | 4842     |
|    total_timesteps | 183296   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.07
Eval num_timesteps=183500, episode_reward=220.05 +/- 68.39
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 183500      |
| train/                  |             |
|    approx_kl            | 0.061524287 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0005      |
|    loss                 | 773         |
|    n_updates            | 857         |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=184000, episode_reward=188.52 +/- 63.70
Episode length: 518.10 +/- 48.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 180      |
|    time_elapsed    | 4879     |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=184500, episode_reward=253.76 +/- 102.91
Episode length: 513.52 +/- 58.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 514         |
|    mean_reward          | 254         |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.057368834 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0005      |
|    loss                 | 255         |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 469         |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=236.43 +/- 109.67
Episode length: 500.22 +/- 98.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 181      |
|    time_elapsed    | 4914     |
|    total_timesteps | 185344   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=185500, episode_reward=115.48 +/- 96.19
Episode length: 310.98 +/- 206.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 311         |
|    mean_reward          | 115         |
| time/                   |             |
|    total_timesteps      | 185500      |
| train/                  |             |
|    approx_kl            | 0.060723916 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.224       |
|    learning_rate        | 0.0005      |
|    loss                 | 980         |
|    n_updates            | 863         |
|    policy_gradient_loss | 0.0166      |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=186000, episode_reward=114.75 +/- 122.54
Episode length: 334.58 +/- 199.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 335      |
|    mean_reward     | 115      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 182      |
|    time_elapsed    | 4936     |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=186500, episode_reward=247.24 +/- 84.67
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.034523364 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0005      |
|    loss                 | 566         |
|    n_updates            | 872         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 990         |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=220.18 +/- 96.01
Episode length: 517.54 +/- 52.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 331      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 183      |
|    time_elapsed    | 4972     |
|    total_timesteps | 187392   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=187500, episode_reward=175.91 +/- 113.23
Episode length: 510.88 +/- 69.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 511         |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 187500      |
| train/                  |             |
|    approx_kl            | 0.058055088 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0005      |
|    loss                 | 510         |
|    n_updates            | 875         |
|    policy_gradient_loss | 0.00375     |
|    value_loss           | 810         |
-----------------------------------------
Eval num_timesteps=188000, episode_reward=173.08 +/- 123.09
Episode length: 518.10 +/- 48.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 335      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 184      |
|    time_elapsed    | 5007     |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
Eval num_timesteps=188500, episode_reward=215.34 +/- 35.80
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.054440636 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0005      |
|    loss                 | 140         |
|    n_updates            | 883         |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 341         |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=204.15 +/- 47.50
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 185      |
|    time_elapsed    | 5043     |
|    total_timesteps | 189440   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
Eval num_timesteps=189500, episode_reward=180.31 +/- 45.66
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 180        |
| time/                   |            |
|    total_timesteps      | 189500     |
| train/                  |            |
|    approx_kl            | 0.06033898 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0005     |
|    loss                 | 136        |
|    n_updates            | 891        |
|    policy_gradient_loss | -0.00202   |
|    value_loss           | 429        |
----------------------------------------
Eval num_timesteps=190000, episode_reward=187.78 +/- 35.23
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 337      |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 186      |
|    time_elapsed    | 5079     |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
Eval num_timesteps=190500, episode_reward=203.28 +/- 68.02
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.042317145 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0005      |
|    loss                 | 563         |
|    n_updates            | 898         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=213.48 +/- 61.77
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 187      |
|    time_elapsed    | 5115     |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=189.24 +/- 28.98
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 191500      |
| train/                  |             |
|    approx_kl            | 0.033783864 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0005      |
|    loss                 | 191         |
|    n_updates            | 908         |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 371         |
-----------------------------------------
Eval num_timesteps=192000, episode_reward=192.55 +/- 28.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=192.27 +/- 21.22
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 346      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 188      |
|    time_elapsed    | 5170     |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.07
Eval num_timesteps=193000, episode_reward=127.52 +/- 86.15
Episode length: 515.58 +/- 65.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 128         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.055994447 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0005      |
|    loss                 | 198         |
|    n_updates            | 915         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 401         |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=131.38 +/- 67.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 189      |
|    time_elapsed    | 5207     |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=169.27 +/- 49.09
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 194000      |
| train/                  |             |
|    approx_kl            | 0.030976798 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0005      |
|    loss                 | 68.2        |
|    n_updates            | 925         |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 420         |
-----------------------------------------
Eval num_timesteps=194500, episode_reward=169.71 +/- 64.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 190      |
|    time_elapsed    | 5243     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=73.36 +/- 94.71
Episode length: 485.20 +/- 134.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 485         |
|    mean_reward          | 73.4        |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.020463068 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0005      |
|    loss                 | 131         |
|    n_updates            | 935         |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 206         |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=84.89 +/- 91.81
Episode length: 505.28 +/- 96.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | 84.9     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 191      |
|    time_elapsed    | 5277     |
|    total_timesteps | 195584   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=196000, episode_reward=119.97 +/- 96.29
Episode length: 496.70 +/- 112.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 497         |
|    mean_reward          | 120         |
| time/                   |             |
|    total_timesteps      | 196000      |
| train/                  |             |
|    approx_kl            | 0.048591804 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.387      |
|    learning_rate        | 0.0005      |
|    loss                 | 346         |
|    n_updates            | 938         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 671         |
-----------------------------------------
Eval num_timesteps=196500, episode_reward=124.49 +/- 83.53
Episode length: 515.68 +/- 65.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | 116      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 192      |
|    time_elapsed    | 5312     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=151.68 +/- 58.05
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 152         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.025090408 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.109      |
|    learning_rate        | 0.0005      |
|    loss                 | 172         |
|    n_updates            | 948         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 449         |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=158.54 +/- 59.17
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 368      |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 193      |
|    time_elapsed    | 5348     |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=192.57 +/- 35.33
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 198000      |
| train/                  |             |
|    approx_kl            | 0.022730544 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.181       |
|    learning_rate        | 0.0005      |
|    loss                 | 213         |
|    n_updates            | 958         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 573         |
-----------------------------------------
Eval num_timesteps=198500, episode_reward=184.73 +/- 45.88
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 371      |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 194      |
|    time_elapsed    | 5384     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=195.34 +/- 7.32
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.022402728 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.156       |
|    learning_rate        | 0.0005      |
|    loss                 | 109         |
|    n_updates            | 968         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 284         |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=195.79 +/- 6.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 372      |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 195      |
|    time_elapsed    | 5419     |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=195.53 +/- 6.56
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 196        |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.01748538 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.89      |
|    explained_variance   | 0.262      |
|    learning_rate        | 0.0005     |
|    loss                 | 258        |
|    n_updates            | 978        |
|    policy_gradient_loss | -0.0068    |
|    value_loss           | 359        |
----------------------------------------
Eval num_timesteps=200500, episode_reward=197.03 +/- 4.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 376      |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 196      |
|    time_elapsed    | 5455     |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.13
Eval num_timesteps=201000, episode_reward=198.30 +/- 2.46
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.040727682 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0005      |
|    loss                 | 86.5        |
|    n_updates            | 983         |
|    policy_gradient_loss | -0.00828    |
|    value_loss           | 181         |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=196.17 +/- 9.21
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 197      |
|    time_elapsed    | 5491     |
|    total_timesteps | 201728   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.08
Eval num_timesteps=202000, episode_reward=194.31 +/- 6.64
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 194        |
| time/                   |            |
|    total_timesteps      | 202000     |
| train/                  |            |
|    approx_kl            | 0.06845387 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0005     |
|    loss                 | 152        |
|    n_updates            | 993        |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 172        |
----------------------------------------
Eval num_timesteps=202500, episode_reward=194.80 +/- 4.39
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 386      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 198      |
|    time_elapsed    | 5527     |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=203000, episode_reward=193.37 +/- 8.14
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 193        |
| time/                   |            |
|    total_timesteps      | 203000     |
| train/                  |            |
|    approx_kl            | 0.05684567 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0005     |
|    loss                 | 131        |
|    n_updates            | 997        |
|    policy_gradient_loss | 0.00877    |
|    value_loss           | 223        |
----------------------------------------
Eval num_timesteps=203500, episode_reward=187.27 +/- 11.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 386      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 199      |
|    time_elapsed    | 5563     |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=199.44 +/- 7.03
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 204000      |
| train/                  |             |
|    approx_kl            | 0.033815607 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.0005      |
|    loss                 | 163         |
|    n_updates            | 1007        |
|    policy_gradient_loss | -0.00438    |
|    value_loss           | 678         |
-----------------------------------------
Eval num_timesteps=204500, episode_reward=200.50 +/- 2.99
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 386      |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 200      |
|    time_elapsed    | 5599     |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=205000, episode_reward=194.01 +/- 8.53
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.051084403 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.0005      |
|    loss                 | 257         |
|    n_updates            | 1012        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 288         |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=193.46 +/- 9.03
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 391      |
|    ep_rew_mean     | 122      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 201      |
|    time_elapsed    | 5635     |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=187.59 +/- 11.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 206000      |
| train/                  |             |
|    approx_kl            | 0.016515687 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0005      |
|    loss                 | 34.6        |
|    n_updates            | 1022        |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=206500, episode_reward=186.14 +/- 11.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 395      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 202      |
|    time_elapsed    | 5671     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=196.97 +/- 5.36
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.019480469 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0005      |
|    loss                 | 185         |
|    n_updates            | 1032        |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 307         |
-----------------------------------------
Eval num_timesteps=207500, episode_reward=197.75 +/- 0.45
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 395      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 203      |
|    time_elapsed    | 5709     |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=193.40 +/- 7.13
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 208000      |
| train/                  |             |
|    approx_kl            | 0.037422955 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0005      |
|    loss                 | 289         |
|    n_updates            | 1042        |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 814         |
-----------------------------------------
Eval num_timesteps=208500, episode_reward=192.05 +/- 12.53
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 399      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 204      |
|    time_elapsed    | 5745     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=193.26 +/- 8.76
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 193        |
| time/                   |            |
|    total_timesteps      | 209000     |
| train/                  |            |
|    approx_kl            | 0.03642759 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | 0.472      |
|    learning_rate        | 0.0005     |
|    loss                 | 165        |
|    n_updates            | 1052       |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 459        |
----------------------------------------
Eval num_timesteps=209500, episode_reward=191.83 +/- 10.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 399      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 205      |
|    time_elapsed    | 5781     |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=197.94 +/- 1.60
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.013176961 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0005      |
|    loss                 | 207         |
|    n_updates            | 1062        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 349         |
-----------------------------------------
Eval num_timesteps=210500, episode_reward=198.05 +/- 2.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 206      |
|    time_elapsed    | 5817     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=195.22 +/- 13.01
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.019898415 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0005      |
|    loss                 | 278         |
|    n_updates            | 1072        |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 495         |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=196.19 +/- 5.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | 127      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 207      |
|    time_elapsed    | 5852     |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=197.46 +/- 1.74
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 212000     |
| train/                  |            |
|    approx_kl            | 0.03078853 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.8       |
|    explained_variance   | 0.43       |
|    learning_rate        | 0.0005     |
|    loss                 | 381        |
|    n_updates            | 1082       |
|    policy_gradient_loss | -0.00959   |
|    value_loss           | 682        |
----------------------------------------
Eval num_timesteps=212500, episode_reward=197.04 +/- 0.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 208      |
|    time_elapsed    | 5888     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=193.18 +/- 23.05
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.030555934 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0005      |
|    loss                 | 937         |
|    n_updates            | 1092        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 2.15e+03    |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=198.34 +/- 2.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=198.51 +/- 2.64
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 401      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 209      |
|    time_elapsed    | 5942     |
|    total_timesteps | 214016   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.08
Eval num_timesteps=214500, episode_reward=198.47 +/- 2.28
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 214500      |
| train/                  |             |
|    approx_kl            | 0.052869536 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.03e+03    |
|    n_updates            | 1101        |
|    policy_gradient_loss | -0.00494    |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=215000, episode_reward=197.95 +/- 1.28
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 408      |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 210      |
|    time_elapsed    | 5978     |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=215500, episode_reward=198.24 +/- 1.65
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 215500     |
| train/                  |            |
|    approx_kl            | 0.06317676 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.75      |
|    explained_variance   | 0.38       |
|    learning_rate        | 0.0005     |
|    loss                 | 346        |
|    n_updates            | 1105       |
|    policy_gradient_loss | -0.00784   |
|    value_loss           | 826        |
----------------------------------------
Eval num_timesteps=216000, episode_reward=197.93 +/- 0.60
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 407      |
|    ep_rew_mean     | 126      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 211      |
|    time_elapsed    | 6014     |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=197.63 +/- 0.90
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 216500      |
| train/                  |             |
|    approx_kl            | 0.019551316 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.264       |
|    learning_rate        | 0.0005      |
|    loss                 | 537         |
|    n_updates            | 1115        |
|    policy_gradient_loss | -0.00406    |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=217000, episode_reward=197.52 +/- 1.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 212      |
|    time_elapsed    | 6050     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=185.79 +/- 46.25
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.021383107 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0005      |
|    loss                 | 450         |
|    n_updates            | 1125        |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 546         |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=190.02 +/- 29.72
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 419      |
|    ep_rew_mean     | 142      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 213      |
|    time_elapsed    | 6086     |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=197.61 +/- 1.14
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 218500      |
| train/                  |             |
|    approx_kl            | 0.023283053 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0005      |
|    loss                 | 121         |
|    n_updates            | 1135        |
|    policy_gradient_loss | -0.0432     |
|    value_loss           | 472         |
-----------------------------------------
Eval num_timesteps=219000, episode_reward=196.44 +/- 5.58
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 148      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 214      |
|    time_elapsed    | 6122     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=198.20 +/- 5.68
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 219500     |
| train/                  |            |
|    approx_kl            | 0.01658392 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.76      |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0005     |
|    loss                 | 290        |
|    n_updates            | 1145       |
|    policy_gradient_loss | -0.0215    |
|    value_loss           | 525        |
----------------------------------------
Eval num_timesteps=220000, episode_reward=197.04 +/- 9.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 215      |
|    time_elapsed    | 6158     |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=275.15 +/- 119.01
Episode length: 473.30 +/- 128.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 473         |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 220500      |
| train/                  |             |
|    approx_kl            | 0.020375866 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.265       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.03e+03    |
|    n_updates            | 1155        |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 1.8e+03     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=221000, episode_reward=267.26 +/- 118.40
Episode length: 427.44 +/- 165.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 413      |
|    ep_rew_mean     | 162      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 216      |
|    time_elapsed    | 6189     |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
Eval num_timesteps=221500, episode_reward=311.59 +/- 180.43
Episode length: 299.56 +/- 208.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | 312         |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.050600685 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.0005      |
|    loss                 | 799         |
|    n_updates            | 1163        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 1.11e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=222000, episode_reward=287.99 +/- 145.62
Episode length: 240.82 +/- 187.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 217      |
|    time_elapsed    | 6208     |
|    total_timesteps | 222208   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.07
Eval num_timesteps=222500, episode_reward=250.80 +/- 138.05
Episode length: 387.44 +/- 177.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 251         |
| time/                   |             |
|    total_timesteps      | 222500      |
| train/                  |             |
|    approx_kl            | 0.053919572 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.429       |
|    learning_rate        | 0.0005      |
|    loss                 | 785         |
|    n_updates            | 1171        |
|    policy_gradient_loss | -0.0518     |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=223000, episode_reward=262.99 +/- 129.85
Episode length: 391.20 +/- 188.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 391      |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 218      |
|    time_elapsed    | 6235     |
|    total_timesteps | 223232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=223500, episode_reward=206.23 +/- 61.22
Episode length: 518.62 +/- 44.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 519         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.022400683 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.0005      |
|    loss                 | 441         |
|    n_updates            | 1172        |
|    policy_gradient_loss | 0.00828     |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=198.98 +/- 66.39
Episode length: 474.42 +/- 117.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 414      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 219      |
|    time_elapsed    | 6269     |
|    total_timesteps | 224256   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.08
Eval num_timesteps=224500, episode_reward=145.60 +/- 67.56
Episode length: 263.02 +/- 174.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 146         |
| time/                   |             |
|    total_timesteps      | 224500      |
| train/                  |             |
|    approx_kl            | 0.047434878 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0005      |
|    loss                 | 719         |
|    n_updates            | 1179        |
|    policy_gradient_loss | -0.0388     |
|    value_loss           | 1.18e+03    |
-----------------------------------------
Eval num_timesteps=225000, episode_reward=146.16 +/- 106.08
Episode length: 261.06 +/- 189.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 146      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 420      |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 220      |
|    time_elapsed    | 6287     |
|    total_timesteps | 225280   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.09
Eval num_timesteps=225500, episode_reward=218.64 +/- 147.46
Episode length: 166.50 +/- 138.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.050153032 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0005      |
|    loss                 | 307         |
|    n_updates            | 1188        |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 448         |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=155.12 +/- 130.48
Episode length: 147.16 +/- 124.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 147      |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 221      |
|    time_elapsed    | 6298     |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=162.22 +/- 129.31
Episode length: 103.24 +/- 50.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 226500      |
| train/                  |             |
|    approx_kl            | 0.038237154 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0005      |
|    loss                 | 631         |
|    n_updates            | 1198        |
|    policy_gradient_loss | -0.000169   |
|    value_loss           | 1.51e+03    |
-----------------------------------------
Eval num_timesteps=227000, episode_reward=158.26 +/- 132.42
Episode length: 113.50 +/- 90.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 406      |
|    ep_rew_mean     | 199      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 222      |
|    time_elapsed    | 6307     |
|    total_timesteps | 227328   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
Eval num_timesteps=227500, episode_reward=105.74 +/- 112.05
Episode length: 80.50 +/- 68.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.5        |
|    mean_reward          | 106         |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.048262864 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0005      |
|    loss                 | 913         |
|    n_updates            | 1205        |
|    policy_gradient_loss | 0.00386     |
|    value_loss           | 1.51e+03    |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=86.88 +/- 85.63
Episode length: 75.50 +/- 66.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | 86.9     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 388      |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 223      |
|    time_elapsed    | 6313     |
|    total_timesteps | 228352   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=228500, episode_reward=143.80 +/- 121.70
Episode length: 86.76 +/- 74.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 86.8       |
|    mean_reward          | 144        |
| time/                   |            |
|    total_timesteps      | 228500     |
| train/                  |            |
|    approx_kl            | 0.03718418 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.16e+03   |
|    n_updates            | 1208       |
|    policy_gradient_loss | -0.0207    |
|    value_loss           | 1.8e+03    |
----------------------------------------
Eval num_timesteps=229000, episode_reward=148.65 +/- 109.89
Episode length: 86.36 +/- 91.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.4     |
|    mean_reward     | 149      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 224      |
|    time_elapsed    | 6320     |
|    total_timesteps | 229376   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=229500, episode_reward=155.89 +/- 99.46
Episode length: 145.90 +/- 121.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 156         |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.078885056 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.31e+03    |
|    n_updates            | 1212        |
|    policy_gradient_loss | 0.0178      |
|    value_loss           | 2.96e+03    |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=135.10 +/- 138.92
Episode length: 110.14 +/- 87.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 135      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 353      |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 225      |
|    time_elapsed    | 6329     |
|    total_timesteps | 230400   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=230500, episode_reward=185.80 +/- 142.26
Episode length: 152.24 +/- 128.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 230500      |
| train/                  |             |
|    approx_kl            | 0.048938323 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0005      |
|    loss                 | 865         |
|    n_updates            | 1215        |
|    policy_gradient_loss | 0.0257      |
|    value_loss           | 2.18e+03    |
-----------------------------------------
Eval num_timesteps=231000, episode_reward=139.03 +/- 112.87
Episode length: 160.42 +/- 139.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 139      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 347      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 226      |
|    time_elapsed    | 6341     |
|    total_timesteps | 231424   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=231500, episode_reward=187.02 +/- 155.94
Episode length: 159.68 +/- 150.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.055321995 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0005      |
|    loss                 | 720         |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=199.18 +/- 154.83
Episode length: 186.04 +/- 173.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 333      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 227      |
|    time_elapsed    | 6353     |
|    total_timesteps | 232448   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=232500, episode_reward=163.93 +/- 121.96
Episode length: 162.52 +/- 152.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 163        |
|    mean_reward          | 164        |
| time/                   |            |
|    total_timesteps      | 232500     |
| train/                  |            |
|    approx_kl            | 0.04900962 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.43      |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.0005     |
|    loss                 | 599        |
|    n_updates            | 1222       |
|    policy_gradient_loss | 0.00788    |
|    value_loss           | 1.41e+03   |
----------------------------------------
Eval num_timesteps=233000, episode_reward=169.12 +/- 138.56
Episode length: 169.52 +/- 158.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 196      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 228      |
|    time_elapsed    | 6365     |
|    total_timesteps | 233472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=233500, episode_reward=145.02 +/- 104.06
Episode length: 394.06 +/- 186.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 394        |
|    mean_reward          | 145        |
| time/                   |            |
|    total_timesteps      | 233500     |
| train/                  |            |
|    approx_kl            | 0.04100997 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.447      |
|    learning_rate        | 0.0005     |
|    loss                 | 871        |
|    n_updates            | 1224       |
|    policy_gradient_loss | 0.0111     |
|    value_loss           | 2.04e+03   |
----------------------------------------
Eval num_timesteps=234000, episode_reward=211.29 +/- 132.75
Episode length: 405.98 +/- 177.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 229      |
|    time_elapsed    | 6392     |
|    total_timesteps | 234496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=234500, episode_reward=228.12 +/- 85.47
Episode length: 516.44 +/- 59.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 234500      |
| train/                  |             |
|    approx_kl            | 0.032014098 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.44       |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0005      |
|    loss                 | 787         |
|    n_updates            | 1225        |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 859         |
-----------------------------------------
Eval num_timesteps=235000, episode_reward=206.63 +/- 53.80
Episode length: 515.86 +/- 63.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=200.65 +/- 40.57
Episode length: 516.60 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 198      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 230      |
|    time_elapsed    | 6444     |
|    total_timesteps | 235520   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=236000, episode_reward=274.28 +/- 139.35
Episode length: 487.94 +/- 111.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 488       |
|    mean_reward          | 274       |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0362514 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39     |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0005    |
|    loss                 | 378       |
|    n_updates            | 1230      |
|    policy_gradient_loss | -0.0169   |
|    value_loss           | 719       |
---------------------------------------
Eval num_timesteps=236500, episode_reward=260.84 +/- 110.20
Episode length: 490.68 +/- 104.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 491      |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 231      |
|    time_elapsed    | 6477     |
|    total_timesteps | 236544   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=237000, episode_reward=240.42 +/- 141.05
Episode length: 451.52 +/- 149.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 452         |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 237000      |
| train/                  |             |
|    approx_kl            | 0.045454178 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.62e+03    |
|    n_updates            | 1233        |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=237500, episode_reward=235.64 +/- 129.50
Episode length: 459.38 +/- 151.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 459      |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 232      |
|    time_elapsed    | 6508     |
|    total_timesteps | 237568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=238000, episode_reward=189.05 +/- 39.34
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.061506674 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.0005      |
|    loss                 | 498         |
|    n_updates            | 1235        |
|    policy_gradient_loss | 0.0181      |
|    value_loss           | 1.2e+03     |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=191.60 +/- 27.18
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 233      |
|    time_elapsed    | 6544     |
|    total_timesteps | 238592   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=239000, episode_reward=198.61 +/- 2.90
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 199        |
| time/                   |            |
|    total_timesteps      | 239000     |
| train/                  |            |
|    approx_kl            | 0.05612004 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.0005     |
|    loss                 | 899        |
|    n_updates            | 1237       |
|    policy_gradient_loss | -0.0002    |
|    value_loss           | 1.55e+03   |
----------------------------------------
Eval num_timesteps=239500, episode_reward=198.71 +/- 3.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 234      |
|    time_elapsed    | 6580     |
|    total_timesteps | 239616   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=240000, episode_reward=197.71 +/- 0.56
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 198        |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.04661727 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.467      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.48e+03   |
|    n_updates            | 1240       |
|    policy_gradient_loss | 0.00239    |
|    value_loss           | 2.42e+03   |
----------------------------------------
Eval num_timesteps=240500, episode_reward=197.71 +/- 0.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 235      |
|    time_elapsed    | 6615     |
|    total_timesteps | 240640   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=241000, episode_reward=195.44 +/- 12.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 241000      |
| train/                  |             |
|    approx_kl            | 0.034103025 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0005      |
|    loss                 | 266         |
|    n_updates            | 1242        |
|    policy_gradient_loss | 0.00395     |
|    value_loss           | 519         |
-----------------------------------------
Eval num_timesteps=241500, episode_reward=197.24 +/- 0.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 283      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 236      |
|    time_elapsed    | 6651     |
|    total_timesteps | 241664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=242000, episode_reward=195.83 +/- 12.96
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.050218105 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0005      |
|    loss                 | 586         |
|    n_updates            | 1244        |
|    policy_gradient_loss | 0.0101      |
|    value_loss           | 853         |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=195.27 +/- 13.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 237      |
|    time_elapsed    | 6687     |
|    total_timesteps | 242688   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=243000, episode_reward=197.79 +/- 0.40
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 243000      |
| train/                  |             |
|    approx_kl            | 0.049912676 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0005      |
|    loss                 | 88.5        |
|    n_updates            | 1248        |
|    policy_gradient_loss | -0.00251    |
|    value_loss           | 221         |
-----------------------------------------
Eval num_timesteps=243500, episode_reward=196.19 +/- 10.27
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 238      |
|    time_elapsed    | 6723     |
|    total_timesteps | 243712   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=244000, episode_reward=197.48 +/- 0.61
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.044707637 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0005      |
|    loss                 | 127         |
|    n_updates            | 1257        |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 303         |
-----------------------------------------
Eval num_timesteps=244500, episode_reward=197.56 +/- 0.55
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 239      |
|    time_elapsed    | 6759     |
|    total_timesteps | 244736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=245000, episode_reward=191.69 +/- 24.83
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 192        |
| time/                   |            |
|    total_timesteps      | 245000     |
| train/                  |            |
|    approx_kl            | 0.07724568 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0005     |
|    loss                 | 72.1       |
|    n_updates            | 1259       |
|    policy_gradient_loss | 0.0223     |
|    value_loss           | 133        |
----------------------------------------
Eval num_timesteps=245500, episode_reward=195.40 +/- 13.49
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 240      |
|    time_elapsed    | 6795     |
|    total_timesteps | 245760   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=246000, episode_reward=189.68 +/- 31.21
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.044281024 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0005      |
|    loss                 | 174         |
|    n_updates            | 1264        |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 373         |
-----------------------------------------
Eval num_timesteps=246500, episode_reward=195.95 +/- 10.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 207      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 241      |
|    time_elapsed    | 6831     |
|    total_timesteps | 246784   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=247000, episode_reward=184.59 +/- 35.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 247000      |
| train/                  |             |
|    approx_kl            | 0.055446383 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0005      |
|    loss                 | 74.9        |
|    n_updates            | 1266        |
|    policy_gradient_loss | 0.0104      |
|    value_loss           | 230         |
-----------------------------------------
Eval num_timesteps=247500, episode_reward=187.07 +/- 24.10
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 209      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 242      |
|    time_elapsed    | 6867     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=200.03 +/- 23.21
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.023944689 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0005      |
|    loss                 | 29.9        |
|    n_updates            | 1276        |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 142         |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=192.63 +/- 26.56
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 243      |
|    time_elapsed    | 6903     |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=199.93 +/- 48.42
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 249000      |
| train/                  |             |
|    approx_kl            | 0.031217393 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.0005      |
|    loss                 | 530         |
|    n_updates            | 1286        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 1.12e+03    |
-----------------------------------------
Eval num_timesteps=249500, episode_reward=211.63 +/- 35.73
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 244      |
|    time_elapsed    | 6940     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=189.75 +/- 41.80
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.026177887 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0005      |
|    loss                 | 186         |
|    n_updates            | 1296        |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 568         |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=194.48 +/- 25.81
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 208      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 245      |
|    time_elapsed    | 6976     |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=204.39 +/- 56.60
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 251000      |
| train/                  |             |
|    approx_kl            | 0.031214384 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0005      |
|    loss                 | 269         |
|    n_updates            | 1306        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 326         |
-----------------------------------------
Eval num_timesteps=251500, episode_reward=185.37 +/- 63.74
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 246      |
|    time_elapsed    | 7011     |
|    total_timesteps | 251904   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.07
Eval num_timesteps=252000, episode_reward=203.82 +/- 60.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.039542753 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.119       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 1316        |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=252500, episode_reward=203.98 +/- 56.97
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 247      |
|    time_elapsed    | 7047     |
|    total_timesteps | 252928   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.06
Eval num_timesteps=253000, episode_reward=219.88 +/- 129.09
Episode length: 491.94 +/- 114.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 492         |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 253000      |
| train/                  |             |
|    approx_kl            | 0.049359825 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0005      |
|    loss                 | 665         |
|    n_updates            | 1322        |
|    policy_gradient_loss | -0.00585    |
|    value_loss           | 848         |
-----------------------------------------
Eval num_timesteps=253500, episode_reward=237.89 +/- 115.74
Episode length: 511.76 +/- 64.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 512      |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 248      |
|    time_elapsed    | 7082     |
|    total_timesteps | 253952   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=254000, episode_reward=244.80 +/- 113.17
Episode length: 505.38 +/- 78.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 505       |
|    mean_reward          | 245       |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0636444 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35     |
|    explained_variance   | 0.427     |
|    learning_rate        | 0.0005    |
|    loss                 | 991       |
|    n_updates            | 1326      |
|    policy_gradient_loss | -0.00695  |
|    value_loss           | 1.64e+03  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=223.35 +/- 101.38
Episode length: 508.76 +/- 79.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 249      |
|    time_elapsed    | 7116     |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=233.32 +/- 95.91
Episode length: 508.42 +/- 81.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 508         |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.027594794 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0005      |
|    loss                 | 325         |
|    n_updates            | 1336        |
|    policy_gradient_loss | -0.00835    |
|    value_loss           | 961         |
-----------------------------------------
Eval num_timesteps=255500, episode_reward=227.46 +/- 81.26
Episode length: 517.04 +/- 55.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=231.24 +/- 92.90
Episode length: 516.92 +/- 56.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 250      |
|    time_elapsed    | 7168     |
|    total_timesteps | 256000   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=256500, episode_reward=209.13 +/- 33.30
Episode length: 516.42 +/- 60.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 516        |
|    mean_reward          | 209        |
| time/                   |            |
|    total_timesteps      | 256500     |
| train/                  |            |
|    approx_kl            | 0.06570779 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0005     |
|    loss                 | 580        |
|    n_updates            | 1345       |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 955        |
----------------------------------------
Eval num_timesteps=257000, episode_reward=229.97 +/- 65.81
Episode length: 516.66 +/- 58.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 251      |
|    time_elapsed    | 7204     |
|    total_timesteps | 257024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=257500, episode_reward=242.28 +/- 108.83
Episode length: 475.70 +/- 133.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 257500      |
| train/                  |             |
|    approx_kl            | 0.027652353 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0005      |
|    loss                 | 876         |
|    n_updates            | 1346        |
|    policy_gradient_loss | 0.0176      |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=258000, episode_reward=232.91 +/- 82.14
Episode length: 492.62 +/- 109.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 252      |
|    time_elapsed    | 7236     |
|    total_timesteps | 258048   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.06
Eval num_timesteps=258500, episode_reward=239.13 +/- 128.63
Episode length: 442.24 +/- 165.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 442         |
|    mean_reward          | 239         |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.040822145 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0005      |
|    loss                 | 500         |
|    n_updates            | 1352        |
|    policy_gradient_loss | 0.00528     |
|    value_loss           | 807         |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=225.82 +/- 93.24
Episode length: 484.06 +/- 122.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | 226      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 319      |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 253      |
|    time_elapsed    | 7268     |
|    total_timesteps | 259072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=259500, episode_reward=211.61 +/- 46.56
Episode length: 517.00 +/- 56.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 517        |
|    mean_reward          | 212        |
| time/                   |            |
|    total_timesteps      | 259500     |
| train/                  |            |
|    approx_kl            | 0.04544327 |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.417      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.68e+03   |
|    n_updates            | 1354       |
|    policy_gradient_loss | -0.00238   |
|    value_loss           | 2.25e+03   |
----------------------------------------
Eval num_timesteps=260000, episode_reward=209.84 +/- 67.48
Episode length: 492.62 +/- 109.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 323      |
|    ep_rew_mean     | 196      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 254      |
|    time_elapsed    | 7302     |
|    total_timesteps | 260096   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.06
Eval num_timesteps=260500, episode_reward=237.93 +/- 146.08
Episode length: 442.50 +/- 165.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 442       |
|    mean_reward          | 238       |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0530016 |
|    clip_fraction        | 0.302     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.28     |
|    explained_variance   | 0.329     |
|    learning_rate        | 0.0005    |
|    loss                 | 491       |
|    n_updates            | 1361      |
|    policy_gradient_loss | 0.00757   |
|    value_loss           | 1.83e+03  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=258.24 +/- 151.18
Episode length: 421.94 +/- 175.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 422      |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 255      |
|    time_elapsed    | 7332     |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=370.65 +/- 149.79
Episode length: 517.92 +/- 49.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | 371         |
| time/                   |             |
|    total_timesteps      | 261500      |
| train/                  |             |
|    approx_kl            | 0.038400106 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.0005      |
|    loss                 | 487         |
|    n_updates            | 1371        |
|    policy_gradient_loss | -0.0432     |
|    value_loss           | 843         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=262000, episode_reward=360.10 +/- 156.27
Episode length: 516.58 +/- 58.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 337      |
|    ep_rew_mean     | 228      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 256      |
|    time_elapsed    | 7367     |
|    total_timesteps | 262144   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=262500, episode_reward=274.34 +/- 125.90
Episode length: 489.64 +/- 106.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 490        |
|    mean_reward          | 274        |
| time/                   |            |
|    total_timesteps      | 262500     |
| train/                  |            |
|    approx_kl            | 0.05174666 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.0005     |
|    loss                 | 820        |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 1.81e+03   |
----------------------------------------
Eval num_timesteps=263000, episode_reward=341.97 +/- 160.23
Episode length: 480.16 +/- 123.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 257      |
|    time_elapsed    | 7401     |
|    total_timesteps | 263168   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=263500, episode_reward=236.14 +/- 105.93
Episode length: 459.36 +/- 140.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 459         |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 263500      |
| train/                  |             |
|    approx_kl            | 0.059246242 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0005      |
|    loss                 | 579         |
|    n_updates            | 1382        |
|    policy_gradient_loss | 0.0204      |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=264000, episode_reward=235.70 +/- 107.61
Episode length: 458.48 +/- 135.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 458      |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 347      |
|    ep_rew_mean     | 236      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 258      |
|    time_elapsed    | 7432     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=313.61 +/- 151.50
Episode length: 428.64 +/- 163.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 429         |
|    mean_reward          | 314         |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.040581264 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0005      |
|    loss                 | 546         |
|    n_updates            | 1392        |
|    policy_gradient_loss | -0.0527     |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=277.60 +/- 120.22
Episode length: 439.40 +/- 153.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 439      |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 355      |
|    ep_rew_mean     | 251      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 259      |
|    time_elapsed    | 7462     |
|    total_timesteps | 265216   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=265500, episode_reward=214.05 +/- 68.06
Episode length: 512.54 +/- 61.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 513        |
|    mean_reward          | 214        |
| time/                   |            |
|    total_timesteps      | 265500     |
| train/                  |            |
|    approx_kl            | 0.06475048 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.0005     |
|    loss                 | 737        |
|    n_updates            | 1397       |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 1.39e+03   |
----------------------------------------
Eval num_timesteps=266000, episode_reward=233.75 +/- 78.98
Episode length: 518.34 +/- 46.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 257      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 260      |
|    time_elapsed    | 7497     |
|    total_timesteps | 266240   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.07
Eval num_timesteps=266500, episode_reward=401.92 +/- 142.61
Episode length: 494.82 +/- 102.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 402         |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.045824975 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.437       |
|    learning_rate        | 0.0005      |
|    loss                 | 736         |
|    n_updates            | 1404        |
|    policy_gradient_loss | -0.0467     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=267000, episode_reward=361.70 +/- 148.93
Episode length: 462.60 +/- 136.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 267      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 261      |
|    time_elapsed    | 7530     |
|    total_timesteps | 267264   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=267500, episode_reward=309.86 +/- 126.01
Episode length: 516.88 +/- 56.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 267500      |
| train/                  |             |
|    approx_kl            | 0.051720247 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.0005      |
|    loss                 | 543         |
|    n_updates            | 1406        |
|    policy_gradient_loss | 0.014       |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=268000, episode_reward=272.29 +/- 99.63
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | 269      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 262      |
|    time_elapsed    | 7565     |
|    total_timesteps | 268288   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=268500, episode_reward=369.40 +/- 168.42
Episode length: 415.84 +/- 176.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.060991377 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0005      |
|    loss                 | 407         |
|    n_updates            | 1410        |
|    policy_gradient_loss | 0.0141      |
|    value_loss           | 846         |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=384.87 +/- 164.70
Episode length: 391.02 +/- 188.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 391      |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | 289      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 263      |
|    time_elapsed    | 7593     |
|    total_timesteps | 269312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=269500, episode_reward=230.83 +/- 94.10
Episode length: 507.22 +/- 87.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 231         |
| time/                   |             |
|    total_timesteps      | 269500      |
| train/                  |             |
|    approx_kl            | 0.043832608 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.49e+03    |
|    n_updates            | 1412        |
|    policy_gradient_loss | 0.0149      |
|    value_loss           | 3.02e+03    |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=248.01 +/- 85.82
Episode length: 518.98 +/- 42.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | 248      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 350      |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 264      |
|    time_elapsed    | 7628     |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=270500, episode_reward=202.99 +/- 25.19
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.028420052 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.912      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.0005      |
|    loss                 | 898         |
|    n_updates            | 1413        |
|    policy_gradient_loss | 0.019       |
|    value_loss           | 2.57e+03    |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=199.87 +/- 4.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 351      |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 265      |
|    time_elapsed    | 7664     |
|    total_timesteps | 271360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=271500, episode_reward=197.21 +/- 10.67
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 271500      |
| train/                  |             |
|    approx_kl            | 0.029752456 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.848      |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0005      |
|    loss                 | 701         |
|    n_updates            | 1414        |
|    policy_gradient_loss | 0.0208      |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=272000, episode_reward=199.16 +/- 3.32
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 266      |
|    time_elapsed    | 7700     |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=272500, episode_reward=196.61 +/- 10.38
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.050492562 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0005      |
|    loss                 | 641         |
|    n_updates            | 1417        |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=198.72 +/- 1.71
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 356      |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 267      |
|    time_elapsed    | 7736     |
|    total_timesteps | 273408   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=273500, episode_reward=278.59 +/- 128.36
Episode length: 428.14 +/- 152.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 273500      |
| train/                  |             |
|    approx_kl            | 0.052974828 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.877      |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.0005      |
|    loss                 | 254         |
|    n_updates            | 1419        |
|    policy_gradient_loss | 0.00527     |
|    value_loss           | 810         |
-----------------------------------------
Eval num_timesteps=274000, episode_reward=282.97 +/- 131.19
Episode length: 418.66 +/- 174.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 419      |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 268      |
|    time_elapsed    | 7765     |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=274500, episode_reward=220.60 +/- 90.26
Episode length: 378.14 +/- 196.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.055526506 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0005      |
|    loss                 | 696         |
|    n_updates            | 1423        |
|    policy_gradient_loss | -0.00831    |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=270.76 +/- 134.38
Episode length: 411.64 +/- 171.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 269      |
|    time_elapsed    | 7792     |
|    total_timesteps | 275456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=275500, episode_reward=317.35 +/- 168.55
Episode length: 184.84 +/- 150.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 317        |
| time/                   |            |
|    total_timesteps      | 275500     |
| train/                  |            |
|    approx_kl            | 0.03241049 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.868     |
|    explained_variance   | 0.391      |
|    learning_rate        | 0.0005     |
|    loss                 | 652        |
|    n_updates            | 1425       |
|    policy_gradient_loss | 0.00659    |
|    value_loss           | 1.54e+03   |
----------------------------------------
Eval num_timesteps=276000, episode_reward=318.28 +/- 175.90
Episode length: 228.04 +/- 173.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 342      |
|    ep_rew_mean     | 381      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 270      |
|    time_elapsed    | 7806     |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=276500, episode_reward=355.39 +/- 171.14
Episode length: 145.38 +/- 128.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 145        |
|    mean_reward          | 355        |
| time/                   |            |
|    total_timesteps      | 276500     |
| train/                  |            |
|    approx_kl            | 0.06458799 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.09e+03   |
|    n_updates            | 1429       |
|    policy_gradient_loss | -0.00605   |
|    value_loss           | 3.73e+03   |
----------------------------------------
Eval num_timesteps=277000, episode_reward=369.79 +/- 200.77
Episode length: 109.52 +/- 63.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=352.96 +/- 174.99
Episode length: 116.54 +/- 85.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 391      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 271      |
|    time_elapsed    | 7820     |
|    total_timesteps | 277504   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=278000, episode_reward=409.80 +/- 194.28
Episode length: 104.36 +/- 30.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 410         |
| time/                   |             |
|    total_timesteps      | 278000      |
| train/                  |             |
|    approx_kl            | 0.035723194 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.81e+03    |
|    n_updates            | 1432        |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 3.08e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=278500, episode_reward=375.38 +/- 190.40
Episode length: 124.58 +/- 102.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 403      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 272      |
|    time_elapsed    | 7829     |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=279000, episode_reward=330.49 +/- 182.00
Episode length: 88.22 +/- 21.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.2        |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.028457467 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.22e+03    |
|    n_updates            | 1433        |
|    policy_gradient_loss | 0.0182      |
|    value_loss           | 2.94e+03    |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=363.97 +/- 202.06
Episode length: 96.72 +/- 65.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 364      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 257      |
|    ep_rew_mean     | 422      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 273      |
|    time_elapsed    | 7836     |
|    total_timesteps | 279552   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=280000, episode_reward=302.46 +/- 199.38
Episode length: 84.32 +/- 65.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 84.3      |
|    mean_reward          | 302       |
| time/                   |           |
|    total_timesteps      | 280000    |
| train/                  |           |
|    approx_kl            | 0.0416492 |
|    clip_fraction        | 0.193     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.556    |
|    explained_variance   | 0.361     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.19e+03  |
|    n_updates            | 1436      |
|    policy_gradient_loss | 0.00745   |
|    value_loss           | 5.9e+03   |
---------------------------------------
Eval num_timesteps=280500, episode_reward=259.00 +/- 161.17
Episode length: 82.46 +/- 65.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.5     |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 274      |
|    time_elapsed    | 7842     |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=281000, episode_reward=366.82 +/- 198.99
Episode length: 123.60 +/- 103.27
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 124      |
|    mean_reward          | 367      |
| time/                   |          |
|    total_timesteps      | 281000   |
| train/                  |          |
|    approx_kl            | 0.019093 |
|    clip_fraction        | 0.179    |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -0.457   |
|    explained_variance   | 0.393    |
|    learning_rate        | 0.0005   |
|    loss                 | 2.04e+03 |
|    n_updates            | 1437     |
|    policy_gradient_loss | 0.0171   |
|    value_loss           | 4.19e+03 |
--------------------------------------
Eval num_timesteps=281500, episode_reward=370.48 +/- 177.65
Episode length: 97.26 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 275      |
|    time_elapsed    | 7851     |
|    total_timesteps | 281600   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.14
Eval num_timesteps=282000, episode_reward=281.04 +/- 168.37
Episode length: 273.92 +/- 203.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 281         |
| time/                   |             |
|    total_timesteps      | 282000      |
| train/                  |             |
|    approx_kl            | 0.041858595 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0005      |
|    loss                 | 682         |
|    n_updates            | 1439        |
|    policy_gradient_loss | 0.000457    |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=282500, episode_reward=278.97 +/- 153.77
Episode length: 211.34 +/- 187.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 279      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 424      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 276      |
|    time_elapsed    | 7868     |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=283000, episode_reward=382.02 +/- 199.70
Episode length: 122.28 +/- 103.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 122        |
|    mean_reward          | 382        |
| time/                   |            |
|    total_timesteps      | 283000     |
| train/                  |            |
|    approx_kl            | 0.03012009 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.682     |
|    explained_variance   | 0.473      |
|    learning_rate        | 0.0005     |
|    loss                 | 884        |
|    n_updates            | 1440       |
|    policy_gradient_loss | 0.0164     |
|    value_loss           | 1.91e+03   |
----------------------------------------
Eval num_timesteps=283500, episode_reward=345.67 +/- 197.50
Episode length: 112.96 +/- 87.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 277      |
|    time_elapsed    | 7876     |
|    total_timesteps | 283648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=284000, episode_reward=389.29 +/- 202.91
Episode length: 202.66 +/- 175.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 284000      |
| train/                  |             |
|    approx_kl            | 0.045102205 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.67e+03    |
|    n_updates            | 1442        |
|    policy_gradient_loss | 0.0196      |
|    value_loss           | 2.94e+03    |
-----------------------------------------
Eval num_timesteps=284500, episode_reward=363.04 +/- 158.69
Episode length: 171.84 +/- 149.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 278      |
|    time_elapsed    | 7890     |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=285000, episode_reward=334.52 +/- 184.36
Episode length: 142.06 +/- 135.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 335         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.033085868 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.22e+03    |
|    n_updates            | 1444        |
|    policy_gradient_loss | 0.00616     |
|    value_loss           | 5.56e+03    |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=371.21 +/- 190.26
Episode length: 139.20 +/- 131.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 395      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 279      |
|    time_elapsed    | 7900     |
|    total_timesteps | 285696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=286000, episode_reward=270.32 +/- 127.65
Episode length: 425.96 +/- 177.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 426        |
|    mean_reward          | 270        |
| time/                   |            |
|    total_timesteps      | 286000     |
| train/                  |            |
|    approx_kl            | 0.06094861 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.537     |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.35e+03   |
|    n_updates            | 1446       |
|    policy_gradient_loss | 0.0122     |
|    value_loss           | 2.67e+03   |
----------------------------------------
Eval num_timesteps=286500, episode_reward=313.31 +/- 163.82
Episode length: 398.60 +/- 184.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 399      |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 280      |
|    time_elapsed    | 7928     |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=287000, episode_reward=315.00 +/- 213.04
Episode length: 272.36 +/- 210.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 272        |
|    mean_reward          | 315        |
| time/                   |            |
|    total_timesteps      | 287000     |
| train/                  |            |
|    approx_kl            | 0.03129845 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.11e+03   |
|    n_updates            | 1448       |
|    policy_gradient_loss | 0.00964    |
|    value_loss           | 1.79e+03   |
----------------------------------------
Eval num_timesteps=287500, episode_reward=318.77 +/- 178.24
Episode length: 299.22 +/- 207.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 281      |
|    time_elapsed    | 7948     |
|    total_timesteps | 287744   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=288000, episode_reward=298.60 +/- 164.09
Episode length: 272.22 +/- 216.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 272        |
|    mean_reward          | 299        |
| time/                   |            |
|    total_timesteps      | 288000     |
| train/                  |            |
|    approx_kl            | 0.03546024 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0005     |
|    loss                 | 698        |
|    n_updates            | 1453       |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 1.73e+03   |
----------------------------------------
Eval num_timesteps=288500, episode_reward=244.49 +/- 149.50
Episode length: 227.92 +/- 205.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 282      |
|    time_elapsed    | 7966     |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=289000, episode_reward=307.79 +/- 142.81
Episode length: 440.18 +/- 162.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 440         |
|    mean_reward          | 308         |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.051614176 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.26e+03    |
|    n_updates            | 1457        |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 2.3e+03     |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=320.16 +/- 150.86
Episode length: 411.10 +/- 180.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 411      |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 283      |
|    time_elapsed    | 7995     |
|    total_timesteps | 289792   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=290000, episode_reward=235.18 +/- 80.11
Episode length: 516.44 +/- 59.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 235         |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.045755826 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.88e+03    |
|    n_updates            | 1462        |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 3.16e+03    |
-----------------------------------------
Eval num_timesteps=290500, episode_reward=238.45 +/- 88.37
Episode length: 516.52 +/- 59.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 284      |
|    time_elapsed    | 8030     |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.06
Eval num_timesteps=291000, episode_reward=365.35 +/- 175.94
Episode length: 432.72 +/- 164.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 433         |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.040679708 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0005      |
|    loss                 | 759         |
|    n_updates            | 1468        |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 1.59e+03    |
-----------------------------------------
Eval num_timesteps=291500, episode_reward=281.62 +/- 143.73
Episode length: 457.34 +/- 155.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 457      |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 285      |
|    time_elapsed    | 8061     |
|    total_timesteps | 291840   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=292000, episode_reward=377.27 +/- 202.36
Episode length: 334.60 +/- 198.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 335        |
|    mean_reward          | 377        |
| time/                   |            |
|    total_timesteps      | 292000     |
| train/                  |            |
|    approx_kl            | 0.06109932 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.704     |
|    explained_variance   | 0.456      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.06e+03   |
|    n_updates            | 1472       |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 1.32e+03   |
----------------------------------------
Eval num_timesteps=292500, episode_reward=288.00 +/- 161.73
Episode length: 430.66 +/- 168.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 286      |
|    time_elapsed    | 8087     |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=293000, episode_reward=263.51 +/- 136.27
Episode length: 486.48 +/- 115.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 486       |
|    mean_reward          | 264       |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0792827 |
|    clip_fraction        | 0.233     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.656    |
|    explained_variance   | 0.389     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.12e+03  |
|    n_updates            | 1476      |
|    policy_gradient_loss | -0.00758  |
|    value_loss           | 2.28e+03  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=260.03 +/- 132.56
Episode length: 469.34 +/- 138.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 424      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 287      |
|    time_elapsed    | 8119     |
|    total_timesteps | 293888   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=294000, episode_reward=465.27 +/- 248.52
Episode length: 119.96 +/- 70.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 465         |
| time/                   |             |
|    total_timesteps      | 294000      |
| train/                  |             |
|    approx_kl            | 0.045440048 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.61e+03    |
|    n_updates            | 1480        |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 2.45e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=294500, episode_reward=492.08 +/- 242.14
Episode length: 121.18 +/- 61.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 288      |
|    time_elapsed    | 8128     |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=295000, episode_reward=437.51 +/- 246.33
Episode length: 96.88 +/- 23.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.9        |
|    mean_reward          | 438         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.061166033 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.0005      |
|    loss                 | 2.46e+03    |
|    n_updates            | 1483        |
|    policy_gradient_loss | -7.39e-05   |
|    value_loss           | 3.4e+03     |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=454.05 +/- 236.20
Episode length: 109.28 +/- 63.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 463      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 289      |
|    time_elapsed    | 8136     |
|    total_timesteps | 295936   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=296000, episode_reward=523.17 +/- 205.73
Episode length: 124.04 +/- 61.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 523         |
| time/                   |             |
|    total_timesteps      | 296000      |
| train/                  |             |
|    approx_kl            | 0.064054005 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.0005      |
|    loss                 | 2.5e+03     |
|    n_updates            | 1485        |
|    policy_gradient_loss | 0.01        |
|    value_loss           | 4.61e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=296500, episode_reward=562.44 +/- 186.21
Episode length: 131.62 +/- 60.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 562      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 481      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 290      |
|    time_elapsed    | 8145     |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=297000, episode_reward=417.84 +/- 197.02
Episode length: 379.46 +/- 187.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 379        |
|    mean_reward          | 418        |
| time/                   |            |
|    total_timesteps      | 297000     |
| train/                  |            |
|    approx_kl            | 0.02926793 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.579     |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.0005     |
|    loss                 | 890        |
|    n_updates            | 1486       |
|    policy_gradient_loss | 0.019      |
|    value_loss           | 1.68e+03   |
----------------------------------------
Eval num_timesteps=297500, episode_reward=365.34 +/- 208.09
Episode length: 381.78 +/- 192.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 382      |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 498      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 291      |
|    time_elapsed    | 8172     |
|    total_timesteps | 297984   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=298000, episode_reward=469.55 +/- 233.79
Episode length: 182.10 +/- 154.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 470         |
| time/                   |             |
|    total_timesteps      | 298000      |
| train/                  |             |
|    approx_kl            | 0.042817064 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.653      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0005      |
|    loss                 | 686         |
|    n_updates            | 1489        |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=298500, episode_reward=451.02 +/- 239.31
Episode length: 184.38 +/- 162.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=486.93 +/- 266.73
Episode length: 161.18 +/- 137.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 495      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 292      |
|    time_elapsed    | 8190     |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=299500, episode_reward=457.43 +/- 236.14
Episode length: 306.70 +/- 196.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 307         |
|    mean_reward          | 457         |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.033112735 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.24e+03    |
|    n_updates            | 1490        |
|    policy_gradient_loss | 0.018       |
|    value_loss           | 4.44e+03    |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=378.18 +/- 228.64
Episode length: 361.82 +/- 194.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 362      |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 503      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 293      |
|    time_elapsed    | 8213     |
|    total_timesteps | 300032   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=300500, episode_reward=339.10 +/- 252.00
Episode length: 178.30 +/- 167.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 178        |
|    mean_reward          | 339        |
| time/                   |            |
|    total_timesteps      | 300500     |
| train/                  |            |
|    approx_kl            | 0.07225014 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.6       |
|    explained_variance   | 0.281      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.51e+03   |
|    n_updates            | 1492       |
|    policy_gradient_loss | 0.00338    |
|    value_loss           | 5.13e+03   |
----------------------------------------
Eval num_timesteps=301000, episode_reward=346.38 +/- 270.83
Episode length: 176.96 +/- 166.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 499      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 294      |
|    time_elapsed    | 8226     |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.15
Eval num_timesteps=301500, episode_reward=529.78 +/- 204.49
Episode length: 197.90 +/- 164.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 198        |
|    mean_reward          | 530        |
| time/                   |            |
|    total_timesteps      | 301500     |
| train/                  |            |
|    approx_kl            | 0.09299151 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0005     |
|    loss                 | 946        |
|    n_updates            | 1495       |
|    policy_gradient_loss | 0.0144     |
|    value_loss           | 1.73e+03   |
----------------------------------------
Eval num_timesteps=302000, episode_reward=534.19 +/- 196.40
Episode length: 234.48 +/- 184.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 534      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 295      |
|    time_elapsed    | 8241     |
|    total_timesteps | 302080   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=302500, episode_reward=567.58 +/- 176.23
Episode length: 206.40 +/- 160.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | 568         |
| time/                   |             |
|    total_timesteps      | 302500      |
| train/                  |             |
|    approx_kl            | 0.043863904 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.661      |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.98e+03    |
|    n_updates            | 1498        |
|    policy_gradient_loss | 0.00121     |
|    value_loss           | 5.35e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=303000, episode_reward=555.15 +/- 202.00
Episode length: 223.96 +/- 173.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 555      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 496      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 296      |
|    time_elapsed    | 8257     |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.08
Eval num_timesteps=303500, episode_reward=523.22 +/- 205.03
Episode length: 136.20 +/- 100.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 523         |
| time/                   |             |
|    total_timesteps      | 303500      |
| train/                  |             |
|    approx_kl            | 0.075227655 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.25e+03    |
|    n_updates            | 1503        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 3.55e+03    |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=543.09 +/- 185.09
Episode length: 163.32 +/- 134.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 163      |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 512      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 297      |
|    time_elapsed    | 8267     |
|    total_timesteps | 304128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=304500, episode_reward=487.61 +/- 234.76
Episode length: 178.44 +/- 152.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 178       |
|    mean_reward          | 488       |
| time/                   |           |
|    total_timesteps      | 304500    |
| train/                  |           |
|    approx_kl            | 0.0335576 |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.668    |
|    explained_variance   | 0.372     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.87e+03  |
|    n_updates            | 1505      |
|    policy_gradient_loss | 0.00573   |
|    value_loss           | 3.44e+03  |
---------------------------------------
Eval num_timesteps=305000, episode_reward=513.05 +/- 210.09
Episode length: 181.32 +/- 151.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 514      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 298      |
|    time_elapsed    | 8280     |
|    total_timesteps | 305152   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=305500, episode_reward=373.27 +/- 211.33
Episode length: 362.04 +/- 209.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 362        |
|    mean_reward          | 373        |
| time/                   |            |
|    total_timesteps      | 305500     |
| train/                  |            |
|    approx_kl            | 0.07093689 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.796     |
|    explained_variance   | 0.42       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.44e+03   |
|    n_updates            | 1508       |
|    policy_gradient_loss | -0.00189   |
|    value_loss           | 2.35e+03   |
----------------------------------------
Eval num_timesteps=306000, episode_reward=402.38 +/- 197.57
Episode length: 361.92 +/- 201.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 362      |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 497      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 299      |
|    time_elapsed    | 8305     |
|    total_timesteps | 306176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=306500, episode_reward=557.23 +/- 191.78
Episode length: 206.54 +/- 160.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 557         |
| time/                   |             |
|    total_timesteps      | 306500      |
| train/                  |             |
|    approx_kl            | 0.055765476 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.0005      |
|    loss                 | 722         |
|    n_updates            | 1510        |
|    policy_gradient_loss | 0.00689     |
|    value_loss           | 1.77e+03    |
-----------------------------------------
Eval num_timesteps=307000, episode_reward=503.97 +/- 223.55
Episode length: 209.18 +/- 169.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 475      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 300      |
|    time_elapsed    | 8320     |
|    total_timesteps | 307200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=307500, episode_reward=503.84 +/- 161.82
Episode length: 378.50 +/- 187.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 504         |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.018850405 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.88e+03    |
|    n_updates            | 1511        |
|    policy_gradient_loss | 0.00279     |
|    value_loss           | 4.52e+03    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=531.67 +/- 208.15
Episode length: 299.68 +/- 192.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 482      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 301      |
|    time_elapsed    | 8343     |
|    total_timesteps | 308224   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=308500, episode_reward=502.95 +/- 219.83
Episode length: 389.66 +/- 189.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 390         |
|    mean_reward          | 503         |
| time/                   |             |
|    total_timesteps      | 308500      |
| train/                  |             |
|    approx_kl            | 0.034691937 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.68e+03    |
|    n_updates            | 1514        |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 3.25e+03    |
-----------------------------------------
Eval num_timesteps=309000, episode_reward=450.74 +/- 186.53
Episode length: 413.08 +/- 181.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 482      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 302      |
|    time_elapsed    | 8370     |
|    total_timesteps | 309248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=309500, episode_reward=271.58 +/- 184.98
Episode length: 350.56 +/- 206.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | 272         |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.029252982 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0005      |
|    loss                 | 683         |
|    n_updates            | 1515        |
|    policy_gradient_loss | 0.0199      |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=326.76 +/- 191.30
Episode length: 414.18 +/- 188.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 471      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 303      |
|    time_elapsed    | 8396     |
|    total_timesteps | 310272   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.13
Eval num_timesteps=310500, episode_reward=351.23 +/- 138.47
Episode length: 502.68 +/- 78.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 503        |
|    mean_reward          | 351        |
| time/                   |            |
|    total_timesteps      | 310500     |
| train/                  |            |
|    approx_kl            | 0.07165249 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.763     |
|    explained_variance   | 0.454      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.39e+03   |
|    n_updates            | 1518       |
|    policy_gradient_loss | -0.00107   |
|    value_loss           | 1.82e+03   |
----------------------------------------
Eval num_timesteps=311000, episode_reward=291.02 +/- 138.89
Episode length: 482.76 +/- 118.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 483      |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 469      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 304      |
|    time_elapsed    | 8430     |
|    total_timesteps | 311296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=311500, episode_reward=297.43 +/- 193.02
Episode length: 383.06 +/- 196.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.061342992 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0005      |
|    loss                 | 368         |
|    n_updates            | 1520        |
|    policy_gradient_loss | 0.0123      |
|    value_loss           | 798         |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=283.61 +/- 168.48
Episode length: 370.46 +/- 195.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 370      |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 471      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 305      |
|    time_elapsed    | 8455     |
|    total_timesteps | 312320   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=312500, episode_reward=499.83 +/- 178.34
Episode length: 476.64 +/- 131.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 477        |
|    mean_reward          | 500        |
| time/                   |            |
|    total_timesteps      | 312500     |
| train/                  |            |
|    approx_kl            | 0.06575191 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.751     |
|    explained_variance   | 0.517      |
|    learning_rate        | 0.0005     |
|    loss                 | 956        |
|    n_updates            | 1522       |
|    policy_gradient_loss | 0.0095     |
|    value_loss           | 1.64e+03   |
----------------------------------------
Eval num_timesteps=313000, episode_reward=439.30 +/- 164.52
Episode length: 495.46 +/- 102.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 466      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 306      |
|    time_elapsed    | 8489     |
|    total_timesteps | 313344   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=313500, episode_reward=467.11 +/- 176.30
Episode length: 451.44 +/- 157.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 451        |
|    mean_reward          | 467        |
| time/                   |            |
|    total_timesteps      | 313500     |
| train/                  |            |
|    approx_kl            | 0.03549964 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0005     |
|    loss                 | 830        |
|    n_updates            | 1526       |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 1.42e+03   |
----------------------------------------
Eval num_timesteps=314000, episode_reward=459.88 +/- 163.57
Episode length: 437.88 +/- 156.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 461      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 307      |
|    time_elapsed    | 8519     |
|    total_timesteps | 314368   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=314500, episode_reward=550.63 +/- 200.33
Episode length: 292.70 +/- 198.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 293         |
|    mean_reward          | 551         |
| time/                   |             |
|    total_timesteps      | 314500      |
| train/                  |             |
|    approx_kl            | 0.048678365 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0005      |
|    loss                 | 393         |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 857         |
-----------------------------------------
Eval num_timesteps=315000, episode_reward=515.62 +/- 196.35
Episode length: 325.22 +/- 201.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 325      |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 469      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 308      |
|    time_elapsed    | 8541     |
|    total_timesteps | 315392   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=315500, episode_reward=543.00 +/- 231.53
Episode length: 217.10 +/- 175.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 543        |
| time/                   |            |
|    total_timesteps      | 315500     |
| train/                  |            |
|    approx_kl            | 0.05986455 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.1e+03    |
|    n_updates            | 1534       |
|    policy_gradient_loss | -0.00637   |
|    value_loss           | 1.82e+03   |
----------------------------------------
Eval num_timesteps=316000, episode_reward=583.15 +/- 182.76
Episode length: 253.20 +/- 179.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 461      |
| time/              |          |
|    fps             | 36       |
|    iterations      | 309      |
|    time_elapsed    | 8557     |
|    total_timesteps | 316416   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=316500, episode_reward=548.93 +/- 207.29
Episode length: 231.38 +/- 170.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 231        |
|    mean_reward          | 549        |
| time/                   |            |
|    total_timesteps      | 316500     |
| train/                  |            |
|    approx_kl            | 0.04945223 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.702     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.23e+03   |
|    n_updates            | 1537       |
|    policy_gradient_loss | -0.00709   |
|    value_loss           | 2.32e+03   |
----------------------------------------
Eval num_timesteps=317000, episode_reward=584.30 +/- 190.40
Episode length: 203.32 +/- 154.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 468      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 310      |
|    time_elapsed    | 8572     |
|    total_timesteps | 317440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=317500, episode_reward=526.68 +/- 183.28
Episode length: 200.90 +/- 163.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 527        |
| time/                   |            |
|    total_timesteps      | 317500     |
| train/                  |            |
|    approx_kl            | 0.02519385 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.359      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.81e+03   |
|    n_updates            | 1538       |
|    policy_gradient_loss | 0.02       |
|    value_loss           | 3.45e+03   |
----------------------------------------
Eval num_timesteps=318000, episode_reward=510.80 +/- 208.38
Episode length: 193.52 +/- 161.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 478      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 311      |
|    time_elapsed    | 8586     |
|    total_timesteps | 318464   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=318500, episode_reward=524.09 +/- 198.07
Episode length: 154.76 +/- 125.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 155        |
|    mean_reward          | 524        |
| time/                   |            |
|    total_timesteps      | 318500     |
| train/                  |            |
|    approx_kl            | 0.05845681 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.613     |
|    explained_variance   | 0.51       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.41e+03   |
|    n_updates            | 1542       |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 2.39e+03   |
----------------------------------------
Eval num_timesteps=319000, episode_reward=552.96 +/- 207.44
Episode length: 143.94 +/- 115.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | 553      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 493      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 312      |
|    time_elapsed    | 8598     |
|    total_timesteps | 319488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=319500, episode_reward=417.76 +/- 181.68
Episode length: 114.70 +/- 105.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 418         |
| time/                   |             |
|    total_timesteps      | 319500      |
| train/                  |             |
|    approx_kl            | 0.037136003 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.87e+03    |
|    n_updates            | 1543        |
|    policy_gradient_loss | 0.0323      |
|    value_loss           | 3.41e+03    |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=442.49 +/- 178.05
Episode length: 99.26 +/- 63.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=427.46 +/- 190.17
Episode length: 88.04 +/- 18.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88       |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 495      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 313      |
|    time_elapsed    | 8609     |
|    total_timesteps | 320512   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=321000, episode_reward=501.17 +/- 177.19
Episode length: 121.40 +/- 104.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 121        |
|    mean_reward          | 501        |
| time/                   |            |
|    total_timesteps      | 321000     |
| train/                  |            |
|    approx_kl            | 0.07083571 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.12e+03   |
|    n_updates            | 1546       |
|    policy_gradient_loss | 0.00376    |
|    value_loss           | 2.43e+03   |
----------------------------------------
Eval num_timesteps=321500, episode_reward=486.94 +/- 170.35
Episode length: 101.78 +/- 62.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 314      |
|    time_elapsed    | 8618     |
|    total_timesteps | 321536   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=322000, episode_reward=475.80 +/- 195.42
Episode length: 100.42 +/- 63.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 100        |
|    mean_reward          | 476        |
| time/                   |            |
|    total_timesteps      | 322000     |
| train/                  |            |
|    approx_kl            | 0.03966321 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.454     |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.33e+03   |
|    n_updates            | 1549       |
|    policy_gradient_loss | -0.00244   |
|    value_loss           | 6.27e+03   |
----------------------------------------
Eval num_timesteps=322500, episode_reward=468.93 +/- 198.68
Episode length: 98.22 +/- 63.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 315      |
|    time_elapsed    | 8625     |
|    total_timesteps | 322560   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=323000, episode_reward=503.14 +/- 185.79
Episode length: 153.82 +/- 138.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 154         |
|    mean_reward          | 503         |
| time/                   |             |
|    total_timesteps      | 323000      |
| train/                  |             |
|    approx_kl            | 0.066916764 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.33e+03    |
|    n_updates            | 1552        |
|    policy_gradient_loss | -0.00697    |
|    value_loss           | 4.61e+03    |
-----------------------------------------
Eval num_timesteps=323500, episode_reward=498.50 +/- 181.60
Episode length: 126.94 +/- 102.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 498      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 316      |
|    time_elapsed    | 8636     |
|    total_timesteps | 323584   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=324000, episode_reward=454.65 +/- 177.43
Episode length: 113.78 +/- 85.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 455         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.042364202 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.639      |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.67e+03    |
|    n_updates            | 1555        |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 3.75e+03    |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=392.01 +/- 202.02
Episode length: 115.84 +/- 104.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 492      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 317      |
|    time_elapsed    | 8644     |
|    total_timesteps | 324608   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.10
Eval num_timesteps=325000, episode_reward=466.69 +/- 212.37
Episode length: 98.98 +/- 25.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99         |
|    mean_reward          | 467        |
| time/                   |            |
|    total_timesteps      | 325000     |
| train/                  |            |
|    approx_kl            | 0.05866735 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.559     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.26e+03   |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 2.38e+03   |
----------------------------------------
Eval num_timesteps=325500, episode_reward=496.79 +/- 166.35
Episode length: 101.12 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 485      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 318      |
|    time_elapsed    | 8652     |
|    total_timesteps | 325632   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=326000, episode_reward=495.31 +/- 209.09
Episode length: 115.68 +/- 85.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 495         |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.042849135 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.31e+03    |
|    n_updates            | 1563        |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 2.12e+03    |
-----------------------------------------
Eval num_timesteps=326500, episode_reward=529.44 +/- 179.52
Episode length: 134.32 +/- 116.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 529      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 319      |
|    time_elapsed    | 8661     |
|    total_timesteps | 326656   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=327000, episode_reward=529.84 +/- 195.24
Episode length: 120.28 +/- 85.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 530         |
| time/                   |             |
|    total_timesteps      | 327000      |
| train/                  |             |
|    approx_kl            | 0.038454227 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.49e+03    |
|    n_updates            | 1567        |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 5.32e+03    |
-----------------------------------------
Eval num_timesteps=327500, episode_reward=521.75 +/- 194.75
Episode length: 102.82 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 474      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 320      |
|    time_elapsed    | 8670     |
|    total_timesteps | 327680   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=328000, episode_reward=549.67 +/- 206.56
Episode length: 211.38 +/- 177.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 550         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.043072753 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.446      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.26e+03    |
|    n_updates            | 1571        |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 4.92e+03    |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=561.99 +/- 206.29
Episode length: 153.76 +/- 128.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 562      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 479      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 321      |
|    time_elapsed    | 8683     |
|    total_timesteps | 328704   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=329000, episode_reward=567.61 +/- 155.62
Episode length: 253.38 +/- 187.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 568         |
| time/                   |             |
|    total_timesteps      | 329000      |
| train/                  |             |
|    approx_kl            | 0.036841713 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.46e+03    |
|    n_updates            | 1573        |
|    policy_gradient_loss | 0.00437     |
|    value_loss           | 3.74e+03    |
-----------------------------------------
Eval num_timesteps=329500, episode_reward=571.05 +/- 157.37
Episode length: 275.74 +/- 195.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 571      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 483      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 322      |
|    time_elapsed    | 8701     |
|    total_timesteps | 329728   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.11
Eval num_timesteps=330000, episode_reward=578.21 +/- 190.93
Episode length: 160.66 +/- 135.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 161        |
|    mean_reward          | 578        |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.05231112 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.567     |
|    explained_variance   | 0.521      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.13e+03   |
|    n_updates            | 1578       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 2.44e+03   |
----------------------------------------
Eval num_timesteps=330500, episode_reward=557.13 +/- 171.56
Episode length: 180.76 +/- 153.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 557      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 495      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 323      |
|    time_elapsed    | 8713     |
|    total_timesteps | 330752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=331000, episode_reward=507.22 +/- 167.50
Episode length: 105.74 +/- 61.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 507         |
| time/                   |             |
|    total_timesteps      | 331000      |
| train/                  |             |
|    approx_kl            | 0.030163443 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.35e+03    |
|    n_updates            | 1579        |
|    policy_gradient_loss | 0.0195      |
|    value_loss           | 3.19e+03    |
-----------------------------------------
Eval num_timesteps=331500, episode_reward=499.49 +/- 202.64
Episode length: 111.90 +/- 86.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 491      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 324      |
|    time_elapsed    | 8722     |
|    total_timesteps | 331776   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.12
Eval num_timesteps=332000, episode_reward=539.15 +/- 174.26
Episode length: 105.74 +/- 61.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 539         |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.060257055 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.41e+03    |
|    n_updates            | 1582        |
|    policy_gradient_loss | 0.00216     |
|    value_loss           | 3.73e+03    |
-----------------------------------------
Eval num_timesteps=332500, episode_reward=451.89 +/- 203.38
Episode length: 89.44 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.4     |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 502      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 325      |
|    time_elapsed    | 8729     |
|    total_timesteps | 332800   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=333000, episode_reward=549.03 +/- 197.77
Episode length: 133.08 +/- 95.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 133        |
|    mean_reward          | 549        |
| time/                   |            |
|    total_timesteps      | 333000     |
| train/                  |            |
|    approx_kl            | 0.07313673 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.69e+03   |
|    n_updates            | 1585       |
|    policy_gradient_loss | 0.0114     |
|    value_loss           | 3.31e+03   |
----------------------------------------
Eval num_timesteps=333500, episode_reward=597.68 +/- 174.03
Episode length: 145.10 +/- 109.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | 598      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 519      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 326      |
|    time_elapsed    | 8739     |
|    total_timesteps | 333824   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=334000, episode_reward=519.46 +/- 201.90
Episode length: 163.76 +/- 136.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 164       |
|    mean_reward          | 519       |
| time/                   |           |
|    total_timesteps      | 334000    |
| train/                  |           |
|    approx_kl            | 0.0813677 |
|    clip_fraction        | 0.199     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.464    |
|    explained_variance   | 0.527     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.3e+03   |
|    n_updates            | 1588      |
|    policy_gradient_loss | 0.0155    |
|    value_loss           | 3.76e+03  |
---------------------------------------
Eval num_timesteps=334500, episode_reward=537.02 +/- 189.94
Episode length: 125.00 +/- 85.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 537      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 525      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 327      |
|    time_elapsed    | 8750     |
|    total_timesteps | 334848   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=335000, episode_reward=392.66 +/- 207.47
Episode length: 121.16 +/- 109.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 121        |
|    mean_reward          | 393        |
| time/                   |            |
|    total_timesteps      | 335000     |
| train/                  |            |
|    approx_kl            | 0.05382517 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.665     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.36e+03   |
|    n_updates            | 1591       |
|    policy_gradient_loss | 0.00319    |
|    value_loss           | 2.02e+03   |
----------------------------------------
Eval num_timesteps=335500, episode_reward=425.63 +/- 208.48
Episode length: 114.08 +/- 94.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 531      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 328      |
|    time_elapsed    | 8759     |
|    total_timesteps | 335872   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=336000, episode_reward=461.44 +/- 194.15
Episode length: 103.80 +/- 41.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 461         |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.047383584 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.03e+03    |
|    n_updates            | 1594        |
|    policy_gradient_loss | 0.00706     |
|    value_loss           | 3e+03       |
-----------------------------------------
Eval num_timesteps=336500, episode_reward=445.04 +/- 217.08
Episode length: 112.70 +/- 79.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 329      |
|    time_elapsed    | 8767     |
|    total_timesteps | 336896   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=337000, episode_reward=552.77 +/- 203.25
Episode length: 208.90 +/- 179.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 553        |
| time/                   |            |
|    total_timesteps      | 337000     |
| train/                  |            |
|    approx_kl            | 0.07090168 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.52      |
|    explained_variance   | 0.434      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.81e+03   |
|    n_updates            | 1596       |
|    policy_gradient_loss | 0.00379    |
|    value_loss           | 3.5e+03    |
----------------------------------------
Eval num_timesteps=337500, episode_reward=510.43 +/- 202.28
Episode length: 213.28 +/- 176.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 525      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 330      |
|    time_elapsed    | 8782     |
|    total_timesteps | 337920   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=338000, episode_reward=583.96 +/- 226.60
Episode length: 250.80 +/- 189.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 584         |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.037369438 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.635      |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0005      |
|    loss                 | 696         |
|    n_updates            | 1599        |
|    policy_gradient_loss | 0.00388     |
|    value_loss           | 2.42e+03    |
-----------------------------------------
Eval num_timesteps=338500, episode_reward=624.06 +/- 175.36
Episode length: 245.20 +/- 176.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 520      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 331      |
|    time_elapsed    | 8799     |
|    total_timesteps | 338944   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=339000, episode_reward=480.91 +/- 186.52
Episode length: 467.16 +/- 143.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 467         |
|    mean_reward          | 481         |
| time/                   |             |
|    total_timesteps      | 339000      |
| train/                  |             |
|    approx_kl            | 0.090330735 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.19e+03    |
|    n_updates            | 1602        |
|    policy_gradient_loss | 0.00796     |
|    value_loss           | 3.37e+03    |
-----------------------------------------
Eval num_timesteps=339500, episode_reward=454.33 +/- 211.76
Episode length: 425.54 +/- 178.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 426      |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 525      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 332      |
|    time_elapsed    | 8829     |
|    total_timesteps | 339968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=340000, episode_reward=400.76 +/- 152.33
Episode length: 479.00 +/- 125.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 479        |
|    mean_reward          | 401        |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.06996329 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.747     |
|    explained_variance   | 0.471      |
|    learning_rate        | 0.0005     |
|    loss                 | 642        |
|    n_updates            | 1604       |
|    policy_gradient_loss | 0.025      |
|    value_loss           | 1.64e+03   |
----------------------------------------
Eval num_timesteps=340500, episode_reward=419.13 +/- 157.26
Episode length: 513.68 +/- 59.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 526      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 333      |
|    time_elapsed    | 8863     |
|    total_timesteps | 340992   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=341000, episode_reward=435.68 +/- 135.08
Episode length: 499.94 +/- 99.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 500        |
|    mean_reward          | 436        |
| time/                   |            |
|    total_timesteps      | 341000     |
| train/                  |            |
|    approx_kl            | 0.06520005 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.835     |
|    explained_variance   | 0.467      |
|    learning_rate        | 0.0005     |
|    loss                 | 472        |
|    n_updates            | 1608       |
|    policy_gradient_loss | -0.00819   |
|    value_loss           | 920        |
----------------------------------------
Eval num_timesteps=341500, episode_reward=445.79 +/- 148.13
Episode length: 491.90 +/- 107.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 492      |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=458.79 +/- 183.01
Episode length: 495.50 +/- 100.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 530      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 334      |
|    time_elapsed    | 8914     |
|    total_timesteps | 342016   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=342500, episode_reward=454.37 +/- 173.41
Episode length: 487.82 +/- 115.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 488         |
|    mean_reward          | 454         |
| time/                   |             |
|    total_timesteps      | 342500      |
| train/                  |             |
|    approx_kl            | 0.040130094 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.635      |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0005      |
|    loss                 | 830         |
|    n_updates            | 1612        |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=343000, episode_reward=439.26 +/- 165.59
Episode length: 470.46 +/- 141.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 531      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 335      |
|    time_elapsed    | 8946     |
|    total_timesteps | 343040   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=343500, episode_reward=423.38 +/- 138.68
Episode length: 518.00 +/- 49.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 518        |
|    mean_reward          | 423        |
| time/                   |            |
|    total_timesteps      | 343500     |
| train/                  |            |
|    approx_kl            | 0.05857911 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.403      |
|    learning_rate        | 0.0005     |
|    loss                 | 698        |
|    n_updates            | 1616       |
|    policy_gradient_loss | -0.00654   |
|    value_loss           | 1.6e+03    |
----------------------------------------
Eval num_timesteps=344000, episode_reward=434.45 +/- 131.27
Episode length: 517.50 +/- 52.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 534      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 336      |
|    time_elapsed    | 8981     |
|    total_timesteps | 344064   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=344500, episode_reward=380.94 +/- 86.14
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 381        |
| time/                   |            |
|    total_timesteps      | 344500     |
| train/                  |            |
|    approx_kl            | 0.06623313 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.572     |
|    explained_variance   | 0.535      |
|    learning_rate        | 0.0005     |
|    loss                 | 529        |
|    n_updates            | 1619       |
|    policy_gradient_loss | 0.00509    |
|    value_loss           | 974        |
----------------------------------------
Eval num_timesteps=345000, episode_reward=355.93 +/- 118.09
Episode length: 508.36 +/- 81.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 534      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 337      |
|    time_elapsed    | 9016     |
|    total_timesteps | 345088   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=345500, episode_reward=315.93 +/- 134.79
Episode length: 515.64 +/- 65.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 345500      |
| train/                  |             |
|    approx_kl            | 0.053245436 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.0005      |
|    loss                 | 525         |
|    n_updates            | 1623        |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=346000, episode_reward=313.42 +/- 111.00
Episode length: 508.44 +/- 81.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 533      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 338      |
|    time_elapsed    | 9051     |
|    total_timesteps | 346112   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.08
Eval num_timesteps=346500, episode_reward=381.55 +/- 108.25
Episode length: 499.72 +/- 100.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 500        |
|    mean_reward          | 382        |
| time/                   |            |
|    total_timesteps      | 346500     |
| train/                  |            |
|    approx_kl            | 0.05635806 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.513      |
|    learning_rate        | 0.0005     |
|    loss                 | 559        |
|    n_updates            | 1629       |
|    policy_gradient_loss | -0.0261    |
|    value_loss           | 1.08e+03   |
----------------------------------------
Eval num_timesteps=347000, episode_reward=373.31 +/- 116.58
Episode length: 515.66 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 539      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 339      |
|    time_elapsed    | 9086     |
|    total_timesteps | 347136   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.13
Eval num_timesteps=347500, episode_reward=315.80 +/- 113.85
Episode length: 507.16 +/- 87.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 347500      |
| train/                  |             |
|    approx_kl            | 0.050415915 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0005      |
|    loss                 | 524         |
|    n_updates            | 1632        |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=348000, episode_reward=290.58 +/- 122.07
Episode length: 497.74 +/- 107.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 545      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 340      |
|    time_elapsed    | 9121     |
|    total_timesteps | 348160   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=348500, episode_reward=338.05 +/- 111.87
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 338         |
| time/                   |             |
|    total_timesteps      | 348500      |
| train/                  |             |
|    approx_kl            | 0.043268237 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.487      |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0005      |
|    loss                 | 327         |
|    n_updates            | 1634        |
|    policy_gradient_loss | 0.00883     |
|    value_loss           | 766         |
-----------------------------------------
Eval num_timesteps=349000, episode_reward=357.94 +/- 113.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 541      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 341      |
|    time_elapsed    | 9156     |
|    total_timesteps | 349184   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=349500, episode_reward=305.81 +/- 108.65
Episode length: 516.18 +/- 61.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 349500      |
| train/                  |             |
|    approx_kl            | 0.041904505 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.0005      |
|    loss                 | 889         |
|    n_updates            | 1636        |
|    policy_gradient_loss | 0.0094      |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=301.56 +/- 114.12
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 542      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 342      |
|    time_elapsed    | 9191     |
|    total_timesteps | 350208   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=350500, episode_reward=253.23 +/- 106.76
Episode length: 507.26 +/- 86.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 253         |
| time/                   |             |
|    total_timesteps      | 350500      |
| train/                  |             |
|    approx_kl            | 0.028641066 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0005      |
|    loss                 | 312         |
|    n_updates            | 1638        |
|    policy_gradient_loss | 0.00624     |
|    value_loss           | 797         |
-----------------------------------------
Eval num_timesteps=351000, episode_reward=241.77 +/- 104.11
Episode length: 498.16 +/- 106.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 547      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 343      |
|    time_elapsed    | 9226     |
|    total_timesteps | 351232   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=351500, episode_reward=234.16 +/- 74.20
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 234         |
| time/                   |             |
|    total_timesteps      | 351500      |
| train/                  |             |
|    approx_kl            | 0.058762807 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0005      |
|    loss                 | 363         |
|    n_updates            | 1642        |
|    policy_gradient_loss | -0.00944    |
|    value_loss           | 792         |
-----------------------------------------
Eval num_timesteps=352000, episode_reward=250.61 +/- 91.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 547      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 344      |
|    time_elapsed    | 9262     |
|    total_timesteps | 352256   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=352500, episode_reward=258.76 +/- 96.17
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 352500      |
| train/                  |             |
|    approx_kl            | 0.034360074 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0005      |
|    loss                 | 548         |
|    n_updates            | 1647        |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=353000, episode_reward=239.48 +/- 83.35
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 239      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 345      |
|    time_elapsed    | 9297     |
|    total_timesteps | 353280   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=353500, episode_reward=195.66 +/- 10.42
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 196        |
| time/                   |            |
|    total_timesteps      | 353500     |
| train/                  |            |
|    approx_kl            | 0.04677076 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.787     |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.0005     |
|    loss                 | 507        |
|    n_updates            | 1651       |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 865        |
----------------------------------------
Eval num_timesteps=354000, episode_reward=196.70 +/- 4.47
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 541      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 346      |
|    time_elapsed    | 9333     |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=354500, episode_reward=197.36 +/- 8.13
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.049725223 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.85       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0005      |
|    loss                 | 162         |
|    n_updates            | 1655        |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 316         |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=198.33 +/- 2.08
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 254      |
|    ep_rew_mean     | 537      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 347      |
|    time_elapsed    | 9369     |
|    total_timesteps | 355328   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=355500, episode_reward=209.55 +/- 46.89
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 210         |
| time/                   |             |
|    total_timesteps      | 355500      |
| train/                  |             |
|    approx_kl            | 0.056306157 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.901      |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0005      |
|    loss                 | 493         |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=356000, episode_reward=218.59 +/- 66.14
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 259      |
|    ep_rew_mean     | 541      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 348      |
|    time_elapsed    | 9404     |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=356500, episode_reward=295.11 +/- 112.84
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 295         |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.044611305 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.885      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0005      |
|    loss                 | 384         |
|    n_updates            | 1662        |
|    policy_gradient_loss | 0.00483     |
|    value_loss           | 795         |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=310.90 +/- 115.30
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 538      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 349      |
|    time_elapsed    | 9440     |
|    total_timesteps | 357376   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=357500, episode_reward=205.77 +/- 35.43
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 357500      |
| train/                  |             |
|    approx_kl            | 0.046990804 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.835      |
|    explained_variance   | 0.478       |
|    learning_rate        | 0.0005      |
|    loss                 | 480         |
|    n_updates            | 1666        |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 960         |
-----------------------------------------
Eval num_timesteps=358000, episode_reward=203.67 +/- 10.16
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 278      |
|    ep_rew_mean     | 537      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 350      |
|    time_elapsed    | 9476     |
|    total_timesteps | 358400   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=358500, episode_reward=245.24 +/- 86.69
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 245        |
| time/                   |            |
|    total_timesteps      | 358500     |
| train/                  |            |
|    approx_kl            | 0.06369999 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.803     |
|    explained_variance   | 0.456      |
|    learning_rate        | 0.0005     |
|    loss                 | 487        |
|    n_updates            | 1669       |
|    policy_gradient_loss | 0.0122     |
|    value_loss           | 914        |
----------------------------------------
Eval num_timesteps=359000, episode_reward=257.89 +/- 95.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | 540      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 351      |
|    time_elapsed    | 9511     |
|    total_timesteps | 359424   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.07
Eval num_timesteps=359500, episode_reward=294.94 +/- 107.18
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 295         |
| time/                   |             |
|    total_timesteps      | 359500      |
| train/                  |             |
|    approx_kl            | 0.050247867 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0005      |
|    loss                 | 779         |
|    n_updates            | 1675        |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=300.28 +/- 113.23
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 551      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 352      |
|    time_elapsed    | 9547     |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.14
Eval num_timesteps=360500, episode_reward=309.92 +/- 102.77
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 360500      |
| train/                  |             |
|    approx_kl            | 0.046748407 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.741      |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0005      |
|    loss                 | 630         |
|    n_updates            | 1677        |
|    policy_gradient_loss | 0.00673     |
|    value_loss           | 1.2e+03     |
-----------------------------------------
Eval num_timesteps=361000, episode_reward=328.69 +/- 129.44
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 558      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 353      |
|    time_elapsed    | 9583     |
|    total_timesteps | 361472   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.09
Eval num_timesteps=361500, episode_reward=334.72 +/- 114.40
Episode length: 517.22 +/- 54.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 517        |
|    mean_reward          | 335        |
| time/                   |            |
|    total_timesteps      | 361500     |
| train/                  |            |
|    approx_kl            | 0.06694427 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.676     |
|    explained_variance   | 0.454      |
|    learning_rate        | 0.0005     |
|    loss                 | 979        |
|    n_updates            | 1682       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 1.19e+03   |
----------------------------------------
Eval num_timesteps=362000, episode_reward=346.42 +/- 113.04
Episode length: 517.38 +/- 53.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 560      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 354      |
|    time_elapsed    | 9618     |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=362500, episode_reward=309.99 +/- 114.63
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 310        |
| time/                   |            |
|    total_timesteps      | 362500     |
| train/                  |            |
|    approx_kl            | 0.06197002 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.477      |
|    learning_rate        | 0.0005     |
|    loss                 | 469        |
|    n_updates            | 1686       |
|    policy_gradient_loss | 0.014      |
|    value_loss           | 1.25e+03   |
----------------------------------------
Eval num_timesteps=363000, episode_reward=313.71 +/- 111.48
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=303.98 +/- 111.26
Episode length: 516.08 +/- 62.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 560      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 355      |
|    time_elapsed    | 9671     |
|    total_timesteps | 363520   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=364000, episode_reward=375.82 +/- 123.99
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 376        |
| time/                   |            |
|    total_timesteps      | 364000     |
| train/                  |            |
|    approx_kl            | 0.05219813 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.791     |
|    explained_variance   | 0.424      |
|    learning_rate        | 0.0005     |
|    loss                 | 698        |
|    n_updates            | 1688       |
|    policy_gradient_loss | 0.0161     |
|    value_loss           | 1.26e+03   |
----------------------------------------
Eval num_timesteps=364500, episode_reward=366.26 +/- 133.37
Episode length: 500.24 +/- 98.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 322      |
|    ep_rew_mean     | 558      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 356      |
|    time_elapsed    | 9705     |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=365000, episode_reward=464.59 +/- 162.58
Episode length: 496.24 +/- 97.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 496         |
|    mean_reward          | 465         |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.045382284 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.805      |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0005      |
|    loss                 | 761         |
|    n_updates            | 1691        |
|    policy_gradient_loss | 0.00473     |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=416.64 +/- 169.35
Episode length: 472.34 +/- 131.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 472      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 562      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 357      |
|    time_elapsed    | 9738     |
|    total_timesteps | 365568   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=366000, episode_reward=396.52 +/- 211.64
Episode length: 478.54 +/- 126.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 366000      |
| train/                  |             |
|    approx_kl            | 0.045905627 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0005      |
|    loss                 | 271         |
|    n_updates            | 1694        |
|    policy_gradient_loss | 0.000189    |
|    value_loss           | 810         |
-----------------------------------------
Eval num_timesteps=366500, episode_reward=371.60 +/- 195.08
Episode length: 455.76 +/- 145.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 456      |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 567      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 358      |
|    time_elapsed    | 9769     |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=367000, episode_reward=471.78 +/- 159.71
Episode length: 486.32 +/- 116.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 472         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.034491826 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0005      |
|    loss                 | 592         |
|    n_updates            | 1697        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=438.90 +/- 179.66
Episode length: 418.10 +/- 173.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 418      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 346      |
|    ep_rew_mean     | 577      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 359      |
|    time_elapsed    | 9800     |
|    total_timesteps | 367616   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.09
Eval num_timesteps=368000, episode_reward=460.71 +/- 156.68
Episode length: 457.60 +/- 144.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | 461         |
| time/                   |             |
|    total_timesteps      | 368000      |
| train/                  |             |
|    approx_kl            | 0.048709847 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.08e+03    |
|    n_updates            | 1701        |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=368500, episode_reward=461.30 +/- 157.63
Episode length: 449.78 +/- 151.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 580      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 360      |
|    time_elapsed    | 9834     |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=369000, episode_reward=341.79 +/- 113.91
Episode length: 494.56 +/- 103.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | 342         |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.035476808 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0005      |
|    loss                 | 517         |
|    n_updates            | 1702        |
|    policy_gradient_loss | 0.029       |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=369500, episode_reward=350.32 +/- 115.92
Episode length: 500.34 +/- 98.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | 573      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 361      |
|    time_elapsed    | 9867     |
|    total_timesteps | 369664   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.10
Eval num_timesteps=370000, episode_reward=369.33 +/- 136.63
Episode length: 480.90 +/- 121.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 481        |
|    mean_reward          | 369        |
| time/                   |            |
|    total_timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.07307241 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.454      |
|    learning_rate        | 0.0005     |
|    loss                 | 806        |
|    n_updates            | 1708       |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 1.35e+03   |
----------------------------------------
Eval num_timesteps=370500, episode_reward=361.59 +/- 141.50
Episode length: 473.46 +/- 128.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 369      |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 362      |
|    time_elapsed    | 9900     |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=371000, episode_reward=365.39 +/- 110.83
Episode length: 510.78 +/- 69.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 511         |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.068364985 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0005      |
|    loss                 | 419         |
|    n_updates            | 1711        |
|    policy_gradient_loss | 0.000447    |
|    value_loss           | 884         |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=392.89 +/- 112.36
Episode length: 514.84 +/- 49.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 393      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 372      |
|    ep_rew_mean     | 566      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 363      |
|    time_elapsed    | 9935     |
|    total_timesteps | 371712   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=372000, episode_reward=360.25 +/- 111.66
Episode length: 517.04 +/- 55.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 517        |
|    mean_reward          | 360        |
| time/                   |            |
|    total_timesteps      | 372000     |
| train/                  |            |
|    approx_kl            | 0.06279324 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.825     |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.0005     |
|    loss                 | 493        |
|    n_updates            | 1714       |
|    policy_gradient_loss | 0.0113     |
|    value_loss           | 1.51e+03   |
----------------------------------------
Eval num_timesteps=372500, episode_reward=331.73 +/- 112.37
Episode length: 522.46 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 522      |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 380      |
|    ep_rew_mean     | 568      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 364      |
|    time_elapsed    | 9970     |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=373000, episode_reward=212.50 +/- 49.39
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 212         |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.027632853 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0005      |
|    loss                 | 481         |
|    n_updates            | 1715        |
|    policy_gradient_loss | 0.0293      |
|    value_loss           | 807         |
-----------------------------------------
Eval num_timesteps=373500, episode_reward=208.04 +/- 49.31
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 386      |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 365      |
|    time_elapsed    | 10005    |
|    total_timesteps | 373760   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.09
Eval num_timesteps=374000, episode_reward=247.13 +/- 88.65
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 247        |
| time/                   |            |
|    total_timesteps      | 374000     |
| train/                  |            |
|    approx_kl            | 0.06126623 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.601     |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.0005     |
|    loss                 | 462        |
|    n_updates            | 1720       |
|    policy_gradient_loss | 0.0119     |
|    value_loss           | 704        |
----------------------------------------
Eval num_timesteps=374500, episode_reward=225.47 +/- 68.46
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 394      |
|    ep_rew_mean     | 573      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 366      |
|    time_elapsed    | 10041    |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=375000, episode_reward=212.08 +/- 47.64
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 212         |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.082368016 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0005      |
|    loss                 | 356         |
|    n_updates            | 1723        |
|    policy_gradient_loss | -0.000998   |
|    value_loss           | 757         |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=218.48 +/- 62.25
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 367      |
|    time_elapsed    | 10077    |
|    total_timesteps | 375808   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=376000, episode_reward=345.89 +/- 136.73
Episode length: 517.80 +/- 50.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | 346         |
| time/                   |             |
|    total_timesteps      | 376000      |
| train/                  |             |
|    approx_kl            | 0.094587006 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0005      |
|    loss                 | 698         |
|    n_updates            | 1726        |
|    policy_gradient_loss | 0.00707     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=376500, episode_reward=321.57 +/- 132.96
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 406      |
|    ep_rew_mean     | 580      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 368      |
|    time_elapsed    | 10113    |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=377000, episode_reward=509.68 +/- 201.69
Episode length: 430.50 +/- 159.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 430       |
|    mean_reward          | 510       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0515258 |
|    clip_fraction        | 0.221     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.84     |
|    explained_variance   | 0.626     |
|    learning_rate        | 0.0005    |
|    loss                 | 763       |
|    n_updates            | 1730      |
|    policy_gradient_loss | -0.0232   |
|    value_loss           | 1.42e+03  |
---------------------------------------
Eval num_timesteps=377500, episode_reward=513.68 +/- 197.96
Episode length: 441.70 +/- 157.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 442      |
|    mean_reward     | 514      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | 581      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 369      |
|    time_elapsed    | 10145    |
|    total_timesteps | 377856   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=378000, episode_reward=528.19 +/- 161.02
Episode length: 463.90 +/- 140.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 464        |
|    mean_reward          | 528        |
| time/                   |            |
|    total_timesteps      | 378000     |
| train/                  |            |
|    approx_kl            | 0.04276189 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.984     |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.0005     |
|    loss                 | 644        |
|    n_updates            | 1734       |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 1.8e+03    |
----------------------------------------
Eval num_timesteps=378500, episode_reward=555.00 +/- 191.27
Episode length: 429.04 +/- 159.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 555      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 417      |
|    ep_rew_mean     | 591      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 370      |
|    time_elapsed    | 10176    |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=379000, episode_reward=503.89 +/- 171.12
Episode length: 456.70 +/- 142.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 457         |
|    mean_reward          | 504         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.049896635 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.875      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.26e+03    |
|    n_updates            | 1738        |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=519.63 +/- 166.35
Episode length: 476.38 +/- 127.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 415      |
|    ep_rew_mean     | 604      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 371      |
|    time_elapsed    | 10207    |
|    total_timesteps | 379904   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=380000, episode_reward=390.98 +/- 162.31
Episode length: 486.14 +/- 116.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 486         |
|    mean_reward          | 391         |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.044668075 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.08e+03    |
|    n_updates            | 1742        |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 2.08e+03    |
-----------------------------------------
Eval num_timesteps=380500, episode_reward=413.03 +/- 158.89
Episode length: 487.40 +/- 114.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 487      |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 420      |
|    ep_rew_mean     | 607      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 372      |
|    time_elapsed    | 10240    |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=381000, episode_reward=399.82 +/- 143.82
Episode length: 493.76 +/- 107.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 494         |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.055571858 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0005      |
|    loss                 | 282         |
|    n_updates            | 1744        |
|    policy_gradient_loss | 0.0202      |
|    value_loss           | 664         |
-----------------------------------------
Eval num_timesteps=381500, episode_reward=404.23 +/- 124.62
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 424      |
|    ep_rew_mean     | 616      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 373      |
|    time_elapsed    | 10275    |
|    total_timesteps | 381952   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=382000, episode_reward=437.19 +/- 151.30
Episode length: 482.58 +/- 123.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 483         |
|    mean_reward          | 437         |
| time/                   |             |
|    total_timesteps      | 382000      |
| train/                  |             |
|    approx_kl            | 0.048695993 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0005      |
|    loss                 | 364         |
|    n_updates            | 1748        |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 855         |
-----------------------------------------
Eval num_timesteps=382500, episode_reward=440.92 +/- 134.51
Episode length: 502.32 +/- 91.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 502      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | 614      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 374      |
|    time_elapsed    | 10309    |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.14
Eval num_timesteps=383000, episode_reward=479.52 +/- 202.46
Episode length: 443.68 +/- 157.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 444        |
|    mean_reward          | 480        |
| time/                   |            |
|    total_timesteps      | 383000     |
| train/                  |            |
|    approx_kl            | 0.07397453 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.559      |
|    learning_rate        | 0.0005     |
|    loss                 | 464        |
|    n_updates            | 1750       |
|    policy_gradient_loss | 0.0041     |
|    value_loss           | 958        |
----------------------------------------
Eval num_timesteps=383500, episode_reward=440.33 +/- 184.37
Episode length: 412.62 +/- 174.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=459.82 +/- 193.97
Episode length: 442.60 +/- 159.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 443      |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 427      |
|    ep_rew_mean     | 618      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 375      |
|    time_elapsed    | 10353    |
|    total_timesteps | 384000   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=384500, episode_reward=464.05 +/- 182.65
Episode length: 456.60 +/- 142.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 457         |
|    mean_reward          | 464         |
| time/                   |             |
|    total_timesteps      | 384500      |
| train/                  |             |
|    approx_kl            | 0.044278853 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0005      |
|    loss                 | 860         |
|    n_updates            | 1753        |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=385000, episode_reward=477.71 +/- 181.42
Episode length: 466.88 +/- 130.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 424      |
|    ep_rew_mean     | 620      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 376      |
|    time_elapsed    | 10384    |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=385500, episode_reward=430.45 +/- 179.50
Episode length: 410.20 +/- 178.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 410        |
|    mean_reward          | 430        |
| time/                   |            |
|    total_timesteps      | 385500     |
| train/                  |            |
|    approx_kl            | 0.05377587 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0005     |
|    loss                 | 520        |
|    n_updates            | 1755       |
|    policy_gradient_loss | 0.0123     |
|    value_loss           | 1.14e+03   |
----------------------------------------
Eval num_timesteps=386000, episode_reward=430.38 +/- 184.92
Episode length: 439.52 +/- 161.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | 612      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 377      |
|    time_elapsed    | 10413    |
|    total_timesteps | 386048   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=386500, episode_reward=509.14 +/- 190.30
Episode length: 327.96 +/- 193.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 328         |
|    mean_reward          | 509         |
| time/                   |             |
|    total_timesteps      | 386500      |
| train/                  |             |
|    approx_kl            | 0.049055826 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.898      |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.37e+03    |
|    n_updates            | 1759        |
|    policy_gradient_loss | 0.00624     |
|    value_loss           | 3.41e+03    |
-----------------------------------------
Eval num_timesteps=387000, episode_reward=471.08 +/- 215.23
Episode length: 239.22 +/- 182.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 406      |
|    ep_rew_mean     | 611      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 378      |
|    time_elapsed    | 10433    |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=387500, episode_reward=494.65 +/- 188.28
Episode length: 334.92 +/- 195.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 335        |
|    mean_reward          | 495        |
| time/                   |            |
|    total_timesteps      | 387500     |
| train/                  |            |
|    approx_kl            | 0.06500501 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.983     |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0005     |
|    loss                 | 476        |
|    n_updates            | 1761       |
|    policy_gradient_loss | 0.00186    |
|    value_loss           | 675        |
----------------------------------------
Eval num_timesteps=388000, episode_reward=463.86 +/- 196.76
Episode length: 322.52 +/- 203.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | 588      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 379      |
|    time_elapsed    | 10455    |
|    total_timesteps | 388096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=388500, episode_reward=424.37 +/- 172.36
Episode length: 458.54 +/- 152.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 459        |
|    mean_reward          | 424        |
| time/                   |            |
|    total_timesteps      | 388500     |
| train/                  |            |
|    approx_kl            | 0.06926426 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.651     |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.0005     |
|    loss                 | 2e+03      |
|    n_updates            | 1763       |
|    policy_gradient_loss | 0.0156     |
|    value_loss           | 3.8e+03    |
----------------------------------------
Eval num_timesteps=389000, episode_reward=419.17 +/- 131.90
Episode length: 482.76 +/- 126.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 483      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 370      |
|    ep_rew_mean     | 582      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 380      |
|    time_elapsed    | 10487    |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=389500, episode_reward=411.97 +/- 140.16
Episode length: 473.74 +/- 138.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | 412        |
| time/                   |            |
|    total_timesteps      | 389500     |
| train/                  |            |
|    approx_kl            | 0.04869479 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.976     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.45e+03   |
|    n_updates            | 1766       |
|    policy_gradient_loss | -0.00552   |
|    value_loss           | 2.42e+03   |
----------------------------------------
Eval num_timesteps=390000, episode_reward=407.86 +/- 165.09
Episode length: 461.14 +/- 146.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 461      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 366      |
|    ep_rew_mean     | 581      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 381      |
|    time_elapsed    | 10519    |
|    total_timesteps | 390144   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=390500, episode_reward=409.45 +/- 145.22
Episode length: 473.10 +/- 140.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 473       |
|    mean_reward          | 409       |
| time/                   |           |
|    total_timesteps      | 390500    |
| train/                  |           |
|    approx_kl            | 0.0538805 |
|    clip_fraction        | 0.242     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.867    |
|    explained_variance   | 0.753     |
|    learning_rate        | 0.0005    |
|    loss                 | 350       |
|    n_updates            | 1771      |
|    policy_gradient_loss | -0.00542  |
|    value_loss           | 741       |
---------------------------------------
Eval num_timesteps=391000, episode_reward=441.69 +/- 153.11
Episode length: 467.22 +/- 143.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | 581      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 382      |
|    time_elapsed    | 10551    |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=391500, episode_reward=422.32 +/- 150.23
Episode length: 454.86 +/- 161.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 455         |
|    mean_reward          | 422         |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.042887457 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0005      |
|    loss                 | 551         |
|    n_updates            | 1774        |
|    policy_gradient_loss | 0.00841     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=474.40 +/- 203.47
Episode length: 434.42 +/- 170.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 434      |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | 578      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 383      |
|    time_elapsed    | 10582    |
|    total_timesteps | 392192   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=392500, episode_reward=422.08 +/- 175.57
Episode length: 349.72 +/- 206.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 350         |
|    mean_reward          | 422         |
| time/                   |             |
|    total_timesteps      | 392500      |
| train/                  |             |
|    approx_kl            | 0.058823433 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0005      |
|    loss                 | 399         |
|    n_updates            | 1776        |
|    policy_gradient_loss | 0.0142      |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=393000, episode_reward=417.74 +/- 170.44
Episode length: 399.58 +/- 191.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 400      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 346      |
|    ep_rew_mean     | 570      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 384      |
|    time_elapsed    | 10608    |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.27
Eval num_timesteps=393500, episode_reward=457.24 +/- 163.00
Episode length: 436.36 +/- 167.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 436         |
|    mean_reward          | 457         |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.066374466 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.07e+03    |
|    n_updates            | 1778        |
|    policy_gradient_loss | 0.00404     |
|    value_loss           | 2.3e+03     |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=392.09 +/- 112.83
Episode length: 448.02 +/- 164.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 567      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 385      |
|    time_elapsed    | 10638    |
|    total_timesteps | 394240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=394500, episode_reward=379.14 +/- 138.34
Episode length: 432.50 +/- 174.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 379         |
| time/                   |             |
|    total_timesteps      | 394500      |
| train/                  |             |
|    approx_kl            | 0.053476866 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.871      |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0005      |
|    loss                 | 619         |
|    n_updates            | 1780        |
|    policy_gradient_loss | 0.00455     |
|    value_loss           | 946         |
-----------------------------------------
Eval num_timesteps=395000, episode_reward=412.42 +/- 134.86
Episode length: 449.18 +/- 161.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 449      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 348      |
|    ep_rew_mean     | 560      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 386      |
|    time_elapsed    | 10668    |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=395500, episode_reward=456.73 +/- 173.44
Episode length: 442.46 +/- 165.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 442         |
|    mean_reward          | 457         |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.051108975 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.03e+03    |
|    n_updates            | 1783        |
|    policy_gradient_loss | 0.000495    |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=444.38 +/- 159.04
Episode length: 449.52 +/- 161.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 450      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 556      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 387      |
|    time_elapsed    | 10699    |
|    total_timesteps | 396288   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=396500, episode_reward=569.30 +/- 177.28
Episode length: 327.04 +/- 198.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 327         |
|    mean_reward          | 569         |
| time/                   |             |
|    total_timesteps      | 396500      |
| train/                  |             |
|    approx_kl            | 0.049497243 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0005      |
|    loss                 | 498         |
|    n_updates            | 1785        |
|    policy_gradient_loss | 0.0152      |
|    value_loss           | 809         |
-----------------------------------------
Eval num_timesteps=397000, episode_reward=532.70 +/- 191.17
Episode length: 330.90 +/- 202.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 331      |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 569      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 388      |
|    time_elapsed    | 10721    |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=397500, episode_reward=650.63 +/- 156.39
Episode length: 241.12 +/- 177.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 651         |
| time/                   |             |
|    total_timesteps      | 397500      |
| train/                  |             |
|    approx_kl            | 0.044019114 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.17e+03    |
|    n_updates            | 1789        |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 2.1e+03     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=398000, episode_reward=557.87 +/- 209.79
Episode length: 236.34 +/- 181.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 574      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 389      |
|    time_elapsed    | 10738    |
|    total_timesteps | 398336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=398500, episode_reward=554.77 +/- 211.25
Episode length: 234.32 +/- 175.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 555         |
| time/                   |             |
|    total_timesteps      | 398500      |
| train/                  |             |
|    approx_kl            | 0.048694838 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.08e+03    |
|    n_updates            | 1791        |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 2.43e+03    |
-----------------------------------------
Eval num_timesteps=399000, episode_reward=609.50 +/- 187.98
Episode length: 207.92 +/- 152.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 565      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 390      |
|    time_elapsed    | 10754    |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=399500, episode_reward=636.45 +/- 171.69
Episode length: 180.80 +/- 129.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 181        |
|    mean_reward          | 636        |
| time/                   |            |
|    total_timesteps      | 399500     |
| train/                  |            |
|    approx_kl            | 0.04561759 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.09e+03   |
|    n_updates            | 1794       |
|    policy_gradient_loss | -0.00421   |
|    value_loss           | 2.51e+03   |
----------------------------------------
Eval num_timesteps=400000, episode_reward=641.14 +/- 179.44
Episode length: 165.24 +/- 107.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 570      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 391      |
|    time_elapsed    | 10766    |
|    total_timesteps | 400384   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=400500, episode_reward=629.83 +/- 145.67
Episode length: 211.06 +/- 157.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 630         |
| time/                   |             |
|    total_timesteps      | 400500      |
| train/                  |             |
|    approx_kl            | 0.041499805 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.901      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0005      |
|    loss                 | 708         |
|    n_updates            | 1797        |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 1.65e+03    |
-----------------------------------------
Eval num_timesteps=401000, episode_reward=600.08 +/- 205.80
Episode length: 200.48 +/- 163.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 392      |
|    time_elapsed    | 10781    |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=401500, episode_reward=624.69 +/- 174.02
Episode length: 188.84 +/- 147.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 625        |
| time/                   |            |
|    total_timesteps      | 401500     |
| train/                  |            |
|    approx_kl            | 0.06674796 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.734     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.75e+03   |
|    n_updates            | 1799       |
|    policy_gradient_loss | 0.00248    |
|    value_loss           | 3.5e+03    |
----------------------------------------
Eval num_timesteps=402000, episode_reward=612.79 +/- 154.38
Episode length: 209.82 +/- 158.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 393      |
|    time_elapsed    | 10795    |
|    total_timesteps | 402432   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=402500, episode_reward=647.08 +/- 188.31
Episode length: 194.02 +/- 156.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 647         |
| time/                   |             |
|    total_timesteps      | 402500      |
| train/                  |             |
|    approx_kl            | 0.050529666 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.63e+03    |
|    n_updates            | 1802        |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 3.31e+03    |
-----------------------------------------
Eval num_timesteps=403000, episode_reward=663.12 +/- 172.76
Episode length: 240.82 +/- 178.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 580      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 394      |
|    time_elapsed    | 10810    |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.12
Eval num_timesteps=403500, episode_reward=591.61 +/- 237.46
Episode length: 209.68 +/- 169.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | 592         |
| time/                   |             |
|    total_timesteps      | 403500      |
| train/                  |             |
|    approx_kl            | 0.059033666 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.28e+03    |
|    n_updates            | 1805        |
|    policy_gradient_loss | 0.000247    |
|    value_loss           | 3.9e+03     |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=526.91 +/- 280.81
Episode length: 197.48 +/- 166.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 569      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 395      |
|    time_elapsed    | 10825    |
|    total_timesteps | 404480   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=404500, episode_reward=611.11 +/- 205.26
Episode length: 224.78 +/- 170.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 611        |
| time/                   |            |
|    total_timesteps      | 404500     |
| train/                  |            |
|    approx_kl            | 0.06448133 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.599     |
|    explained_variance   | 0.413      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.68e+03   |
|    n_updates            | 1808       |
|    policy_gradient_loss | 0.00075    |
|    value_loss           | 3.12e+03   |
----------------------------------------
Eval num_timesteps=405000, episode_reward=652.07 +/- 172.97
Episode length: 195.62 +/- 148.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=629.76 +/- 188.38
Episode length: 219.50 +/- 164.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 251      |
|    ep_rew_mean     | 560      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 396      |
|    time_elapsed    | 10847    |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=406000, episode_reward=603.29 +/- 184.05
Episode length: 249.14 +/- 173.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 603        |
| time/                   |            |
|    total_timesteps      | 406000     |
| train/                  |            |
|    approx_kl            | 0.04677679 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.765     |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0005     |
|    loss                 | 713        |
|    n_updates            | 1811       |
|    policy_gradient_loss | -0.00229   |
|    value_loss           | 1.53e+03   |
----------------------------------------
Eval num_timesteps=406500, episode_reward=633.12 +/- 162.48
Episode length: 265.00 +/- 178.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 545      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 397      |
|    time_elapsed    | 10865    |
|    total_timesteps | 406528   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=407000, episode_reward=542.83 +/- 187.45
Episode length: 404.24 +/- 171.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 404       |
|    mean_reward          | 543       |
| time/                   |           |
|    total_timesteps      | 407000    |
| train/                  |           |
|    approx_kl            | 0.0468088 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.928    |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.57e+03  |
|    n_updates            | 1813      |
|    policy_gradient_loss | 0.0193    |
|    value_loss           | 3.81e+03  |
---------------------------------------
Eval num_timesteps=407500, episode_reward=499.15 +/- 196.44
Episode length: 401.14 +/- 175.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 401      |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 539      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 398      |
|    time_elapsed    | 10892    |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=408000, episode_reward=564.74 +/- 173.75
Episode length: 407.56 +/- 172.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 408         |
|    mean_reward          | 565         |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.047050767 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.43e+03    |
|    n_updates            | 1816        |
|    policy_gradient_loss | -0.00877    |
|    value_loss           | 2.22e+03    |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=565.08 +/- 164.71
Episode length: 395.82 +/- 176.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 396      |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 549      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 399      |
|    time_elapsed    | 10920    |
|    total_timesteps | 408576   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=409000, episode_reward=559.74 +/- 162.33
Episode length: 354.64 +/- 194.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 355        |
|    mean_reward          | 560        |
| time/                   |            |
|    total_timesteps      | 409000     |
| train/                  |            |
|    approx_kl            | 0.08151004 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.803     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.0005     |
|    loss                 | 819        |
|    n_updates            | 1819       |
|    policy_gradient_loss | -0.00227   |
|    value_loss           | 1.42e+03   |
----------------------------------------
Eval num_timesteps=409500, episode_reward=494.96 +/- 192.69
Episode length: 366.24 +/- 192.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 366      |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 554      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 400      |
|    time_elapsed    | 10945    |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=410000, episode_reward=645.97 +/- 173.70
Episode length: 169.48 +/- 125.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 646         |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.043102704 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.0005      |
|    loss                 | 640         |
|    n_updates            | 1821        |
|    policy_gradient_loss | 0.0112      |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=610.98 +/- 208.35
Episode length: 218.44 +/- 172.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 560      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 401      |
|    time_elapsed    | 10959    |
|    total_timesteps | 410624   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=411000, episode_reward=626.58 +/- 202.36
Episode length: 187.86 +/- 148.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 188      |
|    mean_reward          | 627      |
| time/                   |          |
|    total_timesteps      | 411000   |
| train/                  |          |
|    approx_kl            | 0.041429 |
|    clip_fraction        | 0.166    |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -0.655   |
|    explained_variance   | 0.479    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.71e+03 |
|    n_updates            | 1824     |
|    policy_gradient_loss | -0.0118  |
|    value_loss           | 3.4e+03  |
--------------------------------------
Eval num_timesteps=411500, episode_reward=647.02 +/- 182.48
Episode length: 172.08 +/- 131.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 575      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 402      |
|    time_elapsed    | 10972    |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=412000, episode_reward=592.52 +/- 220.80
Episode length: 216.96 +/- 164.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 593        |
| time/                   |            |
|    total_timesteps      | 412000     |
| train/                  |            |
|    approx_kl            | 0.04213604 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.65e+03   |
|    n_updates            | 1827       |
|    policy_gradient_loss | -0.00291   |
|    value_loss           | 3.07e+03   |
----------------------------------------
Eval num_timesteps=412500, episode_reward=624.00 +/- 190.04
Episode length: 237.78 +/- 171.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 583      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 403      |
|    time_elapsed    | 10988    |
|    total_timesteps | 412672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=413000, episode_reward=668.78 +/- 160.99
Episode length: 196.60 +/- 148.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 197         |
|    mean_reward          | 669         |
| time/                   |             |
|    total_timesteps      | 413000      |
| train/                  |             |
|    approx_kl            | 0.054592967 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.01e+03    |
|    n_updates            | 1829        |
|    policy_gradient_loss | 0.00634     |
|    value_loss           | 2.02e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=413500, episode_reward=617.70 +/- 217.09
Episode length: 194.50 +/- 149.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 591      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 404      |
|    time_elapsed    | 11002    |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=414000, episode_reward=538.93 +/- 208.86
Episode length: 257.94 +/- 185.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 258        |
|    mean_reward          | 539        |
| time/                   |            |
|    total_timesteps      | 414000     |
| train/                  |            |
|    approx_kl            | 0.08036911 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.57      |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.17e+03   |
|    n_updates            | 1831       |
|    policy_gradient_loss | 0.00235    |
|    value_loss           | 2.26e+03   |
----------------------------------------
Eval num_timesteps=414500, episode_reward=612.24 +/- 167.65
Episode length: 318.94 +/- 192.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 319      |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 599      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 405      |
|    time_elapsed    | 11022    |
|    total_timesteps | 414720   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=415000, episode_reward=605.27 +/- 205.21
Episode length: 264.30 +/- 188.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 605         |
| time/                   |             |
|    total_timesteps      | 415000      |
| train/                  |             |
|    approx_kl            | 0.043265242 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.03e+03    |
|    n_updates            | 1835        |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=415500, episode_reward=649.93 +/- 179.63
Episode length: 251.62 +/- 180.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 602      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 406      |
|    time_elapsed    | 11040    |
|    total_timesteps | 415744   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=416000, episode_reward=605.94 +/- 218.72
Episode length: 241.90 +/- 186.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 242       |
|    mean_reward          | 606       |
| time/                   |           |
|    total_timesteps      | 416000    |
| train/                  |           |
|    approx_kl            | 0.0347526 |
|    clip_fraction        | 0.175     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.787    |
|    explained_variance   | 0.752     |
|    learning_rate        | 0.0005    |
|    loss                 | 624       |
|    n_updates            | 1837      |
|    policy_gradient_loss | -0.00186  |
|    value_loss           | 1.36e+03  |
---------------------------------------
Eval num_timesteps=416500, episode_reward=561.12 +/- 198.81
Episode length: 247.14 +/- 179.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 606      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 407      |
|    time_elapsed    | 11057    |
|    total_timesteps | 416768   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=417000, episode_reward=603.38 +/- 213.71
Episode length: 211.12 +/- 158.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 603         |
| time/                   |             |
|    total_timesteps      | 417000      |
| train/                  |             |
|    approx_kl            | 0.044665806 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0005      |
|    loss                 | 776         |
|    n_updates            | 1839        |
|    policy_gradient_loss | 0.00647     |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=417500, episode_reward=576.79 +/- 215.65
Episode length: 215.62 +/- 160.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 577      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 607      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 408      |
|    time_elapsed    | 11073    |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=418000, episode_reward=534.89 +/- 218.15
Episode length: 204.20 +/- 163.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 535        |
| time/                   |            |
|    total_timesteps      | 418000     |
| train/                  |            |
|    approx_kl            | 0.04000341 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.826     |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0005     |
|    loss                 | 384        |
|    n_updates            | 1842       |
|    policy_gradient_loss | 0.00421    |
|    value_loss           | 999        |
----------------------------------------
Eval num_timesteps=418500, episode_reward=564.10 +/- 208.34
Episode length: 234.12 +/- 182.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 596      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 409      |
|    time_elapsed    | 11088    |
|    total_timesteps | 418816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=419000, episode_reward=632.23 +/- 195.07
Episode length: 128.12 +/- 83.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 632         |
| time/                   |             |
|    total_timesteps      | 419000      |
| train/                  |             |
|    approx_kl            | 0.023708206 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.58e+03    |
|    n_updates            | 1843        |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 2.87e+03    |
-----------------------------------------
Eval num_timesteps=419500, episode_reward=666.25 +/- 152.30
Episode length: 116.22 +/- 23.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 591      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 410      |
|    time_elapsed    | 11097    |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=420000, episode_reward=588.21 +/- 212.14
Episode length: 106.96 +/- 17.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 107        |
|    mean_reward          | 588        |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.05070642 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.451     |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.28e+03   |
|    n_updates            | 1845       |
|    policy_gradient_loss | 0.00535    |
|    value_loss           | 4.12e+03   |
----------------------------------------
Eval num_timesteps=420500, episode_reward=580.46 +/- 210.42
Episode length: 117.44 +/- 61.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 580      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 597      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 411      |
|    time_elapsed    | 11106    |
|    total_timesteps | 420864   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=421000, episode_reward=605.85 +/- 215.53
Episode length: 117.94 +/- 61.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 606         |
| time/                   |             |
|    total_timesteps      | 421000      |
| train/                  |             |
|    approx_kl            | 0.046555635 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.391      |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.02e+03    |
|    n_updates            | 1848        |
|    policy_gradient_loss | 0.00057     |
|    value_loss           | 4.75e+03    |
-----------------------------------------
Eval num_timesteps=421500, episode_reward=640.85 +/- 151.79
Episode length: 112.74 +/- 14.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 603      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 412      |
|    time_elapsed    | 11114    |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=422000, episode_reward=590.46 +/- 205.35
Episode length: 130.88 +/- 81.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 590         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.032017417 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.24e+03    |
|    n_updates            | 1850        |
|    policy_gradient_loss | 0.00719     |
|    value_loss           | 2.97e+03    |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=543.93 +/- 229.77
Episode length: 120.24 +/- 62.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 544      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 587      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 413      |
|    time_elapsed    | 11124    |
|    total_timesteps | 422912   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=423000, episode_reward=587.81 +/- 188.31
Episode length: 130.28 +/- 82.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 130        |
|    mean_reward          | 588        |
| time/                   |            |
|    total_timesteps      | 423000     |
| train/                  |            |
|    approx_kl            | 0.06691142 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.402     |
|    explained_variance   | 0.514      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.28e+03   |
|    n_updates            | 1852       |
|    policy_gradient_loss | 0.00397    |
|    value_loss           | 6.6e+03    |
----------------------------------------
Eval num_timesteps=423500, episode_reward=650.98 +/- 179.57
Episode length: 132.26 +/- 81.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 602      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 414      |
|    time_elapsed    | 11133    |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=424000, episode_reward=632.73 +/- 168.80
Episode length: 145.44 +/- 96.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 145         |
|    mean_reward          | 633         |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.048173796 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0005      |
|    loss                 | 908         |
|    n_updates            | 1855        |
|    policy_gradient_loss | 0.00203     |
|    value_loss           | 2.95e+03    |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=599.52 +/- 212.61
Episode length: 132.00 +/- 82.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 616      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 415      |
|    time_elapsed    | 11144    |
|    total_timesteps | 424960   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=425000, episode_reward=582.03 +/- 269.82
Episode length: 127.68 +/- 86.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 128        |
|    mean_reward          | 582        |
| time/                   |            |
|    total_timesteps      | 425000     |
| train/                  |            |
|    approx_kl            | 0.06331032 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0005     |
|    loss                 | 890        |
|    n_updates            | 1858       |
|    policy_gradient_loss | 0.00259    |
|    value_loss           | 1.78e+03   |
----------------------------------------
Eval num_timesteps=425500, episode_reward=623.70 +/- 223.50
Episode length: 113.78 +/- 25.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 610      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 416      |
|    time_elapsed    | 11153    |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=426000, episode_reward=569.60 +/- 208.17
Episode length: 266.56 +/- 188.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 267        |
|    mean_reward          | 570        |
| time/                   |            |
|    total_timesteps      | 426000     |
| train/                  |            |
|    approx_kl            | 0.06677287 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.482     |
|    explained_variance   | 0.501      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.65e+03   |
|    n_updates            | 1860       |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 5.97e+03   |
----------------------------------------
Eval num_timesteps=426500, episode_reward=607.30 +/- 175.52
Episode length: 263.36 +/- 182.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 607      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=559.15 +/- 212.40
Episode length: 245.12 +/- 181.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 559      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 605      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 417      |
|    time_elapsed    | 11180    |
|    total_timesteps | 427008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=427500, episode_reward=603.07 +/- 170.18
Episode length: 380.54 +/- 184.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 381        |
|    mean_reward          | 603        |
| time/                   |            |
|    total_timesteps      | 427500     |
| train/                  |            |
|    approx_kl            | 0.06119401 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.539     |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.76e+03   |
|    n_updates            | 1862       |
|    policy_gradient_loss | 0.0234     |
|    value_loss           | 3.34e+03   |
----------------------------------------
Eval num_timesteps=428000, episode_reward=560.36 +/- 170.56
Episode length: 393.68 +/- 183.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 394      |
|    mean_reward     | 560      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 612      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 418      |
|    time_elapsed    | 11206    |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=428500, episode_reward=573.74 +/- 189.60
Episode length: 361.00 +/- 182.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 361         |
|    mean_reward          | 574         |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.062171914 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.642      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0005      |
|    loss                 | 716         |
|    n_updates            | 1866        |
|    policy_gradient_loss | -0.00593    |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=598.91 +/- 186.65
Episode length: 338.30 +/- 190.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 614      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 419      |
|    time_elapsed    | 11230    |
|    total_timesteps | 429056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=429500, episode_reward=555.28 +/- 159.28
Episode length: 401.22 +/- 173.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 401        |
|    mean_reward          | 555        |
| time/                   |            |
|    total_timesteps      | 429500     |
| train/                  |            |
|    approx_kl            | 0.03178083 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.606     |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.42e+03   |
|    n_updates            | 1868       |
|    policy_gradient_loss | 0.00164    |
|    value_loss           | 2.67e+03   |
----------------------------------------
Eval num_timesteps=430000, episode_reward=524.93 +/- 193.10
Episode length: 389.76 +/- 181.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 390      |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 613      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 420      |
|    time_elapsed    | 11257    |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=430500, episode_reward=467.22 +/- 158.38
Episode length: 465.80 +/- 135.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 466        |
|    mean_reward          | 467        |
| time/                   |            |
|    total_timesteps      | 430500     |
| train/                  |            |
|    approx_kl            | 0.07809041 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.68       |
|    learning_rate        | 0.0005     |
|    loss                 | 673        |
|    n_updates            | 1871       |
|    policy_gradient_loss | 0.00262    |
|    value_loss           | 1.17e+03   |
----------------------------------------
Eval num_timesteps=431000, episode_reward=470.35 +/- 176.50
Episode length: 464.48 +/- 140.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 613      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 421      |
|    time_elapsed    | 11289    |
|    total_timesteps | 431104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=431500, episode_reward=414.03 +/- 158.44
Episode length: 487.28 +/- 113.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 487         |
|    mean_reward          | 414         |
| time/                   |             |
|    total_timesteps      | 431500      |
| train/                  |             |
|    approx_kl            | 0.020463882 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.631      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0005      |
|    loss                 | 490         |
|    n_updates            | 1872        |
|    policy_gradient_loss | 0.0153      |
|    value_loss           | 1.26e+03    |
-----------------------------------------
Eval num_timesteps=432000, episode_reward=454.51 +/- 182.61
Episode length: 448.62 +/- 153.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 449      |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 615      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 422      |
|    time_elapsed    | 11323    |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=432500, episode_reward=403.55 +/- 152.69
Episode length: 514.74 +/- 50.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 404         |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.048680354 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0005      |
|    loss                 | 633         |
|    n_updates            | 1875        |
|    policy_gradient_loss | -0.00839    |
|    value_loss           | 1.38e+03    |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=424.97 +/- 174.73
Episode length: 495.72 +/- 99.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 614      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 423      |
|    time_elapsed    | 11357    |
|    total_timesteps | 433152   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=433500, episode_reward=397.74 +/- 150.21
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 398         |
| time/                   |             |
|    total_timesteps      | 433500      |
| train/                  |             |
|    approx_kl            | 0.043484088 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.558      |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.69e+03    |
|    n_updates            | 1878        |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 2.77e+03    |
-----------------------------------------
Eval num_timesteps=434000, episode_reward=404.49 +/- 167.53
Episode length: 518.16 +/- 47.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 609      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 424      |
|    time_elapsed    | 11392    |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.09
Eval num_timesteps=434500, episode_reward=370.10 +/- 175.29
Episode length: 520.52 +/- 31.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 521        |
|    mean_reward          | 370        |
| time/                   |            |
|    total_timesteps      | 434500     |
| train/                  |            |
|    approx_kl            | 0.07167879 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.685     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0005     |
|    loss                 | 474        |
|    n_updates            | 1882       |
|    policy_gradient_loss | -0.00121   |
|    value_loss           | 1.12e+03   |
----------------------------------------
Eval num_timesteps=435000, episode_reward=349.96 +/- 142.90
Episode length: 516.90 +/- 56.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 612      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 425      |
|    time_elapsed    | 11427    |
|    total_timesteps | 435200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=435500, episode_reward=234.26 +/- 74.63
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 234         |
| time/                   |             |
|    total_timesteps      | 435500      |
| train/                  |             |
|    approx_kl            | 0.027157886 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0005      |
|    loss                 | 567         |
|    n_updates            | 1883        |
|    policy_gradient_loss | 0.0059      |
|    value_loss           | 846         |
-----------------------------------------
Eval num_timesteps=436000, episode_reward=262.18 +/- 111.24
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 613      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 426      |
|    time_elapsed    | 11463    |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=436500, episode_reward=411.46 +/- 165.64
Episode length: 499.02 +/- 89.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 499        |
|    mean_reward          | 411        |
| time/                   |            |
|    total_timesteps      | 436500     |
| train/                  |            |
|    approx_kl            | 0.07038244 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0005     |
|    loss                 | 568        |
|    n_updates            | 1885       |
|    policy_gradient_loss | 0.00392    |
|    value_loss           | 1.33e+03   |
----------------------------------------
Eval num_timesteps=437000, episode_reward=448.24 +/- 141.63
Episode length: 505.70 +/- 78.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 602      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 427      |
|    time_elapsed    | 11497    |
|    total_timesteps | 437248   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=437500, episode_reward=461.60 +/- 171.97
Episode length: 472.30 +/- 131.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 472         |
|    mean_reward          | 462         |
| time/                   |             |
|    total_timesteps      | 437500      |
| train/                  |             |
|    approx_kl            | 0.053104445 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.15e+03    |
|    n_updates            | 1888        |
|    policy_gradient_loss | 0.00994     |
|    value_loss           | 2.88e+03    |
-----------------------------------------
Eval num_timesteps=438000, episode_reward=496.65 +/- 200.96
Episode length: 414.86 +/- 161.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 415      |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 605      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 428      |
|    time_elapsed    | 11527    |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=438500, episode_reward=422.50 +/- 166.17
Episode length: 508.28 +/- 81.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 508        |
|    mean_reward          | 423        |
| time/                   |            |
|    total_timesteps      | 438500     |
| train/                  |            |
|    approx_kl            | 0.06101717 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.7       |
|    explained_variance   | 0.548      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.47e+03   |
|    n_updates            | 1891       |
|    policy_gradient_loss | -0.00132   |
|    value_loss           | 2.52e+03   |
----------------------------------------
Eval num_timesteps=439000, episode_reward=452.61 +/- 162.43
Episode length: 509.94 +/- 73.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 603      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 429      |
|    time_elapsed    | 11562    |
|    total_timesteps | 439296   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=439500, episode_reward=430.07 +/- 156.79
Episode length: 510.68 +/- 71.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 511         |
|    mean_reward          | 430         |
| time/                   |             |
|    total_timesteps      | 439500      |
| train/                  |             |
|    approx_kl            | 0.040675975 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0005      |
|    loss                 | 352         |
|    n_updates            | 1895        |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 894         |
-----------------------------------------
Eval num_timesteps=440000, episode_reward=440.11 +/- 132.64
Episode length: 513.76 +/- 56.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 514      |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 592      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 430      |
|    time_elapsed    | 11596    |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=440500, episode_reward=465.96 +/- 155.56
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 466        |
| time/                   |            |
|    total_timesteps      | 440500     |
| train/                  |            |
|    approx_kl            | 0.03822362 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.609     |
|    explained_variance   | 0.543      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.51e+03   |
|    n_updates            | 1898       |
|    policy_gradient_loss | 0.00548    |
|    value_loss           | 2.8e+03    |
----------------------------------------
Eval num_timesteps=441000, episode_reward=422.46 +/- 157.02
Episode length: 516.94 +/- 56.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 596      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 431      |
|    time_elapsed    | 11632    |
|    total_timesteps | 441344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=441500, episode_reward=420.30 +/- 166.00
Episode length: 518.88 +/- 42.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 519        |
|    mean_reward          | 420        |
| time/                   |            |
|    total_timesteps      | 441500     |
| train/                  |            |
|    approx_kl            | 0.04166036 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.95      |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.0005     |
|    loss                 | 358        |
|    n_updates            | 1900       |
|    policy_gradient_loss | 0.0189     |
|    value_loss           | 688        |
----------------------------------------
Eval num_timesteps=442000, episode_reward=425.31 +/- 175.80
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 592      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 432      |
|    time_elapsed    | 11667    |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=442500, episode_reward=328.66 +/- 161.07
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 329        |
| time/                   |            |
|    total_timesteps      | 442500     |
| train/                  |            |
|    approx_kl            | 0.03186556 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.837     |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0005     |
|    loss                 | 231        |
|    n_updates            | 1902       |
|    policy_gradient_loss | -0.00222   |
|    value_loss           | 626        |
----------------------------------------
Eval num_timesteps=443000, episode_reward=336.54 +/- 160.05
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 594      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 433      |
|    time_elapsed    | 11703    |
|    total_timesteps | 443392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=443500, episode_reward=236.11 +/- 83.64
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 443500      |
| train/                  |             |
|    approx_kl            | 0.022806522 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0005      |
|    loss                 | 424         |
|    n_updates            | 1903        |
|    policy_gradient_loss | 0.0333      |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=444000, episode_reward=233.82 +/- 76.20
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 590      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 434      |
|    time_elapsed    | 11738    |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=444500, episode_reward=327.34 +/- 169.27
Episode length: 479.50 +/- 124.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | 327         |
| time/                   |             |
|    total_timesteps      | 444500      |
| train/                  |             |
|    approx_kl            | 0.047981735 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0005      |
|    loss                 | 501         |
|    n_updates            | 1905        |
|    policy_gradient_loss | 0.00859     |
|    value_loss           | 839         |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=331.01 +/- 151.87
Episode length: 499.90 +/- 91.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 500      |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 248      |
|    ep_rew_mean     | 585      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 435      |
|    time_elapsed    | 11772    |
|    total_timesteps | 445440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=445500, episode_reward=440.40 +/- 156.50
Episode length: 436.62 +/- 167.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 437         |
|    mean_reward          | 440         |
| time/                   |             |
|    total_timesteps      | 445500      |
| train/                  |             |
|    approx_kl            | 0.025839994 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0005      |
|    loss                 | 335         |
|    n_updates            | 1906        |
|    policy_gradient_loss | 0.025       |
|    value_loss           | 759         |
-----------------------------------------
Eval num_timesteps=446000, episode_reward=478.60 +/- 158.03
Episode length: 493.06 +/- 97.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 584      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 436      |
|    time_elapsed    | 11804    |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=446500, episode_reward=507.68 +/- 198.85
Episode length: 358.12 +/- 196.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 358         |
|    mean_reward          | 508         |
| time/                   |             |
|    total_timesteps      | 446500      |
| train/                  |             |
|    approx_kl            | 0.051318288 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0005      |
|    loss                 | 829         |
|    n_updates            | 1908        |
|    policy_gradient_loss | 0.00681     |
|    value_loss           | 1.56e+03    |
-----------------------------------------
Eval num_timesteps=447000, episode_reward=525.93 +/- 192.99
Episode length: 307.66 +/- 199.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 261      |
|    ep_rew_mean     | 581      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 437      |
|    time_elapsed    | 11827    |
|    total_timesteps | 447488   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=447500, episode_reward=493.05 +/- 172.01
Episode length: 392.30 +/- 185.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 392        |
|    mean_reward          | 493        |
| time/                   |            |
|    total_timesteps      | 447500     |
| train/                  |            |
|    approx_kl            | 0.04456509 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.2e+03    |
|    n_updates            | 1911       |
|    policy_gradient_loss | -0.00586   |
|    value_loss           | 1.42e+03   |
----------------------------------------
Eval num_timesteps=448000, episode_reward=469.77 +/- 199.60
Episode length: 419.62 +/- 171.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 420      |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=526.02 +/- 157.31
Episode length: 414.36 +/- 177.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 267      |
|    ep_rew_mean     | 578      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 438      |
|    time_elapsed    | 11870    |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.13
Eval num_timesteps=449000, episode_reward=565.14 +/- 157.92
Episode length: 409.16 +/- 178.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 565         |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.054220643 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.802      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0005      |
|    loss                 | 673         |
|    n_updates            | 1913        |
|    policy_gradient_loss | 0.0151      |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=578.60 +/- 175.66
Episode length: 321.74 +/- 195.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 322      |
|    mean_reward     | 579      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 274      |
|    ep_rew_mean     | 579      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 439      |
|    time_elapsed    | 11895    |
|    total_timesteps | 449536   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=450000, episode_reward=582.62 +/- 183.85
Episode length: 311.50 +/- 198.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 312        |
|    mean_reward          | 583        |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.03780596 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.0005     |
|    loss                 | 694        |
|    n_updates            | 1915       |
|    policy_gradient_loss | -0.000805  |
|    value_loss           | 1.29e+03   |
----------------------------------------
Eval num_timesteps=450500, episode_reward=592.00 +/- 152.63
Episode length: 346.66 +/- 193.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 347      |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 576      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 440      |
|    time_elapsed    | 11918    |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=451000, episode_reward=561.19 +/- 173.06
Episode length: 364.68 +/- 189.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 365         |
|    mean_reward          | 561         |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.064838216 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0005      |
|    loss                 | 954         |
|    n_updates            | 1918        |
|    policy_gradient_loss | 0.00381     |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=451500, episode_reward=551.15 +/- 167.86
Episode length: 409.96 +/- 175.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 410      |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 579      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 441      |
|    time_elapsed    | 11944    |
|    total_timesteps | 451584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=452000, episode_reward=444.66 +/- 150.53
Episode length: 485.36 +/- 119.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 485         |
|    mean_reward          | 445         |
| time/                   |             |
|    total_timesteps      | 452000      |
| train/                  |             |
|    approx_kl            | 0.024152013 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0005      |
|    loss                 | 754         |
|    n_updates            | 1919        |
|    policy_gradient_loss | 0.0247      |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=452500, episode_reward=460.38 +/- 171.74
Episode length: 405.54 +/- 176.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 578      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 442      |
|    time_elapsed    | 11976    |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=453000, episode_reward=411.21 +/- 177.08
Episode length: 502.84 +/- 93.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 503       |
|    mean_reward          | 411       |
| time/                   |           |
|    total_timesteps      | 453000    |
| train/                  |           |
|    approx_kl            | 0.0554721 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.912    |
|    explained_variance   | 0.783     |
|    learning_rate        | 0.0005    |
|    loss                 | 634       |
|    n_updates            | 1922      |
|    policy_gradient_loss | -0.00059  |
|    value_loss           | 1.29e+03  |
---------------------------------------
Eval num_timesteps=453500, episode_reward=423.84 +/- 187.78
Episode length: 469.26 +/- 139.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 579      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 443      |
|    time_elapsed    | 12009    |
|    total_timesteps | 453632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=454000, episode_reward=291.57 +/- 178.29
Episode length: 384.66 +/- 197.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 385        |
|    mean_reward          | 292        |
| time/                   |            |
|    total_timesteps      | 454000     |
| train/                  |            |
|    approx_kl            | 0.07881138 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0005     |
|    loss                 | 644        |
|    n_updates            | 1924       |
|    policy_gradient_loss | 0.0148     |
|    value_loss           | 973        |
----------------------------------------
Eval num_timesteps=454500, episode_reward=287.24 +/- 183.05
Episode length: 388.84 +/- 200.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 580      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 444      |
|    time_elapsed    | 12036    |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=455000, episode_reward=238.57 +/- 161.40
Episode length: 400.62 +/- 191.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 401        |
|    mean_reward          | 239        |
| time/                   |            |
|    total_timesteps      | 455000     |
| train/                  |            |
|    approx_kl            | 0.06267419 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.866     |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0005     |
|    loss                 | 292        |
|    n_updates            | 1927       |
|    policy_gradient_loss | 0.00775    |
|    value_loss           | 925        |
----------------------------------------
Eval num_timesteps=455500, episode_reward=290.11 +/- 160.51
Episode length: 388.80 +/- 195.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 389      |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 556      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 445      |
|    time_elapsed    | 12063    |
|    total_timesteps | 455680   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=456000, episode_reward=234.46 +/- 172.51
Episode length: 354.40 +/- 212.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 354        |
|    mean_reward          | 234        |
| time/                   |            |
|    total_timesteps      | 456000     |
| train/                  |            |
|    approx_kl            | 0.04343955 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.854     |
|    explained_variance   | 0.477      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.69e+03   |
|    n_updates            | 1931       |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 3.42e+03   |
----------------------------------------
Eval num_timesteps=456500, episode_reward=216.00 +/- 194.79
Episode length: 328.92 +/- 223.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 329      |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 446      |
|    time_elapsed    | 12087    |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=457000, episode_reward=289.22 +/- 217.35
Episode length: 411.94 +/- 192.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 412        |
|    mean_reward          | 289        |
| time/                   |            |
|    total_timesteps      | 457000     |
| train/                  |            |
|    approx_kl            | 0.06042044 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.978     |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.0005     |
|    loss                 | 516        |
|    n_updates            | 1935       |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 1.31e+03   |
----------------------------------------
Eval num_timesteps=457500, episode_reward=321.02 +/- 187.82
Episode length: 410.06 +/- 187.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 410      |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 322      |
|    ep_rew_mean     | 530      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 447      |
|    time_elapsed    | 12115    |
|    total_timesteps | 457728   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=458000, episode_reward=397.12 +/- 171.52
Episode length: 425.14 +/- 179.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 425         |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 458000      |
| train/                  |             |
|    approx_kl            | 0.048179954 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.961      |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.42e+03    |
|    n_updates            | 1937        |
|    policy_gradient_loss | 0.00984     |
|    value_loss           | 2.88e+03    |
-----------------------------------------
Eval num_timesteps=458500, episode_reward=316.68 +/- 203.52
Episode length: 364.92 +/- 214.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 365      |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 331      |
|    ep_rew_mean     | 532      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 448      |
|    time_elapsed    | 12142    |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=459000, episode_reward=449.32 +/- 181.84
Episode length: 391.92 +/- 192.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 449         |
| time/                   |             |
|    total_timesteps      | 459000      |
| train/                  |             |
|    approx_kl            | 0.032145057 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0005      |
|    loss                 | 240         |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.00198     |
|    value_loss           | 596         |
-----------------------------------------
Eval num_timesteps=459500, episode_reward=468.31 +/- 163.26
Episode length: 428.72 +/- 173.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 337      |
|    ep_rew_mean     | 541      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 449      |
|    time_elapsed    | 12170    |
|    total_timesteps | 459776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=460000, episode_reward=536.19 +/- 173.13
Episode length: 315.72 +/- 198.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 536       |
| time/                   |           |
|    total_timesteps      | 460000    |
| train/                  |           |
|    approx_kl            | 0.0596311 |
|    clip_fraction        | 0.295     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03     |
|    explained_variance   | 0.586     |
|    learning_rate        | 0.0005    |
|    loss                 | 893       |
|    n_updates            | 1942      |
|    policy_gradient_loss | 0.0149    |
|    value_loss           | 2.18e+03  |
---------------------------------------
Eval num_timesteps=460500, episode_reward=503.67 +/- 227.03
Episode length: 244.20 +/- 194.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | 532      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 450      |
|    time_elapsed    | 12190    |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=461000, episode_reward=513.46 +/- 173.10
Episode length: 375.68 +/- 195.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | 513         |
| time/                   |             |
|    total_timesteps      | 461000      |
| train/                  |             |
|    approx_kl            | 0.035396248 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.79e+03    |
|    n_updates            | 1945        |
|    policy_gradient_loss | 0.00142     |
|    value_loss           | 3.33e+03    |
-----------------------------------------
Eval num_timesteps=461500, episode_reward=504.37 +/- 181.09
Episode length: 385.64 +/- 190.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 386      |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 526      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 451      |
|    time_elapsed    | 12216    |
|    total_timesteps | 461824   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=462000, episode_reward=536.93 +/- 186.92
Episode length: 269.34 +/- 186.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 269        |
|    mean_reward          | 537        |
| time/                   |            |
|    total_timesteps      | 462000     |
| train/                  |            |
|    approx_kl            | 0.03981607 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.789     |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.0005     |
|    loss                 | 937        |
|    n_updates            | 1948       |
|    policy_gradient_loss | -0.00444   |
|    value_loss           | 1.77e+03   |
----------------------------------------
Eval num_timesteps=462500, episode_reward=583.89 +/- 205.47
Episode length: 241.98 +/- 175.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 452      |
|    time_elapsed    | 12234    |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=463000, episode_reward=587.05 +/- 192.84
Episode length: 262.18 +/- 189.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 262        |
|    mean_reward          | 587        |
| time/                   |            |
|    total_timesteps      | 463000     |
| train/                  |            |
|    approx_kl            | 0.05495629 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.847     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.37e+03   |
|    n_updates            | 1950       |
|    policy_gradient_loss | 0.00972    |
|    value_loss           | 2.54e+03   |
----------------------------------------
Eval num_timesteps=463500, episode_reward=599.85 +/- 156.82
Episode length: 242.28 +/- 177.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 325      |
|    ep_rew_mean     | 526      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 453      |
|    time_elapsed    | 12251    |
|    total_timesteps | 463872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=464000, episode_reward=557.26 +/- 200.36
Episode length: 376.44 +/- 190.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 376         |
|    mean_reward          | 557         |
| time/                   |             |
|    total_timesteps      | 464000      |
| train/                  |             |
|    approx_kl            | 0.045128774 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.84e+03    |
|    n_updates            | 1952        |
|    policy_gradient_loss | -0.000733   |
|    value_loss           | 3.88e+03    |
-----------------------------------------
Eval num_timesteps=464500, episode_reward=613.03 +/- 176.96
Episode length: 337.00 +/- 189.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 325      |
|    ep_rew_mean     | 523      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 454      |
|    time_elapsed    | 12276    |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=465000, episode_reward=576.69 +/- 161.52
Episode length: 425.56 +/- 170.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 426         |
|    mean_reward          | 577         |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.035964668 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.737      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0005      |
|    loss                 | 754         |
|    n_updates            | 1955        |
|    policy_gradient_loss | -0.000933   |
|    value_loss           | 1.43e+03    |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=573.58 +/- 205.35
Episode length: 392.72 +/- 182.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 330      |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 455      |
|    time_elapsed    | 12304    |
|    total_timesteps | 465920   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=466000, episode_reward=547.79 +/- 202.19
Episode length: 384.56 +/- 183.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 385        |
|    mean_reward          | 548        |
| time/                   |            |
|    total_timesteps      | 466000     |
| train/                  |            |
|    approx_kl            | 0.06020444 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.889     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0005     |
|    loss                 | 550        |
|    n_updates            | 1958       |
|    policy_gradient_loss | 0.000203   |
|    value_loss           | 1.26e+03   |
----------------------------------------
Eval num_timesteps=466500, episode_reward=583.63 +/- 160.30
Episode length: 416.74 +/- 175.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 417      |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 323      |
|    ep_rew_mean     | 526      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 456      |
|    time_elapsed    | 12331    |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=467000, episode_reward=543.62 +/- 197.52
Episode length: 352.22 +/- 192.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 352        |
|    mean_reward          | 544        |
| time/                   |            |
|    total_timesteps      | 467000     |
| train/                  |            |
|    approx_kl            | 0.06541983 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.21e+03   |
|    n_updates            | 1961       |
|    policy_gradient_loss | 0.0104     |
|    value_loss           | 2e+03      |
----------------------------------------
Eval num_timesteps=467500, episode_reward=573.60 +/- 194.97
Episode length: 337.58 +/- 193.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 530      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 457      |
|    time_elapsed    | 12355    |
|    total_timesteps | 467968   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=468000, episode_reward=596.32 +/- 219.89
Episode length: 237.58 +/- 174.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 596        |
| time/                   |            |
|    total_timesteps      | 468000     |
| train/                  |            |
|    approx_kl            | 0.06889211 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0005     |
|    loss                 | 854        |
|    n_updates            | 1964       |
|    policy_gradient_loss | -0.0028    |
|    value_loss           | 1.6e+03    |
----------------------------------------
Eval num_timesteps=468500, episode_reward=649.47 +/- 179.78
Episode length: 219.90 +/- 151.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 458      |
|    time_elapsed    | 12371    |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=469000, episode_reward=564.00 +/- 200.74
Episode length: 279.52 +/- 189.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 564         |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.051709063 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.48e+03    |
|    n_updates            | 1966        |
|    policy_gradient_loss | 0.00861     |
|    value_loss           | 3.06e+03    |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=578.01 +/- 197.69
Episode length: 283.04 +/- 193.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=592.63 +/- 197.64
Episode length: 253.58 +/- 181.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 459      |
|    time_elapsed    | 12399    |
|    total_timesteps | 470016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=470500, episode_reward=537.51 +/- 239.10
Episode length: 183.46 +/- 151.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 538         |
| time/                   |             |
|    total_timesteps      | 470500      |
| train/                  |             |
|    approx_kl            | 0.025247484 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0005      |
|    loss                 | 627         |
|    n_updates            | 1967        |
|    policy_gradient_loss | 0.00905     |
|    value_loss           | 957         |
-----------------------------------------
Eval num_timesteps=471000, episode_reward=528.56 +/- 266.83
Episode length: 165.10 +/- 147.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 529      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 552      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 460      |
|    time_elapsed    | 12411    |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=471500, episode_reward=453.03 +/- 244.78
Episode length: 137.22 +/- 117.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 137         |
|    mean_reward          | 453         |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.025688296 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.428      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.87e+03    |
|    n_updates            | 1968        |
|    policy_gradient_loss | 0.0221      |
|    value_loss           | 4.21e+03    |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=421.03 +/- 249.27
Episode length: 150.04 +/- 138.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 258      |
|    ep_rew_mean     | 546      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 461      |
|    time_elapsed    | 12422    |
|    total_timesteps | 472064   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=472500, episode_reward=509.23 +/- 219.90
Episode length: 144.02 +/- 114.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 144        |
|    mean_reward          | 509        |
| time/                   |            |
|    total_timesteps      | 472500     |
| train/                  |            |
|    approx_kl            | 0.06061763 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.558     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.92e+03   |
|    n_updates            | 1972       |
|    policy_gradient_loss | -0.00273   |
|    value_loss           | 6.03e+03   |
----------------------------------------
Eval num_timesteps=473000, episode_reward=517.31 +/- 230.79
Episode length: 155.84 +/- 125.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 247      |
|    ep_rew_mean     | 532      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 462      |
|    time_elapsed    | 12433    |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=473500, episode_reward=533.26 +/- 206.83
Episode length: 122.76 +/- 84.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 123        |
|    mean_reward          | 533        |
| time/                   |            |
|    total_timesteps      | 473500     |
| train/                  |            |
|    approx_kl            | 0.02407147 |
|    clip_fraction        | 0.0947     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.445     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.02e+03   |
|    n_updates            | 1973       |
|    policy_gradient_loss | 0.0214     |
|    value_loss           | 5.35e+03   |
----------------------------------------
Eval num_timesteps=474000, episode_reward=540.59 +/- 244.57
Episode length: 126.90 +/- 103.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 526      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 463      |
|    time_elapsed    | 12442    |
|    total_timesteps | 474112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=474500, episode_reward=347.97 +/- 240.19
Episode length: 107.78 +/- 90.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 348         |
| time/                   |             |
|    total_timesteps      | 474500      |
| train/                  |             |
|    approx_kl            | 0.033827215 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.21e+03    |
|    n_updates            | 1974        |
|    policy_gradient_loss | 0.03        |
|    value_loss           | 5.76e+03    |
-----------------------------------------
Eval num_timesteps=475000, episode_reward=417.40 +/- 228.98
Episode length: 103.34 +/- 65.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 525      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 464      |
|    time_elapsed    | 12450    |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=475500, episode_reward=404.00 +/- 239.11
Episode length: 108.72 +/- 67.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 404         |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.061228245 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.01e+03    |
|    n_updates            | 1978        |
|    policy_gradient_loss | 0.0103      |
|    value_loss           | 6.41e+03    |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=345.83 +/- 218.52
Episode length: 93.20 +/- 22.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 502      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 465      |
|    time_elapsed    | 12458    |
|    total_timesteps | 476160   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=476500, episode_reward=360.36 +/- 221.85
Episode length: 121.10 +/- 91.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 121         |
|    mean_reward          | 360         |
| time/                   |             |
|    total_timesteps      | 476500      |
| train/                  |             |
|    approx_kl            | 0.058781214 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.71e+03    |
|    n_updates            | 1980        |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 8.43e+03    |
-----------------------------------------
Eval num_timesteps=477000, episode_reward=357.27 +/- 226.37
Episode length: 122.50 +/- 104.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 357      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 486      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 466      |
|    time_elapsed    | 12467    |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=477500, episode_reward=339.44 +/- 198.44
Episode length: 93.86 +/- 63.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.9        |
|    mean_reward          | 339         |
| time/                   |             |
|    total_timesteps      | 477500      |
| train/                  |             |
|    approx_kl            | 0.018937806 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.497      |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.54e+03    |
|    n_updates            | 1981        |
|    policy_gradient_loss | 0.0034      |
|    value_loss           | 4.02e+03    |
-----------------------------------------
Eval num_timesteps=478000, episode_reward=331.59 +/- 226.84
Episode length: 82.42 +/- 23.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 467      |
|    time_elapsed    | 12473    |
|    total_timesteps | 478208   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=478500, episode_reward=421.30 +/- 200.59
Episode length: 94.06 +/- 21.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.1        |
|    mean_reward          | 421         |
| time/                   |             |
|    total_timesteps      | 478500      |
| train/                  |             |
|    approx_kl            | 0.041100807 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.44e+03    |
|    n_updates            | 1983        |
|    policy_gradient_loss | 0.0195      |
|    value_loss           | 8.7e+03     |
-----------------------------------------
Eval num_timesteps=479000, episode_reward=453.00 +/- 211.97
Episode length: 93.76 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 468      |
|    time_elapsed    | 12481    |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=479500, episode_reward=521.52 +/- 196.65
Episode length: 118.88 +/- 60.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 119         |
|    mean_reward          | 522         |
| time/                   |             |
|    total_timesteps      | 479500      |
| train/                  |             |
|    approx_kl            | 0.035975162 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.45e+03    |
|    n_updates            | 1986        |
|    policy_gradient_loss | 0.00393     |
|    value_loss           | 5.36e+03    |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=507.21 +/- 193.10
Episode length: 159.64 +/- 137.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 405      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 469      |
|    time_elapsed    | 12491    |
|    total_timesteps | 480256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=480500, episode_reward=425.77 +/- 228.76
Episode length: 152.42 +/- 95.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 426         |
| time/                   |             |
|    total_timesteps      | 480500      |
| train/                  |             |
|    approx_kl            | 0.037394904 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.52e+03    |
|    n_updates            | 1988        |
|    policy_gradient_loss | 0.00494     |
|    value_loss           | 4.95e+03    |
-----------------------------------------
Eval num_timesteps=481000, episode_reward=509.06 +/- 231.03
Episode length: 164.76 +/- 126.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 470      |
|    time_elapsed    | 12502    |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=481500, episode_reward=511.55 +/- 189.76
Episode length: 223.82 +/- 180.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 512         |
| time/                   |             |
|    total_timesteps      | 481500      |
| train/                  |             |
|    approx_kl            | 0.067283705 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.26e+03    |
|    n_updates            | 1990        |
|    policy_gradient_loss | 0.0158      |
|    value_loss           | 5.62e+03    |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=563.45 +/- 190.65
Episode length: 256.92 +/- 186.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 563      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 471      |
|    time_elapsed    | 12519    |
|    total_timesteps | 482304   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=482500, episode_reward=547.60 +/- 222.73
Episode length: 184.24 +/- 144.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 184         |
|    mean_reward          | 548         |
| time/                   |             |
|    total_timesteps      | 482500      |
| train/                  |             |
|    approx_kl            | 0.039242256 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0005      |
|    loss                 | 816         |
|    n_updates            | 1993        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=483000, episode_reward=529.74 +/- 208.00
Episode length: 195.26 +/- 166.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 472      |
|    time_elapsed    | 12533    |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=483500, episode_reward=445.79 +/- 210.80
Episode length: 227.78 +/- 176.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 446         |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.045055263 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.45e+03    |
|    n_updates            | 1995        |
|    policy_gradient_loss | 0.00565     |
|    value_loss           | 3.15e+03    |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=518.06 +/- 219.65
Episode length: 205.38 +/- 126.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 473      |
|    time_elapsed    | 12548    |
|    total_timesteps | 484352   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=484500, episode_reward=616.51 +/- 198.12
Episode length: 150.76 +/- 103.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 617         |
| time/                   |             |
|    total_timesteps      | 484500      |
| train/                  |             |
|    approx_kl            | 0.054518558 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0005      |
|    loss                 | 997         |
|    n_updates            | 1998        |
|    policy_gradient_loss | -0.000623   |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=485000, episode_reward=586.14 +/- 183.06
Episode length: 161.94 +/- 123.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 586      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 474      |
|    time_elapsed    | 12560    |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=485500, episode_reward=642.76 +/- 163.87
Episode length: 149.82 +/- 111.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 643         |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.058488857 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.64e+03    |
|    n_updates            | 2002        |
|    policy_gradient_loss | -0.00357    |
|    value_loss           | 3.59e+03    |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=583.09 +/- 219.11
Episode length: 168.38 +/- 134.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 429      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 475      |
|    time_elapsed    | 12571    |
|    total_timesteps | 486400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=486500, episode_reward=574.31 +/- 188.48
Episode length: 145.38 +/- 113.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 145         |
|    mean_reward          | 574         |
| time/                   |             |
|    total_timesteps      | 486500      |
| train/                  |             |
|    approx_kl            | 0.046014655 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.56e+03    |
|    n_updates            | 2004        |
|    policy_gradient_loss | 0.00178     |
|    value_loss           | 3.19e+03    |
-----------------------------------------
Eval num_timesteps=487000, episode_reward=574.54 +/- 191.47
Episode length: 164.06 +/- 134.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 476      |
|    time_elapsed    | 12583    |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=487500, episode_reward=506.98 +/- 209.36
Episode length: 117.82 +/- 61.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 507         |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.043143235 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.34       |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0005      |
|    loss                 | 584         |
|    n_updates            | 2007        |
|    policy_gradient_loss | 0.00992     |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=535.84 +/- 187.85
Episode length: 115.92 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 464      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 477      |
|    time_elapsed    | 12591    |
|    total_timesteps | 488448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=488500, episode_reward=634.20 +/- 170.62
Episode length: 128.22 +/- 82.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 634         |
| time/                   |             |
|    total_timesteps      | 488500      |
| train/                  |             |
|    approx_kl            | 0.030630836 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.443      |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.5e+03     |
|    n_updates            | 2008        |
|    policy_gradient_loss | 0.0115      |
|    value_loss           | 4.63e+03    |
-----------------------------------------
Eval num_timesteps=489000, episode_reward=619.85 +/- 176.32
Episode length: 133.22 +/- 100.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 488      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 478      |
|    time_elapsed    | 12601    |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=489500, episode_reward=562.88 +/- 189.40
Episode length: 158.44 +/- 136.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 158        |
|    mean_reward          | 563        |
| time/                   |            |
|    total_timesteps      | 489500     |
| train/                  |            |
|    approx_kl            | 0.06024761 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.431     |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.98e+03   |
|    n_updates            | 2011       |
|    policy_gradient_loss | 6.32e-05   |
|    value_loss           | 4.25e+03   |
----------------------------------------
Eval num_timesteps=490000, episode_reward=607.76 +/- 177.29
Episode length: 164.70 +/- 136.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 511      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 479      |
|    time_elapsed    | 12613    |
|    total_timesteps | 490496   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=490500, episode_reward=566.58 +/- 196.99
Episode length: 225.00 +/- 178.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 567         |
| time/                   |             |
|    total_timesteps      | 490500      |
| train/                  |             |
|    approx_kl            | 0.052919485 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.87e+03    |
|    n_updates            | 2014        |
|    policy_gradient_loss | 0.00682     |
|    value_loss           | 3.89e+03    |
-----------------------------------------
Eval num_timesteps=491000, episode_reward=577.89 +/- 205.04
Episode length: 208.78 +/- 170.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=576.38 +/- 192.85
Episode length: 189.08 +/- 152.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 514      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 480      |
|    time_elapsed    | 12635    |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=492000, episode_reward=646.01 +/- 183.95
Episode length: 230.24 +/- 175.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 230         |
|    mean_reward          | 646         |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.060525954 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.426      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0005      |
|    loss                 | 938         |
|    n_updates            | 2017        |
|    policy_gradient_loss | 0.00152     |
|    value_loss           | 1.82e+03    |
-----------------------------------------
Eval num_timesteps=492500, episode_reward=598.91 +/- 198.81
Episode length: 242.18 +/- 177.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 481      |
|    time_elapsed    | 12651    |
|    total_timesteps | 492544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=493000, episode_reward=468.64 +/- 242.81
Episode length: 252.80 +/- 187.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 253        |
|    mean_reward          | 469        |
| time/                   |            |
|    total_timesteps      | 493000     |
| train/                  |            |
|    approx_kl            | 0.04112924 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.556     |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.22e+03   |
|    n_updates            | 2018       |
|    policy_gradient_loss | 0.024      |
|    value_loss           | 2.58e+03   |
----------------------------------------
Eval num_timesteps=493500, episode_reward=447.24 +/- 222.43
Episode length: 288.08 +/- 193.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 526      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 482      |
|    time_elapsed    | 12670    |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=494000, episode_reward=649.28 +/- 203.21
Episode length: 183.02 +/- 130.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 649         |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.054417048 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.543      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.03e+03    |
|    n_updates            | 2020        |
|    policy_gradient_loss | 0.00271     |
|    value_loss           | 3.73e+03    |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=623.15 +/- 211.12
Episode length: 158.08 +/- 84.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 38       |
|    iterations      | 483      |
|    time_elapsed    | 12682    |
|    total_timesteps | 494592   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=495000, episode_reward=555.78 +/- 216.38
Episode length: 165.72 +/- 123.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 556         |
| time/                   |             |
|    total_timesteps      | 495000      |
| train/                  |             |
|    approx_kl            | 0.032589473 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.54e+03    |
|    n_updates            | 2023        |
|    policy_gradient_loss | 0.011       |
|    value_loss           | 3.46e+03    |
-----------------------------------------
Eval num_timesteps=495500, episode_reward=567.61 +/- 197.43
Episode length: 138.82 +/- 59.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 484      |
|    time_elapsed    | 12693    |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=496000, episode_reward=490.75 +/- 257.19
Episode length: 166.36 +/- 135.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 166        |
|    mean_reward          | 491        |
| time/                   |            |
|    total_timesteps      | 496000     |
| train/                  |            |
|    approx_kl            | 0.04506589 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.428     |
|    explained_variance   | 0.417      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.74e+03   |
|    n_updates            | 2028       |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 4.3e+03    |
----------------------------------------
Eval num_timesteps=496500, episode_reward=537.23 +/- 226.31
Episode length: 183.00 +/- 140.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 537      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 560      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 485      |
|    time_elapsed    | 12706    |
|    total_timesteps | 496640   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=497000, episode_reward=467.85 +/- 233.29
Episode length: 265.70 +/- 189.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 266        |
|    mean_reward          | 468        |
| time/                   |            |
|    total_timesteps      | 497000     |
| train/                  |            |
|    approx_kl            | 0.03563922 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.449     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.04e+03   |
|    n_updates            | 2031       |
|    policy_gradient_loss | 0.00151    |
|    value_loss           | 2.05e+03   |
----------------------------------------
Eval num_timesteps=497500, episode_reward=467.68 +/- 254.30
Episode length: 248.64 +/- 192.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 563      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 486      |
|    time_elapsed    | 12724    |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=498000, episode_reward=537.44 +/- 188.09
Episode length: 283.60 +/- 198.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 284         |
|    mean_reward          | 537         |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.054302514 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.467      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.71e+03    |
|    n_updates            | 2034        |
|    policy_gradient_loss | 0.00664     |
|    value_loss           | 4.04e+03    |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=600.63 +/- 202.97
Episode length: 237.12 +/- 180.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 601      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 571      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 487      |
|    time_elapsed    | 12742    |
|    total_timesteps | 498688   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=499000, episode_reward=523.44 +/- 230.71
Episode length: 217.30 +/- 174.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 217         |
|    mean_reward          | 523         |
| time/                   |             |
|    total_timesteps      | 499000      |
| train/                  |             |
|    approx_kl            | 0.066961706 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.435      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.61e+03    |
|    n_updates            | 2038        |
|    policy_gradient_loss | -0.00742    |
|    value_loss           | 3.69e+03    |
-----------------------------------------
Eval num_timesteps=499500, episode_reward=531.15 +/- 227.92
Episode length: 203.80 +/- 172.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 531      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 577      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 488      |
|    time_elapsed    | 12757    |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=500000, episode_reward=413.60 +/- 242.55
Episode length: 231.90 +/- 187.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 232         |
|    mean_reward          | 414         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.031355776 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.407      |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.65e+03    |
|    n_updates            | 2039        |
|    policy_gradient_loss | 0.0297      |
|    value_loss           | 3.36e+03    |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=481.54 +/- 238.65
Episode length: 273.86 +/- 198.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 574      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 489      |
|    time_elapsed    | 12775    |
|    total_timesteps | 500736   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=501000, episode_reward=422.89 +/- 234.97
Episode length: 243.96 +/- 203.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 423         |
| time/                   |             |
|    total_timesteps      | 501000      |
| train/                  |             |
|    approx_kl            | 0.058728874 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.292      |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0005      |
|    loss                 | 408         |
|    n_updates            | 2042        |
|    policy_gradient_loss | 0.00494     |
|    value_loss           | 1.07e+03    |
-----------------------------------------
Eval num_timesteps=501500, episode_reward=476.73 +/- 220.25
Episode length: 285.46 +/- 200.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 577      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 490      |
|    time_elapsed    | 12793    |
|    total_timesteps | 501760   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=502000, episode_reward=612.29 +/- 184.87
Episode length: 157.12 +/- 123.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 157        |
|    mean_reward          | 612        |
| time/                   |            |
|    total_timesteps      | 502000     |
| train/                  |            |
|    approx_kl            | 0.09858737 |
|    clip_fraction        | 0.096      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.306     |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0005     |
|    loss                 | 705        |
|    n_updates            | 2045       |
|    policy_gradient_loss | -0.00149   |
|    value_loss           | 1.1e+03    |
----------------------------------------
Eval num_timesteps=502500, episode_reward=566.26 +/- 182.79
Episode length: 224.86 +/- 178.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 578      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 491      |
|    time_elapsed    | 12807    |
|    total_timesteps | 502784   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=503000, episode_reward=573.48 +/- 184.46
Episode length: 267.84 +/- 193.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 573         |
| time/                   |             |
|    total_timesteps      | 503000      |
| train/                  |             |
|    approx_kl            | 0.031864956 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.404      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.25e+03    |
|    n_updates            | 2047        |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 2.29e+03    |
-----------------------------------------
Eval num_timesteps=503500, episode_reward=544.40 +/- 213.22
Episode length: 229.00 +/- 185.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 544      |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 579      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 492      |
|    time_elapsed    | 12824    |
|    total_timesteps | 503808   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=504000, episode_reward=566.81 +/- 202.71
Episode length: 154.94 +/- 113.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | 567         |
| time/                   |             |
|    total_timesteps      | 504000      |
| train/                  |             |
|    approx_kl            | 0.063147575 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.502      |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.26e+03    |
|    n_updates            | 2050        |
|    policy_gradient_loss | 0.000449    |
|    value_loss           | 2.42e+03    |
-----------------------------------------
Eval num_timesteps=504500, episode_reward=585.28 +/- 228.37
Episode length: 128.44 +/- 83.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 585      |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 586      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 493      |
|    time_elapsed    | 12834    |
|    total_timesteps | 504832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=505000, episode_reward=602.17 +/- 199.23
Episode length: 107.62 +/- 17.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 602         |
| time/                   |             |
|    total_timesteps      | 505000      |
| train/                  |             |
|    approx_kl            | 0.043097124 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0005      |
|    loss                 | 991         |
|    n_updates            | 2052        |
|    policy_gradient_loss | 0.0029      |
|    value_loss           | 2.08e+03    |
-----------------------------------------
Eval num_timesteps=505500, episode_reward=571.05 +/- 234.14
Episode length: 112.22 +/- 62.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 571      |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 576      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 494      |
|    time_elapsed    | 12843    |
|    total_timesteps | 505856   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=506000, episode_reward=706.59 +/- 159.49
Episode length: 108.00 +/- 12.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 707         |
| time/                   |             |
|    total_timesteps      | 506000      |
| train/                  |             |
|    approx_kl            | 0.039536145 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.485      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.57e+03    |
|    n_updates            | 2054        |
|    policy_gradient_loss | 0.00485     |
|    value_loss           | 5.91e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=506500, episode_reward=614.19 +/- 210.09
Episode length: 140.56 +/- 114.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 141      |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 573      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 495      |
|    time_elapsed    | 12852    |
|    total_timesteps | 506880   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=507000, episode_reward=634.20 +/- 222.61
Episode length: 109.46 +/- 61.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 634         |
| time/                   |             |
|    total_timesteps      | 507000      |
| train/                  |             |
|    approx_kl            | 0.042783342 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0005      |
|    loss                 | 746         |
|    n_updates            | 2056        |
|    policy_gradient_loss | 0.00508     |
|    value_loss           | 1.63e+03    |
-----------------------------------------
Eval num_timesteps=507500, episode_reward=698.54 +/- 177.29
Episode length: 129.54 +/- 100.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 584      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 496      |
|    time_elapsed    | 12861    |
|    total_timesteps | 507904   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.10
Eval num_timesteps=508000, episode_reward=635.21 +/- 204.31
Episode length: 149.42 +/- 115.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 149        |
|    mean_reward          | 635        |
| time/                   |            |
|    total_timesteps      | 508000     |
| train/                  |            |
|    approx_kl            | 0.09733656 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.345     |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.43e+03   |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.00442   |
|    value_loss           | 3.35e+03   |
----------------------------------------
Eval num_timesteps=508500, episode_reward=612.17 +/- 220.30
Episode length: 138.10 +/- 100.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 593      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 497      |
|    time_elapsed    | 12872    |
|    total_timesteps | 508928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=509000, episode_reward=694.98 +/- 164.74
Episode length: 155.68 +/- 113.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 695         |
| time/                   |             |
|    total_timesteps      | 509000      |
| train/                  |             |
|    approx_kl            | 0.041287635 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.361      |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.67e+03    |
|    n_updates            | 2062        |
|    policy_gradient_loss | 0.00149     |
|    value_loss           | 4.69e+03    |
-----------------------------------------
Eval num_timesteps=509500, episode_reward=689.79 +/- 166.72
Episode length: 115.26 +/- 14.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 604      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 498      |
|    time_elapsed    | 12882    |
|    total_timesteps | 509952   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=510000, episode_reward=725.72 +/- 136.41
Episode length: 150.22 +/- 99.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 150        |
|    mean_reward          | 726        |
| time/                   |            |
|    total_timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.05582965 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.18e+03   |
|    n_updates            | 2065       |
|    policy_gradient_loss | 0.0146     |
|    value_loss           | 2.54e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=510500, episode_reward=683.72 +/- 178.44
Episode length: 178.80 +/- 140.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 610      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 499      |
|    time_elapsed    | 12894    |
|    total_timesteps | 510976   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=511000, episode_reward=707.37 +/- 144.50
Episode length: 168.08 +/- 122.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 707         |
| time/                   |             |
|    total_timesteps      | 511000      |
| train/                  |             |
|    approx_kl            | 0.057601042 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.385      |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.84e+03    |
|    n_updates            | 2068        |
|    policy_gradient_loss | 0.0025      |
|    value_loss           | 6.41e+03    |
-----------------------------------------
Eval num_timesteps=511500, episode_reward=632.96 +/- 204.12
Episode length: 188.24 +/- 150.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=644.91 +/- 196.02
Episode length: 178.94 +/- 140.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 614      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 500      |
|    time_elapsed    | 12913    |
|    total_timesteps | 512000   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.13
Eval num_timesteps=512500, episode_reward=649.07 +/- 196.33
Episode length: 169.84 +/- 132.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 170         |
|    mean_reward          | 649         |
| time/                   |             |
|    total_timesteps      | 512500      |
| train/                  |             |
|    approx_kl            | 0.055405084 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.428      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.35e+03    |
|    n_updates            | 2070        |
|    policy_gradient_loss | 0.0279      |
|    value_loss           | 2.16e+03    |
-----------------------------------------
Eval num_timesteps=513000, episode_reward=743.72 +/- 106.28
Episode length: 163.74 +/- 120.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 632      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 501      |
|    time_elapsed    | 12925    |
|    total_timesteps | 513024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=513500, episode_reward=698.72 +/- 159.34
Episode length: 242.46 +/- 176.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 242         |
|    mean_reward          | 699         |
| time/                   |             |
|    total_timesteps      | 513500      |
| train/                  |             |
|    approx_kl            | 0.025977109 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.49e+03    |
|    n_updates            | 2071        |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 2.65e+03    |
-----------------------------------------
Eval num_timesteps=514000, episode_reward=688.98 +/- 176.32
Episode length: 223.56 +/- 170.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 631      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 502      |
|    time_elapsed    | 12942    |
|    total_timesteps | 514048   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=514500, episode_reward=682.54 +/- 151.79
Episode length: 275.32 +/- 187.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 275        |
|    mean_reward          | 683        |
| time/                   |            |
|    total_timesteps      | 514500     |
| train/                  |            |
|    approx_kl            | 0.07978384 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0005     |
|    loss                 | 951        |
|    n_updates            | 2074       |
|    policy_gradient_loss | 0.00781    |
|    value_loss           | 1.65e+03   |
----------------------------------------
Eval num_timesteps=515000, episode_reward=654.43 +/- 194.15
Episode length: 239.62 +/- 178.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 634      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 503      |
|    time_elapsed    | 12960    |
|    total_timesteps | 515072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=515500, episode_reward=707.27 +/- 158.78
Episode length: 201.88 +/- 150.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 707         |
| time/                   |             |
|    total_timesteps      | 515500      |
| train/                  |             |
|    approx_kl            | 0.027150044 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.607      |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.17e+03    |
|    n_updates            | 2075        |
|    policy_gradient_loss | 0.00425     |
|    value_loss           | 2.49e+03    |
-----------------------------------------
Eval num_timesteps=516000, episode_reward=720.87 +/- 142.85
Episode length: 195.28 +/- 144.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 640      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 504      |
|    time_elapsed    | 12974    |
|    total_timesteps | 516096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=516500, episode_reward=685.85 +/- 162.11
Episode length: 304.50 +/- 188.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 304        |
|    mean_reward          | 686        |
| time/                   |            |
|    total_timesteps      | 516500     |
| train/                  |            |
|    approx_kl            | 0.05127446 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.615     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.02e+03   |
|    n_updates            | 2077       |
|    policy_gradient_loss | 0.00594    |
|    value_loss           | 2.03e+03   |
----------------------------------------
Eval num_timesteps=517000, episode_reward=649.05 +/- 195.03
Episode length: 316.48 +/- 192.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 641      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 505      |
|    time_elapsed    | 12995    |
|    total_timesteps | 517120   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=517500, episode_reward=669.88 +/- 153.48
Episode length: 325.24 +/- 193.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | 670         |
| time/                   |             |
|    total_timesteps      | 517500      |
| train/                  |             |
|    approx_kl            | 0.031459253 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.513      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.32e+03    |
|    n_updates            | 2079        |
|    policy_gradient_loss | 0.0144      |
|    value_loss           | 3.42e+03    |
-----------------------------------------
Eval num_timesteps=518000, episode_reward=613.79 +/- 158.20
Episode length: 372.58 +/- 187.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 373      |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 648      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 506      |
|    time_elapsed    | 13019    |
|    total_timesteps | 518144   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=518500, episode_reward=573.22 +/- 135.45
Episode length: 497.54 +/- 93.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 498        |
|    mean_reward          | 573        |
| time/                   |            |
|    total_timesteps      | 518500     |
| train/                  |            |
|    approx_kl            | 0.08601056 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.447     |
|    explained_variance   | 0.566      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.25e+03   |
|    n_updates            | 2081       |
|    policy_gradient_loss | 0.0076     |
|    value_loss           | 2.26e+03   |
----------------------------------------
Eval num_timesteps=519000, episode_reward=547.51 +/- 144.69
Episode length: 473.00 +/- 129.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 650      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 507      |
|    time_elapsed    | 13052    |
|    total_timesteps | 519168   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=519500, episode_reward=533.00 +/- 147.03
Episode length: 485.52 +/- 118.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 486        |
|    mean_reward          | 533        |
| time/                   |            |
|    total_timesteps      | 519500     |
| train/                  |            |
|    approx_kl            | 0.04935338 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0005     |
|    loss                 | 824        |
|    n_updates            | 2084       |
|    policy_gradient_loss | -0.00219   |
|    value_loss           | 1.48e+03   |
----------------------------------------
Eval num_timesteps=520000, episode_reward=529.69 +/- 143.54
Episode length: 485.32 +/- 119.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 485      |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 653      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 508      |
|    time_elapsed    | 13085    |
|    total_timesteps | 520192   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=520500, episode_reward=578.04 +/- 153.41
Episode length: 450.22 +/- 154.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | 578        |
| time/                   |            |
|    total_timesteps      | 520500     |
| train/                  |            |
|    approx_kl            | 0.03879143 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.561     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0005     |
|    loss                 | 745        |
|    n_updates            | 2086       |
|    policy_gradient_loss | 0.00361    |
|    value_loss           | 1.79e+03   |
----------------------------------------
Eval num_timesteps=521000, episode_reward=564.18 +/- 158.89
Episode length: 471.56 +/- 125.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 472      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 649      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 509      |
|    time_elapsed    | 13116    |
|    total_timesteps | 521216   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=521500, episode_reward=594.72 +/- 118.33
Episode length: 509.94 +/- 73.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 510         |
|    mean_reward          | 595         |
| time/                   |             |
|    total_timesteps      | 521500      |
| train/                  |             |
|    approx_kl            | 0.049767237 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0005      |
|    loss                 | 738         |
|    n_updates            | 2089        |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 1.62e+03    |
-----------------------------------------
Eval num_timesteps=522000, episode_reward=579.82 +/- 99.45
Episode length: 510.24 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 580      |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 650      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 510      |
|    time_elapsed    | 13151    |
|    total_timesteps | 522240   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=522500, episode_reward=586.45 +/- 155.41
Episode length: 411.94 +/- 173.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 412         |
|    mean_reward          | 586         |
| time/                   |             |
|    total_timesteps      | 522500      |
| train/                  |             |
|    approx_kl            | 0.044002034 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0005      |
|    loss                 | 309         |
|    n_updates            | 2092        |
|    policy_gradient_loss | 0.00287     |
|    value_loss           | 888         |
-----------------------------------------
Eval num_timesteps=523000, episode_reward=562.67 +/- 142.46
Episode length: 471.26 +/- 126.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 563      |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 652      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 511      |
|    time_elapsed    | 13181    |
|    total_timesteps | 523264   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=523500, episode_reward=580.90 +/- 169.80
Episode length: 407.74 +/- 171.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 408        |
|    mean_reward          | 581        |
| time/                   |            |
|    total_timesteps      | 523500     |
| train/                  |            |
|    approx_kl            | 0.04463466 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.8       |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0005     |
|    loss                 | 469        |
|    n_updates            | 2095       |
|    policy_gradient_loss | -0.000294  |
|    value_loss           | 1.21e+03   |
----------------------------------------
Eval num_timesteps=524000, episode_reward=603.15 +/- 148.82
Episode length: 444.06 +/- 153.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 444      |
|    mean_reward     | 603      |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 656      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 512      |
|    time_elapsed    | 13210    |
|    total_timesteps | 524288   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=524500, episode_reward=626.54 +/- 113.66
Episode length: 473.44 +/- 127.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 473        |
|    mean_reward          | 627        |
| time/                   |            |
|    total_timesteps      | 524500     |
| train/                  |            |
|    approx_kl            | 0.03791527 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.2e+03    |
|    n_updates            | 2098       |
|    policy_gradient_loss | -0.00749   |
|    value_loss           | 2.4e+03    |
----------------------------------------
Eval num_timesteps=525000, episode_reward=592.87 +/- 162.52
Episode length: 430.20 +/- 158.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 430      |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 662      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 513      |
|    time_elapsed    | 13240    |
|    total_timesteps | 525312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=525500, episode_reward=604.48 +/- 218.82
Episode length: 284.30 +/- 178.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 284        |
|    mean_reward          | 604        |
| time/                   |            |
|    total_timesteps      | 525500     |
| train/                  |            |
|    approx_kl            | 0.05469383 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.931     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0005     |
|    loss                 | 519        |
|    n_updates            | 2100       |
|    policy_gradient_loss | 0.0068     |
|    value_loss           | 1.23e+03   |
----------------------------------------
Eval num_timesteps=526000, episode_reward=649.87 +/- 139.23
Episode length: 341.84 +/- 176.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 665      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 514      |
|    time_elapsed    | 13262    |
|    total_timesteps | 526336   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=526500, episode_reward=636.43 +/- 162.78
Episode length: 232.38 +/- 166.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 636        |
| time/                   |            |
|    total_timesteps      | 526500     |
| train/                  |            |
|    approx_kl            | 0.04424541 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0005     |
|    loss                 | 395        |
|    n_updates            | 2104       |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 977        |
----------------------------------------
Eval num_timesteps=527000, episode_reward=612.69 +/- 200.81
Episode length: 239.68 +/- 173.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 680      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 515      |
|    time_elapsed    | 13278    |
|    total_timesteps | 527360   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=527500, episode_reward=665.45 +/- 145.63
Episode length: 303.60 +/- 188.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 304        |
|    mean_reward          | 665        |
| time/                   |            |
|    total_timesteps      | 527500     |
| train/                  |            |
|    approx_kl            | 0.05913698 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.607     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.29e+03   |
|    n_updates            | 2108       |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 2.28e+03   |
----------------------------------------
Eval num_timesteps=528000, episode_reward=613.72 +/- 190.38
Episode length: 338.52 +/- 195.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 692      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 516      |
|    time_elapsed    | 13300    |
|    total_timesteps | 528384   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.06
Eval num_timesteps=528500, episode_reward=669.18 +/- 153.75
Episode length: 254.40 +/- 180.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 254         |
|    mean_reward          | 669         |
| time/                   |             |
|    total_timesteps      | 528500      |
| train/                  |             |
|    approx_kl            | 0.050236322 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0005      |
|    loss                 | 916         |
|    n_updates            | 2117        |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=529000, episode_reward=623.11 +/- 186.56
Episode length: 256.40 +/- 180.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 517      |
|    time_elapsed    | 13318    |
|    total_timesteps | 529408   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=529500, episode_reward=682.68 +/- 154.31
Episode length: 259.54 +/- 182.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | 683         |
| time/                   |             |
|    total_timesteps      | 529500      |
| train/                  |             |
|    approx_kl            | 0.057297014 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.23e+03    |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00983    |
|    value_loss           | 2.68e+03    |
-----------------------------------------
Eval num_timesteps=530000, episode_reward=634.67 +/- 206.89
Episode length: 299.52 +/- 195.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 697      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 518      |
|    time_elapsed    | 13337    |
|    total_timesteps | 530432   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=530500, episode_reward=634.71 +/- 216.78
Episode length: 262.74 +/- 189.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 263        |
|    mean_reward          | 635        |
| time/                   |            |
|    total_timesteps      | 530500     |
| train/                  |            |
|    approx_kl            | 0.04806933 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.592     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.48e+03   |
|    n_updates            | 2125       |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 2.69e+03   |
----------------------------------------
Eval num_timesteps=531000, episode_reward=647.46 +/- 176.72
Episode length: 296.20 +/- 195.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 697      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 519      |
|    time_elapsed    | 13356    |
|    total_timesteps | 531456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=531500, episode_reward=559.28 +/- 208.47
Episode length: 384.92 +/- 187.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 385       |
|    mean_reward          | 559       |
| time/                   |           |
|    total_timesteps      | 531500    |
| train/                  |           |
|    approx_kl            | 0.0439319 |
|    clip_fraction        | 0.207     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.764    |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.0005    |
|    loss                 | 633       |
|    n_updates            | 2127      |
|    policy_gradient_loss | 0.021     |
|    value_loss           | 1.69e+03  |
---------------------------------------
Eval num_timesteps=532000, episode_reward=600.47 +/- 172.84
Episode length: 431.74 +/- 166.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 432      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 699      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 520      |
|    time_elapsed    | 13384    |
|    total_timesteps | 532480   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=532500, episode_reward=519.21 +/- 199.38
Episode length: 445.88 +/- 158.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 446        |
|    mean_reward          | 519        |
| time/                   |            |
|    total_timesteps      | 532500     |
| train/                  |            |
|    approx_kl            | 0.07789455 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.89      |
|    explained_variance   | 0.706      |
|    learning_rate        | 0.0005     |
|    loss                 | 660        |
|    n_updates            | 2129       |
|    policy_gradient_loss | 0.0052     |
|    value_loss           | 1.28e+03   |
----------------------------------------
Eval num_timesteps=533000, episode_reward=548.39 +/- 185.21
Episode length: 462.78 +/- 143.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 463      |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=491.69 +/- 227.92
Episode length: 401.46 +/- 189.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 401      |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 243      |
|    ep_rew_mean     | 695      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 521      |
|    time_elapsed    | 13428    |
|    total_timesteps | 533504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.16
Eval num_timesteps=534000, episode_reward=534.83 +/- 162.72
Episode length: 486.18 +/- 116.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 486        |
|    mean_reward          | 535        |
| time/                   |            |
|    total_timesteps      | 534000     |
| train/                  |            |
|    approx_kl            | 0.10097298 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.814     |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.71e+03   |
|    n_updates            | 2131       |
|    policy_gradient_loss | 0.0108     |
|    value_loss           | 3.23e+03   |
----------------------------------------
Eval num_timesteps=534500, episode_reward=574.81 +/- 167.65
Episode length: 485.96 +/- 117.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 249      |
|    ep_rew_mean     | 697      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 522      |
|    time_elapsed    | 13462    |
|    total_timesteps | 534528   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=535000, episode_reward=581.05 +/- 163.17
Episode length: 453.56 +/- 149.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 454        |
|    mean_reward          | 581        |
| time/                   |            |
|    total_timesteps      | 535000     |
| train/                  |            |
|    approx_kl            | 0.04446275 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.826     |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.0005     |
|    loss                 | 718        |
|    n_updates            | 2133       |
|    policy_gradient_loss | 0.000704   |
|    value_loss           | 1.94e+03   |
----------------------------------------
Eval num_timesteps=535500, episode_reward=588.86 +/- 206.47
Episode length: 414.42 +/- 163.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 414      |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 523      |
|    time_elapsed    | 13491    |
|    total_timesteps | 535552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=536000, episode_reward=526.23 +/- 200.47
Episode length: 396.32 +/- 188.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 396       |
|    mean_reward          | 526       |
| time/                   |           |
|    total_timesteps      | 536000    |
| train/                  |           |
|    approx_kl            | 0.0419042 |
|    clip_fraction        | 0.161     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.78     |
|    explained_variance   | 0.412     |
|    learning_rate        | 0.0005    |
|    loss                 | 874       |
|    n_updates            | 2135      |
|    policy_gradient_loss | 0.000676  |
|    value_loss           | 1.58e+03  |
---------------------------------------
Eval num_timesteps=536500, episode_reward=584.64 +/- 163.29
Episode length: 447.08 +/- 149.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 447      |
|    mean_reward     | 585      |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 263      |
|    ep_rew_mean     | 700      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 524      |
|    time_elapsed    | 13520    |
|    total_timesteps | 536576   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=537000, episode_reward=502.84 +/- 179.39
Episode length: 466.84 +/- 144.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 467        |
|    mean_reward          | 503        |
| time/                   |            |
|    total_timesteps      | 537000     |
| train/                  |            |
|    approx_kl            | 0.04468097 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.646     |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0005     |
|    loss                 | 795        |
|    n_updates            | 2138       |
|    policy_gradient_loss | -0.00455   |
|    value_loss           | 1.88e+03   |
----------------------------------------
Eval num_timesteps=537500, episode_reward=584.84 +/- 186.49
Episode length: 465.94 +/- 134.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 466      |
|    mean_reward     | 585      |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | 700      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 525      |
|    time_elapsed    | 13552    |
|    total_timesteps | 537600   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=538000, episode_reward=410.71 +/- 121.94
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 411         |
| time/                   |             |
|    total_timesteps      | 538000      |
| train/                  |             |
|    approx_kl            | 0.051525943 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0005      |
|    loss                 | 653         |
|    n_updates            | 2140        |
|    policy_gradient_loss | 0.0116      |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=538500, episode_reward=412.24 +/- 145.61
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 279      |
|    ep_rew_mean     | 698      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 526      |
|    time_elapsed    | 13588    |
|    total_timesteps | 538624   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=539000, episode_reward=502.87 +/- 150.25
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | 503        |
| time/                   |            |
|    total_timesteps      | 539000     |
| train/                  |            |
|    approx_kl            | 0.04228279 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.851     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.0005     |
|    loss                 | 814        |
|    n_updates            | 2143       |
|    policy_gradient_loss | 0.00486    |
|    value_loss           | 1.16e+03   |
----------------------------------------
Eval num_timesteps=539500, episode_reward=485.84 +/- 154.07
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 694      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 527      |
|    time_elapsed    | 13624    |
|    total_timesteps | 539648   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=540000, episode_reward=511.44 +/- 163.77
Episode length: 515.80 +/- 64.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 511         |
| time/                   |             |
|    total_timesteps      | 540000      |
| train/                  |             |
|    approx_kl            | 0.042647514 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.877      |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0005      |
|    loss                 | 456         |
|    n_updates            | 2146        |
|    policy_gradient_loss | 0.00564     |
|    value_loss           | 860         |
-----------------------------------------
Eval num_timesteps=540500, episode_reward=510.95 +/- 161.11
Episode length: 516.84 +/- 57.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 695      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 528      |
|    time_elapsed    | 13659    |
|    total_timesteps | 540672   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=541000, episode_reward=463.34 +/- 175.51
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 463         |
| time/                   |             |
|    total_timesteps      | 541000      |
| train/                  |             |
|    approx_kl            | 0.040939137 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0005      |
|    loss                 | 285         |
|    n_updates            | 2149        |
|    policy_gradient_loss | 0.00537     |
|    value_loss           | 849         |
-----------------------------------------
Eval num_timesteps=541500, episode_reward=481.26 +/- 175.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 692      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 529      |
|    time_elapsed    | 13694    |
|    total_timesteps | 541696   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.08
Eval num_timesteps=542000, episode_reward=369.62 +/- 185.55
Episode length: 517.78 +/- 50.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 518        |
|    mean_reward          | 370        |
| time/                   |            |
|    total_timesteps      | 542000     |
| train/                  |            |
|    approx_kl            | 0.05704544 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.987     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.0005     |
|    loss                 | 701        |
|    n_updates            | 2154       |
|    policy_gradient_loss | 0.00522    |
|    value_loss           | 1.4e+03    |
----------------------------------------
Eval num_timesteps=542500, episode_reward=396.96 +/- 187.26
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 690      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 530      |
|    time_elapsed    | 13730    |
|    total_timesteps | 542720   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=543000, episode_reward=521.79 +/- 142.41
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 522         |
| time/                   |             |
|    total_timesteps      | 543000      |
| train/                  |             |
|    approx_kl            | 0.042386115 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.868      |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0005      |
|    loss                 | 882         |
|    n_updates            | 2157        |
|    policy_gradient_loss | 0.0212      |
|    value_loss           | 1.43e+03    |
-----------------------------------------
Eval num_timesteps=543500, episode_reward=540.91 +/- 134.57
Episode length: 509.44 +/- 76.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 686      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 531      |
|    time_elapsed    | 13765    |
|    total_timesteps | 543744   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=544000, episode_reward=394.77 +/- 140.72
Episode length: 502.28 +/- 89.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 502       |
|    mean_reward          | 395       |
| time/                   |           |
|    total_timesteps      | 544000    |
| train/                  |           |
|    approx_kl            | 0.0718459 |
|    clip_fraction        | 0.185     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.824    |
|    explained_variance   | 0.62      |
|    learning_rate        | 0.0005    |
|    loss                 | 686       |
|    n_updates            | 2160      |
|    policy_gradient_loss | 0.00947   |
|    value_loss           | 1.32e+03  |
---------------------------------------
Eval num_timesteps=544500, episode_reward=393.97 +/- 175.09
Episode length: 514.90 +/- 64.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 320      |
|    ep_rew_mean     | 682      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 532      |
|    time_elapsed    | 13799    |
|    total_timesteps | 544768   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.09
Eval num_timesteps=545000, episode_reward=462.19 +/- 162.93
Episode length: 500.40 +/- 97.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 462         |
| time/                   |             |
|    total_timesteps      | 545000      |
| train/                  |             |
|    approx_kl            | 0.051640134 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0005      |
|    loss                 | 772         |
|    n_updates            | 2165        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 1.13e+03    |
-----------------------------------------
Eval num_timesteps=545500, episode_reward=475.12 +/- 160.53
Episode length: 508.36 +/- 81.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 508      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 533      |
|    time_elapsed    | 13834    |
|    total_timesteps | 545792   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=546000, episode_reward=488.00 +/- 163.03
Episode length: 499.52 +/- 101.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 500         |
|    mean_reward          | 488         |
| time/                   |             |
|    total_timesteps      | 546000      |
| train/                  |             |
|    approx_kl            | 0.052043274 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.55e+03    |
|    n_updates            | 2167        |
|    policy_gradient_loss | 0.00236     |
|    value_loss           | 2.76e+03    |
-----------------------------------------
Eval num_timesteps=546500, episode_reward=501.46 +/- 187.30
Episode length: 489.52 +/- 120.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 490      |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 331      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 534      |
|    time_elapsed    | 13867    |
|    total_timesteps | 546816   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=547000, episode_reward=399.91 +/- 161.88
Episode length: 501.56 +/- 95.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 547000      |
| train/                  |             |
|    approx_kl            | 0.051486757 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0005      |
|    loss                 | 534         |
|    n_updates            | 2171        |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 1.01e+03    |
-----------------------------------------
Eval num_timesteps=547500, episode_reward=411.75 +/- 145.54
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 535      |
|    time_elapsed    | 13902    |
|    total_timesteps | 547840   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=548000, episode_reward=522.57 +/- 140.70
Episode length: 517.18 +/- 54.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 517         |
|    mean_reward          | 523         |
| time/                   |             |
|    total_timesteps      | 548000      |
| train/                  |             |
|    approx_kl            | 0.048533708 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0005      |
|    loss                 | 646         |
|    n_updates            | 2174        |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=548500, episode_reward=474.76 +/- 140.55
Episode length: 501.92 +/- 91.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 502      |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | 677      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 536      |
|    time_elapsed    | 13936    |
|    total_timesteps | 548864   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=549000, episode_reward=575.56 +/- 150.88
Episode length: 478.96 +/- 126.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | 576         |
| time/                   |             |
|    total_timesteps      | 549000      |
| train/                  |             |
|    approx_kl            | 0.036548987 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0005      |
|    loss                 | 723         |
|    n_updates            | 2176        |
|    policy_gradient_loss | 0.00621     |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=549500, episode_reward=583.25 +/- 132.47
Episode length: 509.80 +/- 75.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 510      |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 677      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 537      |
|    time_elapsed    | 13970    |
|    total_timesteps | 549888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=550000, episode_reward=556.33 +/- 165.40
Episode length: 468.50 +/- 132.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 468         |
|    mean_reward          | 556         |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.058050722 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0005      |
|    loss                 | 714         |
|    n_updates            | 2178        |
|    policy_gradient_loss | 0.0197      |
|    value_loss           | 1.64e+03    |
-----------------------------------------
Eval num_timesteps=550500, episode_reward=625.81 +/- 131.76
Episode length: 470.22 +/- 135.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 470      |
|    mean_reward     | 626      |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 346      |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 538      |
|    time_elapsed    | 14003    |
|    total_timesteps | 550912   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=551000, episode_reward=656.01 +/- 171.59
Episode length: 381.54 +/- 187.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 382         |
|    mean_reward          | 656         |
| time/                   |             |
|    total_timesteps      | 551000      |
| train/                  |             |
|    approx_kl            | 0.054935396 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.952      |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0005      |
|    loss                 | 931         |
|    n_updates            | 2181        |
|    policy_gradient_loss | -0.00968    |
|    value_loss           | 1.73e+03    |
-----------------------------------------
Eval num_timesteps=551500, episode_reward=644.38 +/- 136.65
Episode length: 440.34 +/- 159.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 440      |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 352      |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 539      |
|    time_elapsed    | 14031    |
|    total_timesteps | 551936   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=552000, episode_reward=681.62 +/- 153.67
Episode length: 336.88 +/- 197.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 337        |
|    mean_reward          | 682        |
| time/                   |            |
|    total_timesteps      | 552000     |
| train/                  |            |
|    approx_kl            | 0.03933447 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.738     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0005     |
|    loss                 | 703        |
|    n_updates            | 2184       |
|    policy_gradient_loss | -4.18e-05  |
|    value_loss           | 1.41e+03   |
----------------------------------------
Eval num_timesteps=552500, episode_reward=597.08 +/- 180.52
Episode length: 386.88 +/- 192.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 387      |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 351      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 540      |
|    time_elapsed    | 14056    |
|    total_timesteps | 552960   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=553000, episode_reward=683.99 +/- 150.26
Episode length: 328.68 +/- 196.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 329        |
|    mean_reward          | 684        |
| time/                   |            |
|    total_timesteps      | 553000     |
| train/                  |            |
|    approx_kl            | 0.04734812 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.3e+03    |
|    n_updates            | 2188       |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 2.24e+03   |
----------------------------------------
Eval num_timesteps=553500, episode_reward=649.11 +/- 171.65
Episode length: 341.80 +/- 198.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | 667      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 541      |
|    time_elapsed    | 14079    |
|    total_timesteps | 553984   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=554000, episode_reward=629.29 +/- 159.42
Episode length: 397.08 +/- 181.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 629         |
| time/                   |             |
|    total_timesteps      | 554000      |
| train/                  |             |
|    approx_kl            | 0.042271864 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.937      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0005      |
|    loss                 | 921         |
|    n_updates            | 2192        |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=554500, episode_reward=636.79 +/- 181.09
Episode length: 354.56 +/- 192.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 355      |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=678.10 +/- 157.34
Episode length: 343.04 +/- 192.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 343      |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 670      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 542      |
|    time_elapsed    | 14116    |
|    total_timesteps | 555008   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=555500, episode_reward=675.20 +/- 180.00
Episode length: 303.42 +/- 196.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 303        |
|    mean_reward          | 675        |
| time/                   |            |
|    total_timesteps      | 555500     |
| train/                  |            |
|    approx_kl            | 0.04045481 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.888     |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0005     |
|    loss                 | 965        |
|    n_updates            | 2197       |
|    policy_gradient_loss | 0.00138    |
|    value_loss           | 1.7e+03    |
----------------------------------------
Eval num_timesteps=556000, episode_reward=660.80 +/- 173.21
Episode length: 329.30 +/- 195.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 329      |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 671      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 543      |
|    time_elapsed    | 14138    |
|    total_timesteps | 556032   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=556500, episode_reward=691.59 +/- 171.12
Episode length: 272.32 +/- 194.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 692         |
| time/                   |             |
|    total_timesteps      | 556500      |
| train/                  |             |
|    approx_kl            | 0.075330526 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0005      |
|    loss                 | 843         |
|    n_updates            | 2199        |
|    policy_gradient_loss | 0.00945     |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=557000, episode_reward=653.24 +/- 163.34
Episode length: 325.52 +/- 202.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 326      |
|    mean_reward     | 653      |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 333      |
|    ep_rew_mean     | 679      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 544      |
|    time_elapsed    | 14159    |
|    total_timesteps | 557056   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=557500, episode_reward=689.57 +/- 171.44
Episode length: 260.68 +/- 192.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 261         |
|    mean_reward          | 690         |
| time/                   |             |
|    total_timesteps      | 557500      |
| train/                  |             |
|    approx_kl            | 0.044096522 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.63e+03    |
|    n_updates            | 2203        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 2.8e+03     |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=687.22 +/- 158.12
Episode length: 287.50 +/- 200.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 684      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 545      |
|    time_elapsed    | 14178    |
|    total_timesteps | 558080   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=558500, episode_reward=641.85 +/- 165.18
Episode length: 331.98 +/- 195.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 332        |
|    mean_reward          | 642        |
| time/                   |            |
|    total_timesteps      | 558500     |
| train/                  |            |
|    approx_kl            | 0.05053454 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.19e+03   |
|    n_updates            | 2207       |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 2.04e+03   |
----------------------------------------
Eval num_timesteps=559000, episode_reward=657.82 +/- 169.98
Episode length: 333.50 +/- 200.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 334      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 682      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 546      |
|    time_elapsed    | 14201    |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=559500, episode_reward=611.51 +/- 162.71
Episode length: 396.40 +/- 187.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | 612         |
| time/                   |             |
|    total_timesteps      | 559500      |
| train/                  |             |
|    approx_kl            | 0.042435993 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.815      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0005      |
|    loss                 | 874         |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.000664   |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=665.48 +/- 167.15
Episode length: 315.56 +/- 201.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 330      |
|    ep_rew_mean     | 678      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 547      |
|    time_elapsed    | 14226    |
|    total_timesteps | 560128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=560500, episode_reward=588.91 +/- 199.29
Episode length: 356.32 +/- 195.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 356         |
|    mean_reward          | 589         |
| time/                   |             |
|    total_timesteps      | 560500      |
| train/                  |             |
|    approx_kl            | 0.026311116 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0005      |
|    loss                 | 858         |
|    n_updates            | 2212        |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Eval num_timesteps=561000, episode_reward=633.41 +/- 214.15
Episode length: 299.92 +/- 199.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 681      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 548      |
|    time_elapsed    | 14248    |
|    total_timesteps | 561152   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=561500, episode_reward=606.87 +/- 206.21
Episode length: 367.28 +/- 193.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 367        |
|    mean_reward          | 607        |
| time/                   |            |
|    total_timesteps      | 561500     |
| train/                  |            |
|    approx_kl            | 0.04551444 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.899     |
|    explained_variance   | 0.87       |
|    learning_rate        | 0.0005     |
|    loss                 | 519        |
|    n_updates            | 2214       |
|    policy_gradient_loss | 0.00685    |
|    value_loss           | 1.13e+03   |
----------------------------------------
Eval num_timesteps=562000, episode_reward=609.17 +/- 182.01
Episode length: 385.90 +/- 185.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 386      |
|    mean_reward     | 609      |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 334      |
|    ep_rew_mean     | 679      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 549      |
|    time_elapsed    | 14274    |
|    total_timesteps | 562176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=562500, episode_reward=593.72 +/- 201.02
Episode length: 392.68 +/- 184.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | 594         |
| time/                   |             |
|    total_timesteps      | 562500      |
| train/                  |             |
|    approx_kl            | 0.034378063 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.955      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0005      |
|    loss                 | 427         |
|    n_updates            | 2216        |
|    policy_gradient_loss | 0.00426     |
|    value_loss           | 788         |
-----------------------------------------
Eval num_timesteps=563000, episode_reward=591.75 +/- 202.90
Episode length: 379.50 +/- 186.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 380      |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 342      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 550      |
|    time_elapsed    | 14301    |
|    total_timesteps | 563200   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=563500, episode_reward=614.14 +/- 174.67
Episode length: 431.98 +/- 162.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 432        |
|    mean_reward          | 614        |
| time/                   |            |
|    total_timesteps      | 563500     |
| train/                  |            |
|    approx_kl            | 0.04540115 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.822     |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.52e+03   |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0047    |
|    value_loss           | 2.51e+03   |
----------------------------------------
Eval num_timesteps=564000, episode_reward=575.63 +/- 181.67
Episode length: 448.44 +/- 153.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 448      |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 345      |
|    ep_rew_mean     | 671      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 551      |
|    time_elapsed    | 14331    |
|    total_timesteps | 564224   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=564500, episode_reward=631.28 +/- 190.55
Episode length: 356.74 +/- 190.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 357         |
|    mean_reward          | 631         |
| time/                   |             |
|    total_timesteps      | 564500      |
| train/                  |             |
|    approx_kl            | 0.057921316 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0005      |
|    loss                 | 663         |
|    n_updates            | 2222        |
|    policy_gradient_loss | 0.0146      |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=565000, episode_reward=616.14 +/- 210.11
Episode length: 350.30 +/- 193.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 350      |
|    mean_reward     | 616      |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 669      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 552      |
|    time_elapsed    | 14355    |
|    total_timesteps | 565248   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=565500, episode_reward=591.12 +/- 171.01
Episode length: 450.24 +/- 146.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 450        |
|    mean_reward          | 591        |
| time/                   |            |
|    total_timesteps      | 565500     |
| train/                  |            |
|    approx_kl            | 0.04705385 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.972     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0005     |
|    loss                 | 636        |
|    n_updates            | 2227       |
|    policy_gradient_loss | -0.00808   |
|    value_loss           | 1.42e+03   |
----------------------------------------
Eval num_timesteps=566000, episode_reward=598.39 +/- 176.84
Episode length: 430.96 +/- 161.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 431      |
|    mean_reward     | 598      |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 356      |
|    ep_rew_mean     | 664      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 553      |
|    time_elapsed    | 14385    |
|    total_timesteps | 566272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=566500, episode_reward=541.85 +/- 128.99
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 542         |
| time/                   |             |
|    total_timesteps      | 566500      |
| train/                  |             |
|    approx_kl            | 0.023093343 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.92       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0005      |
|    loss                 | 394         |
|    n_updates            | 2228        |
|    policy_gradient_loss | 0.0303      |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=567000, episode_reward=467.52 +/- 135.11
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | 661      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 554      |
|    time_elapsed    | 14421    |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=567500, episode_reward=465.06 +/- 171.41
Episode length: 498.82 +/- 91.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 499        |
|    mean_reward          | 465        |
| time/                   |            |
|    total_timesteps      | 567500     |
| train/                  |            |
|    approx_kl            | 0.07304727 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0005     |
|    loss                 | 827        |
|    n_updates            | 2230       |
|    policy_gradient_loss | 0.0265     |
|    value_loss           | 1.41e+03   |
----------------------------------------
Eval num_timesteps=568000, episode_reward=471.99 +/- 149.38
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 367      |
|    ep_rew_mean     | 657      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 555      |
|    time_elapsed    | 14456    |
|    total_timesteps | 568320   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=568500, episode_reward=549.24 +/- 132.68
Episode length: 517.46 +/- 52.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 517        |
|    mean_reward          | 549        |
| time/                   |            |
|    total_timesteps      | 568500     |
| train/                  |            |
|    approx_kl            | 0.04697922 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.996     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0005     |
|    loss                 | 336        |
|    n_updates            | 2235       |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 468        |
----------------------------------------
Eval num_timesteps=569000, episode_reward=496.97 +/- 149.06
Episode length: 503.20 +/- 87.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 503      |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 370      |
|    ep_rew_mean     | 652      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 556      |
|    time_elapsed    | 14491    |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=569500, episode_reward=595.13 +/- 153.72
Episode length: 467.78 +/- 135.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 468         |
|    mean_reward          | 595         |
| time/                   |             |
|    total_timesteps      | 569500      |
| train/                  |             |
|    approx_kl            | 0.041864194 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0005      |
|    loss                 | 632         |
|    n_updates            | 2237        |
|    policy_gradient_loss | 0.00355     |
|    value_loss           | 839         |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=564.30 +/- 188.28
Episode length: 437.90 +/- 159.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 374      |
|    ep_rew_mean     | 647      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 557      |
|    time_elapsed    | 14521    |
|    total_timesteps | 570368   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.10
Eval num_timesteps=570500, episode_reward=656.95 +/- 168.25
Episode length: 347.82 +/- 195.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 348        |
|    mean_reward          | 657        |
| time/                   |            |
|    total_timesteps      | 570500     |
| train/                  |            |
|    approx_kl            | 0.04625091 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0005     |
|    loss                 | 324        |
|    n_updates            | 2241       |
|    policy_gradient_loss | -0.0101    |
|    value_loss           | 824        |
----------------------------------------
Eval num_timesteps=571000, episode_reward=617.99 +/- 178.33
Episode length: 387.54 +/- 183.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 388      |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | 650      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 558      |
|    time_elapsed    | 14546    |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=571500, episode_reward=640.20 +/- 179.19
Episode length: 358.40 +/- 191.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 358        |
|    mean_reward          | 640        |
| time/                   |            |
|    total_timesteps      | 571500     |
| train/                  |            |
|    approx_kl            | 0.06271139 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.986     |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.0005     |
|    loss                 | 544        |
|    n_updates            | 2244       |
|    policy_gradient_loss | -0.00114   |
|    value_loss           | 1.11e+03   |
----------------------------------------
Eval num_timesteps=572000, episode_reward=678.63 +/- 149.77
Episode length: 369.46 +/- 184.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 369      |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | 652      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 559      |
|    time_elapsed    | 14571    |
|    total_timesteps | 572416   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=572500, episode_reward=651.24 +/- 173.05
Episode length: 363.48 +/- 187.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 363         |
|    mean_reward          | 651         |
| time/                   |             |
|    total_timesteps      | 572500      |
| train/                  |             |
|    approx_kl            | 0.046935614 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0005      |
|    loss                 | 714         |
|    n_updates            | 2249        |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 2.26e+03    |
-----------------------------------------
Eval num_timesteps=573000, episode_reward=662.00 +/- 152.87
Episode length: 367.74 +/- 187.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 377      |
|    ep_rew_mean     | 648      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 560      |
|    time_elapsed    | 14597    |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=573500, episode_reward=639.24 +/- 161.65
Episode length: 384.08 +/- 188.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 639         |
| time/                   |             |
|    total_timesteps      | 573500      |
| train/                  |             |
|    approx_kl            | 0.049920015 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.916      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0005      |
|    loss                 | 902         |
|    n_updates            | 2253        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=574000, episode_reward=648.63 +/- 143.46
Episode length: 412.42 +/- 170.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 371      |
|    ep_rew_mean     | 650      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 561      |
|    time_elapsed    | 14624    |
|    total_timesteps | 574464   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.13
Eval num_timesteps=574500, episode_reward=691.77 +/- 162.69
Episode length: 291.82 +/- 200.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 292        |
|    mean_reward          | 692        |
| time/                   |            |
|    total_timesteps      | 574500     |
| train/                  |            |
|    approx_kl            | 0.06283146 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0005     |
|    loss                 | 787        |
|    n_updates            | 2258       |
|    policy_gradient_loss | -0.00616   |
|    value_loss           | 1.29e+03   |
----------------------------------------
Eval num_timesteps=575000, episode_reward=670.78 +/- 163.97
Episode length: 301.90 +/- 200.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 656      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 562      |
|    time_elapsed    | 14645    |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=575500, episode_reward=708.18 +/- 141.22
Episode length: 285.84 +/- 193.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 708         |
| time/                   |             |
|    total_timesteps      | 575500      |
| train/                  |             |
|    approx_kl            | 0.055134106 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.856      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.38e+03    |
|    n_updates            | 2260        |
|    policy_gradient_loss | 0.0028      |
|    value_loss           | 2.65e+03    |
-----------------------------------------
Eval num_timesteps=576000, episode_reward=703.14 +/- 134.58
Episode length: 297.46 +/- 201.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=684.62 +/- 152.81
Episode length: 313.14 +/- 200.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 313      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | 655      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 563      |
|    time_elapsed    | 14676    |
|    total_timesteps | 576512   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=577000, episode_reward=703.08 +/- 155.38
Episode length: 257.34 +/- 179.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 257        |
|    mean_reward          | 703        |
| time/                   |            |
|    total_timesteps      | 577000     |
| train/                  |            |
|    approx_kl            | 0.07240775 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0005     |
|    loss                 | 756        |
|    n_updates            | 2264       |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 1.19e+03   |
----------------------------------------
Eval num_timesteps=577500, episode_reward=637.64 +/- 177.72
Episode length: 309.32 +/- 190.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 353      |
|    ep_rew_mean     | 662      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 564      |
|    time_elapsed    | 14695    |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=578000, episode_reward=647.64 +/- 161.56
Episode length: 369.76 +/- 180.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 370       |
|    mean_reward          | 648       |
| time/                   |           |
|    total_timesteps      | 578000    |
| train/                  |           |
|    approx_kl            | 0.0682043 |
|    clip_fraction        | 0.194     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.769    |
|    explained_variance   | 0.59      |
|    learning_rate        | 0.0005    |
|    loss                 | 928       |
|    n_updates            | 2266      |
|    policy_gradient_loss | 0.0169    |
|    value_loss           | 2.07e+03  |
---------------------------------------
Eval num_timesteps=578500, episode_reward=646.01 +/- 145.99
Episode length: 381.16 +/- 173.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 646      |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 342      |
|    ep_rew_mean     | 671      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 565      |
|    time_elapsed    | 14721    |
|    total_timesteps | 578560   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=579000, episode_reward=626.87 +/- 183.00
Episode length: 363.44 +/- 183.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 363         |
|    mean_reward          | 627         |
| time/                   |             |
|    total_timesteps      | 579000      |
| train/                  |             |
|    approx_kl            | 0.047638576 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.875      |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0005      |
|    loss                 | 627         |
|    n_updates            | 2270        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=579500, episode_reward=626.61 +/- 163.37
Episode length: 411.58 +/- 163.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 412      |
|    mean_reward     | 627      |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 678      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 566      |
|    time_elapsed    | 14747    |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=580000, episode_reward=593.22 +/- 164.76
Episode length: 455.46 +/- 147.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 455         |
|    mean_reward          | 593         |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.035471417 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.02e+03    |
|    n_updates            | 2272        |
|    policy_gradient_loss | 0.00349     |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=580500, episode_reward=607.39 +/- 161.21
Episode length: 468.54 +/- 130.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 469      |
|    mean_reward     | 607      |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | 684      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 567      |
|    time_elapsed    | 14779    |
|    total_timesteps | 580608   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=581000, episode_reward=481.54 +/- 174.09
Episode length: 441.26 +/- 163.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 441        |
|    mean_reward          | 482        |
| time/                   |            |
|    total_timesteps      | 581000     |
| train/                  |            |
|    approx_kl            | 0.05065601 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.593     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.26e+03   |
|    n_updates            | 2276       |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 2.51e+03   |
----------------------------------------
Eval num_timesteps=581500, episode_reward=491.04 +/- 236.20
Episode length: 427.32 +/- 175.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 427      |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 338      |
|    ep_rew_mean     | 681      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 568      |
|    time_elapsed    | 14809    |
|    total_timesteps | 581632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=582000, episode_reward=600.00 +/- 191.81
Episode length: 393.38 +/- 174.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 393        |
|    mean_reward          | 600        |
| time/                   |            |
|    total_timesteps      | 582000     |
| train/                  |            |
|    approx_kl            | 0.03828661 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.982     |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0005     |
|    loss                 | 622        |
|    n_updates            | 2278       |
|    policy_gradient_loss | -0.000298  |
|    value_loss           | 1.09e+03   |
----------------------------------------
Eval num_timesteps=582500, episode_reward=565.93 +/- 172.93
Episode length: 463.60 +/- 138.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 464      |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 334      |
|    ep_rew_mean     | 682      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 569      |
|    time_elapsed    | 14838    |
|    total_timesteps | 582656   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=583000, episode_reward=499.04 +/- 236.31
Episode length: 383.02 +/- 199.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 383         |
|    mean_reward          | 499         |
| time/                   |             |
|    total_timesteps      | 583000      |
| train/                  |             |
|    approx_kl            | 0.051191904 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0005      |
|    loss                 | 598         |
|    n_updates            | 2282        |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 1.43e+03    |
-----------------------------------------
Eval num_timesteps=583500, episode_reward=534.92 +/- 269.70
Episode length: 375.48 +/- 204.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 375      |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 329      |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 570      |
|    time_elapsed    | 14864    |
|    total_timesteps | 583680   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=584000, episode_reward=420.24 +/- 259.73
Episode length: 407.72 +/- 198.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 408         |
|    mean_reward          | 420         |
| time/                   |             |
|    total_timesteps      | 584000      |
| train/                  |             |
|    approx_kl            | 0.033782497 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0005      |
|    loss                 | 956         |
|    n_updates            | 2284        |
|    policy_gradient_loss | 0.0291      |
|    value_loss           | 2.04e+03    |
-----------------------------------------
Eval num_timesteps=584500, episode_reward=361.97 +/- 297.65
Episode length: 339.40 +/- 228.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 633      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 571      |
|    time_elapsed    | 14890    |
|    total_timesteps | 584704   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=585000, episode_reward=516.26 +/- 161.92
Episode length: 476.74 +/- 131.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 477         |
|    mean_reward          | 516         |
| time/                   |             |
|    total_timesteps      | 585000      |
| train/                  |             |
|    approx_kl            | 0.044167012 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.979      |
|    explained_variance   | 0.201       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.73e+03    |
|    n_updates            | 2286        |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 4.84e+03    |
-----------------------------------------
Eval num_timesteps=585500, episode_reward=531.95 +/- 136.93
Episode length: 511.24 +/- 69.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 511      |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 322      |
|    ep_rew_mean     | 636      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 572      |
|    time_elapsed    | 14923    |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=586000, episode_reward=430.55 +/- 151.40
Episode length: 482.92 +/- 126.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 483        |
|    mean_reward          | 431        |
| time/                   |            |
|    total_timesteps      | 586000     |
| train/                  |            |
|    approx_kl            | 0.02345837 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.877     |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.0005     |
|    loss                 | 680        |
|    n_updates            | 2287       |
|    policy_gradient_loss | 0.0193     |
|    value_loss           | 1.52e+03   |
----------------------------------------
Eval num_timesteps=586500, episode_reward=428.17 +/- 166.79
Episode length: 471.20 +/- 136.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 471      |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | 633      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 573      |
|    time_elapsed    | 14956    |
|    total_timesteps | 586752   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=587000, episode_reward=463.82 +/- 171.81
Episode length: 484.04 +/- 115.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 484        |
|    mean_reward          | 464        |
| time/                   |            |
|    total_timesteps      | 587000     |
| train/                  |            |
|    approx_kl            | 0.04417235 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0005     |
|    loss                 | 537        |
|    n_updates            | 2290       |
|    policy_gradient_loss | 0.0143     |
|    value_loss           | 1.19e+03   |
----------------------------------------
Eval num_timesteps=587500, episode_reward=447.91 +/- 163.61
Episode length: 478.48 +/- 126.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 328      |
|    ep_rew_mean     | 622      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 574      |
|    time_elapsed    | 14988    |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=588000, episode_reward=388.54 +/- 171.18
Episode length: 434.02 +/- 164.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 434         |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 588000      |
| train/                  |             |
|    approx_kl            | 0.032655094 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0005      |
|    loss                 | 854         |
|    n_updates            | 2292        |
|    policy_gradient_loss | 0.0076      |
|    value_loss           | 1.89e+03    |
-----------------------------------------
Eval num_timesteps=588500, episode_reward=385.74 +/- 168.12
Episode length: 451.86 +/- 149.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 452      |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | 613      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 575      |
|    time_elapsed    | 15019    |
|    total_timesteps | 588800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=589000, episode_reward=513.07 +/- 141.86
Episode length: 504.02 +/- 84.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | 513         |
| time/                   |             |
|    total_timesteps      | 589000      |
| train/                  |             |
|    approx_kl            | 0.052937053 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.12e+03    |
|    n_updates            | 2294        |
|    policy_gradient_loss | 0.00279     |
|    value_loss           | 2.54e+03    |
-----------------------------------------
Eval num_timesteps=589500, episode_reward=499.75 +/- 124.79
Episode length: 508.58 +/- 66.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 509      |
|    mean_reward     | 500      |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 336      |
|    ep_rew_mean     | 608      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 576      |
|    time_elapsed    | 15053    |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=590000, episode_reward=537.18 +/- 121.90
Episode length: 505.52 +/- 79.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 506        |
|    mean_reward          | 537        |
| time/                   |            |
|    total_timesteps      | 590000     |
| train/                  |            |
|    approx_kl            | 0.05178351 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0005     |
|    loss                 | 358        |
|    n_updates            | 2296       |
|    policy_gradient_loss | 0.0028     |
|    value_loss           | 912        |
----------------------------------------
Eval num_timesteps=590500, episode_reward=547.43 +/- 134.07
Episode length: 518.40 +/- 26.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | 547      |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 604      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 577      |
|    time_elapsed    | 15088    |
|    total_timesteps | 590848   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=591000, episode_reward=583.43 +/- 130.04
Episode length: 481.20 +/- 117.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 481         |
|    mean_reward          | 583         |
| time/                   |             |
|    total_timesteps      | 591000      |
| train/                  |             |
|    approx_kl            | 0.041122213 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.971      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0005      |
|    loss                 | 669         |
|    n_updates            | 2298        |
|    policy_gradient_loss | 0.0109      |
|    value_loss           | 1.58e+03    |
-----------------------------------------
Eval num_timesteps=591500, episode_reward=630.20 +/- 107.59
Episode length: 481.88 +/- 108.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 348      |
|    ep_rew_mean     | 598      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 578      |
|    time_elapsed    | 15120    |
|    total_timesteps | 591872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=592000, episode_reward=577.71 +/- 122.61
Episode length: 475.90 +/- 133.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 578         |
| time/                   |             |
|    total_timesteps      | 592000      |
| train/                  |             |
|    approx_kl            | 0.053183287 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0005      |
|    loss                 | 259         |
|    n_updates            | 2300        |
|    policy_gradient_loss | 0.00742     |
|    value_loss           | 720         |
-----------------------------------------
Eval num_timesteps=592500, episode_reward=592.61 +/- 118.77
Episode length: 494.32 +/- 106.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 587      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 579      |
|    time_elapsed    | 15154    |
|    total_timesteps | 592896   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=593000, episode_reward=541.77 +/- 137.10
Episode length: 491.98 +/- 112.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 492         |
|    mean_reward          | 542         |
| time/                   |             |
|    total_timesteps      | 593000      |
| train/                  |             |
|    approx_kl            | 0.046422437 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.46e+03    |
|    n_updates            | 2304        |
|    policy_gradient_loss | -0.00541    |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=593500, episode_reward=531.80 +/- 135.14
Episode length: 491.76 +/- 112.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 492      |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | 575      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 580      |
|    time_elapsed    | 15188    |
|    total_timesteps | 593920   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=594000, episode_reward=556.76 +/- 163.07
Episode length: 472.80 +/- 141.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 473         |
|    mean_reward          | 557         |
| time/                   |             |
|    total_timesteps      | 594000      |
| train/                  |             |
|    approx_kl            | 0.030511688 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0005      |
|    loss                 | 896         |
|    n_updates            | 2306        |
|    policy_gradient_loss | 0.00295     |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=594500, episode_reward=508.78 +/- 195.95
Episode length: 428.88 +/- 169.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 429      |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | 566      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 581      |
|    time_elapsed    | 15219    |
|    total_timesteps | 594944   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.09
Eval num_timesteps=595000, episode_reward=550.64 +/- 249.72
Episode length: 301.18 +/- 201.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 301         |
|    mean_reward          | 551         |
| time/                   |             |
|    total_timesteps      | 595000      |
| train/                  |             |
|    approx_kl            | 0.057386916 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0005      |
|    loss                 | 511         |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 894         |
-----------------------------------------
Eval num_timesteps=595500, episode_reward=649.71 +/- 194.10
Episode length: 338.22 +/- 186.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | 563      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 582      |
|    time_elapsed    | 15241    |
|    total_timesteps | 595968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=596000, episode_reward=725.59 +/- 136.44
Episode length: 266.64 +/- 167.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 267        |
|    mean_reward          | 726        |
| time/                   |            |
|    total_timesteps      | 596000     |
| train/                  |            |
|    approx_kl            | 0.04519691 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.51e+03   |
|    n_updates            | 2312       |
|    policy_gradient_loss | 0.0103     |
|    value_loss           | 2.28e+03   |
----------------------------------------
Eval num_timesteps=596500, episode_reward=716.13 +/- 153.69
Episode length: 252.36 +/- 166.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 362      |
|    ep_rew_mean     | 563      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 583      |
|    time_elapsed    | 15259    |
|    total_timesteps | 596992   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=597000, episode_reward=730.01 +/- 128.29
Episode length: 284.48 +/- 176.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 284        |
|    mean_reward          | 730        |
| time/                   |            |
|    total_timesteps      | 597000     |
| train/                  |            |
|    approx_kl            | 0.04430709 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0005     |
|    loss                 | 814        |
|    n_updates            | 2317       |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 1.3e+03    |
----------------------------------------
Eval num_timesteps=597500, episode_reward=692.00 +/- 143.09
Episode length: 334.40 +/- 191.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 334      |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=706.07 +/- 146.11
Episode length: 301.74 +/- 184.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 557      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 584      |
|    time_elapsed    | 15290    |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=598500, episode_reward=757.49 +/- 119.75
Episode length: 237.04 +/- 141.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | 757         |
| time/                   |             |
|    total_timesteps      | 598500      |
| train/                  |             |
|    approx_kl            | 0.038377553 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.44e+03    |
|    n_updates            | 2320        |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 2.2e+03     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=599000, episode_reward=717.65 +/- 153.44
Episode length: 270.86 +/- 162.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | 558      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 585      |
|    time_elapsed    | 15308    |
|    total_timesteps | 599040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=599500, episode_reward=661.67 +/- 156.41
Episode length: 398.24 +/- 142.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 398         |
|    mean_reward          | 662         |
| time/                   |             |
|    total_timesteps      | 599500      |
| train/                  |             |
|    approx_kl            | 0.026383478 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0005      |
|    loss                 | 361         |
|    n_updates            | 2321        |
|    policy_gradient_loss | 0.0251      |
|    value_loss           | 730         |
-----------------------------------------
Eval num_timesteps=600000, episode_reward=691.81 +/- 150.27
Episode length: 290.72 +/- 135.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 291      |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 352      |
|    ep_rew_mean     | 554      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 586      |
|    time_elapsed    | 15332    |
|    total_timesteps | 600064   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=600500, episode_reward=692.95 +/- 151.35
Episode length: 293.84 +/- 162.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 294        |
|    mean_reward          | 693        |
| time/                   |            |
|    total_timesteps      | 600500     |
| train/                  |            |
|    approx_kl            | 0.04483666 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0005     |
|    loss                 | 637        |
|    n_updates            | 2326       |
|    policy_gradient_loss | -0.00381   |
|    value_loss           | 1.72e+03   |
----------------------------------------
Eval num_timesteps=601000, episode_reward=722.77 +/- 127.87
Episode length: 268.28 +/- 147.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 601000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 349      |
|    ep_rew_mean     | 558      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 587      |
|    time_elapsed    | 15352    |
|    total_timesteps | 601088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=601500, episode_reward=752.29 +/- 107.40
Episode length: 185.24 +/- 101.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 185         |
|    mean_reward          | 752         |
| time/                   |             |
|    total_timesteps      | 601500      |
| train/                  |             |
|    approx_kl            | 0.022721656 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.938      |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0005      |
|    loss                 | 587         |
|    n_updates            | 2327        |
|    policy_gradient_loss | 0.0208      |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=602000, episode_reward=737.55 +/- 137.07
Episode length: 207.76 +/- 129.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 330      |
|    ep_rew_mean     | 575      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 588      |
|    time_elapsed    | 15366    |
|    total_timesteps | 602112   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=602500, episode_reward=774.84 +/- 92.91
Episode length: 139.34 +/- 55.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 139       |
|    mean_reward          | 775       |
| time/                   |           |
|    total_timesteps      | 602500    |
| train/                  |           |
|    approx_kl            | 0.0487746 |
|    clip_fraction        | 0.232     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.787    |
|    explained_variance   | 0.8       |
|    learning_rate        | 0.0005    |
|    loss                 | 1.03e+03  |
|    n_updates            | 2331      |
|    policy_gradient_loss | -0.00918  |
|    value_loss           | 2.04e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=603000, episode_reward=766.32 +/- 116.67
Episode length: 156.04 +/- 93.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 603000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 588      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 589      |
|    time_elapsed    | 15376    |
|    total_timesteps | 603136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=603500, episode_reward=732.35 +/- 116.42
Episode length: 205.34 +/- 149.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | 732         |
| time/                   |             |
|    total_timesteps      | 603500      |
| train/                  |             |
|    approx_kl            | 0.030391686 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.7        |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.55e+03    |
|    n_updates            | 2333        |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 2.86e+03    |
-----------------------------------------
Eval num_timesteps=604000, episode_reward=786.69 +/- 56.16
Episode length: 159.00 +/- 92.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 594      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 590      |
|    time_elapsed    | 15389    |
|    total_timesteps | 604160   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=604500, episode_reward=773.97 +/- 84.66
Episode length: 191.74 +/- 129.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 192        |
|    mean_reward          | 774        |
| time/                   |            |
|    total_timesteps      | 604500     |
| train/                  |            |
|    approx_kl            | 0.04378087 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.858     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0005     |
|    loss                 | 917        |
|    n_updates            | 2335       |
|    policy_gradient_loss | 0.0105     |
|    value_loss           | 1.54e+03   |
----------------------------------------
Eval num_timesteps=605000, episode_reward=742.89 +/- 142.45
Episode length: 208.14 +/- 146.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 598      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 591      |
|    time_elapsed    | 15404    |
|    total_timesteps | 605184   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=605500, episode_reward=738.97 +/- 140.73
Episode length: 209.12 +/- 131.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 739         |
| time/                   |             |
|    total_timesteps      | 605500      |
| train/                  |             |
|    approx_kl            | 0.042754486 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.927      |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.12e+03    |
|    n_updates            | 2339        |
|    policy_gradient_loss | -0.00889    |
|    value_loss           | 1.85e+03    |
-----------------------------------------
Eval num_timesteps=606000, episode_reward=750.93 +/- 98.78
Episode length: 239.16 +/- 153.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 601      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 592      |
|    time_elapsed    | 15420    |
|    total_timesteps | 606208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=606500, episode_reward=706.77 +/- 154.38
Episode length: 341.72 +/- 137.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 342         |
|    mean_reward          | 707         |
| time/                   |             |
|    total_timesteps      | 606500      |
| train/                  |             |
|    approx_kl            | 0.029165335 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.08e+03    |
|    n_updates            | 2340        |
|    policy_gradient_loss | 0.0368      |
|    value_loss           | 2.12e+03    |
-----------------------------------------
Eval num_timesteps=607000, episode_reward=683.58 +/- 140.15
Episode length: 382.82 +/- 142.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 383      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 607000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 603      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 593      |
|    time_elapsed    | 15444    |
|    total_timesteps | 607232   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=607500, episode_reward=700.05 +/- 146.07
Episode length: 354.46 +/- 139.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 354         |
|    mean_reward          | 700         |
| time/                   |             |
|    total_timesteps      | 607500      |
| train/                  |             |
|    approx_kl            | 0.044832315 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0005      |
|    loss                 | 595         |
|    n_updates            | 2342        |
|    policy_gradient_loss | 0.0147      |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=608000, episode_reward=673.45 +/- 158.15
Episode length: 384.40 +/- 145.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 384      |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 601      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 594      |
|    time_elapsed    | 15470    |
|    total_timesteps | 608256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=608500, episode_reward=286.91 +/- 155.52
Episode length: 511.74 +/- 65.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 512         |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 608500      |
| train/                  |             |
|    approx_kl            | 0.030804833 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0005      |
|    loss                 | 935         |
|    n_updates            | 2343        |
|    policy_gradient_loss | 0.0348      |
|    value_loss           | 2.29e+03    |
-----------------------------------------
Eval num_timesteps=609000, episode_reward=375.97 +/- 201.99
Episode length: 492.62 +/- 97.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 493      |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 609000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 604      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 595      |
|    time_elapsed    | 15504    |
|    total_timesteps | 609280   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=609500, episode_reward=624.25 +/- 152.34
Episode length: 408.90 +/- 153.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 624         |
| time/                   |             |
|    total_timesteps      | 609500      |
| train/                  |             |
|    approx_kl            | 0.029802598 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0005      |
|    loss                 | 493         |
|    n_updates            | 2345        |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=610000, episode_reward=617.62 +/- 166.71
Episode length: 438.34 +/- 136.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 438      |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 610      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 596      |
|    time_elapsed    | 15534    |
|    total_timesteps | 610304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=610500, episode_reward=754.16 +/- 123.94
Episode length: 199.58 +/- 132.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 200        |
|    mean_reward          | 754        |
| time/                   |            |
|    total_timesteps      | 610500     |
| train/                  |            |
|    approx_kl            | 0.03241824 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0005     |
|    loss                 | 965        |
|    n_updates            | 2346       |
|    policy_gradient_loss | 0.0263     |
|    value_loss           | 1.76e+03   |
----------------------------------------
Eval num_timesteps=611000, episode_reward=738.22 +/- 146.26
Episode length: 189.34 +/- 124.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 611000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | 627      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 597      |
|    time_elapsed    | 15547    |
|    total_timesteps | 611328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=611500, episode_reward=721.96 +/- 123.77
Episode length: 305.72 +/- 157.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 306         |
|    mean_reward          | 722         |
| time/                   |             |
|    total_timesteps      | 611500      |
| train/                  |             |
|    approx_kl            | 0.016700078 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.13e+03    |
|    n_updates            | 2347        |
|    policy_gradient_loss | 0.00684     |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=612000, episode_reward=730.58 +/- 123.98
Episode length: 294.46 +/- 143.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 275      |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 598      |
|    time_elapsed    | 15568    |
|    total_timesteps | 612352   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=612500, episode_reward=718.36 +/- 120.90
Episode length: 370.70 +/- 148.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | 718         |
| time/                   |             |
|    total_timesteps      | 612500      |
| train/                  |             |
|    approx_kl            | 0.055637587 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0005      |
|    loss                 | 605         |
|    n_updates            | 2350        |
|    policy_gradient_loss | 0.00505     |
|    value_loss           | 1.65e+03    |
-----------------------------------------
Eval num_timesteps=613000, episode_reward=697.08 +/- 153.57
Episode length: 337.60 +/- 155.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 338      |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 613000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 258      |
|    ep_rew_mean     | 694      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 599      |
|    time_elapsed    | 15593    |
|    total_timesteps | 613376   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=613500, episode_reward=731.12 +/- 123.78
Episode length: 282.14 +/- 159.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 731         |
| time/                   |             |
|    total_timesteps      | 613500      |
| train/                  |             |
|    approx_kl            | 0.112692535 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0005      |
|    loss                 | 887         |
|    n_updates            | 2353        |
|    policy_gradient_loss | 0.0136      |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=614000, episode_reward=705.11 +/- 144.19
Episode length: 318.88 +/- 164.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 319      |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 720      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 600      |
|    time_elapsed    | 15613    |
|    total_timesteps | 614400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=614500, episode_reward=749.99 +/- 141.03
Episode length: 174.02 +/- 107.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 750         |
| time/                   |             |
|    total_timesteps      | 614500      |
| train/                  |             |
|    approx_kl            | 0.030628378 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.631      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0005      |
|    loss                 | 811         |
|    n_updates            | 2354        |
|    policy_gradient_loss | 0.0243      |
|    value_loss           | 2.1e+03     |
-----------------------------------------
Eval num_timesteps=615000, episode_reward=734.77 +/- 139.10
Episode length: 214.08 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 615000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 601      |
|    time_elapsed    | 15627    |
|    total_timesteps | 615424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=615500, episode_reward=791.09 +/- 60.86
Episode length: 159.80 +/- 107.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 160       |
|    mean_reward          | 791       |
| time/                   |           |
|    total_timesteps      | 615500    |
| train/                  |           |
|    approx_kl            | 0.0297206 |
|    clip_fraction        | 0.266     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.54     |
|    explained_variance   | 0.64      |
|    learning_rate        | 0.0005    |
|    loss                 | 873       |
|    n_updates            | 2355      |
|    policy_gradient_loss | 0.0252    |
|    value_loss           | 2.79e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=616000, episode_reward=746.26 +/- 125.47
Episode length: 168.28 +/- 119.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 602      |
|    time_elapsed    | 15639    |
|    total_timesteps | 616448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=616500, episode_reward=711.54 +/- 176.19
Episode length: 149.96 +/- 110.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 712         |
| time/                   |             |
|    total_timesteps      | 616500      |
| train/                  |             |
|    approx_kl            | 0.026121983 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.17e+03    |
|    n_updates            | 2356        |
|    policy_gradient_loss | 0.00529     |
|    value_loss           | 2.7e+03     |
-----------------------------------------
Eval num_timesteps=617000, episode_reward=736.52 +/- 119.41
Episode length: 148.98 +/- 111.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 617000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 603      |
|    time_elapsed    | 15650    |
|    total_timesteps | 617472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=617500, episode_reward=751.04 +/- 111.65
Episode length: 174.86 +/- 152.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | 751         |
| time/                   |             |
|    total_timesteps      | 617500      |
| train/                  |             |
|    approx_kl            | 0.051293578 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.16e+03    |
|    n_updates            | 2358        |
|    policy_gradient_loss | 0.00323     |
|    value_loss           | 5.32e+03    |
-----------------------------------------
Eval num_timesteps=618000, episode_reward=746.77 +/- 132.25
Episode length: 165.50 +/- 145.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 770      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 604      |
|    time_elapsed    | 15662    |
|    total_timesteps | 618496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=618500, episode_reward=743.09 +/- 133.32
Episode length: 179.82 +/- 161.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 743         |
| time/                   |             |
|    total_timesteps      | 618500      |
| train/                  |             |
|    approx_kl            | 0.034251273 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.28e+03    |
|    n_updates            | 2359        |
|    policy_gradient_loss | 0.0359      |
|    value_loss           | 2.38e+03    |
-----------------------------------------
Eval num_timesteps=619000, episode_reward=711.83 +/- 175.74
Episode length: 212.24 +/- 185.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 619000   |
---------------------------------
Eval num_timesteps=619500, episode_reward=705.15 +/- 166.66
Episode length: 180.06 +/- 161.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 619500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 605      |
|    time_elapsed    | 15682    |
|    total_timesteps | 619520   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=620000, episode_reward=606.70 +/- 244.65
Episode length: 211.20 +/- 186.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 607         |
| time/                   |             |
|    total_timesteps      | 620000      |
| train/                  |             |
|    approx_kl            | 0.053263992 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.453      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.29e+03    |
|    n_updates            | 2362        |
|    policy_gradient_loss | 0.018       |
|    value_loss           | 3.25e+03    |
-----------------------------------------
Eval num_timesteps=620500, episode_reward=650.34 +/- 209.70
Episode length: 172.84 +/- 146.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 650      |
| time/              |          |
|    total_timesteps | 620500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 606      |
|    time_elapsed    | 15696    |
|    total_timesteps | 620544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=621000, episode_reward=513.06 +/- 232.56
Episode length: 263.58 +/- 200.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 513         |
| time/                   |             |
|    total_timesteps      | 621000      |
| train/                  |             |
|    approx_kl            | 0.029220194 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0005      |
|    loss                 | 4.44e+03    |
|    n_updates            | 2363        |
|    policy_gradient_loss | 0.0135      |
|    value_loss           | 6.63e+03    |
-----------------------------------------
Eval num_timesteps=621500, episode_reward=541.35 +/- 247.08
Episode length: 212.34 +/- 178.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 621500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 721      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 607      |
|    time_elapsed    | 15713    |
|    total_timesteps | 621568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=622000, episode_reward=532.37 +/- 240.65
Episode length: 368.42 +/- 198.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | 532         |
| time/                   |             |
|    total_timesteps      | 622000      |
| train/                  |             |
|    approx_kl            | 0.032294396 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.65e+03    |
|    n_updates            | 2365        |
|    policy_gradient_loss | 0.00347     |
|    value_loss           | 7.47e+03    |
-----------------------------------------
Eval num_timesteps=622500, episode_reward=621.96 +/- 183.33
Episode length: 352.02 +/- 203.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 352      |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 622500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 707      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 608      |
|    time_elapsed    | 15738    |
|    total_timesteps | 622592   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=623000, episode_reward=527.38 +/- 214.75
Episode length: 440.92 +/- 168.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 441         |
|    mean_reward          | 527         |
| time/                   |             |
|    total_timesteps      | 623000      |
| train/                  |             |
|    approx_kl            | 0.034792233 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.63e+03    |
|    n_updates            | 2367        |
|    policy_gradient_loss | 0.0184      |
|    value_loss           | 3.7e+03     |
-----------------------------------------
Eval num_timesteps=623500, episode_reward=518.74 +/- 210.26
Episode length: 425.10 +/- 178.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 425      |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 623500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 609      |
|    time_elapsed    | 15767    |
|    total_timesteps | 623616   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=624000, episode_reward=591.60 +/- 134.67
Episode length: 484.62 +/- 121.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 485         |
|    mean_reward          | 592         |
| time/                   |             |
|    total_timesteps      | 624000      |
| train/                  |             |
|    approx_kl            | 0.050604694 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.02e+03    |
|    n_updates            | 2370        |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 2.74e+03    |
-----------------------------------------
Eval num_timesteps=624500, episode_reward=576.08 +/- 178.52
Episode length: 475.28 +/- 134.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 475      |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 624500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 684      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 610      |
|    time_elapsed    | 15799    |
|    total_timesteps | 624640   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.18
Eval num_timesteps=625000, episode_reward=634.76 +/- 133.29
Episode length: 460.40 +/- 148.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 460        |
|    mean_reward          | 635        |
| time/                   |            |
|    total_timesteps      | 625000     |
| train/                  |            |
|    approx_kl            | 0.17539248 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.566     |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.58e+03   |
|    n_updates            | 2372       |
|    policy_gradient_loss | 0.0105     |
|    value_loss           | 3.58e+03   |
----------------------------------------
Eval num_timesteps=625500, episode_reward=642.28 +/- 101.38
Episode length: 485.68 +/- 117.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 486      |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 625500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 611      |
|    time_elapsed    | 15831    |
|    total_timesteps | 625664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=626000, episode_reward=727.18 +/- 156.19
Episode length: 195.78 +/- 165.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | 727         |
| time/                   |             |
|    total_timesteps      | 626000      |
| train/                  |             |
|    approx_kl            | 0.028323252 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.27e+03    |
|    n_updates            | 2373        |
|    policy_gradient_loss | 0.0231      |
|    value_loss           | 2.65e+03    |
-----------------------------------------
Eval num_timesteps=626500, episode_reward=752.37 +/- 80.19
Episode length: 272.54 +/- 197.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 626500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 673      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 612      |
|    time_elapsed    | 15848    |
|    total_timesteps | 626688   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=627000, episode_reward=754.87 +/- 85.94
Episode length: 243.08 +/- 184.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 243         |
|    mean_reward          | 755         |
| time/                   |             |
|    total_timesteps      | 627000      |
| train/                  |             |
|    approx_kl            | 0.060953863 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.55e+03    |
|    n_updates            | 2376        |
|    policy_gradient_loss | 0.00215     |
|    value_loss           | 2.65e+03    |
-----------------------------------------
Eval num_timesteps=627500, episode_reward=764.27 +/- 92.40
Episode length: 171.26 +/- 130.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 627500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 613      |
|    time_elapsed    | 15862    |
|    total_timesteps | 627712   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=628000, episode_reward=767.68 +/- 132.39
Episode length: 151.16 +/- 111.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 151        |
|    mean_reward          | 768        |
| time/                   |            |
|    total_timesteps      | 628000     |
| train/                  |            |
|    approx_kl            | 0.04343325 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.56      |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0005     |
|    loss                 | 742        |
|    n_updates            | 2379       |
|    policy_gradient_loss | 0.00901    |
|    value_loss           | 2.04e+03   |
----------------------------------------
Eval num_timesteps=628500, episode_reward=733.02 +/- 136.11
Episode length: 197.38 +/- 164.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 628500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 679      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 614      |
|    time_elapsed    | 15875    |
|    total_timesteps | 628736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=629000, episode_reward=755.01 +/- 149.33
Episode length: 127.72 +/- 100.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 128        |
|    mean_reward          | 755        |
| time/                   |            |
|    total_timesteps      | 629000     |
| train/                  |            |
|    approx_kl            | 0.02753275 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.482     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.54e+03   |
|    n_updates            | 2380       |
|    policy_gradient_loss | 0.0255     |
|    value_loss           | 4.24e+03   |
----------------------------------------
Eval num_timesteps=629500, episode_reward=786.70 +/- 82.16
Episode length: 102.46 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 629500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 679      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 615      |
|    time_elapsed    | 15884    |
|    total_timesteps | 629760   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=630000, episode_reward=677.39 +/- 211.97
Episode length: 95.02 +/- 10.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95          |
|    mean_reward          | 677         |
| time/                   |             |
|    total_timesteps      | 630000      |
| train/                  |             |
|    approx_kl            | 0.050743416 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.482      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.01e+03    |
|    n_updates            | 2383        |
|    policy_gradient_loss | 0.00596     |
|    value_loss           | 3.74e+03    |
-----------------------------------------
Eval num_timesteps=630500, episode_reward=665.16 +/- 204.46
Episode length: 105.62 +/- 61.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 630500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 681      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 616      |
|    time_elapsed    | 15892    |
|    total_timesteps | 630784   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=631000, episode_reward=740.49 +/- 140.57
Episode length: 117.80 +/- 84.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 740         |
| time/                   |             |
|    total_timesteps      | 631000      |
| train/                  |             |
|    approx_kl            | 0.029030345 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.429      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.65e+03    |
|    n_updates            | 2385        |
|    policy_gradient_loss | 0.00647     |
|    value_loss           | 3.17e+03    |
-----------------------------------------
Eval num_timesteps=631500, episode_reward=747.98 +/- 136.69
Episode length: 99.58 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 631500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 617      |
|    time_elapsed    | 15900    |
|    total_timesteps | 631808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=632000, episode_reward=589.66 +/- 230.78
Episode length: 93.90 +/- 13.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.9        |
|    mean_reward          | 590         |
| time/                   |             |
|    total_timesteps      | 632000      |
| train/                  |             |
|    approx_kl            | 0.034224626 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.11e+03    |
|    n_updates            | 2386        |
|    policy_gradient_loss | 0.0245      |
|    value_loss           | 5.44e+03    |
-----------------------------------------
Eval num_timesteps=632500, episode_reward=607.93 +/- 241.89
Episode length: 93.86 +/- 13.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.9     |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 632500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 676      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 618      |
|    time_elapsed    | 15908    |
|    total_timesteps | 632832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=633000, episode_reward=673.42 +/- 217.63
Episode length: 90.98 +/- 9.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 91         |
|    mean_reward          | 673        |
| time/                   |            |
|    total_timesteps      | 633000     |
| train/                  |            |
|    approx_kl            | 0.03760612 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.35e+03   |
|    n_updates            | 2388       |
|    policy_gradient_loss | 0.00973    |
|    value_loss           | 7.49e+03   |
----------------------------------------
Eval num_timesteps=633500, episode_reward=693.49 +/- 182.97
Episode length: 91.62 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 633500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 678      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 619      |
|    time_elapsed    | 15915    |
|    total_timesteps | 633856   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=634000, episode_reward=626.19 +/- 209.94
Episode length: 87.02 +/- 10.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 87         |
|    mean_reward          | 626        |
| time/                   |            |
|    total_timesteps      | 634000     |
| train/                  |            |
|    approx_kl            | 0.06780023 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.19e+03   |
|    n_updates            | 2390       |
|    policy_gradient_loss | 0.00935    |
|    value_loss           | 7.16e+03   |
----------------------------------------
Eval num_timesteps=634500, episode_reward=652.70 +/- 240.50
Episode length: 86.86 +/- 10.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.9     |
|    mean_reward     | 653      |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 681      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 620      |
|    time_elapsed    | 15922    |
|    total_timesteps | 634880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=635000, episode_reward=549.81 +/- 220.64
Episode length: 83.14 +/- 9.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.1        |
|    mean_reward          | 550         |
| time/                   |             |
|    total_timesteps      | 635000      |
| train/                  |             |
|    approx_kl            | 0.023976268 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.25e+03    |
|    n_updates            | 2391        |
|    policy_gradient_loss | 0.0199      |
|    value_loss           | 5.09e+03    |
-----------------------------------------
Eval num_timesteps=635500, episode_reward=579.66 +/- 227.47
Episode length: 84.16 +/- 10.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 580      |
| time/              |          |
|    total_timesteps | 635500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 647      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 621      |
|    time_elapsed    | 15929    |
|    total_timesteps | 635904   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=636000, episode_reward=630.51 +/- 191.65
Episode length: 87.98 +/- 7.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88          |
|    mean_reward          | 631         |
| time/                   |             |
|    total_timesteps      | 636000      |
| train/                  |             |
|    approx_kl            | 0.040658906 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.41e+03    |
|    n_updates            | 2394        |
|    policy_gradient_loss | 0.00674     |
|    value_loss           | 7.68e+03    |
-----------------------------------------
Eval num_timesteps=636500, episode_reward=631.85 +/- 203.59
Episode length: 87.70 +/- 10.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.7     |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 636500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | 615      |
| time/              |          |
|    fps             | 39       |
|    iterations      | 622      |
|    time_elapsed    | 15936    |
|    total_timesteps | 636928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=637000, episode_reward=673.47 +/- 200.78
Episode length: 87.64 +/- 8.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.6        |
|    mean_reward          | 673         |
| time/                   |             |
|    total_timesteps      | 637000      |
| train/                  |             |
|    approx_kl            | 0.049440727 |
|    clip_fraction        | 0.0914      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.9e+03     |
|    n_updates            | 2396        |
|    policy_gradient_loss | 0.00315     |
|    value_loss           | 6.74e+03    |
-----------------------------------------
Eval num_timesteps=637500, episode_reward=669.34 +/- 193.16
Episode length: 89.50 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 637500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.8     |
|    ep_rew_mean     | 621      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 623      |
|    time_elapsed    | 15943    |
|    total_timesteps | 637952   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=638000, episode_reward=751.84 +/- 119.39
Episode length: 93.96 +/- 5.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94          |
|    mean_reward          | 752         |
| time/                   |             |
|    total_timesteps      | 638000      |
| train/                  |             |
|    approx_kl            | 0.071933895 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.408      |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.07e+03    |
|    n_updates            | 2399        |
|    policy_gradient_loss | 0.00211     |
|    value_loss           | 3.97e+03    |
-----------------------------------------
Eval num_timesteps=638500, episode_reward=715.34 +/- 149.59
Episode length: 93.20 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 638500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.5     |
|    ep_rew_mean     | 623      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 624      |
|    time_elapsed    | 15950    |
|    total_timesteps | 638976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=639000, episode_reward=428.91 +/- 242.90
Episode length: 79.74 +/- 14.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.7        |
|    mean_reward          | 429         |
| time/                   |             |
|    total_timesteps      | 639000      |
| train/                  |             |
|    approx_kl            | 0.053219847 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.437      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0005      |
|    loss                 | 1.88e+03    |
|    n_updates            | 2401        |
|    policy_gradient_loss | 0.0142      |
|    value_loss           | 3.99e+03    |
-----------------------------------------
Eval num_timesteps=639500, episode_reward=392.07 +/- 201.33
Episode length: 78.48 +/- 13.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 639500   |
---------------------------------
Eval num_timesteps=640000, episode_reward=456.19 +/- 264.39
Episode length: 79.98 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 596      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 625      |
|    time_elapsed    | 15959    |
|    total_timesteps | 640000   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=640500, episode_reward=634.04 +/- 253.27
Episode length: 89.32 +/- 16.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 89.3       |
|    mean_reward          | 634        |
| time/                   |            |
|    total_timesteps      | 640500     |
| train/                  |            |
|    approx_kl            | 0.05330623 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.438     |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.71e+03   |
|    n_updates            | 2403       |
|    policy_gradient_loss | 0.00385    |
|    value_loss           | 8.39e+03   |
----------------------------------------
Eval num_timesteps=641000, episode_reward=628.26 +/- 208.83
Episode length: 91.84 +/- 11.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.9     |
|    ep_rew_mean     | 582      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 626      |
|    time_elapsed    | 15966    |
|    total_timesteps | 641024   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=641500, episode_reward=774.29 +/- 66.88
Episode length: 102.62 +/- 6.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 103        |
|    mean_reward          | 774        |
| time/                   |            |
|    total_timesteps      | 641500     |
| train/                  |            |
|    approx_kl            | 0.02671065 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.491     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.14e+03   |
|    n_updates            | 2405       |
|    policy_gradient_loss | 0.0179     |
|    value_loss           | 4.37e+03   |
----------------------------------------
Eval num_timesteps=642000, episode_reward=739.21 +/- 151.23
Episode length: 102.50 +/- 8.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.5     |
|    ep_rew_mean     | 592      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 627      |
|    time_elapsed    | 15974    |
|    total_timesteps | 642048   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=642500, episode_reward=722.86 +/- 185.40
Episode length: 108.30 +/- 18.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 723         |
| time/                   |             |
|    total_timesteps      | 642500      |
| train/                  |             |
|    approx_kl            | 0.054099727 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.28e+03    |
|    n_updates            | 2407        |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 5.28e+03    |
-----------------------------------------
Eval num_timesteps=643000, episode_reward=758.29 +/- 134.78
Episode length: 138.62 +/- 100.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.7     |
|    ep_rew_mean     | 607      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 628      |
|    time_elapsed    | 15983    |
|    total_timesteps | 643072   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=643500, episode_reward=711.12 +/- 165.73
Episode length: 145.34 +/- 114.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 145         |
|    mean_reward          | 711         |
| time/                   |             |
|    total_timesteps      | 643500      |
| train/                  |             |
|    approx_kl            | 0.037996273 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.08e+03    |
|    n_updates            | 2411        |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 4.06e+03    |
-----------------------------------------
Eval num_timesteps=644000, episode_reward=717.53 +/- 170.21
Episode length: 138.78 +/- 96.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 644000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.9     |
|    ep_rew_mean     | 612      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 629      |
|    time_elapsed    | 15993    |
|    total_timesteps | 644096   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.15
Eval num_timesteps=644500, episode_reward=664.24 +/- 205.25
Episode length: 216.36 +/- 163.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 664        |
| time/                   |            |
|    total_timesteps      | 644500     |
| train/                  |            |
|    approx_kl            | 0.09764004 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.29e+03   |
|    n_updates            | 2414       |
|    policy_gradient_loss | -0.00207   |
|    value_loss           | 2.76e+03   |
----------------------------------------
Eval num_timesteps=645000, episode_reward=707.85 +/- 174.24
Episode length: 175.56 +/- 126.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 616      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 630      |
|    time_elapsed    | 16007    |
|    total_timesteps | 645120   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=645500, episode_reward=715.31 +/- 132.27
Episode length: 256.36 +/- 193.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 715         |
| time/                   |             |
|    total_timesteps      | 645500      |
| train/                  |             |
|    approx_kl            | 0.048846155 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.27e+03    |
|    n_updates            | 2416        |
|    policy_gradient_loss | 0.00487     |
|    value_loss           | 2.5e+03     |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=681.22 +/- 137.86
Episode length: 238.20 +/- 189.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 646000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 626      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 631      |
|    time_elapsed    | 16024    |
|    total_timesteps | 646144   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=646500, episode_reward=646.77 +/- 152.61
Episode length: 375.30 +/- 200.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 375         |
|    mean_reward          | 647         |
| time/                   |             |
|    total_timesteps      | 646500      |
| train/                  |             |
|    approx_kl            | 0.051069163 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.0005      |
|    loss                 | 881         |
|    n_updates            | 2421        |
|    policy_gradient_loss | -0.00896    |
|    value_loss           | 2.02e+03    |
-----------------------------------------
Eval num_timesteps=647000, episode_reward=644.81 +/- 159.33
Episode length: 282.66 +/- 202.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 625      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 632      |
|    time_elapsed    | 16047    |
|    total_timesteps | 647168   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=647500, episode_reward=644.49 +/- 128.43
Episode length: 397.78 +/- 189.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 398        |
|    mean_reward          | 644        |
| time/                   |            |
|    total_timesteps      | 647500     |
| train/                  |            |
|    approx_kl            | 0.02958562 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0005     |
|    loss                 | 967        |
|    n_updates            | 2424       |
|    policy_gradient_loss | -0.00599   |
|    value_loss           | 1.83e+03   |
----------------------------------------
Eval num_timesteps=648000, episode_reward=667.80 +/- 106.55
Episode length: 378.58 +/- 198.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 630      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 633      |
|    time_elapsed    | 16074    |
|    total_timesteps | 648192   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=648500, episode_reward=669.24 +/- 126.20
Episode length: 355.00 +/- 201.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 669         |
| time/                   |             |
|    total_timesteps      | 648500      |
| train/                  |             |
|    approx_kl            | 0.046156853 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0005      |
|    loss                 | 688         |
|    n_updates            | 2427        |
|    policy_gradient_loss | -0.000575   |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=649000, episode_reward=675.59 +/- 118.77
Episode length: 382.48 +/- 188.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 382      |
|    mean_reward     | 676      |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 637      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 634      |
|    time_elapsed    | 16099    |
|    total_timesteps | 649216   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.09
Eval num_timesteps=649500, episode_reward=752.62 +/- 114.87
Episode length: 235.24 +/- 160.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 235         |
|    mean_reward          | 753         |
| time/                   |             |
|    total_timesteps      | 649500      |
| train/                  |             |
|    approx_kl            | 0.046720266 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0005      |
|    loss                 | 922         |
|    n_updates            | 2432        |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=699.47 +/- 131.67
Episode length: 267.54 +/- 196.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 653      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 635      |
|    time_elapsed    | 16117    |
|    total_timesteps | 650240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=650500, episode_reward=713.10 +/- 176.02
Episode length: 102.48 +/- 15.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 713         |
| time/                   |             |
|    total_timesteps      | 650500      |
| train/                  |             |
|    approx_kl            | 0.025478516 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0005      |
|    loss                 | 911         |
|    n_updates            | 2433        |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 1.81e+03    |
-----------------------------------------
Eval num_timesteps=651000, episode_reward=690.79 +/- 202.25
Episode length: 99.18 +/- 13.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 655      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 636      |
|    time_elapsed    | 16125    |
|    total_timesteps | 651264   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=651500, episode_reward=699.17 +/- 188.63
Episode length: 98.02 +/- 13.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98         |
|    mean_reward          | 699        |
| time/                   |            |
|    total_timesteps      | 651500     |
| train/                  |            |
|    approx_kl            | 0.04767397 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.525     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.32e+03   |
|    n_updates            | 2435       |
|    policy_gradient_loss | -0.00164   |
|    value_loss           | 5.07e+03   |
----------------------------------------
Eval num_timesteps=652000, episode_reward=740.09 +/- 162.30
Episode length: 97.40 +/- 12.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 652000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 677      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 637      |
|    time_elapsed    | 16132    |
|    total_timesteps | 652288   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.28
Eval num_timesteps=652500, episode_reward=718.88 +/- 144.32
Episode length: 101.26 +/- 7.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 719         |
| time/                   |             |
|    total_timesteps      | 652500      |
| train/                  |             |
|    approx_kl            | 0.057281353 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.26e+03    |
|    n_updates            | 2437        |
|    policy_gradient_loss | 0.00916     |
|    value_loss           | 2.65e+03    |
-----------------------------------------
Eval num_timesteps=653000, episode_reward=737.37 +/- 159.28
Episode length: 100.64 +/- 13.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 671      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 638      |
|    time_elapsed    | 16140    |
|    total_timesteps | 653312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=653500, episode_reward=691.67 +/- 177.08
Episode length: 112.28 +/- 60.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 692         |
| time/                   |             |
|    total_timesteps      | 653500      |
| train/                  |             |
|    approx_kl            | 0.036549576 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.555      |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.17e+03    |
|    n_updates            | 2439        |
|    policy_gradient_loss | 0.0169      |
|    value_loss           | 3.83e+03    |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=700.29 +/- 191.35
Episode length: 102.32 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 661      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 639      |
|    time_elapsed    | 16148    |
|    total_timesteps | 654336   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=654500, episode_reward=728.71 +/- 172.29
Episode length: 101.00 +/- 12.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 101        |
|    mean_reward          | 729        |
| time/                   |            |
|    total_timesteps      | 654500     |
| train/                  |            |
|    approx_kl            | 0.10006938 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.442     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0005     |
|    loss                 | 3.35e+03   |
|    n_updates            | 2442       |
|    policy_gradient_loss | 0.00684    |
|    value_loss           | 5.89e+03   |
----------------------------------------
Eval num_timesteps=655000, episode_reward=750.34 +/- 125.30
Episode length: 111.34 +/- 59.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 666      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 640      |
|    time_elapsed    | 16156    |
|    total_timesteps | 655360   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=655500, episode_reward=713.65 +/- 164.27
Episode length: 130.48 +/- 101.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 130         |
|    mean_reward          | 714         |
| time/                   |             |
|    total_timesteps      | 655500      |
| train/                  |             |
|    approx_kl            | 0.051980305 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.506      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.78e+03    |
|    n_updates            | 2444        |
|    policy_gradient_loss | 0.0183      |
|    value_loss           | 3.61e+03    |
-----------------------------------------
Eval num_timesteps=656000, episode_reward=724.58 +/- 148.45
Episode length: 108.10 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 656000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 649      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 641      |
|    time_elapsed    | 16165    |
|    total_timesteps | 656384   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=656500, episode_reward=669.20 +/- 214.04
Episode length: 104.92 +/- 61.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 669         |
| time/                   |             |
|    total_timesteps      | 656500      |
| train/                  |             |
|    approx_kl            | 0.062632956 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.59e+03    |
|    n_updates            | 2448        |
|    policy_gradient_loss | 0.00185     |
|    value_loss           | 8.15e+03    |
-----------------------------------------
Eval num_timesteps=657000, episode_reward=638.95 +/- 219.50
Episode length: 95.50 +/- 13.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 639      |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 644      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 642      |
|    time_elapsed    | 16173    |
|    total_timesteps | 657408   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=657500, episode_reward=457.38 +/- 336.32
Episode length: 94.48 +/- 65.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.5       |
|    mean_reward          | 457        |
| time/                   |            |
|    total_timesteps      | 657500     |
| train/                  |            |
|    approx_kl            | 0.04549225 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.485     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.95e+03   |
|    n_updates            | 2450       |
|    policy_gradient_loss | 0.00525    |
|    value_loss           | 4.09e+03   |
----------------------------------------
Eval num_timesteps=658000, episode_reward=447.19 +/- 313.34
Episode length: 85.38 +/- 24.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 658000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 598      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 643      |
|    time_elapsed    | 16180    |
|    total_timesteps | 658432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=658500, episode_reward=706.35 +/- 176.19
Episode length: 96.48 +/- 8.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.5        |
|    mean_reward          | 706         |
| time/                   |             |
|    total_timesteps      | 658500      |
| train/                  |             |
|    approx_kl            | 0.031958632 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.23e+03    |
|    n_updates            | 2451        |
|    policy_gradient_loss | 0.00996     |
|    value_loss           | 8.41e+03    |
-----------------------------------------
Eval num_timesteps=659000, episode_reward=690.51 +/- 195.16
Episode length: 103.86 +/- 60.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 585      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 644      |
|    time_elapsed    | 16187    |
|    total_timesteps | 659456   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=659500, episode_reward=716.77 +/- 152.32
Episode length: 101.84 +/- 7.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 717        |
| time/                   |            |
|    total_timesteps      | 659500     |
| train/                  |            |
|    approx_kl            | 0.07829267 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.461     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.39e+03   |
|    n_updates            | 2454       |
|    policy_gradient_loss | -0.00118   |
|    value_loss           | 4.9e+03    |
----------------------------------------
Eval num_timesteps=660000, episode_reward=695.17 +/- 188.32
Episode length: 102.22 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 589      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 645      |
|    time_elapsed    | 16195    |
|    total_timesteps | 660480   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=660500, episode_reward=714.06 +/- 150.94
Episode length: 109.68 +/- 60.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 714         |
| time/                   |             |
|    total_timesteps      | 660500      |
| train/                  |             |
|    approx_kl            | 0.029421395 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.467      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.46e+03    |
|    n_updates            | 2456        |
|    policy_gradient_loss | 0.0122      |
|    value_loss           | 3.25e+03    |
-----------------------------------------
Eval num_timesteps=661000, episode_reward=701.73 +/- 185.60
Episode length: 117.72 +/- 84.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=661500, episode_reward=742.25 +/- 131.22
Episode length: 106.62 +/- 27.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 661500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 591      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 646      |
|    time_elapsed    | 16207    |
|    total_timesteps | 661504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=662000, episode_reward=690.10 +/- 192.75
Episode length: 95.26 +/- 8.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 95.3       |
|    mean_reward          | 690        |
| time/                   |            |
|    total_timesteps      | 662000     |
| train/                  |            |
|    approx_kl            | 0.03928677 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.454     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.33e+03   |
|    n_updates            | 2458       |
|    policy_gradient_loss | 0.00579    |
|    value_loss           | 4.83e+03   |
----------------------------------------
Eval num_timesteps=662500, episode_reward=750.03 +/- 136.03
Episode length: 96.94 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 662500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.9     |
|    ep_rew_mean     | 576      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 647      |
|    time_elapsed    | 16214    |
|    total_timesteps | 662528   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=663000, episode_reward=740.03 +/- 138.35
Episode length: 113.22 +/- 60.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 740         |
| time/                   |             |
|    total_timesteps      | 663000      |
| train/                  |             |
|    approx_kl            | 0.030461377 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.422      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.18e+03    |
|    n_updates            | 2460        |
|    policy_gradient_loss | 0.00912     |
|    value_loss           | 3.77e+03    |
-----------------------------------------
Eval num_timesteps=663500, episode_reward=699.55 +/- 172.22
Episode length: 113.04 +/- 62.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 663500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 559      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 648      |
|    time_elapsed    | 16223    |
|    total_timesteps | 663552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=664000, episode_reward=726.69 +/- 156.67
Episode length: 102.54 +/- 31.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 103        |
|    mean_reward          | 727        |
| time/                   |            |
|    total_timesteps      | 664000     |
| train/                  |            |
|    approx_kl            | 0.04360373 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.533     |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.65e+03   |
|    n_updates            | 2462       |
|    policy_gradient_loss | 0.0144     |
|    value_loss           | 5.67e+03   |
----------------------------------------
Eval num_timesteps=664500, episode_reward=633.67 +/- 203.84
Episode length: 97.78 +/- 12.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 634      |
| time/              |          |
|    total_timesteps | 664500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99       |
|    ep_rew_mean     | 550      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 649      |
|    time_elapsed    | 16231    |
|    total_timesteps | 664576   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=665000, episode_reward=684.82 +/- 199.51
Episode length: 98.02 +/- 10.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98          |
|    mean_reward          | 685         |
| time/                   |             |
|    total_timesteps      | 665000      |
| train/                  |             |
|    approx_kl            | 0.058793057 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.15e+03    |
|    n_updates            | 2465        |
|    policy_gradient_loss | -0.00462    |
|    value_loss           | 5.4e+03     |
-----------------------------------------
Eval num_timesteps=665500, episode_reward=623.92 +/- 230.40
Episode length: 94.98 +/- 12.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 665500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99       |
|    ep_rew_mean     | 557      |
| time/              |          |
|    fps             | 40       |
|    iterations      | 650      |
|    time_elapsed    | 16238    |
|    total_timesteps | 665600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=666000, episode_reward=622.85 +/- 222.80
Episode length: 148.52 +/- 123.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 149        |
|    mean_reward          | 623        |
| time/                   |            |
|    total_timesteps      | 666000     |
| train/                  |            |
|    approx_kl            | 0.02236454 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.541     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.5e+03    |
|    n_updates            | 2466       |
|    policy_gradient_loss | 0.00235    |
|    value_loss           | 4.7e+03    |
----------------------------------------
Eval num_timesteps=666500, episode_reward=598.22 +/- 227.44
Episode length: 166.60 +/- 147.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 598      |
| time/              |          |
|    total_timesteps | 666500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 584      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 651      |
|    time_elapsed    | 16249    |
|    total_timesteps | 666624   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.09
Eval num_timesteps=667000, episode_reward=562.45 +/- 213.76
Episode length: 97.52 +/- 14.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.5        |
|    mean_reward          | 562         |
| time/                   |             |
|    total_timesteps      | 667000      |
| train/                  |             |
|    approx_kl            | 0.076851346 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.31e+03    |
|    n_updates            | 2470        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 2.63e+03    |
-----------------------------------------
Eval num_timesteps=667500, episode_reward=591.51 +/- 217.85
Episode length: 103.78 +/- 50.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 667500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 588      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 652      |
|    time_elapsed    | 16257    |
|    total_timesteps | 667648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=668000, episode_reward=646.35 +/- 215.78
Episode length: 96.18 +/- 14.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.2       |
|    mean_reward          | 646        |
| time/                   |            |
|    total_timesteps      | 668000     |
| train/                  |            |
|    approx_kl            | 0.04457693 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.06e+03   |
|    n_updates            | 2472       |
|    policy_gradient_loss | 0.00736    |
|    value_loss           | 2.16e+03   |
----------------------------------------
Eval num_timesteps=668500, episode_reward=644.39 +/- 225.88
Episode length: 94.56 +/- 13.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 668500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 602      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 653      |
|    time_elapsed    | 16264    |
|    total_timesteps | 668672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=669000, episode_reward=447.22 +/- 204.38
Episode length: 83.98 +/- 12.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 84        |
|    mean_reward          | 447       |
| time/                   |           |
|    total_timesteps      | 669000    |
| train/                  |           |
|    approx_kl            | 0.0176513 |
|    clip_fraction        | 0.122     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.48     |
|    explained_variance   | 0.532     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.71e+03  |
|    n_updates            | 2473      |
|    policy_gradient_loss | 0.0225    |
|    value_loss           | 5.5e+03   |
---------------------------------------
Eval num_timesteps=669500, episode_reward=475.44 +/- 179.82
Episode length: 85.24 +/- 9.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 669500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 618      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 654      |
|    time_elapsed    | 16271    |
|    total_timesteps | 669696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=670000, episode_reward=496.80 +/- 229.81
Episode length: 93.06 +/- 36.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.1        |
|    mean_reward          | 497         |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.044411857 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.45e+03    |
|    n_updates            | 2475        |
|    policy_gradient_loss | 0.00697     |
|    value_loss           | 6.38e+03    |
-----------------------------------------
Eval num_timesteps=670500, episode_reward=502.75 +/- 219.45
Episode length: 106.88 +/- 81.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 670500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 590      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 655      |
|    time_elapsed    | 16279    |
|    total_timesteps | 670720   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=671000, episode_reward=581.33 +/- 225.24
Episode length: 91.64 +/- 15.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 91.6       |
|    mean_reward          | 581        |
| time/                   |            |
|    total_timesteps      | 671000     |
| train/                  |            |
|    approx_kl            | 0.03714604 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.531     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.65e+03   |
|    n_updates            | 2478       |
|    policy_gradient_loss | -0.00651   |
|    value_loss           | 5.56e+03   |
----------------------------------------
Eval num_timesteps=671500, episode_reward=595.64 +/- 182.90
Episode length: 93.22 +/- 13.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 596      |
| time/              |          |
|    total_timesteps | 671500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 588      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 656      |
|    time_elapsed    | 16286    |
|    total_timesteps | 671744   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.11
Eval num_timesteps=672000, episode_reward=574.49 +/- 174.44
Episode length: 90.62 +/- 11.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.6        |
|    mean_reward          | 574         |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.073854476 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.81e+03    |
|    n_updates            | 2482        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 4.11e+03    |
-----------------------------------------
Eval num_timesteps=672500, episode_reward=488.83 +/- 224.21
Episode length: 88.66 +/- 18.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.7     |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 672500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 594      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 657      |
|    time_elapsed    | 16293    |
|    total_timesteps | 672768   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=673000, episode_reward=481.32 +/- 193.11
Episode length: 89.18 +/- 15.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.2        |
|    mean_reward          | 481         |
| time/                   |             |
|    total_timesteps      | 673000      |
| train/                  |             |
|    approx_kl            | 0.065618396 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.519       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.27e+03    |
|    n_updates            | 2485        |
|    policy_gradient_loss | 0.00849     |
|    value_loss           | 4.2e+03     |
-----------------------------------------
Eval num_timesteps=673500, episode_reward=527.16 +/- 190.86
Episode length: 88.94 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.9     |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 673500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 584      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 658      |
|    time_elapsed    | 16300    |
|    total_timesteps | 673792   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=674000, episode_reward=460.75 +/- 203.59
Episode length: 84.00 +/- 11.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 84         |
|    mean_reward          | 461        |
| time/                   |            |
|    total_timesteps      | 674000     |
| train/                  |            |
|    approx_kl            | 0.03733211 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.483     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.65e+03   |
|    n_updates            | 2488       |
|    policy_gradient_loss | 0.00661    |
|    value_loss           | 3.24e+03   |
----------------------------------------
Eval num_timesteps=674500, episode_reward=465.65 +/- 205.97
Episode length: 83.78 +/- 12.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.8     |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 674500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 579      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 659      |
|    time_elapsed    | 16307    |
|    total_timesteps | 674816   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=675000, episode_reward=405.46 +/- 209.50
Episode length: 78.32 +/- 13.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.3        |
|    mean_reward          | 405         |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.095705666 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.42e+03    |
|    n_updates            | 2491        |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 4.63e+03    |
-----------------------------------------
Eval num_timesteps=675500, episode_reward=441.90 +/- 222.50
Episode length: 81.84 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.8     |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 675500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 578      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 660      |
|    time_elapsed    | 16314    |
|    total_timesteps | 675840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=676000, episode_reward=567.05 +/- 226.81
Episode length: 88.54 +/- 11.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.5        |
|    mean_reward          | 567         |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.019197945 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.413      |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.0005      |
|    loss                 | 2.85e+03    |
|    n_updates            | 2492        |
|    policy_gradient_loss | 0.00748     |
|    value_loss           | 5.22e+03    |
-----------------------------------------
Eval num_timesteps=676500, episode_reward=573.95 +/- 219.34
Episode length: 88.60 +/- 11.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.6     |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 676500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 560      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 661      |
|    time_elapsed    | 16321    |
|    total_timesteps | 676864   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=677000, episode_reward=604.78 +/- 243.82
Episode length: 92.54 +/- 10.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.5        |
|    mean_reward          | 605         |
| time/                   |             |
|    total_timesteps      | 677000      |
| train/                  |             |
|    approx_kl            | 0.051334042 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.388      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.94e+03    |
|    n_updates            | 2495        |
|    policy_gradient_loss | 0.0123      |
|    value_loss           | 4.47e+03    |
-----------------------------------------
Eval num_timesteps=677500, episode_reward=676.75 +/- 218.77
Episode length: 94.10 +/- 10.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 677500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.7     |
|    ep_rew_mean     | 574      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 662      |
|    time_elapsed    | 16328    |
|    total_timesteps | 677888   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=678000, episode_reward=648.32 +/- 199.02
Episode length: 94.34 +/- 11.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.3       |
|    mean_reward          | 648        |
| time/                   |            |
|    total_timesteps      | 678000     |
| train/                  |            |
|    approx_kl            | 0.04682618 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.451     |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.37e+03   |
|    n_updates            | 2498       |
|    policy_gradient_loss | 0.000979   |
|    value_loss           | 3.03e+03   |
----------------------------------------
Eval num_timesteps=678500, episode_reward=651.26 +/- 220.48
Episode length: 94.94 +/- 13.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 678500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 588      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 663      |
|    time_elapsed    | 16335    |
|    total_timesteps | 678912   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=679000, episode_reward=561.06 +/- 239.58
Episode length: 85.66 +/- 11.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85.7       |
|    mean_reward          | 561        |
| time/                   |            |
|    total_timesteps      | 679000     |
| train/                  |            |
|    approx_kl            | 0.04929212 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.448     |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.47e+03   |
|    n_updates            | 2500       |
|    policy_gradient_loss | 0.00695    |
|    value_loss           | 3.02e+03   |
----------------------------------------
Eval num_timesteps=679500, episode_reward=595.15 +/- 252.09
Episode length: 95.98 +/- 63.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 595      |
| time/              |          |
|    total_timesteps | 679500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97       |
|    ep_rew_mean     | 615      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 664      |
|    time_elapsed    | 16342    |
|    total_timesteps | 679936   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=680000, episode_reward=538.26 +/- 260.57
Episode length: 87.94 +/- 16.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.9        |
|    mean_reward          | 538         |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.048904855 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.57e+03    |
|    n_updates            | 2502        |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 4.9e+03     |
-----------------------------------------
Eval num_timesteps=680500, episode_reward=672.16 +/- 202.03
Episode length: 91.74 +/- 9.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.7     |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 680500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.1     |
|    ep_rew_mean     | 621      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 665      |
|    time_elapsed    | 16349    |
|    total_timesteps | 680960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=681000, episode_reward=669.96 +/- 222.10
Episode length: 106.62 +/- 60.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 107        |
|    mean_reward          | 670        |
| time/                   |            |
|    total_timesteps      | 681000     |
| train/                  |            |
|    approx_kl            | 0.01921666 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.47      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.83e+03   |
|    n_updates            | 2503       |
|    policy_gradient_loss | 0.006      |
|    value_loss           | 4.49e+03   |
----------------------------------------
Eval num_timesteps=681500, episode_reward=625.42 +/- 255.25
Episode length: 130.10 +/- 117.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 681500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | 607      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 666      |
|    time_elapsed    | 16358    |
|    total_timesteps | 681984   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=682000, episode_reward=678.34 +/- 209.38
Episode length: 103.64 +/- 60.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 678         |
| time/                   |             |
|    total_timesteps      | 682000      |
| train/                  |             |
|    approx_kl            | 0.065008156 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.05e+03    |
|    n_updates            | 2505        |
|    policy_gradient_loss | 0.0129      |
|    value_loss           | 5.39e+03    |
-----------------------------------------
Eval num_timesteps=682500, episode_reward=676.54 +/- 189.80
Episode length: 111.70 +/- 84.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 682500   |
---------------------------------
Eval num_timesteps=683000, episode_reward=654.10 +/- 218.06
Episode length: 110.42 +/- 85.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.8     |
|    ep_rew_mean     | 604      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 667      |
|    time_elapsed    | 16370    |
|    total_timesteps | 683008   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=683500, episode_reward=699.37 +/- 186.60
Episode length: 114.22 +/- 84.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 114        |
|    mean_reward          | 699        |
| time/                   |            |
|    total_timesteps      | 683500     |
| train/                  |            |
|    approx_kl            | 0.06819123 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.511     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.89e+03   |
|    n_updates            | 2509       |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 4.2e+03    |
----------------------------------------
Eval num_timesteps=684000, episode_reward=711.70 +/- 194.96
Episode length: 104.06 +/- 60.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.4     |
|    ep_rew_mean     | 615      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 668      |
|    time_elapsed    | 16378    |
|    total_timesteps | 684032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=684500, episode_reward=689.74 +/- 182.49
Episode length: 106.44 +/- 60.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 690         |
| time/                   |             |
|    total_timesteps      | 684500      |
| train/                  |             |
|    approx_kl            | 0.031304702 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.8e+03     |
|    n_updates            | 2510        |
|    policy_gradient_loss | 0.0169      |
|    value_loss           | 3.77e+03    |
-----------------------------------------
Eval num_timesteps=685000, episode_reward=698.48 +/- 181.67
Episode length: 95.16 +/- 10.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.2     |
|    ep_rew_mean     | 635      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 669      |
|    time_elapsed    | 16386    |
|    total_timesteps | 685056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.22
Eval num_timesteps=685500, episode_reward=696.98 +/- 177.35
Episode length: 97.90 +/- 9.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.9       |
|    mean_reward          | 697        |
| time/                   |            |
|    total_timesteps      | 685500     |
| train/                  |            |
|    approx_kl            | 0.06495036 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.569     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.98e+03   |
|    n_updates            | 2512       |
|    policy_gradient_loss | 0.0048     |
|    value_loss           | 4.6e+03    |
----------------------------------------
Eval num_timesteps=686000, episode_reward=688.68 +/- 212.68
Episode length: 95.38 +/- 9.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 651      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 670      |
|    time_elapsed    | 16394    |
|    total_timesteps | 686080   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=686500, episode_reward=775.22 +/- 88.79
Episode length: 130.72 +/- 99.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 775         |
| time/                   |             |
|    total_timesteps      | 686500      |
| train/                  |             |
|    approx_kl            | 0.045302972 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.41e+03    |
|    n_updates            | 2515        |
|    policy_gradient_loss | 0.00231     |
|    value_loss           | 3.94e+03    |
-----------------------------------------
Eval num_timesteps=687000, episode_reward=782.42 +/- 92.37
Episode length: 113.08 +/- 59.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 687000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 671      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 671      |
|    time_elapsed    | 16403    |
|    total_timesteps | 687104   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=687500, episode_reward=767.77 +/- 143.81
Episode length: 100.80 +/- 9.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 101        |
|    mean_reward          | 768        |
| time/                   |            |
|    total_timesteps      | 687500     |
| train/                  |            |
|    approx_kl            | 0.07982863 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.563     |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.17e+03   |
|    n_updates            | 2519       |
|    policy_gradient_loss | -0.00688   |
|    value_loss           | 2.25e+03   |
----------------------------------------
Eval num_timesteps=688000, episode_reward=780.38 +/- 123.50
Episode length: 101.66 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 677      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 672      |
|    time_elapsed    | 16410    |
|    total_timesteps | 688128   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=688500, episode_reward=746.62 +/- 150.34
Episode length: 95.22 +/- 7.61
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 95.2     |
|    mean_reward          | 747      |
| time/                   |          |
|    total_timesteps      | 688500   |
| train/                  |          |
|    approx_kl            | 0.059223 |
|    clip_fraction        | 0.172    |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -0.446   |
|    explained_variance   | 0.757    |
|    learning_rate        | 0.0005   |
|    loss                 | 1.29e+03 |
|    n_updates            | 2522     |
|    policy_gradient_loss | 0.000358 |
|    value_loss           | 2.73e+03 |
--------------------------------------
Eval num_timesteps=689000, episode_reward=773.38 +/- 112.10
Episode length: 95.14 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 689000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 684      |
| time/              |          |
|    fps             | 41       |
|    iterations      | 673      |
|    time_elapsed    | 16418    |
|    total_timesteps | 689152   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=689500, episode_reward=772.26 +/- 108.15
Episode length: 94.52 +/- 4.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.5        |
|    mean_reward          | 772         |
| time/                   |             |
|    total_timesteps      | 689500      |
| train/                  |             |
|    approx_kl            | 0.053784672 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.56e+03    |
|    n_updates            | 2525        |
|    policy_gradient_loss | -0.00819    |
|    value_loss           | 3.14e+03    |
-----------------------------------------
Eval num_timesteps=690000, episode_reward=767.43 +/- 95.38
Episode length: 94.86 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 688      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 674      |
|    time_elapsed    | 16425    |
|    total_timesteps | 690176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=690500, episode_reward=752.04 +/- 129.86
Episode length: 95.32 +/- 6.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.3        |
|    mean_reward          | 752         |
| time/                   |             |
|    total_timesteps      | 690500      |
| train/                  |             |
|    approx_kl            | 0.025698416 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.95e+03    |
|    n_updates            | 2527        |
|    policy_gradient_loss | 0.000551    |
|    value_loss           | 4.2e+03     |
-----------------------------------------
Eval num_timesteps=691000, episode_reward=748.76 +/- 112.44
Episode length: 94.98 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 691000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 702      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 675      |
|    time_elapsed    | 16432    |
|    total_timesteps | 691200   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=691500, episode_reward=798.56 +/- 37.69
Episode length: 115.68 +/- 22.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 116       |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 691500    |
| train/                  |           |
|    approx_kl            | 0.0738537 |
|    clip_fraction        | 0.128     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.453    |
|    explained_variance   | 0.783     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.84e+03  |
|    n_updates            | 2529      |
|    policy_gradient_loss | 0.00313   |
|    value_loss           | 3.11e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=692000, episode_reward=763.32 +/- 111.58
Episode length: 121.84 +/- 49.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 721      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 676      |
|    time_elapsed    | 16441    |
|    total_timesteps | 692224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=692500, episode_reward=732.54 +/- 165.61
Episode length: 113.06 +/- 31.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 733         |
| time/                   |             |
|    total_timesteps      | 692500      |
| train/                  |             |
|    approx_kl            | 0.022970397 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.39e+03    |
|    n_updates            | 2530        |
|    policy_gradient_loss | 0.022       |
|    value_loss           | 2.83e+03    |
-----------------------------------------
Eval num_timesteps=693000, episode_reward=764.93 +/- 104.58
Episode length: 125.82 +/- 64.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 693000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 677      |
|    time_elapsed    | 16450    |
|    total_timesteps | 693248   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=693500, episode_reward=705.19 +/- 177.82
Episode length: 100.98 +/- 21.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 101        |
|    mean_reward          | 705        |
| time/                   |            |
|    total_timesteps      | 693500     |
| train/                  |            |
|    approx_kl            | 0.05198597 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.527     |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.06e+03   |
|    n_updates            | 2532       |
|    policy_gradient_loss | 0.0102     |
|    value_loss           | 2.3e+03    |
----------------------------------------
Eval num_timesteps=694000, episode_reward=709.74 +/- 176.86
Episode length: 97.64 +/- 12.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 745      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 678      |
|    time_elapsed    | 16458    |
|    total_timesteps | 694272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=694500, episode_reward=770.43 +/- 102.84
Episode length: 103.84 +/- 8.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 770         |
| time/                   |             |
|    total_timesteps      | 694500      |
| train/                  |             |
|    approx_kl            | 0.042565264 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.427      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0005      |
|    loss                 | 3.3e+03     |
|    n_updates            | 2534        |
|    policy_gradient_loss | 0.00405     |
|    value_loss           | 6.03e+03    |
-----------------------------------------
Eval num_timesteps=695000, episode_reward=766.84 +/- 107.57
Episode length: 104.06 +/- 10.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 679      |
|    time_elapsed    | 16465    |
|    total_timesteps | 695296   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=695500, episode_reward=783.13 +/- 96.15
Episode length: 124.14 +/- 40.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 695500      |
| train/                  |             |
|    approx_kl            | 0.052857798 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.437      |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.47e+03    |
|    n_updates            | 2537        |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 2.77e+03    |
-----------------------------------------
Eval num_timesteps=696000, episode_reward=788.74 +/- 85.22
Episode length: 135.90 +/- 56.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 680      |
|    time_elapsed    | 16475    |
|    total_timesteps | 696320   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=696500, episode_reward=758.86 +/- 123.18
Episode length: 105.58 +/- 14.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 759        |
| time/                   |            |
|    total_timesteps      | 696500     |
| train/                  |            |
|    approx_kl            | 0.04433729 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.455     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.21e+03   |
|    n_updates            | 2540       |
|    policy_gradient_loss | -6.04e-05  |
|    value_loss           | 2.59e+03   |
----------------------------------------
Eval num_timesteps=697000, episode_reward=757.10 +/- 122.68
Episode length: 107.20 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 697000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 681      |
|    time_elapsed    | 16483    |
|    total_timesteps | 697344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=697500, episode_reward=767.25 +/- 100.52
Episode length: 121.88 +/- 59.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 767         |
| time/                   |             |
|    total_timesteps      | 697500      |
| train/                  |             |
|    approx_kl            | 0.050221235 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.428      |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.18e+03    |
|    n_updates            | 2542        |
|    policy_gradient_loss | 0.00801     |
|    value_loss           | 4.07e+03    |
-----------------------------------------
Eval num_timesteps=698000, episode_reward=743.45 +/- 117.79
Episode length: 122.80 +/- 43.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 682      |
|    time_elapsed    | 16492    |
|    total_timesteps | 698368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=698500, episode_reward=739.49 +/- 83.57
Episode length: 262.00 +/- 173.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 262         |
|    mean_reward          | 739         |
| time/                   |             |
|    total_timesteps      | 698500      |
| train/                  |             |
|    approx_kl            | 0.027656682 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.17e+03    |
|    n_updates            | 2543        |
|    policy_gradient_loss | 0.022       |
|    value_loss           | 2.19e+03    |
-----------------------------------------
Eval num_timesteps=699000, episode_reward=722.02 +/- 115.97
Episode length: 249.70 +/- 169.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 699000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 683      |
|    time_elapsed    | 16510    |
|    total_timesteps | 699392   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=699500, episode_reward=687.90 +/- 166.04
Episode length: 240.18 +/- 172.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 688        |
| time/                   |            |
|    total_timesteps      | 699500     |
| train/                  |            |
|    approx_kl            | 0.05241946 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.865     |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.2e+03    |
|    n_updates            | 2547       |
|    policy_gradient_loss | -0.00395   |
|    value_loss           | 2.87e+03   |
----------------------------------------
Eval num_timesteps=700000, episode_reward=685.10 +/- 142.38
Episode length: 338.52 +/- 186.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 339      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 684      |
|    time_elapsed    | 16531    |
|    total_timesteps | 700416   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=700500, episode_reward=661.61 +/- 114.64
Episode length: 380.24 +/- 193.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 380        |
|    mean_reward          | 662        |
| time/                   |            |
|    total_timesteps      | 700500     |
| train/                  |            |
|    approx_kl            | 0.03991024 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.37e+03   |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.00532   |
|    value_loss           | 2.15e+03   |
----------------------------------------
Eval num_timesteps=701000, episode_reward=657.51 +/- 143.85
Episode length: 356.38 +/- 198.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 701000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 685      |
|    time_elapsed    | 16556    |
|    total_timesteps | 701440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=701500, episode_reward=577.91 +/- 170.47
Episode length: 427.98 +/- 173.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | 578         |
| time/                   |             |
|    total_timesteps      | 701500      |
| train/                  |             |
|    approx_kl            | 0.052724726 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0005      |
|    loss                 | 884         |
|    n_updates            | 2552        |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=702000, episode_reward=581.66 +/- 161.09
Episode length: 460.52 +/- 143.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 461      |
|    mean_reward     | 582      |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 686      |
|    time_elapsed    | 16586    |
|    total_timesteps | 702464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=702500, episode_reward=736.13 +/- 140.27
Episode length: 220.00 +/- 171.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 220        |
|    mean_reward          | 736        |
| time/                   |            |
|    total_timesteps      | 702500     |
| train/                  |            |
|    approx_kl            | 0.08647367 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.704     |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0005     |
|    loss                 | 694        |
|    n_updates            | 2554       |
|    policy_gradient_loss | 0.000727   |
|    value_loss           | 1.75e+03   |
----------------------------------------
Eval num_timesteps=703000, episode_reward=731.18 +/- 129.57
Episode length: 192.32 +/- 156.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 703000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 687      |
|    time_elapsed    | 16601    |
|    total_timesteps | 703488   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=703500, episode_reward=747.54 +/- 137.51
Episode length: 185.08 +/- 133.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 748        |
| time/                   |            |
|    total_timesteps      | 703500     |
| train/                  |            |
|    approx_kl            | 0.04797275 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.665     |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.36e+03   |
|    n_updates            | 2558       |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 2.67e+03   |
----------------------------------------
Eval num_timesteps=704000, episode_reward=745.00 +/- 119.52
Episode length: 220.46 +/- 163.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=704500, episode_reward=753.00 +/- 74.25
Episode length: 183.24 +/- 135.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 704500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 688      |
|    time_elapsed    | 16621    |
|    total_timesteps | 704512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=705000, episode_reward=744.26 +/- 123.95
Episode length: 172.14 +/- 133.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 744         |
| time/                   |             |
|    total_timesteps      | 705000      |
| train/                  |             |
|    approx_kl            | 0.028590176 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.27e+03    |
|    n_updates            | 2560        |
|    policy_gradient_loss | 0.00473     |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=705500, episode_reward=749.12 +/- 79.65
Episode length: 230.80 +/- 176.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 705500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 689      |
|    time_elapsed    | 16636    |
|    total_timesteps | 705536   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=706000, episode_reward=786.79 +/- 60.44
Episode length: 147.62 +/- 97.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 148         |
|    mean_reward          | 787         |
| time/                   |             |
|    total_timesteps      | 706000      |
| train/                  |             |
|    approx_kl            | 0.103531264 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0005      |
|    loss                 | 972         |
|    n_updates            | 2562        |
|    policy_gradient_loss | 0.00858     |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=706500, episode_reward=779.66 +/- 61.92
Episode length: 134.84 +/- 62.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 706500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 690      |
|    time_elapsed    | 16646    |
|    total_timesteps | 706560   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=707000, episode_reward=789.99 +/- 55.11
Episode length: 145.62 +/- 98.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 790         |
| time/                   |             |
|    total_timesteps      | 707000      |
| train/                  |             |
|    approx_kl            | 0.055087954 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.05e+03    |
|    n_updates            | 2564        |
|    policy_gradient_loss | 0.00506     |
|    value_loss           | 2.24e+03    |
-----------------------------------------
Eval num_timesteps=707500, episode_reward=803.38 +/- 32.71
Episode length: 129.78 +/- 32.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 707500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 751      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 691      |
|    time_elapsed    | 16656    |
|    total_timesteps | 707584   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=708000, episode_reward=787.24 +/- 87.03
Episode length: 134.18 +/- 82.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 134        |
|    mean_reward          | 787        |
| time/                   |            |
|    total_timesteps      | 708000     |
| train/                  |            |
|    approx_kl            | 0.06284043 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.553     |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.16e+03   |
|    n_updates            | 2567       |
|    policy_gradient_loss | -0.00253   |
|    value_loss           | 4.63e+03   |
----------------------------------------
Eval num_timesteps=708500, episode_reward=798.10 +/- 40.09
Episode length: 145.54 +/- 98.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 708500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 692      |
|    time_elapsed    | 16666    |
|    total_timesteps | 708608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=709000, episode_reward=770.20 +/- 98.49
Episode length: 140.50 +/- 64.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 140        |
|    mean_reward          | 770        |
| time/                   |            |
|    total_timesteps      | 709000     |
| train/                  |            |
|    approx_kl            | 0.02305826 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.483     |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.35e+03   |
|    n_updates            | 2568       |
|    policy_gradient_loss | 0.0141     |
|    value_loss           | 2.36e+03   |
----------------------------------------
Eval num_timesteps=709500, episode_reward=761.81 +/- 93.63
Episode length: 122.48 +/- 42.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 709500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 693      |
|    time_elapsed    | 16676    |
|    total_timesteps | 709632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=710000, episode_reward=740.06 +/- 99.66
Episode length: 96.40 +/- 8.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.4        |
|    mean_reward          | 740         |
| time/                   |             |
|    total_timesteps      | 710000      |
| train/                  |             |
|    approx_kl            | 0.035676923 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 2570        |
|    policy_gradient_loss | 0.00944     |
|    value_loss           | 2.8e+03     |
-----------------------------------------
Eval num_timesteps=710500, episode_reward=729.75 +/- 132.42
Episode length: 94.78 +/- 8.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 710500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 694      |
|    time_elapsed    | 16683    |
|    total_timesteps | 710656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=711000, episode_reward=749.98 +/- 104.97
Episode length: 95.26 +/- 8.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.3        |
|    mean_reward          | 750         |
| time/                   |             |
|    total_timesteps      | 711000      |
| train/                  |             |
|    approx_kl            | 0.036811978 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.81e+03    |
|    n_updates            | 2572        |
|    policy_gradient_loss | 0.00137     |
|    value_loss           | 3.43e+03    |
-----------------------------------------
Eval num_timesteps=711500, episode_reward=727.49 +/- 138.95
Episode length: 93.24 +/- 9.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 711500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 695      |
|    time_elapsed    | 16691    |
|    total_timesteps | 711680   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=712000, episode_reward=698.88 +/- 196.25
Episode length: 108.40 +/- 62.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 699        |
| time/                   |            |
|    total_timesteps      | 712000     |
| train/                  |            |
|    approx_kl            | 0.03237674 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.09e+03   |
|    n_updates            | 2574       |
|    policy_gradient_loss | 0.0112     |
|    value_loss           | 2.43e+03   |
----------------------------------------
Eval num_timesteps=712500, episode_reward=748.31 +/- 150.47
Episode length: 119.30 +/- 84.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 712500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 696      |
|    time_elapsed    | 16699    |
|    total_timesteps | 712704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=713000, episode_reward=729.77 +/- 128.58
Episode length: 97.58 +/- 15.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 730         |
| time/                   |             |
|    total_timesteps      | 713000      |
| train/                  |             |
|    approx_kl            | 0.024317285 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.63e+03    |
|    n_updates            | 2575        |
|    policy_gradient_loss | 0.00946     |
|    value_loss           | 2.88e+03    |
-----------------------------------------
Eval num_timesteps=713500, episode_reward=748.48 +/- 137.56
Episode length: 97.50 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 713500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 697      |
|    time_elapsed    | 16707    |
|    total_timesteps | 713728   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=714000, episode_reward=720.35 +/- 172.98
Episode length: 94.76 +/- 9.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.8       |
|    mean_reward          | 720        |
| time/                   |            |
|    total_timesteps      | 714000     |
| train/                  |            |
|    approx_kl            | 0.04240805 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.32e+03   |
|    n_updates            | 2577       |
|    policy_gradient_loss | 0.0111     |
|    value_loss           | 5.8e+03    |
----------------------------------------
Eval num_timesteps=714500, episode_reward=715.90 +/- 182.81
Episode length: 95.24 +/- 11.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 714500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 698      |
|    time_elapsed    | 16714    |
|    total_timesteps | 714752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=715000, episode_reward=776.80 +/- 112.85
Episode length: 112.92 +/- 26.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 777         |
| time/                   |             |
|    total_timesteps      | 715000      |
| train/                  |             |
|    approx_kl            | 0.024557915 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.33       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.49e+03    |
|    n_updates            | 2578        |
|    policy_gradient_loss | 0.0175      |
|    value_loss           | 2.85e+03    |
-----------------------------------------
Eval num_timesteps=715500, episode_reward=783.04 +/- 80.12
Episode length: 120.36 +/- 26.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 715500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 699      |
|    time_elapsed    | 16723    |
|    total_timesteps | 715776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=716000, episode_reward=781.60 +/- 76.21
Episode length: 104.24 +/- 11.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 782         |
| time/                   |             |
|    total_timesteps      | 716000      |
| train/                  |             |
|    approx_kl            | 0.019440744 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.1e+03     |
|    n_updates            | 2579        |
|    policy_gradient_loss | 0.011       |
|    value_loss           | 2.17e+03    |
-----------------------------------------
Eval num_timesteps=716500, episode_reward=787.52 +/- 72.13
Episode length: 101.40 +/- 8.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 716500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 700      |
|    time_elapsed    | 16731    |
|    total_timesteps | 716800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=717000, episode_reward=757.31 +/- 143.42
Episode length: 112.70 +/- 59.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 113        |
|    mean_reward          | 757        |
| time/                   |            |
|    total_timesteps      | 717000     |
| train/                  |            |
|    approx_kl            | 0.04316665 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.429     |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.16e+03   |
|    n_updates            | 2581       |
|    policy_gradient_loss | 0.00688    |
|    value_loss           | 2.39e+03   |
----------------------------------------
Eval num_timesteps=717500, episode_reward=784.09 +/- 73.65
Episode length: 104.70 +/- 10.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 717500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 701      |
|    time_elapsed    | 16739    |
|    total_timesteps | 717824   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=718000, episode_reward=782.88 +/- 67.82
Episode length: 99.78 +/- 7.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 718000      |
| train/                  |             |
|    approx_kl            | 0.052196987 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.86e+03    |
|    n_updates            | 2584        |
|    policy_gradient_loss | 0.00184     |
|    value_loss           | 3.39e+03    |
-----------------------------------------
Eval num_timesteps=718500, episode_reward=789.89 +/- 64.49
Episode length: 109.16 +/- 59.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 718500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 702      |
|    time_elapsed    | 16747    |
|    total_timesteps | 718848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=719000, episode_reward=761.11 +/- 86.71
Episode length: 141.90 +/- 113.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 761         |
| time/                   |             |
|    total_timesteps      | 719000      |
| train/                  |             |
|    approx_kl            | 0.022186588 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.04e+03    |
|    n_updates            | 2585        |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 3.12e+03    |
-----------------------------------------
Eval num_timesteps=719500, episode_reward=757.99 +/- 114.11
Episode length: 109.02 +/- 13.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 719500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 703      |
|    time_elapsed    | 16756    |
|    total_timesteps | 719872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=720000, episode_reward=684.37 +/- 148.93
Episode length: 238.60 +/- 179.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 239         |
|    mean_reward          | 684         |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.030831207 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.437      |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.06e+03    |
|    n_updates            | 2586        |
|    policy_gradient_loss | 0.0209      |
|    value_loss           | 2.57e+03    |
-----------------------------------------
Eval num_timesteps=720500, episode_reward=714.31 +/- 132.64
Episode length: 234.02 +/- 163.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 720500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 751      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 704      |
|    time_elapsed    | 16773    |
|    total_timesteps | 720896   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=721000, episode_reward=694.86 +/- 135.40
Episode length: 327.96 +/- 179.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 328        |
|    mean_reward          | 695        |
| time/                   |            |
|    total_timesteps      | 721000     |
| train/                  |            |
|    approx_kl            | 0.07246013 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.14e+03   |
|    n_updates            | 2589       |
|    policy_gradient_loss | 0.000152   |
|    value_loss           | 2.75e+03   |
----------------------------------------
Eval num_timesteps=721500, episode_reward=693.42 +/- 132.61
Episode length: 309.72 +/- 178.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 310      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 721500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 705      |
|    time_elapsed    | 16795    |
|    total_timesteps | 721920   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=722000, episode_reward=649.04 +/- 129.47
Episode length: 354.82 +/- 171.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 649         |
| time/                   |             |
|    total_timesteps      | 722000      |
| train/                  |             |
|    approx_kl            | 0.038651362 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.41e+03    |
|    n_updates            | 2591        |
|    policy_gradient_loss | 0.00561     |
|    value_loss           | 4.85e+03    |
-----------------------------------------
Eval num_timesteps=722500, episode_reward=658.62 +/- 141.75
Episode length: 307.02 +/- 187.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 722500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 706      |
|    time_elapsed    | 16818    |
|    total_timesteps | 722944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=723000, episode_reward=684.52 +/- 123.79
Episode length: 318.98 +/- 176.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 319         |
|    mean_reward          | 685         |
| time/                   |             |
|    total_timesteps      | 723000      |
| train/                  |             |
|    approx_kl            | 0.067788325 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.16e+03    |
|    n_updates            | 2593        |
|    policy_gradient_loss | 0.00953     |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=723500, episode_reward=665.85 +/- 117.91
Episode length: 356.46 +/- 184.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 356      |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 723500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 707      |
|    time_elapsed    | 16841    |
|    total_timesteps | 723968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=724000, episode_reward=679.41 +/- 121.76
Episode length: 322.92 +/- 186.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 679         |
| time/                   |             |
|    total_timesteps      | 724000      |
| train/                  |             |
|    approx_kl            | 0.045437682 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0005      |
|    loss                 | 900         |
|    n_updates            | 2595        |
|    policy_gradient_loss | 0.0156      |
|    value_loss           | 1.93e+03    |
-----------------------------------------
Eval num_timesteps=724500, episode_reward=655.99 +/- 136.19
Episode length: 354.04 +/- 187.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 354      |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 724500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 708      |
|    time_elapsed    | 16864    |
|    total_timesteps | 724992   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.15
Eval num_timesteps=725000, episode_reward=603.58 +/- 197.88
Episode length: 320.72 +/- 181.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 321         |
|    mean_reward          | 604         |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.055527877 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0005      |
|    loss                 | 866         |
|    n_updates            | 2597        |
|    policy_gradient_loss | 0.0294      |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=725500, episode_reward=651.50 +/- 187.32
Episode length: 306.30 +/- 177.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 725500   |
---------------------------------
Eval num_timesteps=726000, episode_reward=642.21 +/- 195.83
Episode length: 313.68 +/- 169.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 314      |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 709      |
|    time_elapsed    | 16896    |
|    total_timesteps | 726016   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=726500, episode_reward=667.04 +/- 160.55
Episode length: 270.70 +/- 184.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 667         |
| time/                   |             |
|    total_timesteps      | 726500      |
| train/                  |             |
|    approx_kl            | 0.038407177 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.5        |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.05e+03    |
|    n_updates            | 2599        |
|    policy_gradient_loss | 0.00228     |
|    value_loss           | 2.13e+03    |
-----------------------------------------
Eval num_timesteps=727000, episode_reward=705.60 +/- 166.42
Episode length: 238.22 +/- 159.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 710      |
|    time_elapsed    | 16914    |
|    total_timesteps | 727040   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=727500, episode_reward=662.71 +/- 117.63
Episode length: 378.56 +/- 176.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 379         |
|    mean_reward          | 663         |
| time/                   |             |
|    total_timesteps      | 727500      |
| train/                  |             |
|    approx_kl            | 0.042324126 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0005      |
|    loss                 | 709         |
|    n_updates            | 2601        |
|    policy_gradient_loss | 0.00242     |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=728000, episode_reward=687.31 +/- 157.71
Episode length: 337.16 +/- 167.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 337      |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 711      |
|    time_elapsed    | 16939    |
|    total_timesteps | 728064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=728500, episode_reward=480.99 +/- 155.91
Episode length: 463.98 +/- 121.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 481         |
| time/                   |             |
|    total_timesteps      | 728500      |
| train/                  |             |
|    approx_kl            | 0.025766697 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0005      |
|    loss                 | 828         |
|    n_updates            | 2602        |
|    policy_gradient_loss | 0.0216      |
|    value_loss           | 1.92e+03    |
-----------------------------------------
Eval num_timesteps=729000, episode_reward=515.38 +/- 150.81
Episode length: 481.38 +/- 112.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | 515      |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 712      |
|    time_elapsed    | 16971    |
|    total_timesteps | 729088   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=729500, episode_reward=699.28 +/- 100.75
Episode length: 347.24 +/- 183.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | 699         |
| time/                   |             |
|    total_timesteps      | 729500      |
| train/                  |             |
|    approx_kl            | 0.046517342 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.476      |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0005      |
|    loss                 | 678         |
|    n_updates            | 2604        |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=730000, episode_reward=717.16 +/- 98.96
Episode length: 323.26 +/- 178.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 323      |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 730000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 713      |
|    time_elapsed    | 16994    |
|    total_timesteps | 730112   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=730500, episode_reward=738.63 +/- 115.75
Episode length: 224.36 +/- 154.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 739        |
| time/                   |            |
|    total_timesteps      | 730500     |
| train/                  |            |
|    approx_kl            | 0.04301798 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.571     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.04e+03   |
|    n_updates            | 2606       |
|    policy_gradient_loss | 0.00231    |
|    value_loss           | 2.4e+03    |
----------------------------------------
Eval num_timesteps=731000, episode_reward=718.06 +/- 134.96
Episode length: 254.68 +/- 170.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 42       |
|    iterations      | 714      |
|    time_elapsed    | 17011    |
|    total_timesteps | 731136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=731500, episode_reward=746.37 +/- 126.33
Episode length: 176.66 +/- 131.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 746         |
| time/                   |             |
|    total_timesteps      | 731500      |
| train/                  |             |
|    approx_kl            | 0.043792214 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.06e+03    |
|    n_updates            | 2608        |
|    policy_gradient_loss | 0.00284     |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=732000, episode_reward=763.10 +/- 89.71
Episode length: 173.04 +/- 131.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 715      |
|    time_elapsed    | 17024    |
|    total_timesteps | 732160   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=732500, episode_reward=749.77 +/- 106.42
Episode length: 176.18 +/- 130.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 750         |
| time/                   |             |
|    total_timesteps      | 732500      |
| train/                  |             |
|    approx_kl            | 0.052468866 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.12e+03    |
|    n_updates            | 2610        |
|    policy_gradient_loss | 0.000784    |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=733000, episode_reward=756.99 +/- 104.32
Episode length: 173.04 +/- 121.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 716      |
|    time_elapsed    | 17037    |
|    total_timesteps | 733184   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=733500, episode_reward=720.42 +/- 127.97
Episode length: 222.92 +/- 167.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 720         |
| time/                   |             |
|    total_timesteps      | 733500      |
| train/                  |             |
|    approx_kl            | 0.036256988 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.26e+03    |
|    n_updates            | 2613        |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 1.98e+03    |
-----------------------------------------
Eval num_timesteps=734000, episode_reward=729.50 +/- 125.77
Episode length: 218.98 +/- 164.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 734000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 717      |
|    time_elapsed    | 17052    |
|    total_timesteps | 734208   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=734500, episode_reward=693.21 +/- 129.92
Episode length: 265.98 +/- 180.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 266         |
|    mean_reward          | 693         |
| time/                   |             |
|    total_timesteps      | 734500      |
| train/                  |             |
|    approx_kl            | 0.046005342 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.673      |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0005      |
|    loss                 | 924         |
|    n_updates            | 2615        |
|    policy_gradient_loss | 0.00287     |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=735000, episode_reward=707.54 +/- 134.65
Episode length: 230.68 +/- 169.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 718      |
|    time_elapsed    | 17070    |
|    total_timesteps | 735232   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=735500, episode_reward=712.81 +/- 125.30
Episode length: 256.24 +/- 179.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 256        |
|    mean_reward          | 713        |
| time/                   |            |
|    total_timesteps      | 735500     |
| train/                  |            |
|    approx_kl            | 0.04989867 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.839      |
|    learning_rate        | 0.0005     |
|    loss                 | 1e+03      |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0167    |
|    value_loss           | 1.74e+03   |
----------------------------------------
Eval num_timesteps=736000, episode_reward=686.35 +/- 124.17
Episode length: 317.96 +/- 199.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 318      |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 736000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 719      |
|    time_elapsed    | 17090    |
|    total_timesteps | 736256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=736500, episode_reward=752.52 +/- 104.28
Episode length: 229.20 +/- 176.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 229         |
|    mean_reward          | 753         |
| time/                   |             |
|    total_timesteps      | 736500      |
| train/                  |             |
|    approx_kl            | 0.035691522 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 2622        |
|    policy_gradient_loss | 0.000813    |
|    value_loss           | 2.08e+03    |
-----------------------------------------
Eval num_timesteps=737000, episode_reward=721.43 +/- 120.79
Episode length: 243.80 +/- 184.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 720      |
|    time_elapsed    | 17106    |
|    total_timesteps | 737280   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=737500, episode_reward=774.41 +/- 63.16
Episode length: 212.28 +/- 176.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 212         |
|    mean_reward          | 774         |
| time/                   |             |
|    total_timesteps      | 737500      |
| train/                  |             |
|    approx_kl            | 0.047899753 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0005      |
|    loss                 | 852         |
|    n_updates            | 2625        |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 1.9e+03     |
-----------------------------------------
Eval num_timesteps=738000, episode_reward=726.54 +/- 103.04
Episode length: 218.50 +/- 182.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 721      |
|    time_elapsed    | 17122    |
|    total_timesteps | 738304   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=738500, episode_reward=709.43 +/- 123.53
Episode length: 285.52 +/- 203.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 286        |
|    mean_reward          | 709        |
| time/                   |            |
|    total_timesteps      | 738500     |
| train/                  |            |
|    approx_kl            | 0.05272731 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.519     |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0005     |
|    loss                 | 909        |
|    n_updates            | 2627       |
|    policy_gradient_loss | -0.00276   |
|    value_loss           | 2.26e+03   |
----------------------------------------
Eval num_timesteps=739000, episode_reward=749.35 +/- 88.34
Episode length: 233.98 +/- 183.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 722      |
|    time_elapsed    | 17140    |
|    total_timesteps | 739328   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=739500, episode_reward=724.06 +/- 72.21
Episode length: 339.22 +/- 201.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 339        |
|    mean_reward          | 724        |
| time/                   |            |
|    total_timesteps      | 739500     |
| train/                  |            |
|    approx_kl            | 0.04461538 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.507     |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.61e+03   |
|    n_updates            | 2629       |
|    policy_gradient_loss | 0.00782    |
|    value_loss           | 2.47e+03   |
----------------------------------------
Eval num_timesteps=740000, episode_reward=739.57 +/- 72.36
Episode length: 319.94 +/- 198.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 320      |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 723      |
|    time_elapsed    | 17162    |
|    total_timesteps | 740352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=740500, episode_reward=737.40 +/- 68.73
Episode length: 317.74 +/- 207.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 318        |
|    mean_reward          | 737        |
| time/                   |            |
|    total_timesteps      | 740500     |
| train/                  |            |
|    approx_kl            | 0.03303705 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.888      |
|    learning_rate        | 0.0005     |
|    loss                 | 616        |
|    n_updates            | 2630       |
|    policy_gradient_loss | 0.0256     |
|    value_loss           | 1.58e+03   |
----------------------------------------
Eval num_timesteps=741000, episode_reward=749.65 +/- 73.03
Episode length: 273.10 +/- 198.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 724      |
|    time_elapsed    | 17183    |
|    total_timesteps | 741376   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=741500, episode_reward=721.77 +/- 97.73
Episode length: 289.48 +/- 201.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 289        |
|    mean_reward          | 722        |
| time/                   |            |
|    total_timesteps      | 741500     |
| train/                  |            |
|    approx_kl            | 0.04667082 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.537     |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.35e+03   |
|    n_updates            | 2632       |
|    policy_gradient_loss | 0.0103     |
|    value_loss           | 2.48e+03   |
----------------------------------------
Eval num_timesteps=742000, episode_reward=684.67 +/- 129.75
Episode length: 353.82 +/- 202.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 354      |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 742000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 725      |
|    time_elapsed    | 17205    |
|    total_timesteps | 742400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=742500, episode_reward=659.23 +/- 99.27
Episode length: 393.24 +/- 192.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 393        |
|    mean_reward          | 659        |
| time/                   |            |
|    total_timesteps      | 742500     |
| train/                  |            |
|    approx_kl            | 0.03558102 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.624     |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.0005     |
|    loss                 | 939        |
|    n_updates            | 2633       |
|    policy_gradient_loss | 0.0159     |
|    value_loss           | 1.98e+03   |
----------------------------------------
Eval num_timesteps=743000, episode_reward=667.54 +/- 129.71
Episode length: 360.00 +/- 202.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 360      |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 726      |
|    time_elapsed    | 17231    |
|    total_timesteps | 743424   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=743500, episode_reward=673.34 +/- 92.31
Episode length: 409.56 +/- 185.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 410        |
|    mean_reward          | 673        |
| time/                   |            |
|    total_timesteps      | 743500     |
| train/                  |            |
|    approx_kl            | 0.05357678 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.895     |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0005     |
|    loss                 | 986        |
|    n_updates            | 2636       |
|    policy_gradient_loss | 0.0207     |
|    value_loss           | 2.25e+03   |
----------------------------------------
Eval num_timesteps=744000, episode_reward=668.96 +/- 84.90
Episode length: 406.20 +/- 182.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 406      |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 727      |
|    time_elapsed    | 17258    |
|    total_timesteps | 744448   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=744500, episode_reward=715.24 +/- 107.33
Episode length: 291.12 +/- 192.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 291         |
|    mean_reward          | 715         |
| time/                   |             |
|    total_timesteps      | 744500      |
| train/                  |             |
|    approx_kl            | 0.061537273 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0005      |
|    loss                 | 963         |
|    n_updates            | 2639        |
|    policy_gradient_loss | -0.00103    |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=745000, episode_reward=736.05 +/- 72.72
Episode length: 307.80 +/- 201.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 728      |
|    time_elapsed    | 17279    |
|    total_timesteps | 745472   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=745500, episode_reward=682.36 +/- 144.47
Episode length: 295.94 +/- 199.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 296         |
|    mean_reward          | 682         |
| time/                   |             |
|    total_timesteps      | 745500      |
| train/                  |             |
|    approx_kl            | 0.047818277 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0005      |
|    loss                 | 838         |
|    n_updates            | 2642        |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=746000, episode_reward=692.92 +/- 115.76
Episode length: 306.12 +/- 202.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 746000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 729      |
|    time_elapsed    | 17300    |
|    total_timesteps | 746496   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=746500, episode_reward=697.29 +/- 90.44
Episode length: 324.50 +/- 201.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 324        |
|    mean_reward          | 697        |
| time/                   |            |
|    total_timesteps      | 746500     |
| train/                  |            |
|    approx_kl            | 0.07263043 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.803     |
|    explained_variance   | 0.81       |
|    learning_rate        | 0.0005     |
|    loss                 | 863        |
|    n_updates            | 2645       |
|    policy_gradient_loss | 0.005      |
|    value_loss           | 1.89e+03   |
----------------------------------------
Eval num_timesteps=747000, episode_reward=723.79 +/- 109.82
Episode length: 309.04 +/- 199.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=747500, episode_reward=691.40 +/- 116.01
Episode length: 316.42 +/- 197.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 747500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 730      |
|    time_elapsed    | 17332    |
|    total_timesteps | 747520   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=748000, episode_reward=714.09 +/- 82.53
Episode length: 333.86 +/- 197.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 334        |
|    mean_reward          | 714        |
| time/                   |            |
|    total_timesteps      | 748000     |
| train/                  |            |
|    approx_kl            | 0.05645198 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.888     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.54e+03   |
|    n_updates            | 2648       |
|    policy_gradient_loss | 8.03e-05   |
|    value_loss           | 2.36e+03   |
----------------------------------------
Eval num_timesteps=748500, episode_reward=705.59 +/- 123.72
Episode length: 298.54 +/- 197.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 748500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 731      |
|    time_elapsed    | 17354    |
|    total_timesteps | 748544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=749000, episode_reward=569.87 +/- 226.17
Episode length: 381.22 +/- 185.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 570         |
| time/                   |             |
|    total_timesteps      | 749000      |
| train/                  |             |
|    approx_kl            | 0.022144467 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.07e+03    |
|    n_updates            | 2649        |
|    policy_gradient_loss | 0.0159      |
|    value_loss           | 2.02e+03    |
-----------------------------------------
Eval num_timesteps=749500, episode_reward=635.17 +/- 190.71
Episode length: 344.16 +/- 191.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 344      |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 749500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 732      |
|    time_elapsed    | 17378    |
|    total_timesteps | 749568   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=750000, episode_reward=722.42 +/- 145.01
Episode length: 241.44 +/- 189.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 722         |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.056384053 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0005      |
|    loss                 | 574         |
|    n_updates            | 2652        |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 1.32e+03    |
-----------------------------------------
Eval num_timesteps=750500, episode_reward=760.24 +/- 76.74
Episode length: 211.24 +/- 176.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 750500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 733      |
|    time_elapsed    | 17394    |
|    total_timesteps | 750592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=751000, episode_reward=776.08 +/- 76.68
Episode length: 121.24 +/- 82.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 121         |
|    mean_reward          | 776         |
| time/                   |             |
|    total_timesteps      | 751000      |
| train/                  |             |
|    approx_kl            | 0.023941971 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.858      |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0005      |
|    loss                 | 891         |
|    n_updates            | 2653        |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=751500, episode_reward=798.37 +/- 54.83
Episode length: 111.20 +/- 59.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 751500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 734      |
|    time_elapsed    | 17403    |
|    total_timesteps | 751616   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=752000, episode_reward=774.03 +/- 82.52
Episode length: 136.28 +/- 114.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 774         |
| time/                   |             |
|    total_timesteps      | 752000      |
| train/                  |             |
|    approx_kl            | 0.044547495 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.03e+03    |
|    n_updates            | 2655        |
|    policy_gradient_loss | 0.00298     |
|    value_loss           | 3.95e+03    |
-----------------------------------------
Eval num_timesteps=752500, episode_reward=782.59 +/- 93.01
Episode length: 102.10 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 752500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 735      |
|    time_elapsed    | 17412    |
|    total_timesteps | 752640   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=753000, episode_reward=749.03 +/- 129.39
Episode length: 133.08 +/- 99.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 133         |
|    mean_reward          | 749         |
| time/                   |             |
|    total_timesteps      | 753000      |
| train/                  |             |
|    approx_kl            | 0.055691373 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.38e+03    |
|    n_updates            | 2658        |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 2.64e+03    |
-----------------------------------------
Eval num_timesteps=753500, episode_reward=750.47 +/- 123.01
Episode length: 162.32 +/- 137.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 753500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 736      |
|    time_elapsed    | 17423    |
|    total_timesteps | 753664   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=754000, episode_reward=791.51 +/- 62.81
Episode length: 125.34 +/- 82.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 792         |
| time/                   |             |
|    total_timesteps      | 754000      |
| train/                  |             |
|    approx_kl            | 0.039594058 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.555      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.05e+03    |
|    n_updates            | 2661        |
|    policy_gradient_loss | 0.000899    |
|    value_loss           | 2.68e+03    |
-----------------------------------------
Eval num_timesteps=754500, episode_reward=789.18 +/- 68.71
Episode length: 115.38 +/- 59.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 754500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 737      |
|    time_elapsed    | 17432    |
|    total_timesteps | 754688   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=755000, episode_reward=756.59 +/- 123.45
Episode length: 160.66 +/- 134.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 757         |
| time/                   |             |
|    total_timesteps      | 755000      |
| train/                  |             |
|    approx_kl            | 0.048151296 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.12e+03    |
|    n_updates            | 2664        |
|    policy_gradient_loss | -0.000916   |
|    value_loss           | 2.42e+03    |
-----------------------------------------
Eval num_timesteps=755500, episode_reward=789.56 +/- 91.83
Episode length: 111.22 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 755500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 738      |
|    time_elapsed    | 17442    |
|    total_timesteps | 755712   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=756000, episode_reward=741.81 +/- 140.83
Episode length: 139.32 +/- 97.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 139         |
|    mean_reward          | 742         |
| time/                   |             |
|    total_timesteps      | 756000      |
| train/                  |             |
|    approx_kl            | 0.051139466 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.17e+03    |
|    n_updates            | 2667        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 2.34e+03    |
-----------------------------------------
Eval num_timesteps=756500, episode_reward=740.66 +/- 163.93
Episode length: 122.26 +/- 58.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 756500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 739      |
|    time_elapsed    | 17452    |
|    total_timesteps | 756736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=757000, episode_reward=761.81 +/- 109.89
Episode length: 118.12 +/- 58.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 762         |
| time/                   |             |
|    total_timesteps      | 757000      |
| train/                  |             |
|    approx_kl            | 0.023237016 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.99e+03    |
|    n_updates            | 2668        |
|    policy_gradient_loss | 0.00379     |
|    value_loss           | 3.32e+03    |
-----------------------------------------
Eval num_timesteps=757500, episode_reward=785.77 +/- 65.14
Episode length: 111.44 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 757500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 740      |
|    time_elapsed    | 17460    |
|    total_timesteps | 757760   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=758000, episode_reward=733.22 +/- 163.70
Episode length: 121.62 +/- 59.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 122        |
|    mean_reward          | 733        |
| time/                   |            |
|    total_timesteps      | 758000     |
| train/                  |            |
|    approx_kl            | 0.04458449 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.517     |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.19e+03   |
|    n_updates            | 2670       |
|    policy_gradient_loss | 0.0197     |
|    value_loss           | 2.56e+03   |
----------------------------------------
Eval num_timesteps=758500, episode_reward=730.16 +/- 151.49
Episode length: 136.38 +/- 98.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 758500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 741      |
|    time_elapsed    | 17470    |
|    total_timesteps | 758784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=759000, episode_reward=553.38 +/- 249.45
Episode length: 99.38 +/- 12.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.4        |
|    mean_reward          | 553         |
| time/                   |             |
|    total_timesteps      | 759000      |
| train/                  |             |
|    approx_kl            | 0.025247218 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.483      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.55e+03    |
|    n_updates            | 2671        |
|    policy_gradient_loss | 0.0212      |
|    value_loss           | 4.22e+03    |
-----------------------------------------
Eval num_timesteps=759500, episode_reward=647.52 +/- 219.39
Episode length: 102.54 +/- 13.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 759500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 742      |
|    time_elapsed    | 17478    |
|    total_timesteps | 759808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=760000, episode_reward=625.00 +/- 236.23
Episode length: 118.58 +/- 83.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 119         |
|    mean_reward          | 625         |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.020953413 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.95e+03    |
|    n_updates            | 2672        |
|    policy_gradient_loss | 0.017       |
|    value_loss           | 4.59e+03    |
-----------------------------------------
Eval num_timesteps=760500, episode_reward=649.17 +/- 232.54
Episode length: 120.70 +/- 83.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 760500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 743      |
|    time_elapsed    | 17486    |
|    total_timesteps | 760832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=761000, episode_reward=581.90 +/- 231.20
Episode length: 92.02 +/- 9.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92          |
|    mean_reward          | 582         |
| time/                   |             |
|    total_timesteps      | 761000      |
| train/                  |             |
|    approx_kl            | 0.058711387 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.94e+03    |
|    n_updates            | 2674        |
|    policy_gradient_loss | 0.00997     |
|    value_loss           | 3.95e+03    |
-----------------------------------------
Eval num_timesteps=761500, episode_reward=521.18 +/- 228.63
Episode length: 87.70 +/- 13.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.7     |
|    mean_reward     | 521      |
| time/              |          |
|    total_timesteps | 761500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 721      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 744      |
|    time_elapsed    | 17493    |
|    total_timesteps | 761856   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=762000, episode_reward=481.91 +/- 267.86
Episode length: 90.90 +/- 13.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 90.9       |
|    mean_reward          | 482        |
| time/                   |            |
|    total_timesteps      | 762000     |
| train/                  |            |
|    approx_kl            | 0.04003912 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.414     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.0005     |
|    loss                 | 3.93e+03   |
|    n_updates            | 2677       |
|    policy_gradient_loss | -0.0051    |
|    value_loss           | 5.86e+03   |
----------------------------------------
Eval num_timesteps=762500, episode_reward=513.13 +/- 252.32
Episode length: 90.94 +/- 12.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 762500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 745      |
|    time_elapsed    | 17501    |
|    total_timesteps | 762880   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=763000, episode_reward=635.64 +/- 252.60
Episode length: 92.46 +/- 9.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.5       |
|    mean_reward          | 636        |
| time/                   |            |
|    total_timesteps      | 763000     |
| train/                  |            |
|    approx_kl            | 0.06379809 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.528     |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.75e+03   |
|    n_updates            | 2681       |
|    policy_gradient_loss | -0.0101    |
|    value_loss           | 9.06e+03   |
----------------------------------------
Eval num_timesteps=763500, episode_reward=530.59 +/- 269.98
Episode length: 89.94 +/- 10.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 531      |
| time/              |          |
|    total_timesteps | 763500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 654      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 746      |
|    time_elapsed    | 17508    |
|    total_timesteps | 763904   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=764000, episode_reward=610.04 +/- 251.72
Episode length: 92.94 +/- 12.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.9       |
|    mean_reward          | 610        |
| time/                   |            |
|    total_timesteps      | 764000     |
| train/                  |            |
|    approx_kl            | 0.04469788 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.466     |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.13e+03   |
|    n_updates            | 2683       |
|    policy_gradient_loss | 0.00973    |
|    value_loss           | 5.91e+03   |
----------------------------------------
Eval num_timesteps=764500, episode_reward=649.40 +/- 229.87
Episode length: 94.38 +/- 10.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 764500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 629      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 747      |
|    time_elapsed    | 17515    |
|    total_timesteps | 764928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=765000, episode_reward=594.41 +/- 240.46
Episode length: 110.28 +/- 60.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 110        |
|    mean_reward          | 594        |
| time/                   |            |
|    total_timesteps      | 765000     |
| train/                  |            |
|    approx_kl            | 0.04497853 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.388     |
|    explained_variance   | 0.566      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.77e+03   |
|    n_updates            | 2685       |
|    policy_gradient_loss | 0.00775    |
|    value_loss           | 6.23e+03   |
----------------------------------------
Eval num_timesteps=765500, episode_reward=628.43 +/- 240.21
Episode length: 104.16 +/- 11.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 765500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 622      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 748      |
|    time_elapsed    | 17523    |
|    total_timesteps | 765952   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=766000, episode_reward=761.04 +/- 138.58
Episode length: 107.18 +/- 7.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 761         |
| time/                   |             |
|    total_timesteps      | 766000      |
| train/                  |             |
|    approx_kl            | 0.041618578 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.408      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.51e+03    |
|    n_updates            | 2687        |
|    policy_gradient_loss | 0.00637     |
|    value_loss           | 3e+03       |
-----------------------------------------
Eval num_timesteps=766500, episode_reward=771.81 +/- 129.02
Episode length: 106.28 +/- 9.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 766500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 614      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 749      |
|    time_elapsed    | 17531    |
|    total_timesteps | 766976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=767000, episode_reward=622.32 +/- 216.43
Episode length: 97.78 +/- 10.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.8        |
|    mean_reward          | 622         |
| time/                   |             |
|    total_timesteps      | 767000      |
| train/                  |             |
|    approx_kl            | 0.033954527 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.31e+03    |
|    n_updates            | 2688        |
|    policy_gradient_loss | 0.0233      |
|    value_loss           | 4.77e+03    |
-----------------------------------------
Eval num_timesteps=767500, episode_reward=600.37 +/- 216.67
Episode length: 95.90 +/- 10.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 767500   |
---------------------------------
Eval num_timesteps=768000, episode_reward=565.30 +/- 258.72
Episode length: 94.74 +/- 10.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 600      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 750      |
|    time_elapsed    | 17542    |
|    total_timesteps | 768000   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=768500, episode_reward=740.80 +/- 148.82
Episode length: 97.30 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 97.3      |
|    mean_reward          | 741       |
| time/                   |           |
|    total_timesteps      | 768500    |
| train/                  |           |
|    approx_kl            | 0.0497836 |
|    clip_fraction        | 0.159     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.403    |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.85e+03  |
|    n_updates            | 2691      |
|    policy_gradient_loss | -0.0001   |
|    value_loss           | 5.36e+03  |
---------------------------------------
Eval num_timesteps=769000, episode_reward=771.81 +/- 114.44
Episode length: 97.50 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 769000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 588      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 751      |
|    time_elapsed    | 17549    |
|    total_timesteps | 769024   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=769500, episode_reward=750.10 +/- 149.96
Episode length: 97.96 +/- 6.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98          |
|    mean_reward          | 750         |
| time/                   |             |
|    total_timesteps      | 769500      |
| train/                  |             |
|    approx_kl            | 0.055998486 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.77e+03    |
|    n_updates            | 2693        |
|    policy_gradient_loss | 0.00685     |
|    value_loss           | 4.36e+03    |
-----------------------------------------
Eval num_timesteps=770000, episode_reward=794.00 +/- 84.72
Episode length: 99.24 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 598      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 752      |
|    time_elapsed    | 17557    |
|    total_timesteps | 770048   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=770500, episode_reward=750.98 +/- 133.43
Episode length: 99.46 +/- 7.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.5       |
|    mean_reward          | 751        |
| time/                   |            |
|    total_timesteps      | 770500     |
| train/                  |            |
|    approx_kl            | 0.04715856 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.409     |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.5e+03    |
|    n_updates            | 2695       |
|    policy_gradient_loss | 0.00217    |
|    value_loss           | 3.15e+03   |
----------------------------------------
Eval num_timesteps=771000, episode_reward=771.44 +/- 103.09
Episode length: 98.90 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 771000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.4     |
|    ep_rew_mean     | 598      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 753      |
|    time_elapsed    | 17565    |
|    total_timesteps | 771072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=771500, episode_reward=772.07 +/- 104.26
Episode length: 98.38 +/- 6.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.4       |
|    mean_reward          | 772        |
| time/                   |            |
|    total_timesteps      | 771500     |
| train/                  |            |
|    approx_kl            | 0.05755608 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.39e+03   |
|    n_updates            | 2697       |
|    policy_gradient_loss | 0.00769    |
|    value_loss           | 3e+03      |
----------------------------------------
Eval num_timesteps=772000, episode_reward=776.55 +/- 119.64
Episode length: 96.46 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 620      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 754      |
|    time_elapsed    | 17572    |
|    total_timesteps | 772096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=772500, episode_reward=779.48 +/- 92.86
Episode length: 96.84 +/- 5.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.8       |
|    mean_reward          | 779        |
| time/                   |            |
|    total_timesteps      | 772500     |
| train/                  |            |
|    approx_kl            | 0.03691773 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.301     |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.84e+03   |
|    n_updates            | 2699       |
|    policy_gradient_loss | 0.00304    |
|    value_loss           | 3.16e+03   |
----------------------------------------
Eval num_timesteps=773000, episode_reward=789.78 +/- 76.11
Episode length: 96.54 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 773000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 661      |
| time/              |          |
|    fps             | 43       |
|    iterations      | 755      |
|    time_elapsed    | 17579    |
|    total_timesteps | 773120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=773500, episode_reward=698.53 +/- 190.64
Episode length: 88.68 +/- 9.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 88.7       |
|    mean_reward          | 699        |
| time/                   |            |
|    total_timesteps      | 773500     |
| train/                  |            |
|    approx_kl            | 0.02189895 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.29e+03   |
|    n_updates            | 2700       |
|    policy_gradient_loss | 0.0254     |
|    value_loss           | 3.53e+03   |
----------------------------------------
Eval num_timesteps=774000, episode_reward=669.93 +/- 208.41
Episode length: 88.04 +/- 9.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88       |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.7     |
|    ep_rew_mean     | 669      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 756      |
|    time_elapsed    | 17586    |
|    total_timesteps | 774144   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=774500, episode_reward=766.88 +/- 89.69
Episode length: 93.26 +/- 4.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.3        |
|    mean_reward          | 767         |
| time/                   |             |
|    total_timesteps      | 774500      |
| train/                  |             |
|    approx_kl            | 0.041168135 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.21e+03    |
|    n_updates            | 2702        |
|    policy_gradient_loss | 0.00571     |
|    value_loss           | 5.01e+03    |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=782.65 +/- 82.66
Episode length: 93.30 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 775000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.2     |
|    ep_rew_mean     | 693      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 757      |
|    time_elapsed    | 17593    |
|    total_timesteps | 775168   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.15
Eval num_timesteps=775500, episode_reward=794.19 +/- 61.55
Episode length: 93.88 +/- 5.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.9        |
|    mean_reward          | 794         |
| time/                   |             |
|    total_timesteps      | 775500      |
| train/                  |             |
|    approx_kl            | 0.052139997 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.46e+03    |
|    n_updates            | 2705        |
|    policy_gradient_loss | 0.0109      |
|    value_loss           | 3.1e+03     |
-----------------------------------------
Eval num_timesteps=776000, episode_reward=766.93 +/- 101.88
Episode length: 91.82 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.9     |
|    ep_rew_mean     | 701      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 758      |
|    time_elapsed    | 17601    |
|    total_timesteps | 776192   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=776500, episode_reward=746.60 +/- 141.69
Episode length: 94.24 +/- 7.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.2       |
|    mean_reward          | 747        |
| time/                   |            |
|    total_timesteps      | 776500     |
| train/                  |            |
|    approx_kl            | 0.06972882 |
|    clip_fraction        | 0.0894     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.272     |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.51e+03   |
|    n_updates            | 2707       |
|    policy_gradient_loss | 0.0117     |
|    value_loss           | 3.08e+03   |
----------------------------------------
Eval num_timesteps=777000, episode_reward=776.96 +/- 98.59
Episode length: 94.14 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 777000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.8     |
|    ep_rew_mean     | 705      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 759      |
|    time_elapsed    | 17608    |
|    total_timesteps | 777216   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=777500, episode_reward=694.04 +/- 212.04
Episode length: 95.48 +/- 9.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 95.5       |
|    mean_reward          | 694        |
| time/                   |            |
|    total_timesteps      | 777500     |
| train/                  |            |
|    approx_kl            | 0.04212853 |
|    clip_fraction        | 0.0891     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.245     |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.19e+03   |
|    n_updates            | 2711       |
|    policy_gradient_loss | -8.21e-07  |
|    value_loss           | 2.21e+03   |
----------------------------------------
Eval num_timesteps=778000, episode_reward=792.21 +/- 81.86
Episode length: 97.70 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 723      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 760      |
|    time_elapsed    | 17615    |
|    total_timesteps | 778240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=778500, episode_reward=790.55 +/- 75.62
Episode length: 102.74 +/- 7.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 103        |
|    mean_reward          | 791        |
| time/                   |            |
|    total_timesteps      | 778500     |
| train/                  |            |
|    approx_kl            | 0.07981089 |
|    clip_fraction        | 0.0694     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.274     |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.57e+03   |
|    n_updates            | 2713       |
|    policy_gradient_loss | 0.00203    |
|    value_loss           | 2.93e+03   |
----------------------------------------
Eval num_timesteps=779000, episode_reward=783.35 +/- 121.52
Episode length: 100.82 +/- 9.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 779000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.7     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 761      |
|    time_elapsed    | 17623    |
|    total_timesteps | 779264   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.12
Eval num_timesteps=779500, episode_reward=784.04 +/- 63.35
Episode length: 107.90 +/- 10.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 784         |
| time/                   |             |
|    total_timesteps      | 779500      |
| train/                  |             |
|    approx_kl            | 0.044519376 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.309      |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.66e+03    |
|    n_updates            | 2716        |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 3.88e+03    |
-----------------------------------------
Eval num_timesteps=780000, episode_reward=776.41 +/- 141.17
Episode length: 110.84 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.5     |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 762      |
|    time_elapsed    | 17631    |
|    total_timesteps | 780288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=780500, episode_reward=813.01 +/- 20.05
Episode length: 106.28 +/- 7.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 813       |
| time/                   |           |
|    total_timesteps      | 780500    |
| train/                  |           |
|    approx_kl            | 0.0261683 |
|    clip_fraction        | 0.122     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.351    |
|    explained_variance   | 0.833     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.14e+03  |
|    n_updates            | 2717      |
|    policy_gradient_loss | 0.00962   |
|    value_loss           | 3.43e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=781000, episode_reward=791.63 +/- 91.00
Episode length: 108.00 +/- 9.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 781000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.2     |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 763      |
|    time_elapsed    | 17639    |
|    total_timesteps | 781312   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=781500, episode_reward=776.92 +/- 104.45
Episode length: 103.58 +/- 9.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | 777        |
| time/                   |            |
|    total_timesteps      | 781500     |
| train/                  |            |
|    approx_kl            | 0.03686503 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.82e+03   |
|    n_updates            | 2719       |
|    policy_gradient_loss | -0.00188   |
|    value_loss           | 2.53e+03   |
----------------------------------------
Eval num_timesteps=782000, episode_reward=798.17 +/- 50.14
Episode length: 104.86 +/- 8.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 764      |
|    time_elapsed    | 17647    |
|    total_timesteps | 782336   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=782500, episode_reward=783.43 +/- 99.99
Episode length: 102.40 +/- 9.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 783        |
| time/                   |            |
|    total_timesteps      | 782500     |
| train/                  |            |
|    approx_kl            | 0.06391295 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.303     |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0005     |
|    loss                 | 939        |
|    n_updates            | 2722       |
|    policy_gradient_loss | 0.0124     |
|    value_loss           | 2.03e+03   |
----------------------------------------
Eval num_timesteps=783000, episode_reward=809.89 +/- 28.49
Episode length: 102.06 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 783000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 765      |
|    time_elapsed    | 17655    |
|    total_timesteps | 783360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=783500, episode_reward=778.40 +/- 119.66
Episode length: 100.46 +/- 8.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 778         |
| time/                   |             |
|    total_timesteps      | 783500      |
| train/                  |             |
|    approx_kl            | 0.022163415 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.81e+03    |
|    n_updates            | 2723        |
|    policy_gradient_loss | 0.00248     |
|    value_loss           | 4.28e+03    |
-----------------------------------------
Eval num_timesteps=784000, episode_reward=797.77 +/- 78.11
Episode length: 100.82 +/- 9.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 770      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 766      |
|    time_elapsed    | 17663    |
|    total_timesteps | 784384   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=784500, episode_reward=798.53 +/- 74.22
Episode length: 121.62 +/- 21.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 799         |
| time/                   |             |
|    total_timesteps      | 784500      |
| train/                  |             |
|    approx_kl            | 0.039547678 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.318      |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.95e+03    |
|    n_updates            | 2726        |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 4.07e+03    |
-----------------------------------------
Eval num_timesteps=785000, episode_reward=768.20 +/- 121.31
Episode length: 119.86 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 767      |
|    time_elapsed    | 17672    |
|    total_timesteps | 785408   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=785500, episode_reward=777.63 +/- 119.01
Episode length: 161.04 +/- 71.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 778         |
| time/                   |             |
|    total_timesteps      | 785500      |
| train/                  |             |
|    approx_kl            | 0.046494316 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.15e+03    |
|    n_updates            | 2728        |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 2.6e+03     |
-----------------------------------------
Eval num_timesteps=786000, episode_reward=808.19 +/- 26.19
Episode length: 165.60 +/- 74.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 768      |
|    time_elapsed    | 17684    |
|    total_timesteps | 786432   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=786500, episode_reward=769.68 +/- 146.91
Episode length: 128.20 +/- 30.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 128        |
|    mean_reward          | 770        |
| time/                   |            |
|    total_timesteps      | 786500     |
| train/                  |            |
|    approx_kl            | 0.06907554 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.547     |
|    explained_variance   | 0.862      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.08e+03   |
|    n_updates            | 2730       |
|    policy_gradient_loss | 0.00575    |
|    value_loss           | 2.36e+03   |
----------------------------------------
Eval num_timesteps=787000, episode_reward=783.64 +/- 86.93
Episode length: 123.98 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 787000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 769      |
|    time_elapsed    | 17693    |
|    total_timesteps | 787456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.13
Eval num_timesteps=787500, episode_reward=797.45 +/- 92.27
Episode length: 146.42 +/- 32.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 797         |
| time/                   |             |
|    total_timesteps      | 787500      |
| train/                  |             |
|    approx_kl            | 0.044560745 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.03e+03    |
|    n_updates            | 2732        |
|    policy_gradient_loss | 0.00715     |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Eval num_timesteps=788000, episode_reward=798.29 +/- 64.43
Episode length: 140.32 +/- 30.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 770      |
|    time_elapsed    | 17703    |
|    total_timesteps | 788480   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=788500, episode_reward=788.82 +/- 110.67
Episode length: 128.40 +/- 25.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 128        |
|    mean_reward          | 789        |
| time/                   |            |
|    total_timesteps      | 788500     |
| train/                  |            |
|    approx_kl            | 0.04070262 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.577     |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.0005     |
|    loss                 | 956        |
|    n_updates            | 2734       |
|    policy_gradient_loss | 0.0048     |
|    value_loss           | 1.66e+03   |
----------------------------------------
Eval num_timesteps=789000, episode_reward=800.35 +/- 71.64
Episode length: 139.20 +/- 34.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 789000   |
---------------------------------
Eval num_timesteps=789500, episode_reward=787.87 +/- 64.75
Episode length: 145.54 +/- 79.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 789500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 771      |
|    time_elapsed    | 17718    |
|    total_timesteps | 789504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=790000, episode_reward=791.73 +/- 58.22
Episode length: 108.38 +/- 9.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 792         |
| time/                   |             |
|    total_timesteps      | 790000      |
| train/                  |             |
|    approx_kl            | 0.081160285 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.05e+03    |
|    n_updates            | 2736        |
|    policy_gradient_loss | 0.0254      |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=790500, episode_reward=776.29 +/- 112.37
Episode length: 115.54 +/- 59.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 790500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 772      |
|    time_elapsed    | 17727    |
|    total_timesteps | 790528   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=791000, episode_reward=795.76 +/- 73.35
Episode length: 104.10 +/- 11.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | 796        |
| time/                   |            |
|    total_timesteps      | 791000     |
| train/                  |            |
|    approx_kl            | 0.04965186 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.39      |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.35e+03   |
|    n_updates            | 2738       |
|    policy_gradient_loss | 0.00788    |
|    value_loss           | 2.11e+03   |
----------------------------------------
Eval num_timesteps=791500, episode_reward=785.06 +/- 103.08
Episode length: 104.20 +/- 12.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 791500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 773      |
|    time_elapsed    | 17735    |
|    total_timesteps | 791552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=792000, episode_reward=781.07 +/- 111.79
Episode length: 113.12 +/- 15.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 781         |
| time/                   |             |
|    total_timesteps      | 792000      |
| train/                  |             |
|    approx_kl            | 0.025793964 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.04e+03    |
|    n_updates            | 2739        |
|    policy_gradient_loss | 0.00796     |
|    value_loss           | 2.12e+03    |
-----------------------------------------
Eval num_timesteps=792500, episode_reward=764.12 +/- 145.10
Episode length: 130.24 +/- 82.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 792500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 774      |
|    time_elapsed    | 17744    |
|    total_timesteps | 792576   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=793000, episode_reward=782.76 +/- 99.85
Episode length: 138.80 +/- 80.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 139         |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 793000      |
| train/                  |             |
|    approx_kl            | 0.036659542 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.503      |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0005      |
|    loss                 | 940         |
|    n_updates            | 2741        |
|    policy_gradient_loss | 0.00944     |
|    value_loss           | 2.12e+03    |
-----------------------------------------
Eval num_timesteps=793500, episode_reward=743.04 +/- 154.07
Episode length: 133.02 +/- 59.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 793500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 775      |
|    time_elapsed    | 17754    |
|    total_timesteps | 793600   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=794000, episode_reward=771.36 +/- 110.14
Episode length: 116.06 +/- 18.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 771         |
| time/                   |             |
|    total_timesteps      | 794000      |
| train/                  |             |
|    approx_kl            | 0.047443785 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0005      |
|    loss                 | 836         |
|    n_updates            | 2744        |
|    policy_gradient_loss | 0.00415     |
|    value_loss           | 1.46e+03    |
-----------------------------------------
Eval num_timesteps=794500, episode_reward=745.11 +/- 112.50
Episode length: 128.14 +/- 81.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 794500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 786      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 776      |
|    time_elapsed    | 17763    |
|    total_timesteps | 794624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=795000, episode_reward=760.53 +/- 117.59
Episode length: 151.82 +/- 78.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 761         |
| time/                   |             |
|    total_timesteps      | 795000      |
| train/                  |             |
|    approx_kl            | 0.038104776 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.06e+03    |
|    n_updates            | 2746        |
|    policy_gradient_loss | 0.00895     |
|    value_loss           | 2.02e+03    |
-----------------------------------------
Eval num_timesteps=795500, episode_reward=731.95 +/- 172.81
Episode length: 173.22 +/- 113.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 795500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 777      |
|    time_elapsed    | 17775    |
|    total_timesteps | 795648   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=796000, episode_reward=769.60 +/- 125.53
Episode length: 155.02 +/- 95.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 155        |
|    mean_reward          | 770        |
| time/                   |            |
|    total_timesteps      | 796000     |
| train/                  |            |
|    approx_kl            | 0.08383413 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.442     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.58e+03   |
|    n_updates            | 2750       |
|    policy_gradient_loss | 0.00686    |
|    value_loss           | 3.03e+03   |
----------------------------------------
Eval num_timesteps=796500, episode_reward=739.82 +/- 193.39
Episode length: 135.32 +/- 31.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 796500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 778      |
|    time_elapsed    | 17786    |
|    total_timesteps | 796672   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=797000, episode_reward=801.18 +/- 30.84
Episode length: 139.76 +/- 25.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 140        |
|    mean_reward          | 801        |
| time/                   |            |
|    total_timesteps      | 797000     |
| train/                  |            |
|    approx_kl            | 0.04373585 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.502     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.84e+03   |
|    n_updates            | 2754       |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 3.22e+03   |
----------------------------------------
Eval num_timesteps=797500, episode_reward=785.84 +/- 71.52
Episode length: 161.12 +/- 78.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 797500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 779      |
|    time_elapsed    | 17797    |
|    total_timesteps | 797696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=798000, episode_reward=794.51 +/- 77.40
Episode length: 114.36 +/- 14.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 114       |
|    mean_reward          | 795       |
| time/                   |           |
|    total_timesteps      | 798000    |
| train/                  |           |
|    approx_kl            | 0.0521597 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.525    |
|    explained_variance   | 0.785     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.25e+03  |
|    n_updates            | 2756      |
|    policy_gradient_loss | 0.00813   |
|    value_loss           | 2.47e+03  |
---------------------------------------
Eval num_timesteps=798500, episode_reward=800.48 +/- 47.90
Episode length: 132.22 +/- 81.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 798500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 780      |
|    time_elapsed    | 17806    |
|    total_timesteps | 798720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=799000, episode_reward=801.69 +/- 39.97
Episode length: 104.40 +/- 6.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | 802        |
| time/                   |            |
|    total_timesteps      | 799000     |
| train/                  |            |
|    approx_kl            | 0.01991861 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.553     |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0005     |
|    loss                 | 869        |
|    n_updates            | 2757       |
|    policy_gradient_loss | 0.00199    |
|    value_loss           | 1.76e+03   |
----------------------------------------
Eval num_timesteps=799500, episode_reward=779.97 +/- 84.38
Episode length: 138.80 +/- 114.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 799500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 781      |
|    time_elapsed    | 17815    |
|    total_timesteps | 799744   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=800000, episode_reward=771.93 +/- 105.20
Episode length: 135.08 +/- 81.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 135         |
|    mean_reward          | 772         |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.052618995 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.36       |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0005      |
|    loss                 | 887         |
|    n_updates            | 2760        |
|    policy_gradient_loss | 0.006       |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=800500, episode_reward=784.81 +/- 61.91
Episode length: 131.44 +/- 81.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 800500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 782      |
|    time_elapsed    | 17825    |
|    total_timesteps | 800768   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=801000, episode_reward=778.59 +/- 67.59
Episode length: 155.36 +/- 110.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | 779         |
| time/                   |             |
|    total_timesteps      | 801000      |
| train/                  |             |
|    approx_kl            | 0.041765258 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.349      |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.01e+03    |
|    n_updates            | 2762        |
|    policy_gradient_loss | 0.00551     |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=801500, episode_reward=770.50 +/- 79.81
Episode length: 135.78 +/- 81.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 801500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 783      |
|    time_elapsed    | 17835    |
|    total_timesteps | 801792   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=802000, episode_reward=742.45 +/- 147.57
Episode length: 113.60 +/- 22.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 742         |
| time/                   |             |
|    total_timesteps      | 802000      |
| train/                  |             |
|    approx_kl            | 0.050201666 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.12e+03    |
|    n_updates            | 2764        |
|    policy_gradient_loss | 0.00926     |
|    value_loss           | 2.42e+03    |
-----------------------------------------
Eval num_timesteps=802500, episode_reward=766.41 +/- 126.79
Episode length: 117.56 +/- 59.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 802500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 44       |
|    iterations      | 784      |
|    time_elapsed    | 17844    |
|    total_timesteps | 802816   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=803000, episode_reward=771.69 +/- 102.34
Episode length: 112.98 +/- 12.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 772         |
| time/                   |             |
|    total_timesteps      | 803000      |
| train/                  |             |
|    approx_kl            | 0.047806952 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.15e+03    |
|    n_updates            | 2766        |
|    policy_gradient_loss | 0.0149      |
|    value_loss           | 2.3e+03     |
-----------------------------------------
Eval num_timesteps=803500, episode_reward=782.89 +/- 98.31
Episode length: 110.34 +/- 9.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 803500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 785      |
|    time_elapsed    | 17853    |
|    total_timesteps | 803840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=804000, episode_reward=784.94 +/- 107.15
Episode length: 128.10 +/- 59.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 785         |
| time/                   |             |
|    total_timesteps      | 804000      |
| train/                  |             |
|    approx_kl            | 0.020702936 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.38e+03    |
|    n_updates            | 2767        |
|    policy_gradient_loss | 0.00533     |
|    value_loss           | 2.58e+03    |
-----------------------------------------
Eval num_timesteps=804500, episode_reward=775.26 +/- 74.11
Episode length: 145.80 +/- 97.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 804500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 786      |
|    time_elapsed    | 17863    |
|    total_timesteps | 804864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=805000, episode_reward=763.27 +/- 64.69
Episode length: 187.38 +/- 137.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 187         |
|    mean_reward          | 763         |
| time/                   |             |
|    total_timesteps      | 805000      |
| train/                  |             |
|    approx_kl            | 0.025097167 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.95e+03    |
|    n_updates            | 2768        |
|    policy_gradient_loss | 0.00234     |
|    value_loss           | 3.36e+03    |
-----------------------------------------
Eval num_timesteps=805500, episode_reward=746.98 +/- 117.84
Episode length: 170.16 +/- 120.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 805500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 787      |
|    time_elapsed    | 17875    |
|    total_timesteps | 805888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=806000, episode_reward=714.59 +/- 104.29
Episode length: 284.92 +/- 188.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 715         |
| time/                   |             |
|    total_timesteps      | 806000      |
| train/                  |             |
|    approx_kl            | 0.025375377 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.37e+03    |
|    n_updates            | 2769        |
|    policy_gradient_loss | 0.0173      |
|    value_loss           | 2.91e+03    |
-----------------------------------------
Eval num_timesteps=806500, episode_reward=725.98 +/- 85.95
Episode length: 261.82 +/- 180.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 806500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 788      |
|    time_elapsed    | 17895    |
|    total_timesteps | 806912   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=807000, episode_reward=724.15 +/- 76.31
Episode length: 313.02 +/- 195.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 313        |
|    mean_reward          | 724        |
| time/                   |            |
|    total_timesteps      | 807000     |
| train/                  |            |
|    approx_kl            | 0.06986962 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.226     |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.03e+03   |
|    n_updates            | 2772       |
|    policy_gradient_loss | 0.00254    |
|    value_loss           | 2.51e+03   |
----------------------------------------
Eval num_timesteps=807500, episode_reward=715.29 +/- 85.54
Episode length: 304.16 +/- 196.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 304      |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 807500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 789      |
|    time_elapsed    | 17916    |
|    total_timesteps | 807936   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=808000, episode_reward=700.90 +/- 83.46
Episode length: 351.96 +/- 195.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 352         |
|    mean_reward          | 701         |
| time/                   |             |
|    total_timesteps      | 808000      |
| train/                  |             |
|    approx_kl            | 0.030156564 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.268      |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0005      |
|    loss                 | 709         |
|    n_updates            | 2774        |
|    policy_gradient_loss | 0.00685     |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=808500, episode_reward=716.11 +/- 86.35
Episode length: 327.84 +/- 197.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 328      |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 808500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 790      |
|    time_elapsed    | 17940    |
|    total_timesteps | 808960   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=809000, episode_reward=742.26 +/- 110.90
Episode length: 213.70 +/- 165.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 742         |
| time/                   |             |
|    total_timesteps      | 809000      |
| train/                  |             |
|    approx_kl            | 0.043101273 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0005      |
|    loss                 | 803         |
|    n_updates            | 2776        |
|    policy_gradient_loss | 0.00259     |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=809500, episode_reward=742.69 +/- 142.22
Episode length: 203.26 +/- 161.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 809500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 791      |
|    time_elapsed    | 17955    |
|    total_timesteps | 809984   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=810000, episode_reward=745.83 +/- 85.54
Episode length: 201.08 +/- 162.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 746        |
| time/                   |            |
|    total_timesteps      | 810000     |
| train/                  |            |
|    approx_kl            | 0.07529837 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0005     |
|    loss                 | 940        |
|    n_updates            | 2779       |
|    policy_gradient_loss | 0.0014     |
|    value_loss           | 1.95e+03   |
----------------------------------------
Eval num_timesteps=810500, episode_reward=768.31 +/- 73.83
Episode length: 179.50 +/- 140.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 810500   |
---------------------------------
Eval num_timesteps=811000, episode_reward=731.47 +/- 111.94
Episode length: 219.44 +/- 172.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 792      |
|    time_elapsed    | 17976    |
|    total_timesteps | 811008   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=811500, episode_reward=740.58 +/- 120.78
Episode length: 189.92 +/- 157.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 190       |
|    mean_reward          | 741       |
| time/                   |           |
|    total_timesteps      | 811500    |
| train/                  |           |
|    approx_kl            | 0.0503179 |
|    clip_fraction        | 0.164     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.434    |
|    explained_variance   | 0.677     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.41e+03  |
|    n_updates            | 2782      |
|    policy_gradient_loss | 0.00645   |
|    value_loss           | 3.96e+03  |
---------------------------------------
Eval num_timesteps=812000, episode_reward=777.66 +/- 93.09
Episode length: 150.02 +/- 111.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 812000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 793      |
|    time_elapsed    | 17988    |
|    total_timesteps | 812032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=812500, episode_reward=785.91 +/- 76.69
Episode length: 141.16 +/- 97.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 141         |
|    mean_reward          | 786         |
| time/                   |             |
|    total_timesteps      | 812500      |
| train/                  |             |
|    approx_kl            | 0.018839331 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0005      |
|    loss                 | 950         |
|    n_updates            | 2783        |
|    policy_gradient_loss | 0.0159      |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=813000, episode_reward=762.89 +/- 103.84
Episode length: 155.14 +/- 113.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 794      |
|    time_elapsed    | 17999    |
|    total_timesteps | 813056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=813500, episode_reward=747.77 +/- 143.58
Episode length: 119.04 +/- 59.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 119        |
|    mean_reward          | 748        |
| time/                   |            |
|    total_timesteps      | 813500     |
| train/                  |            |
|    approx_kl            | 0.05086242 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.33e+03   |
|    n_updates            | 2785       |
|    policy_gradient_loss | 0.0104     |
|    value_loss           | 2.34e+03   |
----------------------------------------
Eval num_timesteps=814000, episode_reward=759.76 +/- 100.43
Episode length: 142.12 +/- 99.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 814000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 795      |
|    time_elapsed    | 18009    |
|    total_timesteps | 814080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=814500, episode_reward=794.76 +/- 65.32
Episode length: 119.88 +/- 59.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 120        |
|    mean_reward          | 795        |
| time/                   |            |
|    total_timesteps      | 814500     |
| train/                  |            |
|    approx_kl            | 0.02144611 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.866      |
|    learning_rate        | 0.0005     |
|    loss                 | 980        |
|    n_updates            | 2786       |
|    policy_gradient_loss | 0.0131     |
|    value_loss           | 1.87e+03   |
----------------------------------------
Eval num_timesteps=815000, episode_reward=796.01 +/- 44.33
Episode length: 124.06 +/- 59.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 796      |
|    time_elapsed    | 18018    |
|    total_timesteps | 815104   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=815500, episode_reward=775.10 +/- 94.50
Episode length: 128.74 +/- 100.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 129         |
|    mean_reward          | 775         |
| time/                   |             |
|    total_timesteps      | 815500      |
| train/                  |             |
|    approx_kl            | 0.046924375 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.361      |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.75e+03    |
|    n_updates            | 2788        |
|    policy_gradient_loss | 0.00288     |
|    value_loss           | 2.25e+03    |
-----------------------------------------
Eval num_timesteps=816000, episode_reward=791.54 +/- 87.57
Episode length: 111.40 +/- 59.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 797      |
|    time_elapsed    | 18027    |
|    total_timesteps | 816128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=816500, episode_reward=755.22 +/- 131.09
Episode length: 141.98 +/- 114.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 142        |
|    mean_reward          | 755        |
| time/                   |            |
|    total_timesteps      | 816500     |
| train/                  |            |
|    approx_kl            | 0.04038871 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.262     |
|    explained_variance   | 0.841      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.49e+03   |
|    n_updates            | 2790       |
|    policy_gradient_loss | 0.015      |
|    value_loss           | 2.37e+03   |
----------------------------------------
Eval num_timesteps=817000, episode_reward=786.96 +/- 72.00
Episode length: 141.88 +/- 113.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 798      |
|    time_elapsed    | 18037    |
|    total_timesteps | 817152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=817500, episode_reward=804.81 +/- 28.97
Episode length: 102.84 +/- 5.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 805         |
| time/                   |             |
|    total_timesteps      | 817500      |
| train/                  |             |
|    approx_kl            | 0.021460798 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.275      |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.04e+03    |
|    n_updates            | 2791        |
|    policy_gradient_loss | 0.00631     |
|    value_loss           | 2.03e+03    |
-----------------------------------------
Eval num_timesteps=818000, episode_reward=779.37 +/- 119.69
Episode length: 130.72 +/- 100.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 818000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 799      |
|    time_elapsed    | 18046    |
|    total_timesteps | 818176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=818500, episode_reward=804.53 +/- 51.57
Episode length: 117.70 +/- 83.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 805         |
| time/                   |             |
|    total_timesteps      | 818500      |
| train/                  |             |
|    approx_kl            | 0.031924613 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 2793        |
|    policy_gradient_loss | 0.000726    |
|    value_loss           | 1.99e+03    |
-----------------------------------------
Eval num_timesteps=819000, episode_reward=806.22 +/- 45.37
Episode length: 109.36 +/- 59.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 800      |
|    time_elapsed    | 18054    |
|    total_timesteps | 819200   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=819500, episode_reward=763.18 +/- 115.94
Episode length: 173.10 +/- 153.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 763        |
| time/                   |            |
|    total_timesteps      | 819500     |
| train/                  |            |
|    approx_kl            | 0.02522471 |
|    clip_fraction        | 0.0814     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.28      |
|    explained_variance   | 0.871      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.04e+03   |
|    n_updates            | 2795       |
|    policy_gradient_loss | 0.00647    |
|    value_loss           | 2.08e+03   |
----------------------------------------
Eval num_timesteps=820000, episode_reward=797.04 +/- 49.25
Episode length: 132.70 +/- 99.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 820000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 801      |
|    time_elapsed    | 18065    |
|    total_timesteps | 820224   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=820500, episode_reward=678.64 +/- 125.43
Episode length: 325.82 +/- 199.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 326         |
|    mean_reward          | 679         |
| time/                   |             |
|    total_timesteps      | 820500      |
| train/                  |             |
|    approx_kl            | 0.037193876 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.877       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.19e+03    |
|    n_updates            | 2797        |
|    policy_gradient_loss | -0.000267   |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=821000, episode_reward=672.07 +/- 106.25
Episode length: 379.56 +/- 194.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 380      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 802      |
|    time_elapsed    | 18090    |
|    total_timesteps | 821248   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=821500, episode_reward=662.67 +/- 131.51
Episode length: 381.24 +/- 191.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 663         |
| time/                   |             |
|    total_timesteps      | 821500      |
| train/                  |             |
|    approx_kl            | 0.041503843 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.19e+03    |
|    n_updates            | 2799        |
|    policy_gradient_loss | 0.00258     |
|    value_loss           | 2.23e+03    |
-----------------------------------------
Eval num_timesteps=822000, episode_reward=704.74 +/- 115.72
Episode length: 301.28 +/- 198.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 301      |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 803      |
|    time_elapsed    | 18113    |
|    total_timesteps | 822272   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=822500, episode_reward=662.11 +/- 128.22
Episode length: 371.58 +/- 196.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 372         |
|    mean_reward          | 662         |
| time/                   |             |
|    total_timesteps      | 822500      |
| train/                  |             |
|    approx_kl            | 0.043218996 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0005      |
|    loss                 | 993         |
|    n_updates            | 2802        |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 1.85e+03    |
-----------------------------------------
Eval num_timesteps=823000, episode_reward=652.38 +/- 153.45
Episode length: 341.54 +/- 198.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 342      |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 804      |
|    time_elapsed    | 18138    |
|    total_timesteps | 823296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=823500, episode_reward=555.96 +/- 142.99
Episode length: 476.96 +/- 130.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 477       |
|    mean_reward          | 556       |
| time/                   |           |
|    total_timesteps      | 823500    |
| train/                  |           |
|    approx_kl            | 0.0257543 |
|    clip_fraction        | 0.174     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.439    |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.15e+03  |
|    n_updates            | 2803      |
|    policy_gradient_loss | 0.0195    |
|    value_loss           | 2.24e+03  |
---------------------------------------
Eval num_timesteps=824000, episode_reward=567.03 +/- 131.91
Episode length: 482.40 +/- 118.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 824000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 805      |
|    time_elapsed    | 18171    |
|    total_timesteps | 824320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=824500, episode_reward=496.61 +/- 190.77
Episode length: 469.06 +/- 139.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 469         |
|    mean_reward          | 497         |
| time/                   |             |
|    total_timesteps      | 824500      |
| train/                  |             |
|    approx_kl            | 0.030399611 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0005      |
|    loss                 | 879         |
|    n_updates            | 2804        |
|    policy_gradient_loss | 0.0239      |
|    value_loss           | 1.42e+03    |
-----------------------------------------
Eval num_timesteps=825000, episode_reward=534.46 +/- 162.57
Episode length: 488.02 +/- 112.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | 534      |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 806      |
|    time_elapsed    | 18203    |
|    total_timesteps | 825344   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=825500, episode_reward=582.12 +/- 163.53
Episode length: 472.34 +/- 129.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 472        |
|    mean_reward          | 582        |
| time/                   |            |
|    total_timesteps      | 825500     |
| train/                  |            |
|    approx_kl            | 0.03907921 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.604     |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.03e+03   |
|    n_updates            | 2807       |
|    policy_gradient_loss | -0.00509   |
|    value_loss           | 1.67e+03   |
----------------------------------------
Eval num_timesteps=826000, episode_reward=573.47 +/- 137.19
Episode length: 496.46 +/- 100.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 826000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 807      |
|    time_elapsed    | 18236    |
|    total_timesteps | 826368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=826500, episode_reward=594.67 +/- 198.50
Episode length: 383.58 +/- 180.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 595         |
| time/                   |             |
|    total_timesteps      | 826500      |
| train/                  |             |
|    approx_kl            | 0.033307217 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0005      |
|    loss                 | 580         |
|    n_updates            | 2808        |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 1.28e+03    |
-----------------------------------------
Eval num_timesteps=827000, episode_reward=597.67 +/- 211.85
Episode length: 378.88 +/- 183.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 379      |
|    mean_reward     | 598      |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 808      |
|    time_elapsed    | 18262    |
|    total_timesteps | 827392   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=827500, episode_reward=748.76 +/- 148.50
Episode length: 214.02 +/- 162.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 749         |
| time/                   |             |
|    total_timesteps      | 827500      |
| train/                  |             |
|    approx_kl            | 0.046953008 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0005      |
|    loss                 | 584         |
|    n_updates            | 2810        |
|    policy_gradient_loss | 0.0187      |
|    value_loss           | 999         |
-----------------------------------------
Eval num_timesteps=828000, episode_reward=724.33 +/- 150.81
Episode length: 264.32 +/- 183.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 809      |
|    time_elapsed    | 18279    |
|    total_timesteps | 828416   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=828500, episode_reward=745.59 +/- 112.92
Episode length: 253.58 +/- 181.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 254        |
|    mean_reward          | 746        |
| time/                   |            |
|    total_timesteps      | 828500     |
| train/                  |            |
|    approx_kl            | 0.05481173 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0005     |
|    loss                 | 963        |
|    n_updates            | 2815       |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 2.3e+03    |
----------------------------------------
Eval num_timesteps=829000, episode_reward=776.27 +/- 84.20
Episode length: 199.48 +/- 153.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 810      |
|    time_elapsed    | 18296    |
|    total_timesteps | 829440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=829500, episode_reward=780.87 +/- 82.71
Episode length: 183.86 +/- 140.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 184         |
|    mean_reward          | 781         |
| time/                   |             |
|    total_timesteps      | 829500      |
| train/                  |             |
|    approx_kl            | 0.045015167 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0005      |
|    loss                 | 669         |
|    n_updates            | 2817        |
|    policy_gradient_loss | 0.00689     |
|    value_loss           | 1.32e+03    |
-----------------------------------------
Eval num_timesteps=830000, episode_reward=785.97 +/- 64.72
Episode length: 193.52 +/- 145.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 830000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 811      |
|    time_elapsed    | 18309    |
|    total_timesteps | 830464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=830500, episode_reward=761.18 +/- 111.30
Episode length: 205.42 +/- 141.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | 761        |
| time/                   |            |
|    total_timesteps      | 830500     |
| train/                  |            |
|    approx_kl            | 0.04030883 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0005     |
|    loss                 | 572        |
|    n_updates            | 2819       |
|    policy_gradient_loss | 0.00965    |
|    value_loss           | 1.36e+03   |
----------------------------------------
Eval num_timesteps=831000, episode_reward=764.27 +/- 74.26
Episode length: 223.36 +/- 160.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 812      |
|    time_elapsed    | 18325    |
|    total_timesteps | 831488   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=831500, episode_reward=762.33 +/- 82.03
Episode length: 243.32 +/- 163.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 243         |
|    mean_reward          | 762         |
| time/                   |             |
|    total_timesteps      | 831500      |
| train/                  |             |
|    approx_kl            | 0.058102153 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0005      |
|    loss                 | 938         |
|    n_updates            | 2822        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 1.91e+03    |
-----------------------------------------
Eval num_timesteps=832000, episode_reward=756.27 +/- 89.62
Episode length: 221.62 +/- 153.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=832500, episode_reward=766.84 +/- 75.89
Episode length: 227.88 +/- 159.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 832500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 813      |
|    time_elapsed    | 18349    |
|    total_timesteps | 832512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=833000, episode_reward=678.80 +/- 87.35
Episode length: 408.76 +/- 170.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 409         |
|    mean_reward          | 679         |
| time/                   |             |
|    total_timesteps      | 833000      |
| train/                  |             |
|    approx_kl            | 0.052298263 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0005      |
|    loss                 | 543         |
|    n_updates            | 2824        |
|    policy_gradient_loss | 0.0195      |
|    value_loss           | 1.36e+03    |
-----------------------------------------
Eval num_timesteps=833500, episode_reward=687.45 +/- 111.35
Episode length: 380.92 +/- 178.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 381      |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 833500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 814      |
|    time_elapsed    | 18376    |
|    total_timesteps | 833536   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=834000, episode_reward=759.29 +/- 82.49
Episode length: 225.62 +/- 150.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 759         |
| time/                   |             |
|    total_timesteps      | 834000      |
| train/                  |             |
|    approx_kl            | 0.057323623 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0005      |
|    loss                 | 572         |
|    n_updates            | 2827        |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=834500, episode_reward=769.95 +/- 78.43
Episode length: 204.68 +/- 140.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 834500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 815      |
|    time_elapsed    | 18391    |
|    total_timesteps | 834560   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=835000, episode_reward=739.56 +/- 93.30
Episode length: 289.00 +/- 171.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 289         |
|    mean_reward          | 740         |
| time/                   |             |
|    total_timesteps      | 835000      |
| train/                  |             |
|    approx_kl            | 0.029424265 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.501      |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0005      |
|    loss                 | 675         |
|    n_updates            | 2829        |
|    policy_gradient_loss | 0.00667     |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=835500, episode_reward=720.26 +/- 103.60
Episode length: 303.24 +/- 181.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 835500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 816      |
|    time_elapsed    | 18412    |
|    total_timesteps | 835584   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=836000, episode_reward=769.25 +/- 81.43
Episode length: 202.98 +/- 131.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 769        |
| time/                   |            |
|    total_timesteps      | 836000     |
| train/                  |            |
|    approx_kl            | 0.04160913 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.45      |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.0005     |
|    loss                 | 912        |
|    n_updates            | 2832       |
|    policy_gradient_loss | 0.00805    |
|    value_loss           | 1.78e+03   |
----------------------------------------
Eval num_timesteps=836500, episode_reward=777.61 +/- 73.18
Episode length: 194.42 +/- 115.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 836500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 817      |
|    time_elapsed    | 18426    |
|    total_timesteps | 836608   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=837000, episode_reward=796.75 +/- 43.97
Episode length: 178.26 +/- 105.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 797         |
| time/                   |             |
|    total_timesteps      | 837000      |
| train/                  |             |
|    approx_kl            | 0.047143776 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0005      |
|    loss                 | 860         |
|    n_updates            | 2835        |
|    policy_gradient_loss | -0.00183    |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=837500, episode_reward=735.50 +/- 168.31
Episode length: 150.90 +/- 79.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 837500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 818      |
|    time_elapsed    | 18438    |
|    total_timesteps | 837632   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=838000, episode_reward=794.41 +/- 45.13
Episode length: 166.22 +/- 108.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 166        |
|    mean_reward          | 794        |
| time/                   |            |
|    total_timesteps      | 838000     |
| train/                  |            |
|    approx_kl            | 0.07069035 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.461     |
|    explained_variance   | 0.857      |
|    learning_rate        | 0.0005     |
|    loss                 | 937        |
|    n_updates            | 2838       |
|    policy_gradient_loss | 0.00243    |
|    value_loss           | 1.95e+03   |
----------------------------------------
Eval num_timesteps=838500, episode_reward=771.10 +/- 111.42
Episode length: 152.94 +/- 91.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 838500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 819      |
|    time_elapsed    | 18450    |
|    total_timesteps | 838656   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.14
Eval num_timesteps=839000, episode_reward=755.29 +/- 162.57
Episode length: 121.02 +/- 17.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 121       |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 839000    |
| train/                  |           |
|    approx_kl            | 0.1438905 |
|    clip_fraction        | 0.149     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.595    |
|    explained_variance   | 0.806     |
|    learning_rate        | 0.0005    |
|    loss                 | 991       |
|    n_updates            | 2841      |
|    policy_gradient_loss | -0.00201  |
|    value_loss           | 2.05e+03  |
---------------------------------------
Eval num_timesteps=839500, episode_reward=779.63 +/- 96.94
Episode length: 141.20 +/- 68.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 141      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 839500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 820      |
|    time_elapsed    | 18459    |
|    total_timesteps | 839680   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=840000, episode_reward=768.19 +/- 131.35
Episode length: 121.74 +/- 59.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 768         |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.039248634 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.614      |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0005      |
|    loss                 | 674         |
|    n_updates            | 2844        |
|    policy_gradient_loss | 0.00325     |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=840500, episode_reward=793.34 +/- 61.77
Episode length: 117.36 +/- 41.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 840500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 821      |
|    time_elapsed    | 18468    |
|    total_timesteps | 840704   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=841000, episode_reward=742.12 +/- 156.67
Episode length: 122.28 +/- 58.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 122        |
|    mean_reward          | 742        |
| time/                   |            |
|    total_timesteps      | 841000     |
| train/                  |            |
|    approx_kl            | 0.05112669 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.07e+03   |
|    n_updates            | 2848       |
|    policy_gradient_loss | -0.00232   |
|    value_loss           | 2.1e+03    |
----------------------------------------
Eval num_timesteps=841500, episode_reward=757.26 +/- 130.17
Episode length: 137.52 +/- 82.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 841500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 822      |
|    time_elapsed    | 18478    |
|    total_timesteps | 841728   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.10
Eval num_timesteps=842000, episode_reward=781.25 +/- 67.99
Episode length: 176.42 +/- 106.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 781        |
| time/                   |            |
|    total_timesteps      | 842000     |
| train/                  |            |
|    approx_kl            | 0.05750636 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.385     |
|    explained_variance   | 0.859      |
|    learning_rate        | 0.0005     |
|    loss                 | 876        |
|    n_updates            | 2852       |
|    policy_gradient_loss | -0.00374   |
|    value_loss           | 2.07e+03   |
----------------------------------------
Eval num_timesteps=842500, episode_reward=790.36 +/- 46.80
Episode length: 155.42 +/- 80.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 842500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 823      |
|    time_elapsed    | 18491    |
|    total_timesteps | 842752   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=843000, episode_reward=772.65 +/- 100.42
Episode length: 190.66 +/- 108.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 191        |
|    mean_reward          | 773        |
| time/                   |            |
|    total_timesteps      | 843000     |
| train/                  |            |
|    approx_kl            | 0.04466025 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.502     |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.5e+03    |
|    n_updates            | 2854       |
|    policy_gradient_loss | 0.0141     |
|    value_loss           | 4.75e+03   |
----------------------------------------
Eval num_timesteps=843500, episode_reward=771.03 +/- 80.58
Episode length: 173.90 +/- 91.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 843500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 824      |
|    time_elapsed    | 18504    |
|    total_timesteps | 843776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=844000, episode_reward=769.47 +/- 71.10
Episode length: 204.56 +/- 124.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | 769         |
| time/                   |             |
|    total_timesteps      | 844000      |
| train/                  |             |
|    approx_kl            | 0.034392726 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0005      |
|    loss                 | 848         |
|    n_updates            | 2856        |
|    policy_gradient_loss | 0.0251      |
|    value_loss           | 1.77e+03    |
-----------------------------------------
Eval num_timesteps=844500, episode_reward=801.61 +/- 25.10
Episode length: 160.40 +/- 58.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 844500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 825      |
|    time_elapsed    | 18517    |
|    total_timesteps | 844800   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=845000, episode_reward=774.09 +/- 103.13
Episode length: 161.00 +/- 108.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 161        |
|    mean_reward          | 774        |
| time/                   |            |
|    total_timesteps      | 845000     |
| train/                  |            |
|    approx_kl            | 0.05610874 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.548     |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0005     |
|    loss                 | 700        |
|    n_updates            | 2859       |
|    policy_gradient_loss | 0.00314    |
|    value_loss           | 1.59e+03   |
----------------------------------------
Eval num_timesteps=845500, episode_reward=790.36 +/- 65.70
Episode length: 154.02 +/- 94.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 845500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 826      |
|    time_elapsed    | 18528    |
|    total_timesteps | 845824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=846000, episode_reward=794.59 +/- 46.20
Episode length: 126.00 +/- 63.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 795         |
| time/                   |             |
|    total_timesteps      | 846000      |
| train/                  |             |
|    approx_kl            | 0.021255275 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0005      |
|    loss                 | 815         |
|    n_updates            | 2860        |
|    policy_gradient_loss | 0.00397     |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=846500, episode_reward=791.22 +/- 50.42
Episode length: 113.00 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 846500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 827      |
|    time_elapsed    | 18537    |
|    total_timesteps | 846848   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=847000, episode_reward=802.08 +/- 36.03
Episode length: 115.38 +/- 9.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 802         |
| time/                   |             |
|    total_timesteps      | 847000      |
| train/                  |             |
|    approx_kl            | 0.051155746 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0005      |
|    loss                 | 933         |
|    n_updates            | 2864        |
|    policy_gradient_loss | -0.0057     |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=847500, episode_reward=799.65 +/- 43.88
Episode length: 116.16 +/- 9.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 847500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 828      |
|    time_elapsed    | 18546    |
|    total_timesteps | 847872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=848000, episode_reward=794.99 +/- 49.96
Episode length: 106.02 +/- 7.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 795        |
| time/                   |            |
|    total_timesteps      | 848000     |
| train/                  |            |
|    approx_kl            | 0.05336362 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.491     |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0005     |
|    loss                 | 885        |
|    n_updates            | 2866       |
|    policy_gradient_loss | -0.00315   |
|    value_loss           | 2.03e+03   |
----------------------------------------
Eval num_timesteps=848500, episode_reward=785.25 +/- 77.42
Episode length: 112.46 +/- 45.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 848500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 829      |
|    time_elapsed    | 18554    |
|    total_timesteps | 848896   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.31
Eval num_timesteps=849000, episode_reward=788.00 +/- 82.30
Episode length: 112.74 +/- 8.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 113        |
|    mean_reward          | 788        |
| time/                   |            |
|    total_timesteps      | 849000     |
| train/                  |            |
|    approx_kl            | 0.06772368 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.42      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.01e+03   |
|    n_updates            | 2868       |
|    policy_gradient_loss | 0.0188     |
|    value_loss           | 2.17e+03   |
----------------------------------------
Eval num_timesteps=849500, episode_reward=776.37 +/- 129.78
Episode length: 110.20 +/- 8.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 849500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 830      |
|    time_elapsed    | 18562    |
|    total_timesteps | 849920   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=850000, episode_reward=791.89 +/- 66.21
Episode length: 108.34 +/- 10.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 792         |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.060879327 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.04e+03    |
|    n_updates            | 2872        |
|    policy_gradient_loss | 0.000976    |
|    value_loss           | 3.47e+03    |
-----------------------------------------
Eval num_timesteps=850500, episode_reward=796.34 +/- 68.02
Episode length: 107.88 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 850500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 831      |
|    time_elapsed    | 18571    |
|    total_timesteps | 850944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=851000, episode_reward=726.49 +/- 149.92
Episode length: 117.50 +/- 11.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 118        |
|    mean_reward          | 726        |
| time/                   |            |
|    total_timesteps      | 851000     |
| train/                  |            |
|    approx_kl            | 0.03191444 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.548     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.34e+03   |
|    n_updates            | 2874       |
|    policy_gradient_loss | -0.00211   |
|    value_loss           | 2.31e+03   |
----------------------------------------
Eval num_timesteps=851500, episode_reward=701.63 +/- 159.43
Episode length: 119.24 +/- 13.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 851500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 832      |
|    time_elapsed    | 18579    |
|    total_timesteps | 851968   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=852000, episode_reward=783.28 +/- 84.62
Episode length: 105.58 +/- 7.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 783        |
| time/                   |            |
|    total_timesteps      | 852000     |
| train/                  |            |
|    approx_kl            | 0.05467744 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.663     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.63e+03   |
|    n_updates            | 2877       |
|    policy_gradient_loss | 0.00486    |
|    value_loss           | 2.9e+03    |
----------------------------------------
Eval num_timesteps=852500, episode_reward=790.13 +/- 70.87
Episode length: 105.12 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 852500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 833      |
|    time_elapsed    | 18587    |
|    total_timesteps | 852992   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=853000, episode_reward=793.58 +/- 66.65
Episode length: 108.22 +/- 7.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 794        |
| time/                   |            |
|    total_timesteps      | 853000     |
| train/                  |            |
|    approx_kl            | 0.07154782 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.579     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.55e+03   |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.00205   |
|    value_loss           | 3.11e+03   |
----------------------------------------
Eval num_timesteps=853500, episode_reward=791.87 +/- 84.98
Episode length: 106.82 +/- 7.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 853500   |
---------------------------------
Eval num_timesteps=854000, episode_reward=803.85 +/- 43.93
Episode length: 105.80 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 834      |
|    time_elapsed    | 18599    |
|    total_timesteps | 854016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=854500, episode_reward=807.82 +/- 38.78
Episode length: 128.78 +/- 16.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 129       |
|    mean_reward          | 808       |
| time/                   |           |
|    total_timesteps      | 854500    |
| train/                  |           |
|    approx_kl            | 0.0293395 |
|    clip_fraction        | 0.194     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.643    |
|    explained_variance   | 0.849     |
|    learning_rate        | 0.0005    |
|    loss                 | 933       |
|    n_updates            | 2881      |
|    policy_gradient_loss | 0.0105    |
|    value_loss           | 1.76e+03  |
---------------------------------------
Eval num_timesteps=855000, episode_reward=793.68 +/- 84.22
Episode length: 149.18 +/- 95.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 855000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 835      |
|    time_elapsed    | 18609    |
|    total_timesteps | 855040   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=855500, episode_reward=790.96 +/- 83.89
Episode length: 156.34 +/- 78.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 156        |
|    mean_reward          | 791        |
| time/                   |            |
|    total_timesteps      | 855500     |
| train/                  |            |
|    approx_kl            | 0.05763806 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.0005     |
|    loss                 | 535        |
|    n_updates            | 2883       |
|    policy_gradient_loss | 0.0136     |
|    value_loss           | 1.52e+03   |
----------------------------------------
Eval num_timesteps=856000, episode_reward=786.64 +/- 104.80
Episode length: 165.50 +/- 95.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 45       |
|    iterations      | 836      |
|    time_elapsed    | 18621    |
|    total_timesteps | 856064   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=856500, episode_reward=788.82 +/- 116.51
Episode length: 124.50 +/- 60.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 124        |
|    mean_reward          | 789        |
| time/                   |            |
|    total_timesteps      | 856500     |
| train/                  |            |
|    approx_kl            | 0.06540694 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.562     |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0005     |
|    loss                 | 559        |
|    n_updates            | 2886       |
|    policy_gradient_loss | 0.00127    |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=857000, episode_reward=809.92 +/- 35.76
Episode length: 131.78 +/- 80.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 857000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 837      |
|    time_elapsed    | 18630    |
|    total_timesteps | 857088   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.10
Eval num_timesteps=857500, episode_reward=805.59 +/- 32.03
Episode length: 117.80 +/- 58.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 806         |
| time/                   |             |
|    total_timesteps      | 857500      |
| train/                  |             |
|    approx_kl            | 0.047020175 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0005      |
|    loss                 | 993         |
|    n_updates            | 2890        |
|    policy_gradient_loss | 0.00464     |
|    value_loss           | 1.93e+03    |
-----------------------------------------
Eval num_timesteps=858000, episode_reward=813.04 +/- 17.65
Episode length: 108.68 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 838      |
|    time_elapsed    | 18639    |
|    total_timesteps | 858112   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=858500, episode_reward=803.97 +/- 37.93
Episode length: 117.04 +/- 10.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 117         |
|    mean_reward          | 804         |
| time/                   |             |
|    total_timesteps      | 858500      |
| train/                  |             |
|    approx_kl            | 0.050719198 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.493      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0005      |
|    loss                 | 2.03e+03    |
|    n_updates            | 2894        |
|    policy_gradient_loss | -0.0071     |
|    value_loss           | 3.89e+03    |
-----------------------------------------
Eval num_timesteps=859000, episode_reward=805.69 +/- 31.53
Episode length: 115.14 +/- 12.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 859000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 839      |
|    time_elapsed    | 18648    |
|    total_timesteps | 859136   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=859500, episode_reward=772.62 +/- 146.99
Episode length: 131.50 +/- 63.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | 773         |
| time/                   |             |
|    total_timesteps      | 859500      |
| train/                  |             |
|    approx_kl            | 0.039321247 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0005      |
|    loss                 | 854         |
|    n_updates            | 2897        |
|    policy_gradient_loss | 0.00492     |
|    value_loss           | 2.17e+03    |
-----------------------------------------
Eval num_timesteps=860000, episode_reward=808.31 +/- 24.32
Episode length: 126.92 +/- 31.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 840      |
|    time_elapsed    | 18658    |
|    total_timesteps | 860160   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=860500, episode_reward=804.03 +/- 33.37
Episode length: 118.02 +/- 15.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 804         |
| time/                   |             |
|    total_timesteps      | 860500      |
| train/                  |             |
|    approx_kl            | 0.048782937 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.03e+03    |
|    n_updates            | 2902        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 2.52e+03    |
-----------------------------------------
Eval num_timesteps=861000, episode_reward=804.18 +/- 50.26
Episode length: 131.58 +/- 81.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 861000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 841      |
|    time_elapsed    | 18667    |
|    total_timesteps | 861184   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=861500, episode_reward=798.19 +/- 50.43
Episode length: 141.60 +/- 98.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 142        |
|    mean_reward          | 798        |
| time/                   |            |
|    total_timesteps      | 861500     |
| train/                  |            |
|    approx_kl            | 0.03701451 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.529     |
|    explained_variance   | 0.88       |
|    learning_rate        | 0.0005     |
|    loss                 | 857        |
|    n_updates            | 2905       |
|    policy_gradient_loss | -0.00309   |
|    value_loss           | 1.72e+03   |
----------------------------------------
Eval num_timesteps=862000, episode_reward=806.95 +/- 34.54
Episode length: 117.52 +/- 29.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 842      |
|    time_elapsed    | 18676    |
|    total_timesteps | 862208   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=862500, episode_reward=758.99 +/- 130.67
Episode length: 136.34 +/- 98.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 136        |
|    mean_reward          | 759        |
| time/                   |            |
|    total_timesteps      | 862500     |
| train/                  |            |
|    approx_kl            | 0.05330542 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.571     |
|    explained_variance   | 0.875      |
|    learning_rate        | 0.0005     |
|    loss                 | 911        |
|    n_updates            | 2907       |
|    policy_gradient_loss | 0.00286    |
|    value_loss           | 1.85e+03   |
----------------------------------------
Eval num_timesteps=863000, episode_reward=780.78 +/- 93.93
Episode length: 148.68 +/- 116.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 863000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 843      |
|    time_elapsed    | 18687    |
|    total_timesteps | 863232   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=863500, episode_reward=797.86 +/- 60.27
Episode length: 126.22 +/- 86.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 798         |
| time/                   |             |
|    total_timesteps      | 863500      |
| train/                  |             |
|    approx_kl            | 0.038067758 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.53       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.45e+03    |
|    n_updates            | 2909        |
|    policy_gradient_loss | 0.00334     |
|    value_loss           | 4.76e+03    |
-----------------------------------------
Eval num_timesteps=864000, episode_reward=805.50 +/- 56.08
Episode length: 114.12 +/- 59.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 844      |
|    time_elapsed    | 18696    |
|    total_timesteps | 864256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=864500, episode_reward=802.61 +/- 34.29
Episode length: 107.38 +/- 8.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 107        |
|    mean_reward          | 803        |
| time/                   |            |
|    total_timesteps      | 864500     |
| train/                  |            |
|    approx_kl            | 0.04266072 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.673     |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0005     |
|    loss                 | 567        |
|    n_updates            | 2911       |
|    policy_gradient_loss | 0.00452    |
|    value_loss           | 1.57e+03   |
----------------------------------------
Eval num_timesteps=865000, episode_reward=799.97 +/- 50.99
Episode length: 109.66 +/- 9.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 865000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 845      |
|    time_elapsed    | 18705    |
|    total_timesteps | 865280   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=865500, episode_reward=806.10 +/- 42.33
Episode length: 111.40 +/- 11.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 806         |
| time/                   |             |
|    total_timesteps      | 865500      |
| train/                  |             |
|    approx_kl            | 0.047069788 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0005      |
|    loss                 | 942         |
|    n_updates            | 2913        |
|    policy_gradient_loss | 0.00336     |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=866000, episode_reward=796.17 +/- 50.11
Episode length: 117.96 +/- 59.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 846      |
|    time_elapsed    | 18715    |
|    total_timesteps | 866304   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=866500, episode_reward=789.88 +/- 71.06
Episode length: 133.04 +/- 99.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 133         |
|    mean_reward          | 790         |
| time/                   |             |
|    total_timesteps      | 866500      |
| train/                  |             |
|    approx_kl            | 0.059120525 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0005      |
|    loss                 | 851         |
|    n_updates            | 2916        |
|    policy_gradient_loss | 0.0191      |
|    value_loss           | 1.89e+03    |
-----------------------------------------
Eval num_timesteps=867000, episode_reward=781.35 +/- 110.41
Episode length: 138.20 +/- 104.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 867000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 847      |
|    time_elapsed    | 18725    |
|    total_timesteps | 867328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=867500, episode_reward=675.99 +/- 118.68
Episode length: 333.94 +/- 200.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 334         |
|    mean_reward          | 676         |
| time/                   |             |
|    total_timesteps      | 867500      |
| train/                  |             |
|    approx_kl            | 0.024212712 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.571      |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0005      |
|    loss                 | 711         |
|    n_updates            | 2917        |
|    policy_gradient_loss | 0.0106      |
|    value_loss           | 1.77e+03    |
-----------------------------------------
Eval num_timesteps=868000, episode_reward=672.36 +/- 105.02
Episode length: 377.04 +/- 190.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 848      |
|    time_elapsed    | 18749    |
|    total_timesteps | 868352   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.09
Eval num_timesteps=868500, episode_reward=755.37 +/- 118.62
Episode length: 192.16 +/- 156.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 192         |
|    mean_reward          | 755         |
| time/                   |             |
|    total_timesteps      | 868500      |
| train/                  |             |
|    approx_kl            | 0.059323687 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0005      |
|    loss                 | 997         |
|    n_updates            | 2923        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 2.22e+03    |
-----------------------------------------
Eval num_timesteps=869000, episode_reward=745.12 +/- 141.56
Episode length: 177.96 +/- 142.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 869000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 849      |
|    time_elapsed    | 18762    |
|    total_timesteps | 869376   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=869500, episode_reward=782.48 +/- 109.83
Episode length: 113.74 +/- 59.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 114        |
|    mean_reward          | 782        |
| time/                   |            |
|    total_timesteps      | 869500     |
| train/                  |            |
|    approx_kl            | 0.06230068 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.792     |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.0005     |
|    loss                 | 714        |
|    n_updates            | 2926       |
|    policy_gradient_loss | 0.00552    |
|    value_loss           | 1.24e+03   |
----------------------------------------
Eval num_timesteps=870000, episode_reward=766.48 +/- 107.60
Episode length: 122.18 +/- 68.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 850      |
|    time_elapsed    | 18771    |
|    total_timesteps | 870400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=870500, episode_reward=711.20 +/- 142.04
Episode length: 269.58 +/- 192.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 711         |
| time/                   |             |
|    total_timesteps      | 870500      |
| train/                  |             |
|    approx_kl            | 0.021412555 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0005      |
|    loss                 | 840         |
|    n_updates            | 2927        |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=871000, episode_reward=721.85 +/- 131.03
Episode length: 230.04 +/- 177.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 871000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 851      |
|    time_elapsed    | 18789    |
|    total_timesteps | 871424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=871500, episode_reward=577.76 +/- 202.10
Episode length: 328.50 +/- 186.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 328         |
|    mean_reward          | 578         |
| time/                   |             |
|    total_timesteps      | 871500      |
| train/                  |             |
|    approx_kl            | 0.027847031 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.872      |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.02e+03    |
|    n_updates            | 2928        |
|    policy_gradient_loss | 0.0378      |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=872000, episode_reward=628.56 +/- 168.78
Episode length: 308.54 +/- 183.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 309      |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 745      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 852      |
|    time_elapsed    | 18811    |
|    total_timesteps | 872448   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=872500, episode_reward=648.85 +/- 186.10
Episode length: 302.62 +/- 186.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 303         |
|    mean_reward          | 649         |
| time/                   |             |
|    total_timesteps      | 872500      |
| train/                  |             |
|    approx_kl            | 0.046425477 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0005      |
|    loss                 | 592         |
|    n_updates            | 2931        |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 1.62e+03    |
-----------------------------------------
Eval num_timesteps=873000, episode_reward=623.51 +/- 203.18
Episode length: 302.68 +/- 190.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 303      |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 873000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 853      |
|    time_elapsed    | 18832    |
|    total_timesteps | 873472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=873500, episode_reward=661.07 +/- 179.07
Episode length: 239.82 +/- 178.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 240         |
|    mean_reward          | 661         |
| time/                   |             |
|    total_timesteps      | 873500      |
| train/                  |             |
|    approx_kl            | 0.033523977 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0005      |
|    loss                 | 525         |
|    n_updates            | 2933        |
|    policy_gradient_loss | 0.00499     |
|    value_loss           | 938         |
-----------------------------------------
Eval num_timesteps=874000, episode_reward=684.38 +/- 156.93
Episode length: 241.32 +/- 177.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 854      |
|    time_elapsed    | 18848    |
|    total_timesteps | 874496   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=874500, episode_reward=770.72 +/- 117.25
Episode length: 138.98 +/- 98.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 139         |
|    mean_reward          | 771         |
| time/                   |             |
|    total_timesteps      | 874500      |
| train/                  |             |
|    approx_kl            | 0.044434935 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.897      |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0005      |
|    loss                 | 640         |
|    n_updates            | 2935        |
|    policy_gradient_loss | 0.0114      |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=778.53 +/- 79.98
Episode length: 137.90 +/- 84.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
Eval num_timesteps=875500, episode_reward=753.21 +/- 139.94
Episode length: 149.20 +/- 111.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 875500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 855      |
|    time_elapsed    | 18864    |
|    total_timesteps | 875520   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=876000, episode_reward=763.17 +/- 103.68
Episode length: 131.96 +/- 99.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | 763         |
| time/                   |             |
|    total_timesteps      | 876000      |
| train/                  |             |
|    approx_kl            | 0.044783823 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0005      |
|    loss                 | 920         |
|    n_updates            | 2939        |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=876500, episode_reward=752.41 +/- 123.51
Episode length: 118.62 +/- 60.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 876500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 743      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 856      |
|    time_elapsed    | 18873    |
|    total_timesteps | 876544   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=877000, episode_reward=716.53 +/- 139.55
Episode length: 167.06 +/- 110.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 167         |
|    mean_reward          | 717         |
| time/                   |             |
|    total_timesteps      | 877000      |
| train/                  |             |
|    approx_kl            | 0.033687882 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.23e+03    |
|    n_updates            | 2941        |
|    policy_gradient_loss | -0.00773    |
|    value_loss           | 2.53e+03    |
-----------------------------------------
Eval num_timesteps=877500, episode_reward=744.62 +/- 132.86
Episode length: 139.94 +/- 57.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 877500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 857      |
|    time_elapsed    | 18884    |
|    total_timesteps | 877568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=878000, episode_reward=690.69 +/- 211.43
Episode length: 167.96 +/- 109.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 691        |
| time/                   |            |
|    total_timesteps      | 878000     |
| train/                  |            |
|    approx_kl            | 0.04732583 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.785     |
|    explained_variance   | 0.57       |
|    learning_rate        | 0.0005     |
|    loss                 | 1.62e+03   |
|    n_updates            | 2943       |
|    policy_gradient_loss | 0.0201     |
|    value_loss           | 3.55e+03   |
----------------------------------------
Eval num_timesteps=878500, episode_reward=721.72 +/- 184.64
Episode length: 159.06 +/- 98.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 878500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 743      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 858      |
|    time_elapsed    | 18896    |
|    total_timesteps | 878592   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=879000, episode_reward=684.61 +/- 158.93
Episode length: 325.20 +/- 180.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 325        |
|    mean_reward          | 685        |
| time/                   |            |
|    total_timesteps      | 879000     |
| train/                  |            |
|    approx_kl            | 0.04908393 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0005     |
|    loss                 | 827        |
|    n_updates            | 2945       |
|    policy_gradient_loss | 0.00919    |
|    value_loss           | 1.66e+03   |
----------------------------------------
Eval num_timesteps=879500, episode_reward=641.36 +/- 148.99
Episode length: 393.16 +/- 172.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 393      |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 879500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 738      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 859      |
|    time_elapsed    | 18920    |
|    total_timesteps | 879616   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=880000, episode_reward=718.72 +/- 115.82
Episode length: 304.34 +/- 175.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 304         |
|    mean_reward          | 719         |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.062236235 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0005      |
|    loss                 | 780         |
|    n_updates            | 2948        |
|    policy_gradient_loss | 0.0101      |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=880500, episode_reward=727.63 +/- 108.17
Episode length: 287.54 +/- 175.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 880500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 735      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 860      |
|    time_elapsed    | 18941    |
|    total_timesteps | 880640   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=881000, episode_reward=744.48 +/- 113.48
Episode length: 250.32 +/- 175.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 744        |
| time/                   |            |
|    total_timesteps      | 881000     |
| train/                  |            |
|    approx_kl            | 0.06302281 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0005     |
|    loss                 | 679        |
|    n_updates            | 2951       |
|    policy_gradient_loss | 0.00991    |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=881500, episode_reward=785.00 +/- 68.93
Episode length: 200.86 +/- 133.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 881500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 729      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 861      |
|    time_elapsed    | 18957    |
|    total_timesteps | 881664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=882000, episode_reward=785.20 +/- 105.90
Episode length: 135.98 +/- 80.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 785         |
| time/                   |             |
|    total_timesteps      | 882000      |
| train/                  |             |
|    approx_kl            | 0.026629474 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0005      |
|    loss                 | 951         |
|    n_updates            | 2952        |
|    policy_gradient_loss | 0.0105      |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=882500, episode_reward=779.47 +/- 85.44
Episode length: 137.56 +/- 80.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 882500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 731      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 862      |
|    time_elapsed    | 18967    |
|    total_timesteps | 882688   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=883000, episode_reward=775.85 +/- 67.80
Episode length: 110.86 +/- 11.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 776         |
| time/                   |             |
|    total_timesteps      | 883000      |
| train/                  |             |
|    approx_kl            | 0.045529254 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.06e+03    |
|    n_updates            | 2954        |
|    policy_gradient_loss | 0.012       |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=883500, episode_reward=783.20 +/- 75.58
Episode length: 118.70 +/- 59.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 883500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 863      |
|    time_elapsed    | 18975    |
|    total_timesteps | 883712   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=884000, episode_reward=783.54 +/- 75.03
Episode length: 106.44 +/- 10.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 784         |
| time/                   |             |
|    total_timesteps      | 884000      |
| train/                  |             |
|    approx_kl            | 0.043675542 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.34e+03    |
|    n_updates            | 2958        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 2.45e+03    |
-----------------------------------------
Eval num_timesteps=884500, episode_reward=765.22 +/- 110.09
Episode length: 106.74 +/- 13.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 884500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 864      |
|    time_elapsed    | 18984    |
|    total_timesteps | 884736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=885000, episode_reward=787.46 +/- 74.06
Episode length: 106.76 +/- 59.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 107       |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 885000    |
| train/                  |           |
|    approx_kl            | 0.0474407 |
|    clip_fraction        | 0.152     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.57     |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.41e+03  |
|    n_updates            | 2960      |
|    policy_gradient_loss | 0.0105    |
|    value_loss           | 2.49e+03  |
---------------------------------------
Eval num_timesteps=885500, episode_reward=783.22 +/- 111.67
Episode length: 98.48 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 885500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 743      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 865      |
|    time_elapsed    | 18991    |
|    total_timesteps | 885760   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=886000, episode_reward=804.82 +/- 48.01
Episode length: 99.40 +/- 5.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.4        |
|    mean_reward          | 805         |
| time/                   |             |
|    total_timesteps      | 886000      |
| train/                  |             |
|    approx_kl            | 0.038794637 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0005      |
|    loss                 | 947         |
|    n_updates            | 2962        |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 2.03e+03    |
-----------------------------------------
Eval num_timesteps=886500, episode_reward=786.57 +/- 69.08
Episode length: 110.08 +/- 59.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 886500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 866      |
|    time_elapsed    | 18999    |
|    total_timesteps | 886784   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=887000, episode_reward=792.74 +/- 59.10
Episode length: 113.46 +/- 59.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 793         |
| time/                   |             |
|    total_timesteps      | 887000      |
| train/                  |             |
|    approx_kl            | 0.044499263 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0005      |
|    loss                 | 998         |
|    n_updates            | 2964        |
|    policy_gradient_loss | 0.00406     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=887500, episode_reward=789.51 +/- 77.84
Episode length: 104.76 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 887500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 867      |
|    time_elapsed    | 19007    |
|    total_timesteps | 887808   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=888000, episode_reward=809.65 +/- 21.46
Episode length: 101.04 +/- 6.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 810         |
| time/                   |             |
|    total_timesteps      | 888000      |
| train/                  |             |
|    approx_kl            | 0.062140763 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.414      |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.27e+03    |
|    n_updates            | 2966        |
|    policy_gradient_loss | 0.0151      |
|    value_loss           | 2.64e+03    |
-----------------------------------------
Eval num_timesteps=888500, episode_reward=795.40 +/- 60.12
Episode length: 102.30 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 888500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 868      |
|    time_elapsed    | 19015    |
|    total_timesteps | 888832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=889000, episode_reward=783.25 +/- 87.75
Episode length: 130.72 +/- 99.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 889000      |
| train/                  |             |
|    approx_kl            | 0.040630385 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0005      |
|    loss                 | 1.03e+03    |
|    n_updates            | 2968        |
|    policy_gradient_loss | 0.00974     |
|    value_loss           | 1.58e+03    |
-----------------------------------------
Eval num_timesteps=889500, episode_reward=787.67 +/- 86.22
Episode length: 129.86 +/- 100.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 889500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 869      |
|    time_elapsed    | 19025    |
|    total_timesteps | 889856   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=890000, episode_reward=797.63 +/- 56.14
Episode length: 110.48 +/- 59.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 798         |
| time/                   |             |
|    total_timesteps      | 890000      |
| train/                  |             |
|    approx_kl            | 0.039757576 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0005      |
|    loss                 | 2.44e+03    |
|    n_updates            | 2970        |
|    policy_gradient_loss | 0.00806     |
|    value_loss           | 3.57e+03    |
-----------------------------------------
Eval num_timesteps=890500, episode_reward=805.09 +/- 34.15
Episode length: 101.92 +/- 8.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 890500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 870      |
|    time_elapsed    | 19033    |
|    total_timesteps | 890880   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.13
Eval num_timesteps=891000, episode_reward=738.13 +/- 136.11
Episode length: 173.98 +/- 142.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 738         |
| time/                   |             |
|    total_timesteps      | 891000      |
| train/                  |             |
|    approx_kl            | 0.054230046 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.79e+03    |
|    n_updates            | 2973        |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 3.11e+03    |
-----------------------------------------
Eval num_timesteps=891500, episode_reward=748.21 +/- 141.18
Episode length: 163.92 +/- 127.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 891500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 871      |
|    time_elapsed    | 19045    |
|    total_timesteps | 891904   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.20
Eval num_timesteps=892000, episode_reward=761.86 +/- 118.77
Episode length: 153.94 +/- 124.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 154        |
|    mean_reward          | 762        |
| time/                   |            |
|    total_timesteps      | 892000     |
| train/                  |            |
|    approx_kl            | 0.09070021 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.887     |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0005     |
|    loss                 | 571        |
|    n_updates            | 2976       |
|    policy_gradient_loss | 0.00489    |
|    value_loss           | 1.42e+03   |
----------------------------------------
Eval num_timesteps=892500, episode_reward=758.13 +/- 113.90
Episode length: 152.90 +/- 124.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 892500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 872      |
|    time_elapsed    | 19056    |
|    total_timesteps | 892928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=893000, episode_reward=752.71 +/- 134.26
Episode length: 169.24 +/- 134.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 753         |
| time/                   |             |
|    total_timesteps      | 893000      |
| train/                  |             |
|    approx_kl            | 0.024110788 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.46e+03    |
|    n_updates            | 2977        |
|    policy_gradient_loss | 0.0028      |
|    value_loss           | 2.65e+03    |
-----------------------------------------
Eval num_timesteps=893500, episode_reward=790.47 +/- 79.73
Episode length: 162.56 +/- 128.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 163      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 893500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 873      |
|    time_elapsed    | 19068    |
|    total_timesteps | 893952   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=894000, episode_reward=788.61 +/- 93.59
Episode length: 116.90 +/- 62.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 117        |
|    mean_reward          | 789        |
| time/                   |            |
|    total_timesteps      | 894000     |
| train/                  |            |
|    approx_kl            | 0.05204339 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.897      |
|    learning_rate        | 0.0005     |
|    loss                 | 789        |
|    n_updates            | 2979       |
|    policy_gradient_loss | 0.0223     |
|    value_loss           | 1.57e+03   |
----------------------------------------
Eval num_timesteps=894500, episode_reward=768.86 +/- 115.26
Episode length: 138.54 +/- 114.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 894500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 874      |
|    time_elapsed    | 19077    |
|    total_timesteps | 894976   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=895000, episode_reward=793.46 +/- 79.14
Episode length: 124.56 +/- 83.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 793         |
| time/                   |             |
|    total_timesteps      | 895000      |
| train/                  |             |
|    approx_kl            | 0.046037972 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.08e+03    |
|    n_updates            | 2982        |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 1.92e+03    |
-----------------------------------------
Eval num_timesteps=895500, episode_reward=794.21 +/- 111.70
Episode length: 113.70 +/- 60.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 895500   |
---------------------------------
Eval num_timesteps=896000, episode_reward=810.06 +/- 25.27
Episode length: 116.20 +/- 59.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 875      |
|    time_elapsed    | 19090    |
|    total_timesteps | 896000   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=896500, episode_reward=802.48 +/- 49.48
Episode length: 139.28 +/- 99.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 139        |
|    mean_reward          | 802        |
| time/                   |            |
|    total_timesteps      | 896500     |
| train/                  |            |
|    approx_kl            | 0.07101468 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.15e+03   |
|    n_updates            | 2986       |
|    policy_gradient_loss | -0.00639   |
|    value_loss           | 1.96e+03   |
----------------------------------------
Eval num_timesteps=897000, episode_reward=810.57 +/- 21.26
Episode length: 115.58 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 876      |
|    time_elapsed    | 19100    |
|    total_timesteps | 897024   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.08
Eval num_timesteps=897500, episode_reward=792.20 +/- 62.40
Episode length: 154.32 +/- 118.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 154        |
|    mean_reward          | 792        |
| time/                   |            |
|    total_timesteps      | 897500     |
| train/                  |            |
|    approx_kl            | 0.05317951 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.751     |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0005     |
|    loss                 | 624        |
|    n_updates            | 2992       |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 1.41e+03   |
----------------------------------------
Eval num_timesteps=898000, episode_reward=781.11 +/- 83.10
Episode length: 161.26 +/- 134.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 898000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 46       |
|    iterations      | 877      |
|    time_elapsed    | 19111    |
|    total_timesteps | 898048   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=898500, episode_reward=816.44 +/- 6.17
Episode length: 101.32 +/- 6.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 816         |
| time/                   |             |
|    total_timesteps      | 898500      |
| train/                  |             |
|    approx_kl            | 0.047248784 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.913       |
|    learning_rate        | 0.0005      |
|    loss                 | 758         |
|    n_updates            | 2994        |
|    policy_gradient_loss | 0.0136      |
|    value_loss           | 1.48e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=899000, episode_reward=808.62 +/- 38.27
Episode length: 110.08 +/- 59.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 878      |
|    time_elapsed    | 19119    |
|    total_timesteps | 899072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=899500, episode_reward=773.71 +/- 83.69
Episode length: 183.40 +/- 160.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 183        |
|    mean_reward          | 774        |
| time/                   |            |
|    total_timesteps      | 899500     |
| train/                  |            |
|    approx_kl            | 0.03619608 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.672     |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0005     |
|    loss                 | 933        |
|    n_updates            | 2996       |
|    policy_gradient_loss | 0.00558    |
|    value_loss           | 1.69e+03   |
----------------------------------------
Eval num_timesteps=900000, episode_reward=780.38 +/- 86.82
Episode length: 155.00 +/- 126.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 879      |
|    time_elapsed    | 19131    |
|    total_timesteps | 900096   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=900500, episode_reward=781.63 +/- 60.60
Episode length: 211.88 +/- 176.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 212       |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 900500    |
| train/                  |           |
|    approx_kl            | 0.0350492 |
|    clip_fraction        | 0.188     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.856    |
|    explained_variance   | 0.911     |
|    learning_rate        | 0.0005    |
|    loss                 | 881       |
|    n_updates            | 2999      |
|    policy_gradient_loss | -0.004    |
|    value_loss           | 1.52e+03  |
---------------------------------------
Eval num_timesteps=901000, episode_reward=757.01 +/- 107.12
Episode length: 211.18 +/- 176.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 880      |
|    time_elapsed    | 19146    |
|    total_timesteps | 901120   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=901500, episode_reward=760.00 +/- 84.40
Episode length: 243.58 +/- 184.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 760         |
| time/                   |             |
|    total_timesteps      | 901500      |
| train/                  |             |
|    approx_kl            | 0.040949915 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.653      |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.0005      |
|    loss                 | 879         |
|    n_updates            | 3001        |
|    policy_gradient_loss | 0.0106      |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=902000, episode_reward=727.29 +/- 96.36
Episode length: 283.14 +/- 197.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 902000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 881      |
|    time_elapsed    | 19165    |
|    total_timesteps | 902144   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=902500, episode_reward=746.42 +/- 100.02
Episode length: 295.18 +/- 195.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 295         |
|    mean_reward          | 746         |
| time/                   |             |
|    total_timesteps      | 902500      |
| train/                  |             |
|    approx_kl            | 0.052081365 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0005      |
|    loss                 | 455         |
|    n_updates            | 3004        |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=903000, episode_reward=758.76 +/- 68.11
Episode length: 286.04 +/- 195.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 882      |
|    time_elapsed    | 19185    |
|    total_timesteps | 903168   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=903500, episode_reward=776.44 +/- 105.91
Episode length: 162.20 +/- 146.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 162        |
|    mean_reward          | 776        |
| time/                   |            |
|    total_timesteps      | 903500     |
| train/                  |            |
|    approx_kl            | 0.06729619 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.94       |
|    learning_rate        | 0.0005     |
|    loss                 | 520        |
|    n_updates            | 3007       |
|    policy_gradient_loss | 0.00646    |
|    value_loss           | 1.12e+03   |
----------------------------------------
Eval num_timesteps=904000, episode_reward=797.24 +/- 57.82
Episode length: 129.60 +/- 100.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 904000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 883      |
|    time_elapsed    | 19195    |
|    total_timesteps | 904192   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=904500, episode_reward=676.47 +/- 197.95
Episode length: 166.10 +/- 156.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 166        |
|    mean_reward          | 676        |
| time/                   |            |
|    total_timesteps      | 904500     |
| train/                  |            |
|    approx_kl            | 0.07462591 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.565     |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0005     |
|    loss                 | 953        |
|    n_updates            | 3009       |
|    policy_gradient_loss | 0.0131     |
|    value_loss           | 3.1e+03    |
----------------------------------------
Eval num_timesteps=905000, episode_reward=709.58 +/- 176.09
Episode length: 166.58 +/- 156.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 884      |
|    time_elapsed    | 19207    |
|    total_timesteps | 905216   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=905500, episode_reward=739.00 +/- 137.69
Episode length: 152.02 +/- 138.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 739         |
| time/                   |             |
|    total_timesteps      | 905500      |
| train/                  |             |
|    approx_kl            | 0.042613488 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.66e+03    |
|    n_updates            | 3012        |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 7.13e+03    |
-----------------------------------------
Eval num_timesteps=906000, episode_reward=760.91 +/- 85.80
Episode length: 144.94 +/- 127.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 885      |
|    time_elapsed    | 19218    |
|    total_timesteps | 906240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=906500, episode_reward=724.56 +/- 106.38
Episode length: 283.50 +/- 205.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 284        |
|    mean_reward          | 725        |
| time/                   |            |
|    total_timesteps      | 906500     |
| train/                  |            |
|    approx_kl            | 0.07347928 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.99e+03   |
|    n_updates            | 3014       |
|    policy_gradient_loss | -0.00655   |
|    value_loss           | 3.49e+03   |
----------------------------------------
Eval num_timesteps=907000, episode_reward=745.94 +/- 106.66
Episode length: 242.90 +/- 193.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 886      |
|    time_elapsed    | 19236    |
|    total_timesteps | 907264   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=907500, episode_reward=762.13 +/- 140.34
Episode length: 142.30 +/- 127.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 762         |
| time/                   |             |
|    total_timesteps      | 907500      |
| train/                  |             |
|    approx_kl            | 0.043361854 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | 955         |
|    n_updates            | 3017        |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 1.49e+03    |
-----------------------------------------
Eval num_timesteps=908000, episode_reward=793.29 +/- 82.47
Episode length: 124.42 +/- 101.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 908000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 887      |
|    time_elapsed    | 19246    |
|    total_timesteps | 908288   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=908500, episode_reward=785.76 +/- 67.21
Episode length: 176.62 +/- 163.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 786         |
| time/                   |             |
|    total_timesteps      | 908500      |
| train/                  |             |
|    approx_kl            | 0.026900565 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0005      |
|    loss                 | 852         |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=909000, episode_reward=798.94 +/- 44.12
Episode length: 136.32 +/- 114.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 888      |
|    time_elapsed    | 19258    |
|    total_timesteps | 909312   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=909500, episode_reward=768.41 +/- 89.22
Episode length: 164.08 +/- 145.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 164         |
|    mean_reward          | 768         |
| time/                   |             |
|    total_timesteps      | 909500      |
| train/                  |             |
|    approx_kl            | 0.033918053 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0005      |
|    loss                 | 523         |
|    n_updates            | 3023        |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=787.70 +/- 65.67
Episode length: 179.86 +/- 161.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 889      |
|    time_elapsed    | 19270    |
|    total_timesteps | 910336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=910500, episode_reward=773.71 +/- 73.20
Episode length: 180.90 +/- 161.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 181         |
|    mean_reward          | 774         |
| time/                   |             |
|    total_timesteps      | 910500      |
| train/                  |             |
|    approx_kl            | 0.044328954 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0005      |
|    loss                 | 479         |
|    n_updates            | 3025        |
|    policy_gradient_loss | 0.00261     |
|    value_loss           | 905         |
-----------------------------------------
Eval num_timesteps=911000, episode_reward=780.39 +/- 75.96
Episode length: 189.68 +/- 167.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 890      |
|    time_elapsed    | 19283    |
|    total_timesteps | 911360   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=911500, episode_reward=769.27 +/- 81.73
Episode length: 206.58 +/- 179.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 769         |
| time/                   |             |
|    total_timesteps      | 911500      |
| train/                  |             |
|    approx_kl            | 0.047077842 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0005      |
|    loss                 | 467         |
|    n_updates            | 3027        |
|    policy_gradient_loss | 0.000811    |
|    value_loss           | 1.2e+03     |
-----------------------------------------
Eval num_timesteps=912000, episode_reward=767.38 +/- 89.37
Episode length: 189.96 +/- 167.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 891      |
|    time_elapsed    | 19297    |
|    total_timesteps | 912384   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=912500, episode_reward=767.61 +/- 112.24
Episode length: 194.36 +/- 165.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 768         |
| time/                   |             |
|    total_timesteps      | 912500      |
| train/                  |             |
|    approx_kl            | 0.062730476 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0005      |
|    loss                 | 566         |
|    n_updates            | 3029        |
|    policy_gradient_loss | 0.00785     |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=913000, episode_reward=749.16 +/- 130.85
Episode length: 194.22 +/- 165.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 892      |
|    time_elapsed    | 19311    |
|    total_timesteps | 913408   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=913500, episode_reward=761.37 +/- 130.09
Episode length: 138.80 +/- 98.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 139       |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 913500    |
| train/                  |           |
|    approx_kl            | 0.0402762 |
|    clip_fraction        | 0.191     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.72     |
|    explained_variance   | 0.952     |
|    learning_rate        | 0.0005    |
|    loss                 | 568       |
|    n_updates            | 3033      |
|    policy_gradient_loss | -0.0086   |
|    value_loss           | 1.27e+03  |
---------------------------------------
Eval num_timesteps=914000, episode_reward=737.14 +/- 144.33
Episode length: 195.44 +/- 165.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 914000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 893      |
|    time_elapsed    | 19323    |
|    total_timesteps | 914432   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=914500, episode_reward=728.17 +/- 172.38
Episode length: 134.96 +/- 115.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 135         |
|    mean_reward          | 728         |
| time/                   |             |
|    total_timesteps      | 914500      |
| train/                  |             |
|    approx_kl            | 0.042306114 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | 911         |
|    n_updates            | 3035        |
|    policy_gradient_loss | 0.0151      |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=915000, episode_reward=762.72 +/- 102.39
Episode length: 124.34 +/- 101.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 894      |
|    time_elapsed    | 19333    |
|    total_timesteps | 915456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=915500, episode_reward=780.56 +/- 91.59
Episode length: 130.10 +/- 100.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 130         |
|    mean_reward          | 781         |
| time/                   |             |
|    total_timesteps      | 915500      |
| train/                  |             |
|    approx_kl            | 0.037957236 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.28e+03    |
|    n_updates            | 3037        |
|    policy_gradient_loss | 0.000992    |
|    value_loss           | 2.92e+03    |
-----------------------------------------
Eval num_timesteps=916000, episode_reward=737.33 +/- 125.90
Episode length: 174.44 +/- 153.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 916000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 895      |
|    time_elapsed    | 19344    |
|    total_timesteps | 916480   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=916500, episode_reward=776.90 +/- 83.36
Episode length: 151.00 +/- 125.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 777         |
| time/                   |             |
|    total_timesteps      | 916500      |
| train/                  |             |
|    approx_kl            | 0.042929396 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.91e+03    |
|    n_updates            | 3039        |
|    policy_gradient_loss | 0.0133      |
|    value_loss           | 4.04e+03    |
-----------------------------------------
Eval num_timesteps=917000, episode_reward=774.98 +/- 94.18
Episode length: 149.30 +/- 125.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=917500, episode_reward=761.88 +/- 101.29
Episode length: 152.14 +/- 125.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 152      |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 917500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 896      |
|    time_elapsed    | 19360    |
|    total_timesteps | 917504   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=918000, episode_reward=672.92 +/- 163.97
Episode length: 289.46 +/- 209.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 289        |
|    mean_reward          | 673        |
| time/                   |            |
|    total_timesteps      | 918000     |
| train/                  |            |
|    approx_kl            | 0.08732229 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.535     |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.23e+03   |
|    n_updates            | 3042       |
|    policy_gradient_loss | 0.000636   |
|    value_loss           | 2.49e+03   |
----------------------------------------
Eval num_timesteps=918500, episode_reward=685.66 +/- 136.14
Episode length: 284.32 +/- 204.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 918500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 897      |
|    time_elapsed    | 19380    |
|    total_timesteps | 918528   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=919000, episode_reward=748.67 +/- 133.73
Episode length: 141.56 +/- 128.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 142        |
|    mean_reward          | 749        |
| time/                   |            |
|    total_timesteps      | 919000     |
| train/                  |            |
|    approx_kl            | 0.06189574 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0005     |
|    loss                 | 860        |
|    n_updates            | 3045       |
|    policy_gradient_loss | 0.00447    |
|    value_loss           | 1.81e+03   |
----------------------------------------
Eval num_timesteps=919500, episode_reward=732.03 +/- 126.37
Episode length: 175.16 +/- 163.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 919500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 898      |
|    time_elapsed    | 19391    |
|    total_timesteps | 919552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=920000, episode_reward=766.38 +/- 107.59
Episode length: 177.98 +/- 162.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 766         |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.034505386 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.453      |
|    explained_variance   | 0.902       |
|    learning_rate        | 0.0005      |
|    loss                 | 986         |
|    n_updates            | 3047        |
|    policy_gradient_loss | 0.0144      |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=920500, episode_reward=759.27 +/- 107.59
Episode length: 171.00 +/- 154.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 920500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 899      |
|    time_elapsed    | 19404    |
|    total_timesteps | 920576   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=921000, episode_reward=760.33 +/- 120.55
Episode length: 155.24 +/- 136.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | 760         |
| time/                   |             |
|    total_timesteps      | 921000      |
| train/                  |             |
|    approx_kl            | 0.040218014 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.723      |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0005      |
|    loss                 | 793         |
|    n_updates            | 3049        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=921500, episode_reward=783.55 +/- 82.20
Episode length: 165.18 +/- 145.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 921500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 900      |
|    time_elapsed    | 19415    |
|    total_timesteps | 921600   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=922000, episode_reward=785.01 +/- 74.04
Episode length: 155.44 +/- 136.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | 785         |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.045554314 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.426      |
|    explained_variance   | 0.901       |
|    learning_rate        | 0.0005      |
|    loss                 | 622         |
|    n_updates            | 3052        |
|    policy_gradient_loss | -0.000802   |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=922500, episode_reward=760.32 +/- 110.80
Episode length: 167.02 +/- 145.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 922500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 901      |
|    time_elapsed    | 19427    |
|    total_timesteps | 922624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=923000, episode_reward=769.24 +/- 99.37
Episode length: 172.42 +/- 142.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 769         |
| time/                   |             |
|    total_timesteps      | 923000      |
| train/                  |             |
|    approx_kl            | 0.068549335 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0005      |
|    loss                 | 412         |
|    n_updates            | 3054        |
|    policy_gradient_loss | 0.0329      |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=923500, episode_reward=754.72 +/- 132.68
Episode length: 192.16 +/- 159.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 923500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 902      |
|    time_elapsed    | 19440    |
|    total_timesteps | 923648   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=924000, episode_reward=764.08 +/- 96.45
Episode length: 200.90 +/- 172.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 764         |
| time/                   |             |
|    total_timesteps      | 924000      |
| train/                  |             |
|    approx_kl            | 0.053221356 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.895       |
|    learning_rate        | 0.0005      |
|    loss                 | 941         |
|    n_updates            | 3057        |
|    policy_gradient_loss | 0.00146     |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=924500, episode_reward=765.57 +/- 101.26
Episode length: 168.10 +/- 144.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 924500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 903      |
|    time_elapsed    | 19453    |
|    total_timesteps | 924672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=925000, episode_reward=794.54 +/- 66.47
Episode length: 126.08 +/- 100.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 795         |
| time/                   |             |
|    total_timesteps      | 925000      |
| train/                  |             |
|    approx_kl            | 0.043199528 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.482      |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.0005      |
|    loss                 | 726         |
|    n_updates            | 3059        |
|    policy_gradient_loss | 0.00439     |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=925500, episode_reward=802.77 +/- 52.82
Episode length: 118.58 +/- 83.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 925500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 904      |
|    time_elapsed    | 19462    |
|    total_timesteps | 925696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=926000, episode_reward=759.68 +/- 101.18
Episode length: 196.56 +/- 164.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 197         |
|    mean_reward          | 760         |
| time/                   |             |
|    total_timesteps      | 926000      |
| train/                  |             |
|    approx_kl            | 0.040590625 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.467      |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0005      |
|    loss                 | 712         |
|    n_updates            | 3061        |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=926500, episode_reward=748.85 +/- 110.27
Episode length: 195.00 +/- 165.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 926500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 905      |
|    time_elapsed    | 19476    |
|    total_timesteps | 926720   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=927000, episode_reward=661.58 +/- 173.53
Episode length: 296.40 +/- 203.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 296        |
|    mean_reward          | 662        |
| time/                   |            |
|    total_timesteps      | 927000     |
| train/                  |            |
|    approx_kl            | 0.04932625 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.581     |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.01e+03   |
|    n_updates            | 3063       |
|    policy_gradient_loss | 0.0198     |
|    value_loss           | 1.62e+03   |
----------------------------------------
Eval num_timesteps=927500, episode_reward=657.54 +/- 180.91
Episode length: 310.68 +/- 206.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 311      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 927500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 906      |
|    time_elapsed    | 19496    |
|    total_timesteps | 927744   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=928000, episode_reward=777.63 +/- 86.73
Episode length: 153.64 +/- 124.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 154         |
|    mean_reward          | 778         |
| time/                   |             |
|    total_timesteps      | 928000      |
| train/                  |             |
|    approx_kl            | 0.039417423 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.88e+03    |
|    n_updates            | 3066        |
|    policy_gradient_loss | 0.00262     |
|    value_loss           | 2.43e+03    |
-----------------------------------------
Eval num_timesteps=928500, episode_reward=771.80 +/- 93.14
Episode length: 178.98 +/- 151.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 179      |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 928500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 907      |
|    time_elapsed    | 19508    |
|    total_timesteps | 928768   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=929000, episode_reward=700.68 +/- 127.01
Episode length: 320.96 +/- 204.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 321         |
|    mean_reward          | 701         |
| time/                   |             |
|    total_timesteps      | 929000      |
| train/                  |             |
|    approx_kl            | 0.059488576 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.28e+03    |
|    n_updates            | 3068        |
|    policy_gradient_loss | 0.0135      |
|    value_loss           | 2.46e+03    |
-----------------------------------------
Eval num_timesteps=929500, episode_reward=688.21 +/- 149.41
Episode length: 321.22 +/- 203.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 321      |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 929500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 908      |
|    time_elapsed    | 19530    |
|    total_timesteps | 929792   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=930000, episode_reward=747.26 +/- 113.47
Episode length: 214.36 +/- 174.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 747         |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.050357167 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.62       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0005      |
|    loss                 | 978         |
|    n_updates            | 3071        |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 2.38e+03    |
-----------------------------------------
Eval num_timesteps=930500, episode_reward=744.09 +/- 118.64
Episode length: 221.62 +/- 179.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 930500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 909      |
|    time_elapsed    | 19547    |
|    total_timesteps | 930816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=931000, episode_reward=605.25 +/- 170.52
Episode length: 296.64 +/- 195.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 297         |
|    mean_reward          | 605         |
| time/                   |             |
|    total_timesteps      | 931000      |
| train/                  |             |
|    approx_kl            | 0.027604869 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.64e+03    |
|    n_updates            | 3072        |
|    policy_gradient_loss | 0.0164      |
|    value_loss           | 2.76e+03    |
-----------------------------------------
Eval num_timesteps=931500, episode_reward=630.48 +/- 182.76
Episode length: 304.58 +/- 196.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 305      |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 931500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 910      |
|    time_elapsed    | 19567    |
|    total_timesteps | 931840   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=932000, episode_reward=600.74 +/- 142.10
Episode length: 376.70 +/- 185.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 377        |
|    mean_reward          | 601        |
| time/                   |            |
|    total_timesteps      | 932000     |
| train/                  |            |
|    approx_kl            | 0.04420504 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.625     |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | 833        |
|    n_updates            | 3074       |
|    policy_gradient_loss | 0.0327     |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=932500, episode_reward=647.42 +/- 144.44
Episode length: 327.06 +/- 191.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 327      |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 932500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 911      |
|    time_elapsed    | 19592    |
|    total_timesteps | 932864   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.08
Eval num_timesteps=933000, episode_reward=426.86 +/- 172.34
Episode length: 468.12 +/- 125.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 468       |
|    mean_reward          | 427       |
| time/                   |           |
|    total_timesteps      | 933000    |
| train/                  |           |
|    approx_kl            | 0.0426222 |
|    clip_fraction        | 0.238     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.848    |
|    explained_variance   | 0.79      |
|    learning_rate        | 0.0005    |
|    loss                 | 1.92e+03  |
|    n_updates            | 3079      |
|    policy_gradient_loss | -0.00229  |
|    value_loss           | 2.94e+03  |
---------------------------------------
Eval num_timesteps=933500, episode_reward=427.61 +/- 185.23
Episode length: 460.10 +/- 131.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 460      |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 933500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 912      |
|    time_elapsed    | 19623    |
|    total_timesteps | 933888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=934000, episode_reward=712.13 +/- 128.61
Episode length: 265.58 +/- 179.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 266         |
|    mean_reward          | 712         |
| time/                   |             |
|    total_timesteps      | 934000      |
| train/                  |             |
|    approx_kl            | 0.042562686 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.08e+03    |
|    n_updates            | 3081        |
|    policy_gradient_loss | 0.0115      |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=934500, episode_reward=695.38 +/- 124.10
Episode length: 300.36 +/- 191.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 934500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 913      |
|    time_elapsed    | 19643    |
|    total_timesteps | 934912   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=935000, episode_reward=657.84 +/- 138.84
Episode length: 355.12 +/- 191.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 658         |
| time/                   |             |
|    total_timesteps      | 935000      |
| train/                  |             |
|    approx_kl            | 0.037259016 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0005      |
|    loss                 | 778         |
|    n_updates            | 3083        |
|    policy_gradient_loss | 0.00364     |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=935500, episode_reward=670.68 +/- 133.62
Episode length: 329.78 +/- 195.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 330      |
|    mean_reward     | 671      |
| time/              |          |
|    total_timesteps | 935500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 914      |
|    time_elapsed    | 19667    |
|    total_timesteps | 935936   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=936000, episode_reward=764.24 +/- 95.96
Episode length: 177.00 +/- 152.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 764         |
| time/                   |             |
|    total_timesteps      | 936000      |
| train/                  |             |
|    approx_kl            | 0.041264005 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0005      |
|    loss                 | 815         |
|    n_updates            | 3086        |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=936500, episode_reward=756.58 +/- 105.91
Episode length: 201.40 +/- 172.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 936500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 751      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 915      |
|    time_elapsed    | 19680    |
|    total_timesteps | 936960   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=937000, episode_reward=758.14 +/- 105.72
Episode length: 197.28 +/- 174.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 758        |
| time/                   |            |
|    total_timesteps      | 937000     |
| train/                  |            |
|    approx_kl            | 0.05945612 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.45e+03   |
|    n_updates            | 3089       |
|    policy_gradient_loss | -0.00583   |
|    value_loss           | 3.06e+03   |
----------------------------------------
Eval num_timesteps=937500, episode_reward=774.73 +/- 93.95
Episode length: 172.48 +/- 153.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 937500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 916      |
|    time_elapsed    | 19694    |
|    total_timesteps | 937984   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=938000, episode_reward=773.64 +/- 107.93
Episode length: 158.10 +/- 135.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 774         |
| time/                   |             |
|    total_timesteps      | 938000      |
| train/                  |             |
|    approx_kl            | 0.033276618 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.592      |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.16e+03    |
|    n_updates            | 3092        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 2.09e+03    |
-----------------------------------------
Eval num_timesteps=938500, episode_reward=765.97 +/- 101.33
Episode length: 166.54 +/- 144.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 938500   |
---------------------------------
Eval num_timesteps=939000, episode_reward=780.73 +/- 108.52
Episode length: 132.00 +/- 99.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 917      |
|    time_elapsed    | 19710    |
|    total_timesteps | 939008   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=939500, episode_reward=788.59 +/- 71.73
Episode length: 146.04 +/- 126.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 789         |
| time/                   |             |
|    total_timesteps      | 939500      |
| train/                  |             |
|    approx_kl            | 0.059953123 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.16e+03    |
|    n_updates            | 3095        |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 2.25e+03    |
-----------------------------------------
Eval num_timesteps=940000, episode_reward=785.61 +/- 72.63
Episode length: 138.14 +/- 114.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 918      |
|    time_elapsed    | 19720    |
|    total_timesteps | 940032   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=940500, episode_reward=759.08 +/- 91.67
Episode length: 172.96 +/- 153.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 173         |
|    mean_reward          | 759         |
| time/                   |             |
|    total_timesteps      | 940500      |
| train/                  |             |
|    approx_kl            | 0.034528315 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0005      |
|    loss                 | 859         |
|    n_updates            | 3099        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=941000, episode_reward=768.02 +/- 95.46
Episode length: 164.08 +/- 145.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 941000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 919      |
|    time_elapsed    | 19732    |
|    total_timesteps | 941056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.15
Eval num_timesteps=941500, episode_reward=752.48 +/- 113.71
Episode length: 153.02 +/- 137.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 153         |
|    mean_reward          | 752         |
| time/                   |             |
|    total_timesteps      | 941500      |
| train/                  |             |
|    approx_kl            | 0.050621748 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0005      |
|    loss                 | 579         |
|    n_updates            | 3101        |
|    policy_gradient_loss | 0.0112      |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=942000, episode_reward=763.54 +/- 97.36
Episode length: 169.56 +/- 155.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 920      |
|    time_elapsed    | 19744    |
|    total_timesteps | 942080   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=942500, episode_reward=757.50 +/- 121.18
Episode length: 97.52 +/- 6.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.5       |
|    mean_reward          | 758        |
| time/                   |            |
|    total_timesteps      | 942500     |
| train/                  |            |
|    approx_kl            | 0.07543287 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.452     |
|    explained_variance   | 0.918      |
|    learning_rate        | 0.0005     |
|    loss                 | 503        |
|    n_updates            | 3104       |
|    policy_gradient_loss | 0.0118     |
|    value_loss           | 1.32e+03   |
----------------------------------------
Eval num_timesteps=943000, episode_reward=737.59 +/- 145.41
Episode length: 119.60 +/- 88.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 943000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 921      |
|    time_elapsed    | 19752    |
|    total_timesteps | 943104   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=943500, episode_reward=783.83 +/- 112.30
Episode length: 142.46 +/- 127.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 784         |
| time/                   |             |
|    total_timesteps      | 943500      |
| train/                  |             |
|    approx_kl            | 0.034672085 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.371      |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.87e+03    |
|    n_updates            | 3106        |
|    policy_gradient_loss | 0.00626     |
|    value_loss           | 2.62e+03    |
-----------------------------------------
Eval num_timesteps=944000, episode_reward=765.01 +/- 117.48
Episode length: 176.06 +/- 163.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 922      |
|    time_elapsed    | 19764    |
|    total_timesteps | 944128   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=944500, episode_reward=752.49 +/- 129.13
Episode length: 174.76 +/- 164.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | 752         |
| time/                   |             |
|    total_timesteps      | 944500      |
| train/                  |             |
|    approx_kl            | 0.040311582 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0005      |
|    loss                 | 895         |
|    n_updates            | 3109        |
|    policy_gradient_loss | 0.0112      |
|    value_loss           | 1.2e+03     |
-----------------------------------------
Eval num_timesteps=945000, episode_reward=803.16 +/- 52.88
Episode length: 106.48 +/- 59.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 945000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 923      |
|    time_elapsed    | 19774    |
|    total_timesteps | 945152   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=945500, episode_reward=775.60 +/- 91.41
Episode length: 156.22 +/- 148.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 776         |
| time/                   |             |
|    total_timesteps      | 945500      |
| train/                  |             |
|    approx_kl            | 0.045739282 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.452      |
|    explained_variance   | 0.934       |
|    learning_rate        | 0.0005      |
|    loss                 | 665         |
|    n_updates            | 3112        |
|    policy_gradient_loss | 0.000734    |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=946000, episode_reward=782.94 +/- 81.02
Episode length: 139.66 +/- 128.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 924      |
|    time_elapsed    | 19785    |
|    total_timesteps | 946176   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=946500, episode_reward=784.64 +/- 75.95
Episode length: 134.86 +/- 115.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 135         |
|    mean_reward          | 785         |
| time/                   |             |
|    total_timesteps      | 946500      |
| train/                  |             |
|    approx_kl            | 0.033726245 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.99e+03    |
|    n_updates            | 3114        |
|    policy_gradient_loss | -0.000915   |
|    value_loss           | 3.43e+03    |
-----------------------------------------
Eval num_timesteps=947000, episode_reward=779.29 +/- 92.85
Episode length: 159.68 +/- 147.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 947000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 925      |
|    time_elapsed    | 19796    |
|    total_timesteps | 947200   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=947500, episode_reward=775.94 +/- 99.89
Episode length: 134.28 +/- 115.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 776         |
| time/                   |             |
|    total_timesteps      | 947500      |
| train/                  |             |
|    approx_kl            | 0.041258715 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0005      |
|    loss                 | 506         |
|    n_updates            | 3116        |
|    policy_gradient_loss | 0.0108      |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=948000, episode_reward=757.80 +/- 110.35
Episode length: 160.34 +/- 147.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 926      |
|    time_elapsed    | 19806    |
|    total_timesteps | 948224   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=948500, episode_reward=735.22 +/- 149.01
Episode length: 125.26 +/- 118.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 125        |
|    mean_reward          | 735        |
| time/                   |            |
|    total_timesteps      | 948500     |
| train/                  |            |
|    approx_kl            | 0.06406906 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.551     |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.12e+03   |
|    n_updates            | 3118       |
|    policy_gradient_loss | 0.0147     |
|    value_loss           | 1.77e+03   |
----------------------------------------
Eval num_timesteps=949000, episode_reward=748.56 +/- 126.46
Episode length: 92.50 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 949000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 927      |
|    time_elapsed    | 19815    |
|    total_timesteps | 949248   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=949500, episode_reward=760.30 +/- 94.90
Episode length: 118.46 +/- 102.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 118        |
|    mean_reward          | 760        |
| time/                   |            |
|    total_timesteps      | 949500     |
| train/                  |            |
|    approx_kl            | 0.03868752 |
|    clip_fraction        | 0.0838     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.383     |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.35e+03   |
|    n_updates            | 3120       |
|    policy_gradient_loss | 0.00789    |
|    value_loss           | 3.36e+03   |
----------------------------------------
Eval num_timesteps=950000, episode_reward=753.74 +/- 122.84
Episode length: 118.64 +/- 102.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 928      |
|    time_elapsed    | 19823    |
|    total_timesteps | 950272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=950500, episode_reward=738.30 +/- 183.24
Episode length: 99.84 +/- 62.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | 738         |
| time/                   |             |
|    total_timesteps      | 950500      |
| train/                  |             |
|    approx_kl            | 0.042234834 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.33       |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.64e+03    |
|    n_updates            | 3122        |
|    policy_gradient_loss | 0.0026      |
|    value_loss           | 3.96e+03    |
-----------------------------------------
Eval num_timesteps=951000, episode_reward=730.34 +/- 156.21
Episode length: 102.10 +/- 61.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 951000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 47       |
|    iterations      | 929      |
|    time_elapsed    | 19831    |
|    total_timesteps | 951296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=951500, episode_reward=747.85 +/- 101.73
Episode length: 108.10 +/- 85.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 748         |
| time/                   |             |
|    total_timesteps      | 951500      |
| train/                  |             |
|    approx_kl            | 0.016414678 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.368      |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.24e+03    |
|    n_updates            | 3123        |
|    policy_gradient_loss | 0.00818     |
|    value_loss           | 2.59e+03    |
-----------------------------------------
Eval num_timesteps=952000, episode_reward=753.66 +/- 110.13
Episode length: 109.24 +/- 84.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 930      |
|    time_elapsed    | 19839    |
|    total_timesteps | 952320   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=952500, episode_reward=746.37 +/- 136.74
Episode length: 108.60 +/- 85.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 746         |
| time/                   |             |
|    total_timesteps      | 952500      |
| train/                  |             |
|    approx_kl            | 0.061556716 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.365      |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 3125        |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 2.09e+03    |
-----------------------------------------
Eval num_timesteps=953000, episode_reward=723.55 +/- 182.57
Episode length: 115.36 +/- 103.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 953000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 931      |
|    time_elapsed    | 19848    |
|    total_timesteps | 953344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=953500, episode_reward=771.05 +/- 97.00
Episode length: 120.34 +/- 102.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 771         |
| time/                   |             |
|    total_timesteps      | 953500      |
| train/                  |             |
|    approx_kl            | 0.047676414 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.59e+03    |
|    n_updates            | 3127        |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 5.41e+03    |
-----------------------------------------
Eval num_timesteps=954000, episode_reward=761.39 +/- 85.89
Episode length: 110.88 +/- 84.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 932      |
|    time_elapsed    | 19856    |
|    total_timesteps | 954368   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=954500, episode_reward=779.91 +/- 106.80
Episode length: 114.08 +/- 87.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 780         |
| time/                   |             |
|    total_timesteps      | 954500      |
| train/                  |             |
|    approx_kl            | 0.029203946 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.391      |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.16e+03    |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=955000, episode_reward=792.22 +/- 109.58
Episode length: 91.84 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 955000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 933      |
|    time_elapsed    | 19864    |
|    total_timesteps | 955392   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=955500, episode_reward=804.90 +/- 63.19
Episode length: 109.90 +/- 84.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 805         |
| time/                   |             |
|    total_timesteps      | 955500      |
| train/                  |             |
|    approx_kl            | 0.035720915 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.467      |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0005      |
|    loss                 | 983         |
|    n_updates            | 3132        |
|    policy_gradient_loss | 0.000788    |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=956000, episode_reward=805.96 +/- 32.14
Episode length: 102.58 +/- 60.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 934      |
|    time_elapsed    | 19872    |
|    total_timesteps | 956416   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=956500, episode_reward=807.22 +/- 44.57
Episode length: 111.10 +/- 85.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 111        |
|    mean_reward          | 807        |
| time/                   |            |
|    total_timesteps      | 956500     |
| train/                  |            |
|    approx_kl            | 0.09286715 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.468     |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0005     |
|    loss                 | 866        |
|    n_updates            | 3135       |
|    policy_gradient_loss | 0.0112     |
|    value_loss           | 1.98e+03   |
----------------------------------------
Eval num_timesteps=957000, episode_reward=789.92 +/- 79.89
Episode length: 109.56 +/- 84.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 957000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 935      |
|    time_elapsed    | 19880    |
|    total_timesteps | 957440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=957500, episode_reward=795.19 +/- 60.52
Episode length: 119.98 +/- 83.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 795         |
| time/                   |             |
|    total_timesteps      | 957500      |
| train/                  |             |
|    approx_kl            | 0.031344693 |
|    clip_fraction        | 0.0923      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.58e+03    |
|    n_updates            | 3137        |
|    policy_gradient_loss | 0.0143      |
|    value_loss           | 2.67e+03    |
-----------------------------------------
Eval num_timesteps=958000, episode_reward=773.12 +/- 104.60
Episode length: 131.48 +/- 100.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 936      |
|    time_elapsed    | 19890    |
|    total_timesteps | 958464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=958500, episode_reward=802.96 +/- 39.29
Episode length: 96.56 +/- 13.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.6        |
|    mean_reward          | 803         |
| time/                   |             |
|    total_timesteps      | 958500      |
| train/                  |             |
|    approx_kl            | 0.062482275 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.364      |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0005      |
|    loss                 | 871         |
|    n_updates            | 3139        |
|    policy_gradient_loss | 0.016       |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Eval num_timesteps=959000, episode_reward=804.92 +/- 45.56
Episode length: 111.52 +/- 85.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 959000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 937      |
|    time_elapsed    | 19898    |
|    total_timesteps | 959488   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=959500, episode_reward=796.70 +/- 80.59
Episode length: 103.92 +/- 60.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 797         |
| time/                   |             |
|    total_timesteps      | 959500      |
| train/                  |             |
|    approx_kl            | 0.044455238 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.261      |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.0005      |
|    loss                 | 884         |
|    n_updates            | 3142        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=802.28 +/- 52.40
Episode length: 107.00 +/- 61.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=960500, episode_reward=784.44 +/- 78.68
Episode length: 98.34 +/- 14.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 960500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 938      |
|    time_elapsed    | 19909    |
|    total_timesteps | 960512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=961000, episode_reward=762.41 +/- 127.13
Episode length: 106.24 +/- 60.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 762        |
| time/                   |            |
|    total_timesteps      | 961000     |
| train/                  |            |
|    approx_kl            | 0.06124592 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0005     |
|    loss                 | 794        |
|    n_updates            | 3144       |
|    policy_gradient_loss | 0.00382    |
|    value_loss           | 2.1e+03    |
----------------------------------------
Eval num_timesteps=961500, episode_reward=763.84 +/- 109.95
Episode length: 108.86 +/- 61.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 961500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 939      |
|    time_elapsed    | 19917    |
|    total_timesteps | 961536   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.16
Eval num_timesteps=962000, episode_reward=749.29 +/- 96.28
Episode length: 107.74 +/- 9.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 749        |
| time/                   |            |
|    total_timesteps      | 962000     |
| train/                  |            |
|    approx_kl            | 0.04875847 |
|    clip_fraction        | 0.0962     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.327     |
|    explained_variance   | 0.878      |
|    learning_rate        | 0.0005     |
|    loss                 | 951        |
|    n_updates            | 3146       |
|    policy_gradient_loss | 0.0187     |
|    value_loss           | 2.17e+03   |
----------------------------------------
Eval num_timesteps=962500, episode_reward=717.57 +/- 120.73
Episode length: 124.76 +/- 82.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 962500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 940      |
|    time_elapsed    | 19926    |
|    total_timesteps | 962560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=963000, episode_reward=761.00 +/- 99.11
Episode length: 174.24 +/- 141.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 761         |
| time/                   |             |
|    total_timesteps      | 963000      |
| train/                  |             |
|    approx_kl            | 0.023094675 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.388      |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.04e+03    |
|    n_updates            | 3147        |
|    policy_gradient_loss | 0.0181      |
|    value_loss           | 2.09e+03    |
-----------------------------------------
Eval num_timesteps=963500, episode_reward=776.14 +/- 97.38
Episode length: 139.04 +/- 97.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 963500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 941      |
|    time_elapsed    | 19937    |
|    total_timesteps | 963584   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=964000, episode_reward=797.62 +/- 42.49
Episode length: 110.50 +/- 8.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 798         |
| time/                   |             |
|    total_timesteps      | 964000      |
| train/                  |             |
|    approx_kl            | 0.053912103 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0005      |
|    loss                 | 658         |
|    n_updates            | 3149        |
|    policy_gradient_loss | 0.00921     |
|    value_loss           | 1.49e+03    |
-----------------------------------------
Eval num_timesteps=964500, episode_reward=774.65 +/- 104.18
Episode length: 111.16 +/- 10.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 964500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 942      |
|    time_elapsed    | 19946    |
|    total_timesteps | 964608   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=965000, episode_reward=793.92 +/- 41.98
Episode length: 107.68 +/- 10.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 794        |
| time/                   |            |
|    total_timesteps      | 965000     |
| train/                  |            |
|    approx_kl            | 0.06433456 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.492     |
|    explained_variance   | 0.795      |
|    learning_rate        | 0.0005     |
|    loss                 | 982        |
|    n_updates            | 3151       |
|    policy_gradient_loss | 0.00715    |
|    value_loss           | 2.24e+03   |
----------------------------------------
Eval num_timesteps=965500, episode_reward=789.92 +/- 72.23
Episode length: 114.58 +/- 59.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 965500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 943      |
|    time_elapsed    | 19954    |
|    total_timesteps | 965632   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=966000, episode_reward=770.90 +/- 136.59
Episode length: 103.12 +/- 9.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 771         |
| time/                   |             |
|    total_timesteps      | 966000      |
| train/                  |             |
|    approx_kl            | 0.039971862 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.403      |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0005      |
|    loss                 | 949         |
|    n_updates            | 3154        |
|    policy_gradient_loss | -0.00702    |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=966500, episode_reward=796.59 +/- 59.98
Episode length: 103.56 +/- 8.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 966500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 944      |
|    time_elapsed    | 19962    |
|    total_timesteps | 966656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=967000, episode_reward=797.04 +/- 65.72
Episode length: 99.14 +/- 8.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.1        |
|    mean_reward          | 797         |
| time/                   |             |
|    total_timesteps      | 967000      |
| train/                  |             |
|    approx_kl            | 0.043483943 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.388      |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0005      |
|    loss                 | 845         |
|    n_updates            | 3156        |
|    policy_gradient_loss | 0.00681     |
|    value_loss           | 1.85e+03    |
-----------------------------------------
Eval num_timesteps=967500, episode_reward=798.42 +/- 47.42
Episode length: 97.86 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 967500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 945      |
|    time_elapsed    | 19969    |
|    total_timesteps | 967680   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=968000, episode_reward=791.36 +/- 96.90
Episode length: 97.72 +/- 8.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.7        |
|    mean_reward          | 791         |
| time/                   |             |
|    total_timesteps      | 968000      |
| train/                  |             |
|    approx_kl            | 0.057771325 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0005      |
|    loss                 | 845         |
|    n_updates            | 3159        |
|    policy_gradient_loss | 0.00184     |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=968500, episode_reward=784.24 +/- 109.48
Episode length: 105.92 +/- 60.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 968500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 946      |
|    time_elapsed    | 19977    |
|    total_timesteps | 968704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=969000, episode_reward=771.23 +/- 123.82
Episode length: 95.82 +/- 8.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.8        |
|    mean_reward          | 771         |
| time/                   |             |
|    total_timesteps      | 969000      |
| train/                  |             |
|    approx_kl            | 0.028039046 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.74e+03    |
|    n_updates            | 3160        |
|    policy_gradient_loss | 0.0223      |
|    value_loss           | 2.94e+03    |
-----------------------------------------
Eval num_timesteps=969500, episode_reward=755.67 +/- 149.53
Episode length: 94.40 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 969500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 947      |
|    time_elapsed    | 19985    |
|    total_timesteps | 969728   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=970000, episode_reward=768.18 +/- 128.56
Episode length: 96.14 +/- 9.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.1        |
|    mean_reward          | 768         |
| time/                   |             |
|    total_timesteps      | 970000      |
| train/                  |             |
|    approx_kl            | 0.048574332 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.295      |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.42e+03    |
|    n_updates            | 3163        |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 2.84e+03    |
-----------------------------------------
Eval num_timesteps=970500, episode_reward=749.03 +/- 153.03
Episode length: 103.54 +/- 61.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 970500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 948      |
|    time_elapsed    | 19992    |
|    total_timesteps | 970752   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=971000, episode_reward=792.80 +/- 78.27
Episode length: 126.46 +/- 117.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 126        |
|    mean_reward          | 793        |
| time/                   |            |
|    total_timesteps      | 971000     |
| train/                  |            |
|    approx_kl            | 0.05449909 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.315     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.03e+03   |
|    n_updates            | 3167       |
|    policy_gradient_loss | -0.00755   |
|    value_loss           | 6.55e+03   |
----------------------------------------
Eval num_timesteps=971500, episode_reward=810.72 +/- 26.87
Episode length: 93.06 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 971500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 949      |
|    time_elapsed    | 20001    |
|    total_timesteps | 971776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=972000, episode_reward=806.06 +/- 56.61
Episode length: 112.20 +/- 84.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 112        |
|    mean_reward          | 806        |
| time/                   |            |
|    total_timesteps      | 972000     |
| train/                  |            |
|    approx_kl            | 0.03540877 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0005     |
|    loss                 | 790        |
|    n_updates            | 3169       |
|    policy_gradient_loss | 0.00223    |
|    value_loss           | 1.66e+03   |
----------------------------------------
Eval num_timesteps=972500, episode_reward=794.88 +/- 91.20
Episode length: 103.36 +/- 60.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 972500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 950      |
|    time_elapsed    | 20009    |
|    total_timesteps | 972800   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.08
Eval num_timesteps=973000, episode_reward=799.60 +/- 68.50
Episode length: 112.70 +/- 84.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 800         |
| time/                   |             |
|    total_timesteps      | 973000      |
| train/                  |             |
|    approx_kl            | 0.053901337 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.398      |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0005      |
|    loss                 | 806         |
|    n_updates            | 3174        |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.27e+03    |
-----------------------------------------
Eval num_timesteps=973500, episode_reward=801.16 +/- 56.14
Episode length: 110.88 +/- 84.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 973500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 951      |
|    time_elapsed    | 20017    |
|    total_timesteps | 973824   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.15
Eval num_timesteps=974000, episode_reward=683.10 +/- 197.33
Episode length: 103.04 +/- 61.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 103        |
|    mean_reward          | 683        |
| time/                   |            |
|    total_timesteps      | 974000     |
| train/                  |            |
|    approx_kl            | 0.05605044 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.465     |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0005     |
|    loss                 | 748        |
|    n_updates            | 3177       |
|    policy_gradient_loss | 0.00151    |
|    value_loss           | 2.19e+03   |
----------------------------------------
Eval num_timesteps=974500, episode_reward=665.74 +/- 191.03
Episode length: 130.22 +/- 116.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 974500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 952      |
|    time_elapsed    | 20026    |
|    total_timesteps | 974848   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=975000, episode_reward=750.40 +/- 127.15
Episode length: 154.98 +/- 137.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 155        |
|    mean_reward          | 750        |
| time/                   |            |
|    total_timesteps      | 975000     |
| train/                  |            |
|    approx_kl            | 0.06439422 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.22e+03   |
|    n_updates            | 3180       |
|    policy_gradient_loss | 0.00763    |
|    value_loss           | 3.74e+03   |
----------------------------------------
Eval num_timesteps=975500, episode_reward=747.61 +/- 122.55
Episode length: 149.42 +/- 126.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 975500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 953      |
|    time_elapsed    | 20037    |
|    total_timesteps | 975872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=976000, episode_reward=794.14 +/- 72.15
Episode length: 133.68 +/- 115.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 794         |
| time/                   |             |
|    total_timesteps      | 976000      |
| train/                  |             |
|    approx_kl            | 0.057804495 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.11e+03    |
|    n_updates            | 3182        |
|    policy_gradient_loss | -0.00472    |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=976500, episode_reward=806.22 +/- 35.33
Episode length: 117.32 +/- 83.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 976500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 954      |
|    time_elapsed    | 20046    |
|    total_timesteps | 976896   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=977000, episode_reward=790.95 +/- 60.88
Episode length: 143.80 +/- 127.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 791         |
| time/                   |             |
|    total_timesteps      | 977000      |
| train/                  |             |
|    approx_kl            | 0.037397992 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.479      |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0005      |
|    loss                 | 896         |
|    n_updates            | 3184        |
|    policy_gradient_loss | -0.000674   |
|    value_loss           | 1.42e+03    |
-----------------------------------------
Eval num_timesteps=977500, episode_reward=794.54 +/- 69.16
Episode length: 124.76 +/- 101.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 977500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 955      |
|    time_elapsed    | 20056    |
|    total_timesteps | 977920   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=978000, episode_reward=804.59 +/- 51.98
Episode length: 126.08 +/- 100.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 805         |
| time/                   |             |
|    total_timesteps      | 978000      |
| train/                  |             |
|    approx_kl            | 0.042143073 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.937       |
|    learning_rate        | 0.0005      |
|    loss                 | 770         |
|    n_updates            | 3186        |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=978500, episode_reward=780.45 +/- 84.44
Episode length: 161.78 +/- 146.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 978500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 956      |
|    time_elapsed    | 20066    |
|    total_timesteps | 978944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=979000, episode_reward=781.05 +/- 71.85
Episode length: 143.52 +/- 113.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 781         |
| time/                   |             |
|    total_timesteps      | 979000      |
| train/                  |             |
|    approx_kl            | 0.044355605 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0005      |
|    loss                 | 716         |
|    n_updates            | 3188        |
|    policy_gradient_loss | 0.006       |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=979500, episode_reward=747.89 +/- 144.86
Episode length: 151.48 +/- 122.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 979500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 957      |
|    time_elapsed    | 20077    |
|    total_timesteps | 979968   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=980000, episode_reward=676.49 +/- 231.11
Episode length: 139.62 +/- 114.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 140        |
|    mean_reward          | 676        |
| time/                   |            |
|    total_timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.06402424 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.939      |
|    learning_rate        | 0.0005     |
|    loss                 | 662        |
|    n_updates            | 3191       |
|    policy_gradient_loss | 0.0221     |
|    value_loss           | 1.17e+03   |
----------------------------------------
Eval num_timesteps=980500, episode_reward=729.77 +/- 200.10
Episode length: 144.44 +/- 113.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 980500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 958      |
|    time_elapsed    | 20087    |
|    total_timesteps | 980992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=981000, episode_reward=591.71 +/- 204.35
Episode length: 148.44 +/- 99.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 148         |
|    mean_reward          | 592         |
| time/                   |             |
|    total_timesteps      | 981000      |
| train/                  |             |
|    approx_kl            | 0.026915673 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.672      |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.01e+03    |
|    n_updates            | 3192        |
|    policy_gradient_loss | 0.0132      |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=981500, episode_reward=647.72 +/- 180.79
Episode length: 181.26 +/- 139.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 981500   |
---------------------------------
Eval num_timesteps=982000, episode_reward=557.09 +/- 266.00
Episode length: 162.70 +/- 136.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 163      |
|    mean_reward     | 557      |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 959      |
|    time_elapsed    | 20105    |
|    total_timesteps | 982016   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=982500, episode_reward=682.87 +/- 188.54
Episode length: 125.92 +/- 20.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 683         |
| time/                   |             |
|    total_timesteps      | 982500      |
| train/                  |             |
|    approx_kl            | 0.043605007 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.62e+03    |
|    n_updates            | 3194        |
|    policy_gradient_loss | 0.0338      |
|    value_loss           | 7.24e+03    |
-----------------------------------------
Eval num_timesteps=983000, episode_reward=617.10 +/- 240.30
Episode length: 156.54 +/- 112.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 157      |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 723      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 960      |
|    time_elapsed    | 20115    |
|    total_timesteps | 983040   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=983500, episode_reward=679.78 +/- 169.65
Episode length: 193.76 +/- 145.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 194        |
|    mean_reward          | 680        |
| time/                   |            |
|    total_timesteps      | 983500     |
| train/                  |            |
|    approx_kl            | 0.04549239 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.702     |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.59e+03   |
|    n_updates            | 3197       |
|    policy_gradient_loss | -0.00958   |
|    value_loss           | 5.52e+03   |
----------------------------------------
Eval num_timesteps=984000, episode_reward=673.19 +/- 192.28
Episode length: 163.74 +/- 121.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 705      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 961      |
|    time_elapsed    | 20128    |
|    total_timesteps | 984064   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=984500, episode_reward=628.37 +/- 157.25
Episode length: 152.64 +/- 110.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 153         |
|    mean_reward          | 628         |
| time/                   |             |
|    total_timesteps      | 984500      |
| train/                  |             |
|    approx_kl            | 0.055636488 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.01e+03    |
|    n_updates            | 3199        |
|    policy_gradient_loss | 0.0191      |
|    value_loss           | 4.66e+03    |
-----------------------------------------
Eval num_timesteps=985000, episode_reward=657.69 +/- 172.53
Episode length: 154.52 +/- 110.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 679      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 962      |
|    time_elapsed    | 20139    |
|    total_timesteps | 985088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=985500, episode_reward=686.94 +/- 131.74
Episode length: 319.08 +/- 183.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 319         |
|    mean_reward          | 687         |
| time/                   |             |
|    total_timesteps      | 985500      |
| train/                  |             |
|    approx_kl            | 0.033142302 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.564      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.18e+03    |
|    n_updates            | 3200        |
|    policy_gradient_loss | 0.00872     |
|    value_loss           | 4.44e+03    |
-----------------------------------------
Eval num_timesteps=986000, episode_reward=690.07 +/- 158.91
Episode length: 283.54 +/- 175.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 986000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 674      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 963      |
|    time_elapsed    | 20160    |
|    total_timesteps | 986112   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=986500, episode_reward=620.41 +/- 188.90
Episode length: 308.04 +/- 180.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 620       |
| time/                   |           |
|    total_timesteps      | 986500    |
| train/                  |           |
|    approx_kl            | 0.0709726 |
|    clip_fraction        | 0.201     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.877    |
|    explained_variance   | 0.809     |
|    learning_rate        | 0.0005    |
|    loss                 | 662       |
|    n_updates            | 3203      |
|    policy_gradient_loss | 0.00125   |
|    value_loss           | 1.27e+03  |
---------------------------------------
Eval num_timesteps=987000, episode_reward=665.43 +/- 194.00
Episode length: 271.46 +/- 176.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 663      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 964      |
|    time_elapsed    | 20180    |
|    total_timesteps | 987136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=987500, episode_reward=628.72 +/- 158.52
Episode length: 229.80 +/- 167.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 629        |
| time/                   |            |
|    total_timesteps      | 987500     |
| train/                  |            |
|    approx_kl            | 0.03654079 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0005     |
|    loss                 | 2.04e+03   |
|    n_updates            | 3205       |
|    policy_gradient_loss | 0.00423    |
|    value_loss           | 4.55e+03   |
----------------------------------------
Eval num_timesteps=988000, episode_reward=628.60 +/- 171.05
Episode length: 292.90 +/- 190.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 988000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 663      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 965      |
|    time_elapsed    | 20198    |
|    total_timesteps | 988160   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=988500, episode_reward=725.99 +/- 135.83
Episode length: 237.06 +/- 171.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | 726         |
| time/                   |             |
|    total_timesteps      | 988500      |
| train/                  |             |
|    approx_kl            | 0.056657948 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.888      |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0005      |
|    loss                 | 740         |
|    n_updates            | 3209        |
|    policy_gradient_loss | 0.0057      |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=989000, episode_reward=722.88 +/- 118.85
Episode length: 242.50 +/- 170.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 663      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 966      |
|    time_elapsed    | 20215    |
|    total_timesteps | 989184   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=989500, episode_reward=769.20 +/- 100.78
Episode length: 165.82 +/- 145.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 769         |
| time/                   |             |
|    total_timesteps      | 989500      |
| train/                  |             |
|    approx_kl            | 0.043489195 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0005      |
|    loss                 | 609         |
|    n_updates            | 3211        |
|    policy_gradient_loss | 0.00177     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=990000, episode_reward=763.16 +/- 116.90
Episode length: 175.96 +/- 152.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 664      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 967      |
|    time_elapsed    | 20228    |
|    total_timesteps | 990208   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=990500, episode_reward=715.67 +/- 141.70
Episode length: 225.84 +/- 177.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 716         |
| time/                   |             |
|    total_timesteps      | 990500      |
| train/                  |             |
|    approx_kl            | 0.045653533 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | 656         |
|    n_updates            | 3216        |
|    policy_gradient_loss | -0.00789    |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=991000, episode_reward=739.46 +/- 139.53
Episode length: 201.92 +/- 161.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 664      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 968      |
|    time_elapsed    | 20244    |
|    total_timesteps | 991232   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=991500, episode_reward=692.30 +/- 153.32
Episode length: 256.08 +/- 184.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 692         |
| time/                   |             |
|    total_timesteps      | 991500      |
| train/                  |             |
|    approx_kl            | 0.053426653 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.0005      |
|    loss                 | 695         |
|    n_updates            | 3218        |
|    policy_gradient_loss | 0.0233      |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=992000, episode_reward=681.08 +/- 152.89
Episode length: 297.60 +/- 193.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 298      |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 661      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 969      |
|    time_elapsed    | 20263    |
|    total_timesteps | 992256   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=992500, episode_reward=710.00 +/- 134.86
Episode length: 270.54 +/- 179.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 271        |
|    mean_reward          | 710        |
| time/                   |            |
|    total_timesteps      | 992500     |
| train/                  |            |
|    approx_kl            | 0.04696857 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0005     |
|    loss                 | 607        |
|    n_updates            | 3221       |
|    policy_gradient_loss | 0.01       |
|    value_loss           | 1.07e+03   |
----------------------------------------
Eval num_timesteps=993000, episode_reward=690.37 +/- 124.49
Episode length: 306.18 +/- 190.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 658      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 970      |
|    time_elapsed    | 20283    |
|    total_timesteps | 993280   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=993500, episode_reward=676.66 +/- 152.63
Episode length: 282.06 +/- 190.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 282        |
|    mean_reward          | 677        |
| time/                   |            |
|    total_timesteps      | 993500     |
| train/                  |            |
|    approx_kl            | 0.05330394 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.839     |
|    explained_variance   | 0.923      |
|    learning_rate        | 0.0005     |
|    loss                 | 533        |
|    n_updates            | 3224       |
|    policy_gradient_loss | 0.00823    |
|    value_loss           | 1.15e+03   |
----------------------------------------
Eval num_timesteps=994000, episode_reward=677.24 +/- 151.35
Episode length: 302.50 +/- 197.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 302      |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 994000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 654      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 971      |
|    time_elapsed    | 20303    |
|    total_timesteps | 994304   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=994500, episode_reward=636.56 +/- 187.37
Episode length: 277.68 +/- 194.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 278         |
|    mean_reward          | 637         |
| time/                   |             |
|    total_timesteps      | 994500      |
| train/                  |             |
|    approx_kl            | 0.049909197 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0005      |
|    loss                 | 766         |
|    n_updates            | 3227        |
|    policy_gradient_loss | 0.01        |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=995000, episode_reward=648.14 +/- 202.10
Episode length: 265.34 +/- 195.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 654      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 972      |
|    time_elapsed    | 20322    |
|    total_timesteps | 995328   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=995500, episode_reward=562.45 +/- 318.27
Episode length: 156.08 +/- 147.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 562         |
| time/                   |             |
|    total_timesteps      | 995500      |
| train/                  |             |
|    approx_kl            | 0.053648107 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0005      |
|    loss                 | 962         |
|    n_updates            | 3229        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=996000, episode_reward=465.35 +/- 309.43
Episode length: 217.80 +/- 203.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 654      |
| time/              |          |
|    fps             | 48       |
|    iterations      | 973      |
|    time_elapsed    | 20335    |
|    total_timesteps | 996352   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=996500, episode_reward=685.83 +/- 229.29
Episode length: 198.66 +/- 164.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 686         |
| time/                   |             |
|    total_timesteps      | 996500      |
| train/                  |             |
|    approx_kl            | 0.055921037 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.544      |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.1e+03     |
|    n_updates            | 3232        |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=997000, episode_reward=693.29 +/- 189.74
Episode length: 249.82 +/- 190.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 664      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 974      |
|    time_elapsed    | 20351    |
|    total_timesteps | 997376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=997500, episode_reward=618.94 +/- 309.36
Episode length: 130.92 +/- 119.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 619         |
| time/                   |             |
|    total_timesteps      | 997500      |
| train/                  |             |
|    approx_kl            | 0.017981641 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0005      |
|    loss                 | 859         |
|    n_updates            | 3233        |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=998000, episode_reward=654.90 +/- 271.72
Episode length: 134.20 +/- 117.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 998000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 676      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 975      |
|    time_elapsed    | 20360    |
|    total_timesteps | 998400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=998500, episode_reward=660.86 +/- 271.84
Episode length: 118.50 +/- 86.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 661         |
| time/                   |             |
|    total_timesteps      | 998500      |
| train/                  |             |
|    approx_kl            | 0.037652574 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.715      |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.42e+03    |
|    n_updates            | 3235        |
|    policy_gradient_loss | 0.00368     |
|    value_loss           | 4.93e+03    |
-----------------------------------------
Eval num_timesteps=999000, episode_reward=734.36 +/- 162.67
Episode length: 180.46 +/- 161.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 698      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 976      |
|    time_elapsed    | 20371    |
|    total_timesteps | 999424   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=999500, episode_reward=779.00 +/- 84.91
Episode length: 161.46 +/- 134.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 779         |
| time/                   |             |
|    total_timesteps      | 999500      |
| train/                  |             |
|    approx_kl            | 0.035390507 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.25e+03    |
|    n_updates            | 3238        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=742.61 +/- 168.55
Episode length: 159.62 +/- 127.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 701      |
| time/              |          |
|    fps             | 49       |
|    iterations      | 977      |
|    time_elapsed    | 20383    |
|    total_timesteps | 1000448  |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-stop-3-3/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 1 due to reaching max kl: 0.07
