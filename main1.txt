/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55       |
|    ep_rew_mean      | 219      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3619     |
|    time_elapsed     | 0        |
|    total_timesteps  | 220      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48       |
|    ep_rew_mean      | 191      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 3658     |
|    time_elapsed     | 0        |
|    total_timesteps  | 384      |
----------------------------------
Eval num_timesteps=500, episode_reward=181.68 +/- 49.61
Episode length: 45.80 +/- 12.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.1     |
|    ep_rew_mean      | 231      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 12       |
|    fps              | 247      |
|    time_elapsed     | 2        |
|    total_timesteps  | 697      |
----------------------------------
Eval num_timesteps=1000, episode_reward=189.06 +/- 44.63
Episode length: 47.60 +/- 11.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.8     |
|    ep_rew_mean      | 250      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 191      |
|    time_elapsed     | 5        |
|    total_timesteps  | 1004     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.2     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 20       |
|    fps              | 247      |
|    time_elapsed     | 5        |
|    total_timesteps  | 1324     |
----------------------------------
Eval num_timesteps=1500, episode_reward=190.64 +/- 59.37
Episode length: 48.00 +/- 14.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48       |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65       |
|    ep_rew_mean      | 259      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 24       |
|    fps              | 193      |
|    time_elapsed     | 8        |
|    total_timesteps  | 1560     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.4     |
|    ep_rew_mean      | 260      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 28       |
|    fps              | 225      |
|    time_elapsed     | 8        |
|    total_timesteps  | 1830     |
----------------------------------
Eval num_timesteps=2000, episode_reward=178.66 +/- 58.35
Episode length: 45.04 +/- 14.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | 266      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 32       |
|    fps              | 204      |
|    time_elapsed     | 10       |
|    total_timesteps  | 2138     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.2     |
|    ep_rew_mean      | 260      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 36       |
|    fps              | 223      |
|    time_elapsed     | 10       |
|    total_timesteps  | 2349     |
----------------------------------
Eval num_timesteps=2500, episode_reward=188.30 +/- 56.73
Episode length: 47.48 +/- 14.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.1     |
|    ep_rew_mean      | 263      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 40       |
|    fps              | 202      |
|    time_elapsed     | 13       |
|    total_timesteps  | 2644     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.7     |
|    ep_rew_mean      | 257      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 44       |
|    fps              | 216      |
|    time_elapsed     | 13       |
|    total_timesteps  | 2848     |
----------------------------------
Eval num_timesteps=3000, episode_reward=180.38 +/- 44.56
Episode length: 45.44 +/- 11.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.4     |
|    ep_rew_mean      | 256      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 48       |
|    fps              | 199      |
|    time_elapsed     | 15       |
|    total_timesteps  | 3091     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.9     |
|    ep_rew_mean      | 258      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 52       |
|    fps              | 216      |
|    time_elapsed     | 15       |
|    total_timesteps  | 3373     |
----------------------------------
Eval num_timesteps=3500, episode_reward=190.52 +/- 48.20
Episode length: 47.98 +/- 12.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48       |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64       |
|    ep_rew_mean      | 254      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 56       |
|    fps              | 197      |
|    time_elapsed     | 18       |
|    total_timesteps  | 3582     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.1     |
|    ep_rew_mean      | 251      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 60       |
|    fps              | 208      |
|    time_elapsed     | 18       |
|    total_timesteps  | 3787     |
----------------------------------
Eval num_timesteps=4000, episode_reward=184.76 +/- 37.98
Episode length: 46.54 +/- 9.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | 255      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 64       |
|    fps              | 193      |
|    time_elapsed     | 21       |
|    total_timesteps  | 4106     |
----------------------------------
Eval num_timesteps=4500, episode_reward=170.66 +/- 34.83
Episode length: 43.00 +/- 8.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.2     |
|    ep_rew_mean      | 263      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 68       |
|    fps              | 191      |
|    time_elapsed     | 23       |
|    total_timesteps  | 4502     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.8     |
|    ep_rew_mean      | 258      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 72       |
|    fps              | 197      |
|    time_elapsed     | 23       |
|    total_timesteps  | 4664     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.9     |
|    ep_rew_mean      | 258      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 76       |
|    fps              | 208      |
|    time_elapsed     | 23       |
|    total_timesteps  | 4936     |
----------------------------------
Eval num_timesteps=5000, episode_reward=178.30 +/- 59.77
Episode length: 44.94 +/- 14.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.1     |
|    ep_rew_mean      | 255      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 80       |
|    fps              | 197      |
|    time_elapsed     | 26       |
|    total_timesteps  | 5126     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.5     |
|    ep_rew_mean      | 252      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 84       |
|    fps              | 204      |
|    time_elapsed     | 26       |
|    total_timesteps  | 5330     |
----------------------------------
Eval num_timesteps=5500, episode_reward=176.00 +/- 38.58
Episode length: 44.38 +/- 9.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | 260      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 199      |
|    time_elapsed     | 28       |
|    total_timesteps  | 5765     |
----------------------------------
Eval num_timesteps=6000, episode_reward=187.54 +/- 42.02
Episode length: 47.30 +/- 10.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.4     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 92       |
|    fps              | 193      |
|    time_elapsed     | 31       |
|    total_timesteps  | 6108     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.5     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 201      |
|    time_elapsed     | 31       |
|    total_timesteps  | 6381     |
----------------------------------
Eval num_timesteps=6500, episode_reward=173.16 +/- 38.58
Episode length: 43.66 +/- 9.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | 266      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 196      |
|    time_elapsed     | 34       |
|    total_timesteps  | 6699     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | 265      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 104      |
|    fps              | 201      |
|    time_elapsed     | 34       |
|    total_timesteps  | 6890     |
----------------------------------
Eval num_timesteps=7000, episode_reward=171.94 +/- 45.27
Episode length: 43.38 +/- 11.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.4     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 108      |
|    fps              | 197      |
|    time_elapsed     | 36       |
|    total_timesteps  | 7223     |
----------------------------------
Eval num_timesteps=7500, episode_reward=192.40 +/- 62.13
Episode length: 48.46 +/- 15.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.5     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 112      |
|    fps              | 191      |
|    time_elapsed     | 39       |
|    total_timesteps  | 7546     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.9     |
|    ep_rew_mean      | 266      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 116      |
|    fps              | 194      |
|    time_elapsed     | 39       |
|    total_timesteps  | 7697     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.1     |
|    ep_rew_mean      | 263      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 120      |
|    fps              | 200      |
|    time_elapsed     | 39       |
|    total_timesteps  | 7935     |
----------------------------------
Eval num_timesteps=8000, episode_reward=185.36 +/- 61.10
Episode length: 46.70 +/- 15.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 124      |
|    fps              | 195      |
|    time_elapsed     | 42       |
|    total_timesteps  | 8301     |
----------------------------------
Eval num_timesteps=8500, episode_reward=176.84 +/- 40.50
Episode length: 44.52 +/- 10.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.3     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 128      |
|    fps              | 189      |
|    time_elapsed     | 45       |
|    total_timesteps  | 8562     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | 266      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 132      |
|    fps              | 195      |
|    time_elapsed     | 45       |
|    total_timesteps  | 8835     |
----------------------------------
Eval num_timesteps=9000, episode_reward=173.74 +/- 37.12
Episode length: 43.82 +/- 9.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 136      |
|    fps              | 191      |
|    time_elapsed     | 48       |
|    total_timesteps  | 9215     |
----------------------------------
Eval num_timesteps=9500, episode_reward=175.24 +/- 59.48
Episode length: 44.16 +/- 14.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.8     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 140      |
|    fps              | 188      |
|    time_elapsed     | 50       |
|    total_timesteps  | 9529     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.1     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 144      |
|    fps              | 192      |
|    time_elapsed     | 50       |
|    total_timesteps  | 9762     |
----------------------------------
Eval num_timesteps=10000, episode_reward=176.78 +/- 47.02
Episode length: 44.60 +/- 11.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 148      |
|    fps              | 189      |
|    time_elapsed     | 53       |
|    total_timesteps  | 10042    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 193      |
|    time_elapsed     | 53       |
|    total_timesteps  | 10299    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.6     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 156      |
|    fps              | 196      |
|    time_elapsed     | 53       |
|    total_timesteps  | 10445    |
----------------------------------
Eval num_timesteps=10500, episode_reward=166.54 +/- 44.69
Episode length: 42.02 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 160      |
|    fps              | 191      |
|    time_elapsed     | 55       |
|    total_timesteps  | 10738    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 164      |
|    fps              | 194      |
|    time_elapsed     | 55       |
|    total_timesteps  | 10877    |
----------------------------------
Eval num_timesteps=11000, episode_reward=182.46 +/- 65.40
Episode length: 46.00 +/- 16.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.3     |
|    ep_rew_mean      | 263      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 168      |
|    fps              | 189      |
|    time_elapsed     | 58       |
|    total_timesteps  | 11131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.9     |
|    ep_rew_mean      | 270      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 172      |
|    fps              | 194      |
|    time_elapsed     | 58       |
|    total_timesteps  | 11455    |
----------------------------------
Eval num_timesteps=11500, episode_reward=165.86 +/- 47.64
Episode length: 41.84 +/- 11.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | 267      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 176      |
|    fps              | 189      |
|    time_elapsed     | 61       |
|    total_timesteps  | 11659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.1     |
|    ep_rew_mean      | 271      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 180      |
|    fps              | 194      |
|    time_elapsed     | 61       |
|    total_timesteps  | 11938    |
----------------------------------
Eval num_timesteps=12000, episode_reward=175.38 +/- 49.72
Episode length: 44.20 +/- 12.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 184      |
|    fps              | 191      |
|    time_elapsed     | 64       |
|    total_timesteps  | 12258    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.5     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 188      |
|    fps              | 193      |
|    time_elapsed     | 64       |
|    total_timesteps  | 12413    |
----------------------------------
Eval num_timesteps=12500, episode_reward=178.22 +/- 59.02
Episode length: 44.94 +/- 14.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.8     |
|    ep_rew_mean      | 262      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 192      |
|    fps              | 190      |
|    time_elapsed     | 66       |
|    total_timesteps  | 12687    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | 261      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 196      |
|    fps              | 193      |
|    time_elapsed     | 66       |
|    total_timesteps  | 12934    |
----------------------------------
Eval num_timesteps=13000, episode_reward=176.66 +/- 48.47
Episode length: 44.56 +/- 12.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.2     |
|    ep_rew_mean      | 259      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 200      |
|    fps              | 190      |
|    time_elapsed     | 69       |
|    total_timesteps  | 13224    |
----------------------------------
Eval num_timesteps=13500, episode_reward=180.42 +/- 44.60
Episode length: 45.50 +/- 11.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | 266      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 204      |
|    fps              | 188      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13586    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.3     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 208      |
|    fps              | 191      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13857    |
----------------------------------
Eval num_timesteps=14000, episode_reward=187.60 +/- 53.83
Episode length: 47.32 +/- 13.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.3     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 188      |
|    time_elapsed     | 75       |
|    total_timesteps  | 14174    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.8     |
|    ep_rew_mean      | 270      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 216      |
|    fps              | 192      |
|    time_elapsed     | 75       |
|    total_timesteps  | 14476    |
----------------------------------
Eval num_timesteps=14500, episode_reward=178.62 +/- 62.70
Episode length: 44.96 +/- 15.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.4     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 189      |
|    time_elapsed     | 78       |
|    total_timesteps  | 14775    |
----------------------------------
Eval num_timesteps=15000, episode_reward=177.32 +/- 41.08
Episode length: 44.70 +/- 10.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.6     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 186      |
|    time_elapsed     | 80       |
|    total_timesteps  | 15061    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.8     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 191      |
|    time_elapsed     | 80       |
|    total_timesteps  | 15447    |
----------------------------------
Eval num_timesteps=15500, episode_reward=188.48 +/- 57.02
Episode length: 47.56 +/- 14.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.5     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 232      |
|    fps              | 187      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15686    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.3     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 236      |
|    fps              | 190      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15942    |
----------------------------------
Eval num_timesteps=16000, episode_reward=179.16 +/- 46.63
Episode length: 45.16 +/- 11.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 240      |
|    fps              | 188      |
|    time_elapsed     | 86       |
|    total_timesteps  | 16269    |
----------------------------------
Eval num_timesteps=16500, episode_reward=184.34 +/- 52.79
Episode length: 46.46 +/- 13.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 244      |
|    fps              | 187      |
|    time_elapsed     | 89       |
|    total_timesteps  | 16699    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 248      |
|    fps              | 190      |
|    time_elapsed     | 89       |
|    total_timesteps  | 16979    |
----------------------------------
Eval num_timesteps=17000, episode_reward=178.76 +/- 39.94
Episode length: 45.08 +/- 9.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.7     |
|    ep_rew_mean      | 277      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 252      |
|    fps              | 187      |
|    time_elapsed     | 91       |
|    total_timesteps  | 17269    |
----------------------------------
Eval num_timesteps=17500, episode_reward=183.26 +/- 50.10
Episode length: 46.28 +/- 12.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71       |
|    ep_rew_mean      | 283      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 256      |
|    fps              | 185      |
|    time_elapsed     | 94       |
|    total_timesteps  | 17548    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 260      |
|    fps              | 187      |
|    time_elapsed     | 94       |
|    total_timesteps  | 17747    |
----------------------------------
Eval num_timesteps=18000, episode_reward=174.30 +/- 47.15
Episode length: 43.94 +/- 11.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.4     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 264      |
|    fps              | 184      |
|    time_elapsed     | 97       |
|    total_timesteps  | 18013    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.7     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 268      |
|    fps              | 187      |
|    time_elapsed     | 97       |
|    total_timesteps  | 18299    |
----------------------------------
Eval num_timesteps=18500, episode_reward=194.94 +/- 56.10
Episode length: 49.12 +/- 14.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18500    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.9     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 184      |
|    time_elapsed     | 100      |
|    total_timesteps  | 18546    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.8     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 276      |
|    fps              | 186      |
|    time_elapsed     | 100      |
|    total_timesteps  | 18744    |
----------------------------------
Eval num_timesteps=19000, episode_reward=176.12 +/- 46.87
Episode length: 44.40 +/- 11.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.9     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 280      |
|    fps              | 184      |
|    time_elapsed     | 103      |
|    total_timesteps  | 19024    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.3     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 186      |
|    time_elapsed     | 103      |
|    total_timesteps  | 19289    |
----------------------------------
Eval num_timesteps=19500, episode_reward=181.50 +/- 36.73
Episode length: 45.68 +/- 9.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 286      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 288      |
|    fps              | 184      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19608    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 292      |
|    fps              | 186      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19806    |
----------------------------------
Eval num_timesteps=20000, episode_reward=179.96 +/- 43.81
Episode length: 45.40 +/- 10.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.3     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 296      |
|    fps              | 184      |
|    time_elapsed     | 108      |
|    total_timesteps  | 20065    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.7     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 300      |
|    fps              | 186      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20292    |
----------------------------------
Eval num_timesteps=20500, episode_reward=183.52 +/- 46.53
Episode length: 46.28 +/- 11.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | 277      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 304      |
|    fps              | 183      |
|    time_elapsed     | 112      |
|    total_timesteps  | 20541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 278      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 308      |
|    fps              | 186      |
|    time_elapsed     | 112      |
|    total_timesteps  | 20857    |
----------------------------------
Eval num_timesteps=21000, episode_reward=179.72 +/- 49.88
Episode length: 45.32 +/- 12.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 312      |
|    fps              | 183      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.9     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 316      |
|    fps              | 185      |
|    time_elapsed     | 115      |
|    total_timesteps  | 21365    |
----------------------------------
Eval num_timesteps=21500, episode_reward=177.68 +/- 41.26
Episode length: 44.84 +/- 10.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 320      |
|    fps              | 183      |
|    time_elapsed     | 117      |
|    total_timesteps  | 21640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.6     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 324      |
|    fps              | 186      |
|    time_elapsed     | 117      |
|    total_timesteps  | 21920    |
----------------------------------
Eval num_timesteps=22000, episode_reward=179.30 +/- 55.45
Episode length: 45.26 +/- 13.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | 271      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 328      |
|    fps              | 184      |
|    time_elapsed     | 120      |
|    total_timesteps  | 22272    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.1     |
|    ep_rew_mean      | 271      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 332      |
|    fps              | 186      |
|    time_elapsed     | 120      |
|    total_timesteps  | 22494    |
----------------------------------
Eval num_timesteps=22500, episode_reward=184.44 +/- 45.18
Episode length: 46.52 +/- 11.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.2     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 336      |
|    fps              | 185      |
|    time_elapsed     | 123      |
|    total_timesteps  | 22861    |
----------------------------------
Eval num_timesteps=23000, episode_reward=185.76 +/- 49.14
Episode length: 46.94 +/- 12.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.9     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 340      |
|    fps              | 183      |
|    time_elapsed     | 126      |
|    total_timesteps  | 23156    |
----------------------------------
Eval num_timesteps=23500, episode_reward=173.24 +/- 46.93
Episode length: 43.66 +/- 11.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | 271      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 344      |
|    fps              | 182      |
|    time_elapsed     | 128      |
|    total_timesteps  | 23514    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.1     |
|    ep_rew_mean      | 267      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 348      |
|    fps              | 183      |
|    time_elapsed     | 128      |
|    total_timesteps  | 23691    |
----------------------------------
Eval num_timesteps=24000, episode_reward=178.88 +/- 47.36
Episode length: 45.10 +/- 11.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 182      |
|    time_elapsed     | 131      |
|    total_timesteps  | 24039    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.9     |
|    ep_rew_mean      | 266      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 356      |
|    fps              | 184      |
|    time_elapsed     | 131      |
|    total_timesteps  | 24238    |
----------------------------------
Eval num_timesteps=24500, episode_reward=176.20 +/- 51.33
Episode length: 44.36 +/- 12.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 183      |
|    time_elapsed     | 134      |
|    total_timesteps  | 24699    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.6     |
|    ep_rew_mean      | 277      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 364      |
|    fps              | 185      |
|    time_elapsed     | 134      |
|    total_timesteps  | 24972    |
----------------------------------
Eval num_timesteps=25000, episode_reward=187.06 +/- 52.98
Episode length: 47.14 +/- 13.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.2     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 368      |
|    fps              | 183      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25221    |
----------------------------------
Eval num_timesteps=25500, episode_reward=173.56 +/- 42.41
Episode length: 43.80 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 279      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 372      |
|    fps              | 182      |
|    time_elapsed     | 140      |
|    total_timesteps  | 25549    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 376      |
|    fps              | 185      |
|    time_elapsed     | 140      |
|    total_timesteps  | 25936    |
----------------------------------
Eval num_timesteps=26000, episode_reward=176.66 +/- 42.58
Episode length: 44.44 +/- 10.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 380      |
|    fps              | 183      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26251    |
----------------------------------
Eval num_timesteps=26500, episode_reward=173.38 +/- 51.47
Episode length: 43.78 +/- 12.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 290      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 384      |
|    fps              | 182      |
|    time_elapsed     | 145      |
|    total_timesteps  | 26588    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 388      |
|    fps              | 185      |
|    time_elapsed     | 145      |
|    total_timesteps  | 26941    |
----------------------------------
Eval num_timesteps=27000, episode_reward=173.44 +/- 42.84
Episode length: 43.70 +/- 10.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 392      |
|    fps              | 184      |
|    time_elapsed     | 148      |
|    total_timesteps  | 27413    |
----------------------------------
Eval num_timesteps=27500, episode_reward=182.02 +/- 42.52
Episode length: 45.90 +/- 10.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 396      |
|    fps              | 183      |
|    time_elapsed     | 150      |
|    total_timesteps  | 27733    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 400      |
|    fps              | 185      |
|    time_elapsed     | 151      |
|    total_timesteps  | 27945    |
----------------------------------
Eval num_timesteps=28000, episode_reward=179.18 +/- 52.97
Episode length: 45.16 +/- 13.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 404      |
|    fps              | 183      |
|    time_elapsed     | 153      |
|    total_timesteps  | 28232    |
----------------------------------
Eval num_timesteps=28500, episode_reward=183.92 +/- 51.04
Episode length: 46.38 +/- 12.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 408      |
|    fps              | 182      |
|    time_elapsed     | 156      |
|    total_timesteps  | 28569    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 412      |
|    fps              | 184      |
|    time_elapsed     | 156      |
|    total_timesteps  | 28831    |
----------------------------------
Eval num_timesteps=29000, episode_reward=167.54 +/- 42.02
Episode length: 42.20 +/- 10.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 416      |
|    fps              | 183      |
|    time_elapsed     | 159      |
|    total_timesteps  | 29207    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 420      |
|    fps              | 184      |
|    time_elapsed     | 159      |
|    total_timesteps  | 29412    |
----------------------------------
Eval num_timesteps=29500, episode_reward=178.14 +/- 46.35
Episode length: 44.94 +/- 11.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 424      |
|    fps              | 183      |
|    time_elapsed     | 162      |
|    total_timesteps  | 29752    |
----------------------------------
Eval num_timesteps=30000, episode_reward=178.78 +/- 47.29
Episode length: 45.10 +/- 11.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 428      |
|    fps              | 182      |
|    time_elapsed     | 164      |
|    total_timesteps  | 30107    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 432      |
|    fps              | 184      |
|    time_elapsed     | 165      |
|    total_timesteps  | 30434    |
----------------------------------
Eval num_timesteps=30500, episode_reward=180.20 +/- 43.98
Episode length: 45.42 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 436      |
|    fps              | 183      |
|    time_elapsed     | 167      |
|    total_timesteps  | 30763    |
----------------------------------
Eval num_timesteps=31000, episode_reward=171.20 +/- 47.56
Episode length: 43.26 +/- 11.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 440      |
|    fps              | 182      |
|    time_elapsed     | 170      |
|    total_timesteps  | 31063    |
----------------------------------
Eval num_timesteps=31500, episode_reward=193.34 +/- 61.45
Episode length: 48.70 +/- 15.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 444      |
|    fps              | 181      |
|    time_elapsed     | 173      |
|    total_timesteps  | 31525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 448      |
|    fps              | 183      |
|    time_elapsed     | 173      |
|    total_timesteps  | 31847    |
----------------------------------
Eval num_timesteps=32000, episode_reward=177.50 +/- 44.05
Episode length: 44.82 +/- 11.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 452      |
|    fps              | 182      |
|    time_elapsed     | 176      |
|    total_timesteps  | 32147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 456      |
|    fps              | 183      |
|    time_elapsed     | 176      |
|    total_timesteps  | 32370    |
----------------------------------
Eval num_timesteps=32500, episode_reward=178.28 +/- 43.60
Episode length: 44.96 +/- 10.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 460      |
|    fps              | 182      |
|    time_elapsed     | 178      |
|    total_timesteps  | 32629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 464      |
|    fps              | 184      |
|    time_elapsed     | 178      |
|    total_timesteps  | 32924    |
----------------------------------
Eval num_timesteps=33000, episode_reward=170.40 +/- 45.45
Episode length: 43.02 +/- 11.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 468      |
|    fps              | 182      |
|    time_elapsed     | 181      |
|    total_timesteps  | 33156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 472      |
|    fps              | 184      |
|    time_elapsed     | 181      |
|    total_timesteps  | 33472    |
----------------------------------
Eval num_timesteps=33500, episode_reward=168.44 +/- 51.53
Episode length: 42.52 +/- 12.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 476      |
|    fps              | 182      |
|    time_elapsed     | 184      |
|    total_timesteps  | 33701    |
----------------------------------
Eval num_timesteps=34000, episode_reward=179.84 +/- 44.81
Episode length: 45.28 +/- 11.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 480      |
|    fps              | 181      |
|    time_elapsed     | 186      |
|    total_timesteps  | 34016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 484      |
|    fps              | 183      |
|    time_elapsed     | 187      |
|    total_timesteps  | 34328    |
----------------------------------
Eval num_timesteps=34500, episode_reward=170.96 +/- 41.81
Episode length: 43.12 +/- 10.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 488      |
|    fps              | 182      |
|    time_elapsed     | 189      |
|    total_timesteps  | 34732    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 492      |
|    fps              | 184      |
|    time_elapsed     | 189      |
|    total_timesteps  | 34966    |
----------------------------------
Eval num_timesteps=35000, episode_reward=173.92 +/- 48.30
Episode length: 43.88 +/- 12.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 496      |
|    fps              | 182      |
|    time_elapsed     | 192      |
|    total_timesteps  | 35220    |
----------------------------------
Eval num_timesteps=35500, episode_reward=175.90 +/- 52.05
Episode length: 44.38 +/- 13.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 500      |
|    fps              | 182      |
|    time_elapsed     | 195      |
|    total_timesteps  | 35612    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 504      |
|    fps              | 183      |
|    time_elapsed     | 195      |
|    total_timesteps  | 35935    |
----------------------------------
Eval num_timesteps=36000, episode_reward=171.92 +/- 42.13
Episode length: 43.34 +/- 10.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 508      |
|    fps              | 182      |
|    time_elapsed     | 198      |
|    total_timesteps  | 36241    |
----------------------------------
Eval num_timesteps=36500, episode_reward=183.84 +/- 45.06
Episode length: 46.28 +/- 11.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 512      |
|    fps              | 181      |
|    time_elapsed     | 200      |
|    total_timesteps  | 36512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 516      |
|    fps              | 183      |
|    time_elapsed     | 201      |
|    total_timesteps  | 36851    |
----------------------------------
Eval num_timesteps=37000, episode_reward=182.92 +/- 46.10
Episode length: 46.10 +/- 11.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 520      |
|    fps              | 183      |
|    time_elapsed     | 203      |
|    total_timesteps  | 37430    |
----------------------------------
Eval num_timesteps=37500, episode_reward=177.24 +/- 48.60
Episode length: 44.72 +/- 12.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 524      |
|    fps              | 182      |
|    time_elapsed     | 206      |
|    total_timesteps  | 37685    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 528      |
|    fps              | 183      |
|    time_elapsed     | 206      |
|    total_timesteps  | 37920    |
----------------------------------
Eval num_timesteps=38000, episode_reward=186.74 +/- 75.70
Episode length: 47.00 +/- 18.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 532      |
|    fps              | 182      |
|    time_elapsed     | 209      |
|    total_timesteps  | 38147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 536      |
|    fps              | 183      |
|    time_elapsed     | 209      |
|    total_timesteps  | 38451    |
----------------------------------
Eval num_timesteps=38500, episode_reward=182.84 +/- 39.14
Episode length: 46.08 +/- 9.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 540      |
|    fps              | 182      |
|    time_elapsed     | 211      |
|    total_timesteps  | 38683    |
----------------------------------
Eval num_timesteps=39000, episode_reward=183.40 +/- 50.61
Episode length: 46.22 +/- 12.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 544      |
|    fps              | 182      |
|    time_elapsed     | 214      |
|    total_timesteps  | 39100    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 548      |
|    fps              | 182      |
|    time_elapsed     | 214      |
|    total_timesteps  | 39322    |
----------------------------------
Eval num_timesteps=39500, episode_reward=176.40 +/- 48.11
Episode length: 44.42 +/- 12.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 552      |
|    fps              | 182      |
|    time_elapsed     | 217      |
|    total_timesteps  | 39610    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 556      |
|    fps              | 182      |
|    time_elapsed     | 217      |
|    total_timesteps  | 39790    |
----------------------------------
Eval num_timesteps=40000, episode_reward=167.86 +/- 39.06
Episode length: 42.38 +/- 9.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 560      |
|    fps              | 181      |
|    time_elapsed     | 220      |
|    total_timesteps  | 40103    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.15     |
|    n_updates        | 25       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 564      |
|    fps              | 182      |
|    time_elapsed     | 220      |
|    total_timesteps  | 40405    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 101      |
----------------------------------
Eval num_timesteps=40500, episode_reward=159.58 +/- 41.37
Episode length: 40.26 +/- 10.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.3     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0409   |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 568      |
|    fps              | 182      |
|    time_elapsed     | 223      |
|    total_timesteps  | 40753    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 188      |
----------------------------------
Eval num_timesteps=41000, episode_reward=175.12 +/- 53.14
Episode length: 44.26 +/- 13.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00315  |
|    n_updates        | 249      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 572      |
|    fps              | 181      |
|    time_elapsed     | 226      |
|    total_timesteps  | 41070    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0558   |
|    n_updates        | 267      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 576      |
|    fps              | 182      |
|    time_elapsed     | 227      |
|    total_timesteps  | 41367    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00316  |
|    n_updates        | 341      |
----------------------------------
Eval num_timesteps=41500, episode_reward=191.38 +/- 58.81
Episode length: 48.22 +/- 14.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00858  |
|    n_updates        | 374      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 580      |
|    fps              | 180      |
|    time_elapsed     | 230      |
|    total_timesteps  | 41629    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 407      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 584      |
|    fps              | 181      |
|    time_elapsed     | 230      |
|    total_timesteps  | 41962    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00152  |
|    n_updates        | 490      |
----------------------------------
Eval num_timesteps=42000, episode_reward=169.40 +/- 45.17
Episode length: 42.76 +/- 11.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00238  |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 588      |
|    fps              | 181      |
|    time_elapsed     | 233      |
|    total_timesteps  | 42268    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0018   |
|    n_updates        | 566      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 592      |
|    fps              | 181      |
|    time_elapsed     | 233      |
|    total_timesteps  | 42421    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00251  |
|    n_updates        | 605      |
----------------------------------
Eval num_timesteps=42500, episode_reward=177.94 +/- 43.87
Episode length: 44.90 +/- 10.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00777  |
|    n_updates        | 624      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 596      |
|    fps              | 180      |
|    time_elapsed     | 236      |
|    total_timesteps  | 42799    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 699      |
----------------------------------
Eval num_timesteps=43000, episode_reward=160.14 +/- 53.74
Episode length: 40.46 +/- 13.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00272  |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 600      |
|    fps              | 179      |
|    time_elapsed     | 239      |
|    total_timesteps  | 43010    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00122  |
|    n_updates        | 752      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.2     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 604      |
|    fps              | 180      |
|    time_elapsed     | 239      |
|    total_timesteps  | 43256    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 813      |
----------------------------------
Eval num_timesteps=43500, episode_reward=156.34 +/- 58.82
Episode length: 39.50 +/- 14.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.5     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0676   |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 608      |
|    fps              | 179      |
|    time_elapsed     | 242      |
|    total_timesteps  | 43572    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000974 |
|    n_updates        | 892      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 612      |
|    fps              | 180      |
|    time_elapsed     | 242      |
|    total_timesteps  | 43907    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00279  |
|    n_updates        | 976      |
----------------------------------
Eval num_timesteps=44000, episode_reward=165.60 +/- 54.07
Episode length: 41.78 +/- 13.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 999      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 616      |
|    fps              | 180      |
|    time_elapsed     | 245      |
|    total_timesteps  | 44245    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00164  |
|    n_updates        | 1061     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.3     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 620      |
|    fps              | 180      |
|    time_elapsed     | 245      |
|    total_timesteps  | 44460    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00217  |
|    n_updates        | 1114     |
----------------------------------
Eval num_timesteps=44500, episode_reward=162.36 +/- 39.60
Episode length: 41.02 +/- 9.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00119  |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.6     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 624      |
|    fps              | 179      |
|    time_elapsed     | 248      |
|    total_timesteps  | 44745    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 1186     |
----------------------------------
Eval num_timesteps=45000, episode_reward=185.58 +/- 54.47
Episode length: 46.72 +/- 13.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0569   |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 628      |
|    fps              | 178      |
|    time_elapsed     | 251      |
|    total_timesteps  | 45041    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 1260     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71       |
|    ep_rew_mean      | 282      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 632      |
|    fps              | 179      |
|    time_elapsed     | 251      |
|    total_timesteps  | 45248    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00199  |
|    n_updates        | 1311     |
----------------------------------
Eval num_timesteps=45500, episode_reward=183.52 +/- 49.74
Episode length: 46.28 +/- 12.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00224  |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.1     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 636      |
|    fps              | 178      |
|    time_elapsed     | 255      |
|    total_timesteps  | 45558    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00348  |
|    n_updates        | 1389     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 640      |
|    fps              | 179      |
|    time_elapsed     | 255      |
|    total_timesteps  | 45869    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00682  |
|    n_updates        | 1467     |
----------------------------------
Eval num_timesteps=46000, episode_reward=181.42 +/- 55.40
Episode length: 45.68 +/- 13.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 644      |
|    fps              | 179      |
|    time_elapsed     | 258      |
|    total_timesteps  | 46343    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000992 |
|    n_updates        | 1585     |
----------------------------------
Eval num_timesteps=46500, episode_reward=182.48 +/- 42.74
Episode length: 46.06 +/- 10.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00158  |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 648      |
|    fps              | 178      |
|    time_elapsed     | 261      |
|    total_timesteps  | 46612    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 1652     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.7     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 652      |
|    fps              | 178      |
|    time_elapsed     | 261      |
|    total_timesteps  | 46878    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00779  |
|    n_updates        | 1719     |
----------------------------------
Eval num_timesteps=47000, episode_reward=173.20 +/- 50.31
Episode length: 43.66 +/- 12.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00602  |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 656      |
|    fps              | 177      |
|    time_elapsed     | 264      |
|    total_timesteps  | 47098    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000733 |
|    n_updates        | 1774     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 660      |
|    fps              | 178      |
|    time_elapsed     | 264      |
|    total_timesteps  | 47328    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00847  |
|    n_updates        | 1831     |
----------------------------------
Eval num_timesteps=47500, episode_reward=178.70 +/- 44.76
Episode length: 44.96 +/- 11.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 664      |
|    fps              | 177      |
|    time_elapsed     | 267      |
|    total_timesteps  | 47587    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00359  |
|    n_updates        | 1896     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.6     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 668      |
|    fps              | 178      |
|    time_elapsed     | 268      |
|    total_timesteps  | 47913    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000771 |
|    n_updates        | 1978     |
----------------------------------
Eval num_timesteps=48000, episode_reward=174.70 +/- 59.43
Episode length: 44.04 +/- 14.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0373   |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.3     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 672      |
|    fps              | 177      |
|    time_elapsed     | 271      |
|    total_timesteps  | 48198    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0255   |
|    n_updates        | 2049     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 676      |
|    fps              | 178      |
|    time_elapsed     | 271      |
|    total_timesteps  | 48491    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000746 |
|    n_updates        | 2122     |
----------------------------------
Eval num_timesteps=48500, episode_reward=226.56 +/- 68.89
Episode length: 57.02 +/- 17.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57       |
|    mean_reward      | 227      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00636  |
|    n_updates        | 2124     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.5     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 680      |
|    fps              | 176      |
|    time_elapsed     | 275      |
|    total_timesteps  | 48684    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00261  |
|    n_updates        | 2170     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 684      |
|    fps              | 177      |
|    time_elapsed     | 275      |
|    total_timesteps  | 48959    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0577   |
|    n_updates        | 2239     |
----------------------------------
Eval num_timesteps=49000, episode_reward=183.74 +/- 78.04
Episode length: 46.34 +/- 19.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0415   |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 688      |
|    fps              | 176      |
|    time_elapsed     | 278      |
|    total_timesteps  | 49203    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00136  |
|    n_updates        | 2300     |
----------------------------------
Eval num_timesteps=49500, episode_reward=174.96 +/- 56.08
Episode length: 44.08 +/- 14.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 692      |
|    fps              | 176      |
|    time_elapsed     | 281      |
|    total_timesteps  | 49754    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0018   |
|    n_updates        | 2438     |
----------------------------------
Eval num_timesteps=50000, episode_reward=181.28 +/- 45.50
Episode length: 45.62 +/- 11.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 696      |
|    fps              | 175      |
|    time_elapsed     | 284      |
|    total_timesteps  | 50074    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.274    |
|    n_updates        | 2518     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.2     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 700      |
|    fps              | 176      |
|    time_elapsed     | 285      |
|    total_timesteps  | 50331    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.222    |
|    n_updates        | 2582     |
----------------------------------
Eval num_timesteps=50500, episode_reward=156.86 +/- 46.07
Episode length: 39.60 +/- 11.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.6     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0852   |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 704      |
|    fps              | 175      |
|    time_elapsed     | 287      |
|    total_timesteps  | 50569    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0903   |
|    n_updates        | 2642     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 708      |
|    fps              | 176      |
|    time_elapsed     | 287      |
|    total_timesteps  | 50810    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0656   |
|    n_updates        | 2702     |
----------------------------------
Eval num_timesteps=51000, episode_reward=175.68 +/- 64.75
Episode length: 44.28 +/- 16.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.167    |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 712      |
|    fps              | 175      |
|    time_elapsed     | 291      |
|    total_timesteps  | 51023    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0768   |
|    n_updates        | 2755     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.6     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 716      |
|    fps              | 176      |
|    time_elapsed     | 291      |
|    total_timesteps  | 51401    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 2850     |
----------------------------------
Eval num_timesteps=51500, episode_reward=201.52 +/- 53.42
Episode length: 50.78 +/- 13.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.8     |
|    mean_reward      | 202      |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00434  |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.7     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 720      |
|    fps              | 175      |
|    time_elapsed     | 294      |
|    total_timesteps  | 51625    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 2906     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 724      |
|    fps              | 175      |
|    time_elapsed     | 295      |
|    total_timesteps  | 51927    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00349  |
|    n_updates        | 2981     |
----------------------------------
Eval num_timesteps=52000, episode_reward=197.44 +/- 61.99
Episode length: 49.70 +/- 15.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.142    |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 728      |
|    fps              | 175      |
|    time_elapsed     | 298      |
|    total_timesteps  | 52261    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0838   |
|    n_updates        | 3065     |
----------------------------------
Eval num_timesteps=52500, episode_reward=157.50 +/- 53.57
Episode length: 39.74 +/- 13.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.212    |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 732      |
|    fps              | 174      |
|    time_elapsed     | 301      |
|    total_timesteps  | 52581    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00215  |
|    n_updates        | 3145     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 736      |
|    fps              | 175      |
|    time_elapsed     | 301      |
|    total_timesteps  | 52806    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 3201     |
----------------------------------
Eval num_timesteps=53000, episode_reward=197.06 +/- 58.11
Episode length: 49.60 +/- 14.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00299  |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 740      |
|    fps              | 174      |
|    time_elapsed     | 304      |
|    total_timesteps  | 53117    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00605  |
|    n_updates        | 3279     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.4     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 744      |
|    fps              | 174      |
|    time_elapsed     | 305      |
|    total_timesteps  | 53384    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00345  |
|    n_updates        | 3345     |
----------------------------------
Eval num_timesteps=53500, episode_reward=178.34 +/- 41.42
Episode length: 44.92 +/- 10.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00327  |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 748      |
|    fps              | 174      |
|    time_elapsed     | 308      |
|    total_timesteps  | 53899    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00251  |
|    n_updates        | 3474     |
----------------------------------
Eval num_timesteps=54000, episode_reward=175.84 +/- 36.16
Episode length: 44.36 +/- 9.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00607  |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 752      |
|    fps              | 174      |
|    time_elapsed     | 311      |
|    total_timesteps  | 54329    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 3582     |
----------------------------------
Eval num_timesteps=54500, episode_reward=243.78 +/- 150.88
Episode length: 61.34 +/- 37.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.3     |
|    mean_reward      | 244      |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00241  |
|    n_updates        | 3624     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 756      |
|    fps              | 172      |
|    time_elapsed     | 315      |
|    total_timesteps  | 54609    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0482   |
|    n_updates        | 3652     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 760      |
|    fps              | 173      |
|    time_elapsed     | 315      |
|    total_timesteps  | 54849    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00194  |
|    n_updates        | 3712     |
----------------------------------
Eval num_timesteps=55000, episode_reward=187.16 +/- 38.27
Episode length: 47.12 +/- 9.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 764      |
|    fps              | 172      |
|    time_elapsed     | 319      |
|    total_timesteps  | 55178    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.218    |
|    n_updates        | 3794     |
----------------------------------
Eval num_timesteps=55500, episode_reward=185.16 +/- 73.10
Episode length: 46.70 +/- 18.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 768      |
|    fps              | 172      |
|    time_elapsed     | 322      |
|    total_timesteps  | 55547    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.19     |
|    n_updates        | 3886     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 772      |
|    fps              | 173      |
|    time_elapsed     | 322      |
|    total_timesteps  | 55861    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 3965     |
----------------------------------
Eval num_timesteps=56000, episode_reward=183.00 +/- 44.77
Episode length: 46.18 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00563  |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 776      |
|    fps              | 172      |
|    time_elapsed     | 325      |
|    total_timesteps  | 56068    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0577   |
|    n_updates        | 4016     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 780      |
|    fps              | 172      |
|    time_elapsed     | 326      |
|    total_timesteps  | 56350    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00817  |
|    n_updates        | 4087     |
----------------------------------
Eval num_timesteps=56500, episode_reward=186.16 +/- 59.87
Episode length: 46.92 +/- 14.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0974   |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 784      |
|    fps              | 172      |
|    time_elapsed     | 329      |
|    total_timesteps  | 56672    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0526   |
|    n_updates        | 4167     |
----------------------------------
Eval num_timesteps=57000, episode_reward=170.08 +/- 39.62
Episode length: 42.80 +/- 9.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.232    |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 788      |
|    fps              | 171      |
|    time_elapsed     | 332      |
|    total_timesteps  | 57133    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0689   |
|    n_updates        | 4283     |
----------------------------------
Eval num_timesteps=57500, episode_reward=170.10 +/- 44.16
Episode length: 42.94 +/- 11.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00885  |
|    n_updates        | 4374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 792      |
|    fps              | 171      |
|    time_elapsed     | 335      |
|    total_timesteps  | 57537    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0782   |
|    n_updates        | 4384     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 796      |
|    fps              | 172      |
|    time_elapsed     | 335      |
|    total_timesteps  | 57937    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 4484     |
----------------------------------
Eval num_timesteps=58000, episode_reward=186.50 +/- 42.46
Episode length: 47.00 +/- 10.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0741   |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 800      |
|    fps              | 172      |
|    time_elapsed     | 339      |
|    total_timesteps  | 58359    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0659   |
|    n_updates        | 4589     |
----------------------------------
Eval num_timesteps=58500, episode_reward=166.66 +/- 57.55
Episode length: 42.00 +/- 14.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0875   |
|    n_updates        | 4624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 804      |
|    fps              | 171      |
|    time_elapsed     | 342      |
|    total_timesteps  | 58591    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 4647     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 808      |
|    fps              | 171      |
|    time_elapsed     | 342      |
|    total_timesteps  | 58846    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 4711     |
----------------------------------
Eval num_timesteps=59000, episode_reward=164.56 +/- 41.63
Episode length: 41.52 +/- 10.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 812      |
|    fps              | 171      |
|    time_elapsed     | 345      |
|    total_timesteps  | 59120    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 4779     |
----------------------------------
Eval num_timesteps=59500, episode_reward=175.54 +/- 55.94
Episode length: 44.22 +/- 14.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0604   |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 816      |
|    fps              | 171      |
|    time_elapsed     | 348      |
|    total_timesteps  | 59626    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0607   |
|    n_updates        | 4906     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 820      |
|    fps              | 171      |
|    time_elapsed     | 348      |
|    total_timesteps  | 59914    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00358  |
|    n_updates        | 4978     |
----------------------------------
Eval num_timesteps=60000, episode_reward=196.28 +/- 97.04
Episode length: 49.54 +/- 24.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.5     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.173    |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 824      |
|    fps              | 170      |
|    time_elapsed     | 351      |
|    total_timesteps  | 60187    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 5046     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 828      |
|    fps              | 171      |
|    time_elapsed     | 352      |
|    total_timesteps  | 60430    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 5107     |
----------------------------------
Eval num_timesteps=60500, episode_reward=229.74 +/- 112.86
Episode length: 57.88 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.9     |
|    mean_reward      | 230      |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 832      |
|    fps              | 170      |
|    time_elapsed     | 356      |
|    total_timesteps  | 60747    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 5186     |
----------------------------------
Eval num_timesteps=61000, episode_reward=208.00 +/- 67.59
Episode length: 52.36 +/- 16.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.4     |
|    mean_reward      | 208      |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.26     |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 836      |
|    fps              | 169      |
|    time_elapsed     | 359      |
|    total_timesteps  | 61051    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 5262     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 840      |
|    fps              | 170      |
|    time_elapsed     | 360      |
|    total_timesteps  | 61313    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00931  |
|    n_updates        | 5328     |
----------------------------------
Eval num_timesteps=61500, episode_reward=172.84 +/- 43.40
Episode length: 43.60 +/- 10.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 844      |
|    fps              | 169      |
|    time_elapsed     | 363      |
|    total_timesteps  | 61714    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 5428     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 848      |
|    fps              | 170      |
|    time_elapsed     | 363      |
|    total_timesteps  | 61953    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00595  |
|    n_updates        | 5488     |
----------------------------------
Eval num_timesteps=62000, episode_reward=194.04 +/- 64.26
Episode length: 48.88 +/- 16.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00598  |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 852      |
|    fps              | 169      |
|    time_elapsed     | 367      |
|    total_timesteps  | 62382    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 5595     |
----------------------------------
Eval num_timesteps=62500, episode_reward=174.94 +/- 57.81
Episode length: 44.06 +/- 14.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.166    |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 856      |
|    fps              | 169      |
|    time_elapsed     | 370      |
|    total_timesteps  | 62585    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 5646     |
----------------------------------
Eval num_timesteps=63000, episode_reward=191.64 +/- 79.17
Episode length: 48.34 +/- 19.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00238  |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 860      |
|    fps              | 168      |
|    time_elapsed     | 373      |
|    total_timesteps  | 63026    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 5756     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 864      |
|    fps              | 169      |
|    time_elapsed     | 373      |
|    total_timesteps  | 63242    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.397    |
|    n_updates        | 5810     |
----------------------------------
Eval num_timesteps=63500, episode_reward=232.90 +/- 83.39
Episode length: 58.60 +/- 20.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.6     |
|    mean_reward      | 233      |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.379    |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 868      |
|    fps              | 168      |
|    time_elapsed     | 377      |
|    total_timesteps  | 63586    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00249  |
|    n_updates        | 5896     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 872      |
|    fps              | 168      |
|    time_elapsed     | 377      |
|    total_timesteps  | 63772    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0058   |
|    n_updates        | 5942     |
----------------------------------
Eval num_timesteps=64000, episode_reward=234.52 +/- 92.97
Episode length: 59.02 +/- 23.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | 235      |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00625  |
|    n_updates        | 5999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 876      |
|    fps              | 168      |
|    time_elapsed     | 381      |
|    total_timesteps  | 64081    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 6020     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 880      |
|    fps              | 168      |
|    time_elapsed     | 381      |
|    total_timesteps  | 64239    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00315  |
|    n_updates        | 6059     |
----------------------------------
Eval num_timesteps=64500, episode_reward=189.34 +/- 52.11
Episode length: 47.72 +/- 13.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.321    |
|    n_updates        | 6124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 884      |
|    fps              | 167      |
|    time_elapsed     | 384      |
|    total_timesteps  | 64593    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.409    |
|    n_updates        | 6148     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 888      |
|    fps              | 168      |
|    time_elapsed     | 385      |
|    total_timesteps  | 64882    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 6220     |
----------------------------------
Eval num_timesteps=65000, episode_reward=185.08 +/- 44.90
Episode length: 46.68 +/- 11.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00456  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 892      |
|    fps              | 167      |
|    time_elapsed     | 388      |
|    total_timesteps  | 65168    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00484  |
|    n_updates        | 6291     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 896      |
|    fps              | 168      |
|    time_elapsed     | 388      |
|    total_timesteps  | 65455    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 6363     |
----------------------------------
Eval num_timesteps=65500, episode_reward=169.74 +/- 46.72
Episode length: 42.78 +/- 11.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.8     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 900      |
|    fps              | 167      |
|    time_elapsed     | 391      |
|    total_timesteps  | 65742    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 6435     |
----------------------------------
Eval num_timesteps=66000, episode_reward=186.00 +/- 57.15
Episode length: 46.94 +/- 14.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.968    |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.017    |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 904      |
|    fps              | 167      |
|    time_elapsed     | 395      |
|    total_timesteps  | 66004    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.316    |
|    n_updates        | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 908      |
|    fps              | 167      |
|    time_elapsed     | 395      |
|    total_timesteps  | 66305    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 6576     |
----------------------------------
Eval num_timesteps=66500, episode_reward=188.72 +/- 88.22
Episode length: 47.62 +/- 22.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.302    |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 912      |
|    fps              | 166      |
|    time_elapsed     | 398      |
|    total_timesteps  | 66518    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.264    |
|    n_updates        | 6629     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 916      |
|    fps              | 167      |
|    time_elapsed     | 398      |
|    total_timesteps  | 66860    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0044   |
|    n_updates        | 6714     |
----------------------------------
Eval num_timesteps=67000, episode_reward=164.08 +/- 40.60
Episode length: 41.40 +/- 10.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 920      |
|    fps              | 167      |
|    time_elapsed     | 401      |
|    total_timesteps  | 67168    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.224    |
|    n_updates        | 6791     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.1     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 924      |
|    fps              | 167      |
|    time_elapsed     | 402      |
|    total_timesteps  | 67401    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 6850     |
----------------------------------
Eval num_timesteps=67500, episode_reward=185.02 +/- 63.24
Episode length: 46.66 +/- 15.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0072   |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 928      |
|    fps              | 167      |
|    time_elapsed     | 405      |
|    total_timesteps  | 67780    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 6944     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.1     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 932      |
|    fps              | 167      |
|    time_elapsed     | 405      |
|    total_timesteps  | 67959    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 6989     |
----------------------------------
Eval num_timesteps=68000, episode_reward=199.22 +/- 49.89
Episode length: 50.20 +/- 12.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.2     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 936      |
|    fps              | 166      |
|    time_elapsed     | 409      |
|    total_timesteps  | 68299    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 7074     |
----------------------------------
Eval num_timesteps=68500, episode_reward=194.20 +/- 92.62
Episode length: 48.84 +/- 23.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.8     |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0188   |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 940      |
|    fps              | 166      |
|    time_elapsed     | 412      |
|    total_timesteps  | 68613    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.167    |
|    n_updates        | 7153     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71       |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 944      |
|    fps              | 166      |
|    time_elapsed     | 412      |
|    total_timesteps  | 68819    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 7204     |
----------------------------------
Eval num_timesteps=69000, episode_reward=177.10 +/- 40.14
Episode length: 44.68 +/- 10.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.963    |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.249    |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 948      |
|    fps              | 166      |
|    time_elapsed     | 415      |
|    total_timesteps  | 69076    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.161    |
|    n_updates        | 7268     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 952      |
|    fps              | 166      |
|    time_elapsed     | 416      |
|    total_timesteps  | 69321    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00313  |
|    n_updates        | 7330     |
----------------------------------
Eval num_timesteps=69500, episode_reward=241.72 +/- 125.85
Episode length: 60.84 +/- 31.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.8     |
|    mean_reward      | 242      |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.8     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 956      |
|    fps              | 165      |
|    time_elapsed     | 420      |
|    total_timesteps  | 69569    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.242    |
|    n_updates        | 7392     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.3     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 960      |
|    fps              | 166      |
|    time_elapsed     | 420      |
|    total_timesteps  | 69860    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.256    |
|    n_updates        | 7464     |
----------------------------------
Eval num_timesteps=70000, episode_reward=194.02 +/- 44.04
Episode length: 48.88 +/- 11.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.234    |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 964      |
|    fps              | 165      |
|    time_elapsed     | 423      |
|    total_timesteps  | 70185    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.064    |
|    n_updates        | 7546     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.5     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 968      |
|    fps              | 166      |
|    time_elapsed     | 424      |
|    total_timesteps  | 70437    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.222    |
|    n_updates        | 7609     |
----------------------------------
Eval num_timesteps=70500, episode_reward=183.30 +/- 54.04
Episode length: 46.16 +/- 13.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.96     |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0232   |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.9     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 972      |
|    fps              | 165      |
|    time_elapsed     | 427      |
|    total_timesteps  | 70664    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.193    |
|    n_updates        | 7665     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.6     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 976      |
|    fps              | 165      |
|    time_elapsed     | 427      |
|    total_timesteps  | 70944    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0549   |
|    n_updates        | 7735     |
----------------------------------
Eval num_timesteps=71000, episode_reward=179.90 +/- 46.13
Episode length: 45.34 +/- 11.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.199    |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 980      |
|    fps              | 165      |
|    time_elapsed     | 430      |
|    total_timesteps  | 71181    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 7795     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.3     |
|    ep_rew_mean      | 271      |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 984      |
|    fps              | 165      |
|    time_elapsed     | 431      |
|    total_timesteps  | 71419    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.222    |
|    n_updates        | 7854     |
----------------------------------
Eval num_timesteps=71500, episode_reward=231.18 +/- 82.67
Episode length: 58.12 +/- 20.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | 231      |
| rollout/            |          |
|    exploration_rate | 0.958    |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00642  |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.6     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 988      |
|    fps              | 164      |
|    time_elapsed     | 435      |
|    total_timesteps  | 71639    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.185    |
|    n_updates        | 7909     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | 267      |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 992      |
|    fps              | 165      |
|    time_elapsed     | 435      |
|    total_timesteps  | 71886    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.363    |
|    n_updates        | 7971     |
----------------------------------
Eval num_timesteps=72000, episode_reward=174.54 +/- 47.06
Episode length: 44.02 +/- 11.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00697  |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | 267      |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 996      |
|    fps              | 164      |
|    time_elapsed     | 438      |
|    total_timesteps  | 72172    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 8042     |
----------------------------------
Eval num_timesteps=72500, episode_reward=170.62 +/- 38.75
Episode length: 43.02 +/- 9.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00683  |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.4     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 164      |
|    time_elapsed     | 441      |
|    total_timesteps  | 72585    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.201    |
|    n_updates        | 8146     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 165      |
|    time_elapsed     | 442      |
|    total_timesteps  | 72954    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0244   |
|    n_updates        | 8238     |
----------------------------------
Eval num_timesteps=73000, episode_reward=170.44 +/- 61.34
Episode length: 42.94 +/- 15.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0435   |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.8     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 164      |
|    time_elapsed     | 445      |
|    total_timesteps  | 73290    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 8322     |
----------------------------------
Eval num_timesteps=73500, episode_reward=191.64 +/- 85.23
Episode length: 48.34 +/- 21.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.181    |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.1     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 164      |
|    time_elapsed     | 448      |
|    total_timesteps  | 73626    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.172    |
|    n_updates        | 8406     |
----------------------------------
Eval num_timesteps=74000, episode_reward=195.70 +/- 77.40
Episode length: 49.30 +/- 19.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.203    |
|    n_updates        | 8499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.6     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 163      |
|    time_elapsed     | 452      |
|    total_timesteps  | 74019    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.394    |
|    n_updates        | 8504     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 164      |
|    time_elapsed     | 452      |
|    total_timesteps  | 74462    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.37     |
|    n_updates        | 8615     |
----------------------------------
Eval num_timesteps=74500, episode_reward=162.80 +/- 44.69
Episode length: 41.06 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00596  |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 164      |
|    time_elapsed     | 455      |
|    total_timesteps  | 74906    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 8726     |
----------------------------------
Eval num_timesteps=75000, episode_reward=197.16 +/- 55.59
Episode length: 49.62 +/- 13.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.951    |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.8     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 163      |
|    time_elapsed     | 459      |
|    total_timesteps  | 75165    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.179    |
|    n_updates        | 8791     |
----------------------------------
Eval num_timesteps=75500, episode_reward=178.92 +/- 50.79
Episode length: 45.02 +/- 12.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.19     |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 163      |
|    time_elapsed     | 462      |
|    total_timesteps  | 75578    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 8894     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 163      |
|    time_elapsed     | 462      |
|    total_timesteps  | 75865    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.386    |
|    n_updates        | 8966     |
----------------------------------
Eval num_timesteps=76000, episode_reward=175.86 +/- 43.48
Episode length: 44.28 +/- 10.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.949    |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.215    |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 163      |
|    time_elapsed     | 465      |
|    total_timesteps  | 76165    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.212    |
|    n_updates        | 9041     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 164      |
|    time_elapsed     | 466      |
|    total_timesteps  | 76483    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.181    |
|    n_updates        | 9120     |
----------------------------------
Eval num_timesteps=76500, episode_reward=218.16 +/- 62.67
Episode length: 54.92 +/- 15.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.9     |
|    mean_reward      | 218      |
| rollout/            |          |
|    exploration_rate | 0.947    |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.189    |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 163      |
|    time_elapsed     | 469      |
|    total_timesteps  | 76711    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 9177     |
----------------------------------
Eval num_timesteps=77000, episode_reward=190.92 +/- 77.95
Episode length: 48.12 +/- 19.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00456  |
|    n_updates        | 9249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 162      |
|    time_elapsed     | 473      |
|    total_timesteps  | 77004    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.387    |
|    n_updates        | 9250     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 162      |
|    time_elapsed     | 473      |
|    total_timesteps  | 77166    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.426    |
|    n_updates        | 9291     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 163      |
|    time_elapsed     | 473      |
|    total_timesteps  | 77449    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00424  |
|    n_updates        | 9362     |
----------------------------------
Eval num_timesteps=77500, episode_reward=177.00 +/- 51.07
Episode length: 44.70 +/- 12.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.945    |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 163      |
|    time_elapsed     | 476      |
|    total_timesteps  | 77751    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.238    |
|    n_updates        | 9437     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 163      |
|    time_elapsed     | 477      |
|    total_timesteps  | 77956    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.215    |
|    n_updates        | 9488     |
----------------------------------
Eval num_timesteps=78000, episode_reward=251.76 +/- 104.08
Episode length: 63.30 +/- 26.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.3     |
|    mean_reward      | 252      |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.565    |
|    n_updates        | 9499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 162      |
|    time_elapsed     | 481      |
|    total_timesteps  | 78226    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.17     |
|    n_updates        | 9556     |
----------------------------------
Eval num_timesteps=78500, episode_reward=209.80 +/- 54.63
Episode length: 52.88 +/- 13.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.943    |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 161      |
|    time_elapsed     | 485      |
|    total_timesteps  | 78559    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.223    |
|    n_updates        | 9639     |
----------------------------------
Eval num_timesteps=79000, episode_reward=210.96 +/- 80.27
Episode length: 53.10 +/- 20.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.1     |
|    mean_reward      | 211      |
| rollout/            |          |
|    exploration_rate | 0.942    |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.185    |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 161      |
|    time_elapsed     | 488      |
|    total_timesteps  | 79022    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0275   |
|    n_updates        | 9755     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 162      |
|    time_elapsed     | 488      |
|    total_timesteps  | 79383    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.792    |
|    n_updates        | 9845     |
----------------------------------
Eval num_timesteps=79500, episode_reward=203.16 +/- 88.64
Episode length: 51.16 +/- 22.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | 203      |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.382    |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 161      |
|    time_elapsed     | 492      |
|    total_timesteps  | 79662    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.208    |
|    n_updates        | 9915     |
----------------------------------
Eval num_timesteps=80000, episode_reward=177.56 +/- 60.86
Episode length: 44.76 +/- 15.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.94     |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 161      |
|    time_elapsed     | 495      |
|    total_timesteps  | 80013    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.28     |
|    n_updates        | 10003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 161      |
|    time_elapsed     | 495      |
|    total_timesteps  | 80236    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0216   |
|    n_updates        | 10058    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 162      |
|    time_elapsed     | 495      |
|    total_timesteps  | 80457    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.955    |
|    n_updates        | 10114    |
----------------------------------
Eval num_timesteps=80500, episode_reward=186.20 +/- 77.93
Episode length: 46.94 +/- 19.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.939    |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.291    |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 161      |
|    time_elapsed     | 499      |
|    total_timesteps  | 80774    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0526   |
|    n_updates        | 10193    |
----------------------------------
Eval num_timesteps=81000, episode_reward=349.02 +/- 209.62
Episode length: 87.64 +/- 52.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.937    |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 10249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 160      |
|    time_elapsed     | 504      |
|    total_timesteps  | 81119    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0446   |
|    n_updates        | 10279    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 161      |
|    time_elapsed     | 504      |
|    total_timesteps  | 81489    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.468    |
|    n_updates        | 10372    |
----------------------------------
Eval num_timesteps=81500, episode_reward=223.28 +/- 71.96
Episode length: 56.12 +/- 17.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.1     |
|    mean_reward      | 223      |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.261    |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 160      |
|    time_elapsed     | 508      |
|    total_timesteps  | 81730    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.528    |
|    n_updates        | 10432    |
----------------------------------
Eval num_timesteps=82000, episode_reward=193.42 +/- 85.14
Episode length: 48.72 +/- 21.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.5      |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 160      |
|    time_elapsed     | 511      |
|    total_timesteps  | 82002    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.453    |
|    n_updates        | 10500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.7     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 160      |
|    time_elapsed     | 511      |
|    total_timesteps  | 82177    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.461    |
|    n_updates        | 10544    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.5     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 160      |
|    time_elapsed     | 512      |
|    total_timesteps  | 82318    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 10579    |
----------------------------------
Eval num_timesteps=82500, episode_reward=196.40 +/- 65.99
Episode length: 49.50 +/- 16.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.5     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.934    |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.767    |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 160      |
|    time_elapsed     | 515      |
|    total_timesteps  | 82504    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.245    |
|    n_updates        | 10625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 160      |
|    time_elapsed     | 515      |
|    total_timesteps  | 82734    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.288    |
|    n_updates        | 10683    |
----------------------------------
Eval num_timesteps=83000, episode_reward=181.58 +/- 55.40
Episode length: 45.70 +/- 13.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.538    |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69       |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 160      |
|    time_elapsed     | 518      |
|    total_timesteps  | 83067    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0482   |
|    n_updates        | 10766    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.6     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 160      |
|    time_elapsed     | 519      |
|    total_timesteps  | 83342    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.256    |
|    n_updates        | 10835    |
----------------------------------
Eval num_timesteps=83500, episode_reward=171.18 +/- 71.90
Episode length: 43.12 +/- 17.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.932    |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.471    |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.2     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 160      |
|    time_elapsed     | 521      |
|    total_timesteps  | 83731    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.255    |
|    n_updates        | 10932    |
----------------------------------
Eval num_timesteps=84000, episode_reward=281.14 +/- 139.37
Episode length: 70.70 +/- 34.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.7     |
|    mean_reward      | 281      |
| rollout/            |          |
|    exploration_rate | 0.93     |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.78     |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.2     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 159      |
|    time_elapsed     | 526      |
|    total_timesteps  | 84022    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00973  |
|    n_updates        | 11005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.4     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 159      |
|    time_elapsed     | 527      |
|    total_timesteps  | 84207    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0409   |
|    n_updates        | 11051    |
----------------------------------
Eval num_timesteps=84500, episode_reward=204.20 +/- 96.97
Episode length: 51.44 +/- 24.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | 204      |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0513   |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.1     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 159      |
|    time_elapsed     | 530      |
|    total_timesteps  | 84563    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 11140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.8     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 159      |
|    time_elapsed     | 530      |
|    total_timesteps  | 84829    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.526    |
|    n_updates        | 11207    |
----------------------------------
Eval num_timesteps=85000, episode_reward=250.66 +/- 85.27
Episode length: 63.08 +/- 21.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.1     |
|    mean_reward      | 251      |
| rollout/            |          |
|    exploration_rate | 0.928    |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00457  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.3     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 159      |
|    time_elapsed     | 534      |
|    total_timesteps  | 85084    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 11270    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 159      |
|    time_elapsed     | 535      |
|    total_timesteps  | 85452    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 11362    |
----------------------------------
Eval num_timesteps=85500, episode_reward=257.60 +/- 135.67
Episode length: 64.76 +/- 34.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.8     |
|    mean_reward      | 258      |
| rollout/            |          |
|    exploration_rate | 0.927    |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0255   |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.3     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 158      |
|    time_elapsed     | 539      |
|    total_timesteps  | 85693    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00939  |
|    n_updates        | 11423    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 159      |
|    time_elapsed     | 539      |
|    total_timesteps  | 85957    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.283    |
|    n_updates        | 11489    |
----------------------------------
Eval num_timesteps=86000, episode_reward=227.96 +/- 104.41
Episode length: 57.30 +/- 26.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.3     |
|    mean_reward      | 228      |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0926   |
|    n_updates        | 11499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.6     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 158      |
|    time_elapsed     | 543      |
|    total_timesteps  | 86243    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0126   |
|    n_updates        | 11560    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | 271      |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 159      |
|    time_elapsed     | 543      |
|    total_timesteps  | 86477    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 11619    |
----------------------------------
Eval num_timesteps=86500, episode_reward=177.64 +/- 49.03
Episode length: 44.74 +/- 12.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.924    |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.253    |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 158      |
|    time_elapsed     | 547      |
|    total_timesteps  | 86766    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.253    |
|    n_updates        | 11691    |
----------------------------------
Eval num_timesteps=87000, episode_reward=261.98 +/- 134.34
Episode length: 65.84 +/- 33.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.8     |
|    mean_reward      | 262      |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.263    |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.6     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 157      |
|    time_elapsed     | 551      |
|    total_timesteps  | 87097    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.279    |
|    n_updates        | 11774    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.8     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 158      |
|    time_elapsed     | 551      |
|    total_timesteps  | 87332    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 11832    |
----------------------------------
Eval num_timesteps=87500, episode_reward=229.10 +/- 49.80
Episode length: 57.56 +/- 12.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | 229      |
| rollout/            |          |
|    exploration_rate | 0.922    |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69       |
|    ep_rew_mean      | 274      |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 157      |
|    time_elapsed     | 555      |
|    total_timesteps  | 87671    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.25     |
|    n_updates        | 11917    |
----------------------------------
Eval num_timesteps=88000, episode_reward=172.76 +/- 47.50
Episode length: 43.58 +/- 11.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.252    |
|    n_updates        | 11999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | 277      |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 157      |
|    time_elapsed     | 559      |
|    total_timesteps  | 88071    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.791    |
|    n_updates        | 12017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.6     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 157      |
|    time_elapsed     | 559      |
|    total_timesteps  | 88350    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0167   |
|    n_updates        | 12087    |
----------------------------------
Eval num_timesteps=88500, episode_reward=303.36 +/- 148.57
Episode length: 76.20 +/- 37.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.2     |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00607  |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.2     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 157      |
|    time_elapsed     | 564      |
|    total_timesteps  | 88648    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.265    |
|    n_updates        | 12161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.8     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 157      |
|    time_elapsed     | 564      |
|    total_timesteps  | 88983    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.278    |
|    n_updates        | 12245    |
----------------------------------
Eval num_timesteps=89000, episode_reward=189.94 +/- 55.83
Episode length: 47.92 +/- 13.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.918    |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.312    |
|    n_updates        | 12249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 157      |
|    time_elapsed     | 568      |
|    total_timesteps  | 89453    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0244   |
|    n_updates        | 12363    |
----------------------------------
Eval num_timesteps=89500, episode_reward=174.98 +/- 43.79
Episode length: 44.14 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.917    |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 157      |
|    time_elapsed     | 571      |
|    total_timesteps  | 89813    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.246    |
|    n_updates        | 12453    |
----------------------------------
Eval num_timesteps=90000, episode_reward=197.62 +/- 56.01
Episode length: 49.82 +/- 14.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.8     |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.916    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 156      |
|    time_elapsed     | 574      |
|    total_timesteps  | 90107    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.656    |
|    n_updates        | 12526    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 157      |
|    time_elapsed     | 574      |
|    total_timesteps  | 90482    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.389    |
|    n_updates        | 12620    |
----------------------------------
Eval num_timesteps=90500, episode_reward=197.00 +/- 59.61
Episode length: 49.72 +/- 14.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.914    |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 157      |
|    time_elapsed     | 578      |
|    total_timesteps  | 90796    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00799  |
|    n_updates        | 12698    |
----------------------------------
Eval num_timesteps=91000, episode_reward=240.26 +/- 98.06
Episode length: 60.44 +/- 24.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.4     |
|    mean_reward      | 240      |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 156      |
|    time_elapsed     | 582      |
|    total_timesteps  | 91077    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.056    |
|    n_updates        | 12769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 156      |
|    time_elapsed     | 582      |
|    total_timesteps  | 91340    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0322   |
|    n_updates        | 12834    |
----------------------------------
Eval num_timesteps=91500, episode_reward=173.82 +/- 59.58
Episode length: 43.78 +/- 14.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.912    |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.288    |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 156      |
|    time_elapsed     | 585      |
|    total_timesteps  | 91682    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.302    |
|    n_updates        | 12920    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 156      |
|    time_elapsed     | 585      |
|    total_timesteps  | 91905    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.342    |
|    n_updates        | 12976    |
----------------------------------
Eval num_timesteps=92000, episode_reward=192.34 +/- 99.31
Episode length: 48.46 +/- 24.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0602   |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 156      |
|    time_elapsed     | 589      |
|    total_timesteps  | 92123    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00773  |
|    n_updates        | 13030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 156      |
|    time_elapsed     | 589      |
|    total_timesteps  | 92379    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.904    |
|    n_updates        | 13094    |
----------------------------------
Eval num_timesteps=92500, episode_reward=203.04 +/- 67.70
Episode length: 51.24 +/- 16.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | 203      |
| rollout/            |          |
|    exploration_rate | 0.909    |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.633    |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 156      |
|    time_elapsed     | 592      |
|    total_timesteps  | 92609    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 13152    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 156      |
|    time_elapsed     | 593      |
|    total_timesteps  | 92882    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.317    |
|    n_updates        | 13220    |
----------------------------------
Eval num_timesteps=93000, episode_reward=187.62 +/- 49.73
Episode length: 47.26 +/- 12.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.908    |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0461   |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 156      |
|    time_elapsed     | 596      |
|    total_timesteps  | 93143    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.294    |
|    n_updates        | 13285    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 156      |
|    time_elapsed     | 596      |
|    total_timesteps  | 93386    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00983  |
|    n_updates        | 13346    |
----------------------------------
Eval num_timesteps=93500, episode_reward=289.54 +/- 148.47
Episode length: 72.74 +/- 37.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.7     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00908  |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 155      |
|    time_elapsed     | 601      |
|    total_timesteps  | 93690    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.671    |
|    n_updates        | 13422    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 156      |
|    time_elapsed     | 601      |
|    total_timesteps  | 93927    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.33     |
|    n_updates        | 13481    |
----------------------------------
Eval num_timesteps=94000, episode_reward=182.78 +/- 46.58
Episode length: 46.06 +/- 11.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 155      |
|    time_elapsed     | 604      |
|    total_timesteps  | 94161    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 13540    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.4     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 156      |
|    time_elapsed     | 605      |
|    total_timesteps  | 94439    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.321    |
|    n_updates        | 13609    |
----------------------------------
Eval num_timesteps=94500, episode_reward=253.50 +/- 117.26
Episode length: 63.76 +/- 29.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.8     |
|    mean_reward      | 254      |
| rollout/            |          |
|    exploration_rate | 0.904    |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 13624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 155      |
|    time_elapsed     | 609      |
|    total_timesteps  | 94761    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.36     |
|    n_updates        | 13690    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 155      |
|    time_elapsed     | 609      |
|    total_timesteps  | 94997    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 13749    |
----------------------------------
Eval num_timesteps=95000, episode_reward=257.12 +/- 139.99
Episode length: 64.66 +/- 35.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.7     |
|    mean_reward      | 257      |
| rollout/            |          |
|    exploration_rate | 0.903    |
| time/               |          |
|    total_timesteps  | 95000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.6     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 155      |
|    time_elapsed     | 614      |
|    total_timesteps  | 95229    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0335   |
|    n_updates        | 13807    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.9     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 155      |
|    time_elapsed     | 614      |
|    total_timesteps  | 95440    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.687    |
|    n_updates        | 13859    |
----------------------------------
Eval num_timesteps=95500, episode_reward=172.88 +/- 45.97
Episode length: 43.56 +/- 11.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.901    |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.353    |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 155      |
|    time_elapsed     | 617      |
|    total_timesteps  | 95841    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.286    |
|    n_updates        | 13960    |
----------------------------------
Eval num_timesteps=96000, episode_reward=159.96 +/- 31.52
Episode length: 40.38 +/- 7.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.4     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0498   |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 154      |
|    time_elapsed     | 620      |
|    total_timesteps  | 96187    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.603    |
|    n_updates        | 14046    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 155      |
|    time_elapsed     | 620      |
|    total_timesteps  | 96395    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.344    |
|    n_updates        | 14098    |
----------------------------------
Eval num_timesteps=96500, episode_reward=180.24 +/- 46.35
Episode length: 45.48 +/- 11.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.899    |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.311    |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 155      |
|    time_elapsed     | 624      |
|    total_timesteps  | 96823    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.343    |
|    n_updates        | 14205    |
----------------------------------
Eval num_timesteps=97000, episode_reward=179.62 +/- 56.50
Episode length: 45.34 +/- 14.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.897    |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.345    |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.9     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 154      |
|    time_elapsed     | 626      |
|    total_timesteps  | 97095    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.36     |
|    n_updates        | 14273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.9     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 155      |
|    time_elapsed     | 627      |
|    total_timesteps  | 97375    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 14343    |
----------------------------------
Eval num_timesteps=97500, episode_reward=186.66 +/- 79.43
Episode length: 47.08 +/- 19.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.341    |
|    n_updates        | 14374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.9     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 155      |
|    time_elapsed     | 630      |
|    total_timesteps  | 97785    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.63     |
|    n_updates        | 14446    |
----------------------------------
Eval num_timesteps=98000, episode_reward=220.24 +/- 98.46
Episode length: 55.38 +/- 24.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.4     |
|    mean_reward      | 220      |
| rollout/            |          |
|    exploration_rate | 0.895    |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.294    |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 154      |
|    time_elapsed     | 634      |
|    total_timesteps  | 98006    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 14501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.1     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 154      |
|    time_elapsed     | 634      |
|    total_timesteps  | 98252    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 14562    |
----------------------------------
Eval num_timesteps=98500, episode_reward=288.40 +/- 128.90
Episode length: 72.48 +/- 32.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.5     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.965    |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 154      |
|    time_elapsed     | 639      |
|    total_timesteps  | 98547    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.28     |
|    n_updates        | 14636    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.7     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 154      |
|    time_elapsed     | 640      |
|    total_timesteps  | 98971    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00796  |
|    n_updates        | 14742    |
----------------------------------
Eval num_timesteps=99000, episode_reward=187.20 +/- 54.98
Episode length: 47.20 +/- 13.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.892    |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.5     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 154      |
|    time_elapsed     | 643      |
|    total_timesteps  | 99271    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.359    |
|    n_updates        | 14817    |
----------------------------------
Eval num_timesteps=99500, episode_reward=242.30 +/- 77.94
Episode length: 60.90 +/- 19.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.9     |
|    mean_reward      | 242      |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.7     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 154      |
|    time_elapsed     | 647      |
|    total_timesteps  | 99753    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.293    |
|    n_updates        | 14938    |
----------------------------------
Eval num_timesteps=100000, episode_reward=182.54 +/- 54.12
Episode length: 46.04 +/- 13.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.889    |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.34     |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 153      |
|    time_elapsed     | 650      |
|    total_timesteps  | 100119   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0953   |
|    n_updates        | 15029    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 154      |
|    time_elapsed     | 651      |
|    total_timesteps  | 100409   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 15102    |
----------------------------------
Eval num_timesteps=100500, episode_reward=376.98 +/- 251.36
Episode length: 94.60 +/- 62.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 377      |
| rollout/            |          |
|    exploration_rate | 0.888    |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0452   |
|    n_updates        | 15124    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 153      |
|    time_elapsed     | 657      |
|    total_timesteps  | 100807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.437    |
|    n_updates        | 15201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 153      |
|    time_elapsed     | 657      |
|    total_timesteps  | 100991   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 15247    |
----------------------------------
Eval num_timesteps=101000, episode_reward=185.20 +/- 49.81
Episode length: 46.74 +/- 12.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.886    |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 153      |
|    time_elapsed     | 660      |
|    total_timesteps  | 101324   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.335    |
|    n_updates        | 15330    |
----------------------------------
Eval num_timesteps=101500, episode_reward=165.92 +/- 50.45
Episode length: 41.84 +/- 12.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.885    |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 153      |
|    time_elapsed     | 663      |
|    total_timesteps  | 101694   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.026    |
|    n_updates        | 15423    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 153      |
|    time_elapsed     | 663      |
|    total_timesteps  | 101952   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 15487    |
----------------------------------
Eval num_timesteps=102000, episode_reward=279.70 +/- 152.84
Episode length: 70.32 +/- 38.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.3     |
|    mean_reward      | 280      |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 152      |
|    time_elapsed     | 668      |
|    total_timesteps  | 102257   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 15564    |
----------------------------------
Eval num_timesteps=102500, episode_reward=180.42 +/- 48.97
Episode length: 45.48 +/- 12.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.882    |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 152      |
|    time_elapsed     | 671      |
|    total_timesteps  | 102510   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.11     |
|    n_updates        | 15627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 153      |
|    time_elapsed     | 672      |
|    total_timesteps  | 102894   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.418    |
|    n_updates        | 15723    |
----------------------------------
Eval num_timesteps=103000, episode_reward=210.28 +/- 102.74
Episode length: 52.98 +/- 25.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53       |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.881    |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.41     |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 152      |
|    time_elapsed     | 675      |
|    total_timesteps  | 103175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 15793    |
----------------------------------
Eval num_timesteps=103500, episode_reward=201.00 +/- 63.29
Episode length: 50.66 +/- 15.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.7     |
|    mean_reward      | 201      |
| rollout/            |          |
|    exploration_rate | 0.879    |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.723    |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 152      |
|    time_elapsed     | 679      |
|    total_timesteps  | 103537   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.696    |
|    n_updates        | 15884    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 152      |
|    time_elapsed     | 679      |
|    total_timesteps  | 103731   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.365    |
|    n_updates        | 15932    |
----------------------------------
Eval num_timesteps=104000, episode_reward=175.80 +/- 42.82
Episode length: 44.32 +/- 10.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.878    |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0576   |
|    n_updates        | 15999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 152      |
|    time_elapsed     | 682      |
|    total_timesteps  | 104040   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0667   |
|    n_updates        | 16009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 152      |
|    time_elapsed     | 682      |
|    total_timesteps  | 104233   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.499    |
|    n_updates        | 16058    |
----------------------------------
Eval num_timesteps=104500, episode_reward=159.62 +/- 39.80
Episode length: 40.28 +/- 9.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.3     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.403    |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 152      |
|    time_elapsed     | 685      |
|    total_timesteps  | 104501   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0621   |
|    n_updates        | 16125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 152      |
|    time_elapsed     | 685      |
|    total_timesteps  | 104740   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 16184    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 152      |
|    time_elapsed     | 686      |
|    total_timesteps  | 104977   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0575   |
|    n_updates        | 16244    |
----------------------------------
Eval num_timesteps=105000, episode_reward=355.64 +/- 199.06
Episode length: 89.30 +/- 49.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.3     |
|    mean_reward      | 356      |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.391    |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 151      |
|    time_elapsed     | 692      |
|    total_timesteps  | 105137   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.073    |
|    n_updates        | 16284    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.4     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 152      |
|    time_elapsed     | 692      |
|    total_timesteps  | 105344   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 16335    |
----------------------------------
Eval num_timesteps=105500, episode_reward=166.62 +/- 35.92
Episode length: 42.02 +/- 8.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.874    |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.385    |
|    n_updates        | 16374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 151      |
|    time_elapsed     | 695      |
|    total_timesteps  | 105606   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.665    |
|    n_updates        | 16401    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 152      |
|    time_elapsed     | 695      |
|    total_timesteps  | 105882   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.756    |
|    n_updates        | 16470    |
----------------------------------
Eval num_timesteps=106000, episode_reward=237.40 +/- 116.21
Episode length: 59.74 +/- 29.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.7     |
|    mean_reward      | 237      |
| rollout/            |          |
|    exploration_rate | 0.872    |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0853   |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 151      |
|    time_elapsed     | 699      |
|    total_timesteps  | 106248   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.743    |
|    n_updates        | 16561    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.871    |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 152      |
|    time_elapsed     | 699      |
|    total_timesteps  | 106474   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.714    |
|    n_updates        | 16618    |
----------------------------------
Eval num_timesteps=106500, episode_reward=262.08 +/- 125.25
Episode length: 65.88 +/- 31.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.9     |
|    mean_reward      | 262      |
| rollout/            |          |
|    exploration_rate | 0.871    |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.32     |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 151      |
|    time_elapsed     | 704      |
|    total_timesteps  | 106875   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00948  |
|    n_updates        | 16718    |
----------------------------------
Eval num_timesteps=107000, episode_reward=188.32 +/- 76.65
Episode length: 47.48 +/- 19.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.869    |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.727    |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 151      |
|    time_elapsed     | 707      |
|    total_timesteps  | 107054   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.651    |
|    n_updates        | 16763    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.1     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 151      |
|    time_elapsed     | 707      |
|    total_timesteps  | 107320   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 16829    |
----------------------------------
Eval num_timesteps=107500, episode_reward=253.62 +/- 107.43
Episode length: 63.80 +/- 26.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.8     |
|    mean_reward      | 254      |
| rollout/            |          |
|    exploration_rate | 0.868    |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.665    |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.9     |
|    ep_rew_mean      | 266      |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 151      |
|    time_elapsed     | 711      |
|    total_timesteps  | 107501   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 16875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | 266      |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 151      |
|    time_elapsed     | 712      |
|    total_timesteps  | 107687   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 16921    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.3     |
|    ep_rew_mean      | 260      |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 151      |
|    time_elapsed     | 712      |
|    total_timesteps  | 107854   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.387    |
|    n_updates        | 16963    |
----------------------------------
Eval num_timesteps=108000, episode_reward=198.60 +/- 63.63
Episode length: 50.06 +/- 15.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.866    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.357    |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.5     |
|    ep_rew_mean      | 256      |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 151      |
|    time_elapsed     | 715      |
|    total_timesteps  | 108143   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.699    |
|    n_updates        | 17035    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.7     |
|    ep_rew_mean      | 257      |
|    exploration_rate | 0.865    |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 151      |
|    time_elapsed     | 715      |
|    total_timesteps  | 108426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.791    |
|    n_updates        | 17106    |
----------------------------------
Eval num_timesteps=108500, episode_reward=227.36 +/- 80.18
Episode length: 57.12 +/- 20.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.1     |
|    mean_reward      | 227      |
| rollout/            |          |
|    exploration_rate | 0.865    |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.5     |
|    ep_rew_mean      | 257      |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 151      |
|    time_elapsed     | 719      |
|    total_timesteps  | 108709   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.335    |
|    n_updates        | 17177    |
----------------------------------
Eval num_timesteps=109000, episode_reward=186.22 +/- 51.13
Episode length: 46.88 +/- 12.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.863    |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.423    |
|    n_updates        | 17249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65       |
|    ep_rew_mean      | 259      |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 150      |
|    time_elapsed     | 722      |
|    total_timesteps  | 109013   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 17253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | 261      |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 151      |
|    time_elapsed     | 723      |
|    total_timesteps  | 109449   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.798    |
|    n_updates        | 17362    |
----------------------------------
Eval num_timesteps=109500, episode_reward=185.52 +/- 49.10
Episode length: 46.76 +/- 12.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0726   |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | 265      |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 151      |
|    time_elapsed     | 726      |
|    total_timesteps  | 109846   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.379    |
|    n_updates        | 17461    |
----------------------------------
Eval num_timesteps=110000, episode_reward=180.98 +/- 46.92
Episode length: 45.66 +/- 11.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.86     |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.1     |
|    ep_rew_mean      | 259      |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 150      |
|    time_elapsed     | 729      |
|    total_timesteps  | 110045   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.3      |
|    n_updates        | 17511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.5     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 151      |
|    time_elapsed     | 729      |
|    total_timesteps  | 110379   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.421    |
|    n_updates        | 17594    |
----------------------------------
Eval num_timesteps=110500, episode_reward=175.34 +/- 70.41
Episode length: 44.28 +/- 17.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.859    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.83     |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.1     |
|    ep_rew_mean      | 259      |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 150      |
|    time_elapsed     | 732      |
|    total_timesteps  | 110554   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.497    |
|    n_updates        | 17638    |
----------------------------------
Eval num_timesteps=111000, episode_reward=191.42 +/- 56.67
Episode length: 48.22 +/- 14.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.857    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.485    |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.8     |
|    ep_rew_mean      | 270      |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 150      |
|    time_elapsed     | 736      |
|    total_timesteps  | 111018   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.401    |
|    n_updates        | 17754    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 150      |
|    time_elapsed     | 736      |
|    total_timesteps  | 111250   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.428    |
|    n_updates        | 17812    |
----------------------------------
Eval num_timesteps=111500, episode_reward=190.48 +/- 75.67
Episode length: 48.00 +/- 18.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48       |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.836    |
|    n_updates        | 17874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | 271      |
|    exploration_rate | 0.856    |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 150      |
|    time_elapsed     | 740      |
|    total_timesteps  | 111560   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.27     |
|    n_updates        | 17889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 151      |
|    time_elapsed     | 740      |
|    total_timesteps  | 111987   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.435    |
|    n_updates        | 17996    |
----------------------------------
Eval num_timesteps=112000, episode_reward=173.48 +/- 49.69
Episode length: 43.78 +/- 12.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.854    |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.445    |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.1     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 150      |
|    time_elapsed     | 743      |
|    total_timesteps  | 112245   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.046    |
|    n_updates        | 18061    |
----------------------------------
Eval num_timesteps=112500, episode_reward=178.84 +/- 47.00
Episode length: 45.12 +/- 11.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0964   |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.1     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.853    |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 150      |
|    time_elapsed     | 746      |
|    total_timesteps  | 112556   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.23     |
|    n_updates        | 18138    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 150      |
|    time_elapsed     | 747      |
|    total_timesteps  | 112837   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 18209    |
----------------------------------
Eval num_timesteps=113000, episode_reward=171.54 +/- 52.69
Episode length: 43.20 +/- 13.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.851    |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.513    |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.7     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 150      |
|    time_elapsed     | 750      |
|    total_timesteps  | 113053   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.802    |
|    n_updates        | 18263    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.7     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 151      |
|    time_elapsed     | 750      |
|    total_timesteps  | 113414   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 18353    |
----------------------------------
Eval num_timesteps=113500, episode_reward=166.92 +/- 40.68
Episode length: 42.08 +/- 10.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.85     |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.047    |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.1     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 150      |
|    time_elapsed     | 753      |
|    total_timesteps  | 113683   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0363   |
|    n_updates        | 18420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 151      |
|    time_elapsed     | 753      |
|    total_timesteps  | 113871   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.456    |
|    n_updates        | 18467    |
----------------------------------
Eval num_timesteps=114000, episode_reward=173.14 +/- 53.73
Episode length: 43.78 +/- 13.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.848    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0322   |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.5     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 150      |
|    time_elapsed     | 756      |
|    total_timesteps  | 114107   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0687   |
|    n_updates        | 18526    |
----------------------------------
Eval num_timesteps=114500, episode_reward=305.96 +/- 148.75
Episode length: 76.94 +/- 37.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.9     |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 0.847    |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0445   |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 150      |
|    time_elapsed     | 761      |
|    total_timesteps  | 114608   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.455    |
|    n_updates        | 18651    |
----------------------------------
Eval num_timesteps=115000, episode_reward=172.16 +/- 47.12
Episode length: 43.44 +/- 11.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.845    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 150      |
|    time_elapsed     | 764      |
|    total_timesteps  | 115108   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.43     |
|    n_updates        | 18776    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.844    |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 150      |
|    time_elapsed     | 764      |
|    total_timesteps  | 115439   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0455   |
|    n_updates        | 18859    |
----------------------------------
Eval num_timesteps=115500, episode_reward=188.48 +/- 53.37
Episode length: 47.58 +/- 13.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.844    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0845   |
|    n_updates        | 18874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 150      |
|    time_elapsed     | 767      |
|    total_timesteps  | 115725   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.465    |
|    n_updates        | 18931    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.842    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 150      |
|    time_elapsed     | 768      |
|    total_timesteps  | 115975   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.16     |
|    n_updates        | 18993    |
----------------------------------
Eval num_timesteps=116000, episode_reward=246.00 +/- 128.74
Episode length: 61.94 +/- 32.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.9     |
|    mean_reward      | 246      |
| rollout/            |          |
|    exploration_rate | 0.842    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.499    |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 150      |
|    time_elapsed     | 772      |
|    total_timesteps  | 116226   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.439    |
|    n_updates        | 19056    |
----------------------------------
Eval num_timesteps=116500, episode_reward=169.20 +/- 46.34
Episode length: 42.64 +/- 11.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.841    |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.464    |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 150      |
|    time_elapsed     | 775      |
|    total_timesteps  | 116543   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.436    |
|    n_updates        | 19135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 150      |
|    time_elapsed     | 775      |
|    total_timesteps  | 116780   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0322   |
|    n_updates        | 19194    |
----------------------------------
Eval num_timesteps=117000, episode_reward=180.46 +/- 76.03
Episode length: 45.44 +/- 19.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.839    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0429   |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 150      |
|    time_elapsed     | 779      |
|    total_timesteps  | 117175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.434    |
|    n_updates        | 19293    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 150      |
|    time_elapsed     | 779      |
|    total_timesteps  | 117452   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.841    |
|    n_updates        | 19362    |
----------------------------------
Eval num_timesteps=117500, episode_reward=186.12 +/- 49.83
Episode length: 46.92 +/- 12.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.837    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.597    |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 150      |
|    time_elapsed     | 782      |
|    total_timesteps  | 117752   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.872    |
|    n_updates        | 19437    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.836    |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 150      |
|    time_elapsed     | 782      |
|    total_timesteps  | 117969   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0824   |
|    n_updates        | 19492    |
----------------------------------
Eval num_timesteps=118000, episode_reward=201.18 +/- 142.95
Episode length: 50.66 +/- 35.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.7     |
|    mean_reward      | 201      |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.782    |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 150      |
|    time_elapsed     | 786      |
|    total_timesteps  | 118254   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.46     |
|    n_updates        | 19563    |
----------------------------------
Eval num_timesteps=118500, episode_reward=260.32 +/- 157.44
Episode length: 65.46 +/- 39.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.5     |
|    mean_reward      | 260      |
| rollout/            |          |
|    exploration_rate | 0.834    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0888   |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 149      |
|    time_elapsed     | 790      |
|    total_timesteps  | 118554   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.24     |
|    n_updates        | 19638    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 150      |
|    time_elapsed     | 790      |
|    total_timesteps  | 118774   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.438    |
|    n_updates        | 19693    |
----------------------------------
Eval num_timesteps=119000, episode_reward=154.14 +/- 53.08
Episode length: 38.94 +/- 13.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.9     |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 150      |
|    time_elapsed     | 793      |
|    total_timesteps  | 119195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.462    |
|    n_updates        | 19798    |
----------------------------------
Eval num_timesteps=119500, episode_reward=173.12 +/- 44.95
Episode length: 43.66 +/- 11.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.831    |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.442    |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 150      |
|    time_elapsed     | 796      |
|    total_timesteps  | 119609   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.508    |
|    n_updates        | 19902    |
----------------------------------
Eval num_timesteps=120000, episode_reward=183.96 +/- 57.26
Episode length: 46.38 +/- 14.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.829    |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.873    |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.829    |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 150      |
|    time_elapsed     | 799      |
|    total_timesteps  | 120000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 150      |
|    time_elapsed     | 800      |
|    total_timesteps  | 120363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 20090    |
----------------------------------
Eval num_timesteps=120500, episode_reward=181.62 +/- 54.41
Episode length: 45.80 +/- 13.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 150      |
|    time_elapsed     | 803      |
|    total_timesteps  | 120727   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0648   |
|    n_updates        | 20181    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 150      |
|    time_elapsed     | 803      |
|    total_timesteps  | 120962   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.908    |
|    n_updates        | 20240    |
----------------------------------
Eval num_timesteps=121000, episode_reward=188.04 +/- 53.76
Episode length: 47.38 +/- 13.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.4     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.826    |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 20249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 150      |
|    time_elapsed     | 806      |
|    total_timesteps  | 121173   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0966   |
|    n_updates        | 20293    |
----------------------------------
Eval num_timesteps=121500, episode_reward=277.36 +/- 180.37
Episode length: 69.68 +/- 45.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.7     |
|    mean_reward      | 277      |
| rollout/            |          |
|    exploration_rate | 0.825    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.37     |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.825    |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 149      |
|    time_elapsed     | 811      |
|    total_timesteps  | 121540   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 20384    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 150      |
|    time_elapsed     | 811      |
|    total_timesteps  | 121782   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 20445    |
----------------------------------
Eval num_timesteps=122000, episode_reward=204.28 +/- 89.05
Episode length: 51.48 +/- 22.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.5     |
|    mean_reward      | 204      |
| rollout/            |          |
|    exploration_rate | 0.823    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.551    |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 149      |
|    time_elapsed     | 815      |
|    total_timesteps  | 122114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 20528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 149      |
|    time_elapsed     | 815      |
|    total_timesteps  | 122307   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.513    |
|    n_updates        | 20576    |
----------------------------------
Eval num_timesteps=122500, episode_reward=204.26 +/- 82.78
Episode length: 51.42 +/- 20.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | 204      |
| rollout/            |          |
|    exploration_rate | 0.821    |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0346   |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 149      |
|    time_elapsed     | 819      |
|    total_timesteps  | 122760   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.988    |
|    n_updates        | 20689    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 149      |
|    time_elapsed     | 819      |
|    total_timesteps  | 122962   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0731   |
|    n_updates        | 20740    |
----------------------------------
Eval num_timesteps=123000, episode_reward=176.40 +/- 61.85
Episode length: 44.44 +/- 15.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.82     |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.484    |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 149      |
|    time_elapsed     | 822      |
|    total_timesteps  | 123161   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0442   |
|    n_updates        | 20790    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.818    |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 149      |
|    time_elapsed     | 823      |
|    total_timesteps  | 123408   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.916    |
|    n_updates        | 20851    |
----------------------------------
Eval num_timesteps=123500, episode_reward=177.28 +/- 48.53
Episode length: 44.68 +/- 12.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.951    |
|    n_updates        | 20874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 149      |
|    time_elapsed     | 825      |
|    total_timesteps  | 123723   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 20930    |
----------------------------------
Eval num_timesteps=124000, episode_reward=179.58 +/- 45.99
Episode length: 45.22 +/- 11.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.817    |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0336   |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.816    |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 149      |
|    time_elapsed     | 829      |
|    total_timesteps  | 124089   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.959    |
|    n_updates        | 21022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 149      |
|    time_elapsed     | 829      |
|    total_timesteps  | 124352   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 21087    |
----------------------------------
Eval num_timesteps=124500, episode_reward=170.22 +/- 41.30
Episode length: 42.94 +/- 10.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.532    |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 149      |
|    time_elapsed     | 832      |
|    total_timesteps  | 124573   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0631   |
|    n_updates        | 21143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 150      |
|    time_elapsed     | 832      |
|    total_timesteps  | 124908   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.487    |
|    n_updates        | 21226    |
----------------------------------
Eval num_timesteps=125000, episode_reward=261.62 +/- 76.64
Episode length: 65.80 +/- 19.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.8     |
|    mean_reward      | 262      |
| rollout/            |          |
|    exploration_rate | 0.813    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0508   |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 149      |
|    time_elapsed     | 836      |
|    total_timesteps  | 125155   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.471    |
|    n_updates        | 21288    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 149      |
|    time_elapsed     | 837      |
|    total_timesteps  | 125392   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0535   |
|    n_updates        | 21347    |
----------------------------------
Eval num_timesteps=125500, episode_reward=277.28 +/- 153.15
Episode length: 69.70 +/- 38.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.7     |
|    mean_reward      | 277      |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0982   |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.811    |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 149      |
|    time_elapsed     | 841      |
|    total_timesteps  | 125676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0497   |
|    n_updates        | 21418    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 149      |
|    time_elapsed     | 841      |
|    total_timesteps  | 125947   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.652    |
|    n_updates        | 21486    |
----------------------------------
Eval num_timesteps=126000, episode_reward=177.82 +/- 71.09
Episode length: 44.88 +/- 17.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.81     |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.552    |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 149      |
|    time_elapsed     | 845      |
|    total_timesteps  | 126267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 21566    |
----------------------------------
Eval num_timesteps=126500, episode_reward=180.32 +/- 40.34
Episode length: 45.46 +/- 10.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.808    |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 149      |
|    time_elapsed     | 847      |
|    total_timesteps  | 126605   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.44     |
|    n_updates        | 21651    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.1     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 149      |
|    time_elapsed     | 848      |
|    total_timesteps  | 126822   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.467    |
|    n_updates        | 21705    |
----------------------------------
Eval num_timesteps=127000, episode_reward=174.92 +/- 44.46
Episode length: 44.04 +/- 11.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.807    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0497   |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.8     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 149      |
|    time_elapsed     | 851      |
|    total_timesteps  | 127084   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.96     |
|    n_updates        | 21770    |
----------------------------------
Eval num_timesteps=127500, episode_reward=233.16 +/- 111.62
Episode length: 58.66 +/- 27.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.7     |
|    mean_reward      | 233      |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.988    |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.805    |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 149      |
|    time_elapsed     | 855      |
|    total_timesteps  | 127558   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.936    |
|    n_updates        | 21889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.7     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 149      |
|    time_elapsed     | 855      |
|    total_timesteps  | 127898   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0632   |
|    n_updates        | 21974    |
----------------------------------
Eval num_timesteps=128000, episode_reward=181.22 +/- 73.33
Episode length: 45.66 +/- 18.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.803    |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.802    |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 149      |
|    time_elapsed     | 859      |
|    total_timesteps  | 128478   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 22119    |
----------------------------------
Eval num_timesteps=128500, episode_reward=174.02 +/- 37.68
Episode length: 43.88 +/- 9.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.802    |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.998    |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.801    |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 149      |
|    time_elapsed     | 862      |
|    total_timesteps  | 128807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 22201    |
----------------------------------
Eval num_timesteps=129000, episode_reward=191.02 +/- 48.76
Episode length: 48.08 +/- 12.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.8      |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.882    |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 149      |
|    time_elapsed     | 865      |
|    total_timesteps  | 129122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0971   |
|    n_updates        | 22280    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 149      |
|    time_elapsed     | 866      |
|    total_timesteps  | 129439   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0686   |
|    n_updates        | 22359    |
----------------------------------
Eval num_timesteps=129500, episode_reward=292.10 +/- 153.60
Episode length: 73.32 +/- 38.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.3     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.041    |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 149      |
|    time_elapsed     | 870      |
|    total_timesteps  | 129777   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.465    |
|    n_updates        | 22444    |
----------------------------------
Eval num_timesteps=130000, episode_reward=178.14 +/- 48.00
Episode length: 44.96 +/- 11.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.797    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.465    |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 148      |
|    time_elapsed     | 873      |
|    total_timesteps  | 130091   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.042    |
|    n_updates        | 22522    |
----------------------------------
Eval num_timesteps=130500, episode_reward=159.80 +/- 43.79
Episode length: 40.34 +/- 11.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.3     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.795    |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.658    |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.795    |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 148      |
|    time_elapsed     | 876      |
|    total_timesteps  | 130558   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 22639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 149      |
|    time_elapsed     | 877      |
|    total_timesteps  | 130883   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 22720    |
----------------------------------
Eval num_timesteps=131000, episode_reward=266.66 +/- 140.37
Episode length: 67.02 +/- 35.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67       |
|    mean_reward      | 267      |
| rollout/            |          |
|    exploration_rate | 0.793    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.5      |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.793    |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 148      |
|    time_elapsed     | 881      |
|    total_timesteps  | 131136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 22783    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.792    |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 148      |
|    time_elapsed     | 882      |
|    total_timesteps  | 131314   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0711   |
|    n_updates        | 22828    |
----------------------------------
Eval num_timesteps=131500, episode_reward=192.02 +/- 92.16
Episode length: 48.38 +/- 23.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.791    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0533   |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.791    |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 148      |
|    time_elapsed     | 885      |
|    total_timesteps  | 131651   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.479    |
|    n_updates        | 22912    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.79     |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 148      |
|    time_elapsed     | 886      |
|    total_timesteps  | 131984   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.581    |
|    n_updates        | 22995    |
----------------------------------
Eval num_timesteps=132000, episode_reward=180.58 +/- 43.69
Episode length: 45.52 +/- 10.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0418   |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.788    |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 148      |
|    time_elapsed     | 889      |
|    total_timesteps  | 132468   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.635    |
|    n_updates        | 23116    |
----------------------------------
Eval num_timesteps=132500, episode_reward=169.74 +/- 66.44
Episode length: 42.86 +/- 16.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.788    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0377   |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.787    |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 148      |
|    time_elapsed     | 892      |
|    total_timesteps  | 132696   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.554    |
|    n_updates        | 23173    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 148      |
|    time_elapsed     | 892      |
|    total_timesteps  | 132944   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.577    |
|    n_updates        | 23235    |
----------------------------------
Eval num_timesteps=133000, episode_reward=319.66 +/- 142.12
Episode length: 80.24 +/- 35.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.2     |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.786    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 148      |
|    time_elapsed     | 898      |
|    total_timesteps  | 133226   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.604    |
|    n_updates        | 23306    |
----------------------------------
Eval num_timesteps=133500, episode_reward=172.26 +/- 44.64
Episode length: 43.50 +/- 11.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.785    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.572    |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.784    |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 148      |
|    time_elapsed     | 901      |
|    total_timesteps  | 133693   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.55     |
|    n_updates        | 23423    |
----------------------------------
Eval num_timesteps=134000, episode_reward=172.36 +/- 35.84
Episode length: 43.50 +/- 8.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.783    |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0563   |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 148      |
|    time_elapsed     | 904      |
|    total_timesteps  | 134119   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.12     |
|    n_updates        | 23529    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 148      |
|    time_elapsed     | 904      |
|    total_timesteps  | 134394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.659    |
|    n_updates        | 23598    |
----------------------------------
Eval num_timesteps=134500, episode_reward=291.80 +/- 148.64
Episode length: 73.30 +/- 37.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.3     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.781    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.487    |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 148      |
|    time_elapsed     | 909      |
|    total_timesteps  | 134655   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 23663    |
----------------------------------
Eval num_timesteps=135000, episode_reward=179.78 +/- 47.27
Episode length: 45.26 +/- 11.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0729   |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 147      |
|    time_elapsed     | 912      |
|    total_timesteps  | 135008   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0679   |
|    n_updates        | 23751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 148      |
|    time_elapsed     | 913      |
|    total_timesteps  | 135217   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0794   |
|    n_updates        | 23804    |
----------------------------------
Eval num_timesteps=135500, episode_reward=205.46 +/- 96.01
Episode length: 51.72 +/- 23.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.7     |
|    mean_reward      | 205      |
| rollout/            |          |
|    exploration_rate | 0.778    |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0442   |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.777    |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 147      |
|    time_elapsed     | 916      |
|    total_timesteps  | 135623   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.637    |
|    n_updates        | 23905    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.777    |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 148      |
|    time_elapsed     | 917      |
|    total_timesteps  | 135788   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.477    |
|    n_updates        | 23946    |
----------------------------------
Eval num_timesteps=136000, episode_reward=188.80 +/- 52.93
Episode length: 47.56 +/- 13.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 147      |
|    time_elapsed     | 920      |
|    total_timesteps  | 136094   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0427   |
|    n_updates        | 24023    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 148      |
|    time_elapsed     | 920      |
|    total_timesteps  | 136370   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 24092    |
----------------------------------
Eval num_timesteps=136500, episode_reward=181.76 +/- 48.33
Episode length: 45.82 +/- 12.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.774    |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.614    |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.774    |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 147      |
|    time_elapsed     | 923      |
|    total_timesteps  | 136662   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.594    |
|    n_updates        | 24165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 148      |
|    time_elapsed     | 923      |
|    total_timesteps  | 136868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0659   |
|    n_updates        | 24216    |
----------------------------------
Eval num_timesteps=137000, episode_reward=192.68 +/- 50.68
Episode length: 48.52 +/- 12.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.772    |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.5      |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 148      |
|    time_elapsed     | 927      |
|    total_timesteps  | 137264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 24315    |
----------------------------------
Eval num_timesteps=137500, episode_reward=180.90 +/- 48.86
Episode length: 45.62 +/- 12.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.771    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.54     |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 147      |
|    time_elapsed     | 930      |
|    total_timesteps  | 137544   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.06     |
|    n_updates        | 24385    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 148      |
|    time_elapsed     | 930      |
|    total_timesteps  | 137714   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.602    |
|    n_updates        | 24428    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 148      |
|    time_elapsed     | 930      |
|    total_timesteps  | 137945   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 24486    |
----------------------------------
Eval num_timesteps=138000, episode_reward=203.22 +/- 71.63
Episode length: 51.20 +/- 17.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | 203      |
| rollout/            |          |
|    exploration_rate | 0.769    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.544    |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.768    |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 148      |
|    time_elapsed     | 934      |
|    total_timesteps  | 138370   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 24592    |
----------------------------------
Eval num_timesteps=138500, episode_reward=186.94 +/- 84.13
Episode length: 47.12 +/- 21.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.767    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.497    |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 147      |
|    time_elapsed     | 938      |
|    total_timesteps  | 138847   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0838   |
|    n_updates        | 24711    |
----------------------------------
Eval num_timesteps=139000, episode_reward=302.36 +/- 183.70
Episode length: 75.94 +/- 45.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.9     |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.59     |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 147      |
|    time_elapsed     | 943      |
|    total_timesteps  | 139259   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.571    |
|    n_updates        | 24814    |
----------------------------------
Eval num_timesteps=139500, episode_reward=171.06 +/- 47.00
Episode length: 43.16 +/- 11.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.763    |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0416   |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 147      |
|    time_elapsed     | 946      |
|    total_timesteps  | 139621   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 24905    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.762    |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 147      |
|    time_elapsed     | 947      |
|    total_timesteps  | 139891   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.603    |
|    n_updates        | 24972    |
----------------------------------
Eval num_timesteps=140000, episode_reward=177.90 +/- 43.34
Episode length: 44.84 +/- 10.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.762    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 147      |
|    time_elapsed     | 950      |
|    total_timesteps  | 140110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.33     |
|    n_updates        | 25027    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.76     |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 147      |
|    time_elapsed     | 950      |
|    total_timesteps  | 140434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0878   |
|    n_updates        | 25108    |
----------------------------------
Eval num_timesteps=140500, episode_reward=273.46 +/- 121.71
Episode length: 68.68 +/- 30.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.7     |
|    mean_reward      | 273      |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.575    |
|    n_updates        | 25124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 147      |
|    time_elapsed     | 955      |
|    total_timesteps  | 140927   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.047    |
|    n_updates        | 25231    |
----------------------------------
Eval num_timesteps=141000, episode_reward=177.50 +/- 44.09
Episode length: 44.80 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.758    |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.757    |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 147      |
|    time_elapsed     | 958      |
|    total_timesteps  | 141291   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 25322    |
----------------------------------
Eval num_timesteps=141500, episode_reward=167.16 +/- 47.05
Episode length: 42.22 +/- 11.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.756    |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.16     |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 147      |
|    time_elapsed     | 960      |
|    total_timesteps  | 141536   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.22     |
|    n_updates        | 25383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 147      |
|    time_elapsed     | 961      |
|    total_timesteps  | 141816   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.29     |
|    n_updates        | 25453    |
----------------------------------
Eval num_timesteps=142000, episode_reward=290.48 +/- 155.96
Episode length: 73.02 +/- 38.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73       |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.755    |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.737    |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.754    |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 147      |
|    time_elapsed     | 965      |
|    total_timesteps  | 142059   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0526   |
|    n_updates        | 25514    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.753    |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 147      |
|    time_elapsed     | 966      |
|    total_timesteps  | 142303   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 25575    |
----------------------------------
Eval num_timesteps=142500, episode_reward=177.74 +/- 43.43
Episode length: 44.82 +/- 10.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.618    |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 147      |
|    time_elapsed     | 968      |
|    total_timesteps  | 142589   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0518   |
|    n_updates        | 25647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 147      |
|    time_elapsed     | 969      |
|    total_timesteps  | 142859   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.238    |
|    n_updates        | 25714    |
----------------------------------
Eval num_timesteps=143000, episode_reward=281.30 +/- 113.46
Episode length: 70.74 +/- 28.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.7     |
|    mean_reward      | 281      |
| rollout/            |          |
|    exploration_rate | 0.751    |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0209   |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.75     |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 147      |
|    time_elapsed     | 973      |
|    total_timesteps  | 143171   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 25792    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.749    |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 147      |
|    time_elapsed     | 974      |
|    total_timesteps  | 143436   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.585    |
|    n_updates        | 25858    |
----------------------------------
Eval num_timesteps=143500, episode_reward=348.76 +/- 156.71
Episode length: 87.62 +/- 39.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.749    |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.18     |
|    n_updates        | 25874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.749    |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 146      |
|    time_elapsed     | 979      |
|    total_timesteps  | 143652   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.077    |
|    n_updates        | 25912    |
----------------------------------
Eval num_timesteps=144000, episode_reward=262.20 +/- 152.62
Episode length: 65.94 +/- 38.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.9     |
|    mean_reward      | 262      |
| rollout/            |          |
|    exploration_rate | 0.747    |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0593   |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 146      |
|    time_elapsed     | 983      |
|    total_timesteps  | 144019   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.539    |
|    n_updates        | 26004    |
----------------------------------
Eval num_timesteps=144500, episode_reward=289.42 +/- 137.03
Episode length: 72.72 +/- 34.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.7     |
|    mean_reward      | 289      |
| rollout/            |          |
|    exploration_rate | 0.745    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.635    |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 146      |
|    time_elapsed     | 988      |
|    total_timesteps  | 144587   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 26146    |
----------------------------------
Eval num_timesteps=145000, episode_reward=170.16 +/- 39.15
Episode length: 42.90 +/- 9.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.743    |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 146      |
|    time_elapsed     | 991      |
|    total_timesteps  | 145045   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.621    |
|    n_updates        | 26261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.742    |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 146      |
|    time_elapsed     | 991      |
|    total_timesteps  | 145335   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 26333    |
----------------------------------
Eval num_timesteps=145500, episode_reward=188.62 +/- 47.64
Episode length: 47.48 +/- 11.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.742    |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.741    |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 146      |
|    time_elapsed     | 995      |
|    total_timesteps  | 145671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 26417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 146      |
|    time_elapsed     | 995      |
|    total_timesteps  | 145900   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.11     |
|    n_updates        | 26474    |
----------------------------------
Eval num_timesteps=146000, episode_reward=246.28 +/- 126.80
Episode length: 61.96 +/- 31.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62       |
|    mean_reward      | 246      |
| rollout/            |          |
|    exploration_rate | 0.74     |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.19     |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.739    |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 146      |
|    time_elapsed     | 999      |
|    total_timesteps  | 146259   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.549    |
|    n_updates        | 26564    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 146      |
|    time_elapsed     | 999      |
|    total_timesteps  | 146472   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.652    |
|    n_updates        | 26617    |
----------------------------------
Eval num_timesteps=146500, episode_reward=207.26 +/- 60.39
Episode length: 52.24 +/- 15.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | 207      |
| rollout/            |          |
|    exploration_rate | 0.738    |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.69     |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.737    |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 146      |
|    time_elapsed     | 1002     |
|    total_timesteps  | 146672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.19     |
|    n_updates        | 26667    |
----------------------------------
Eval num_timesteps=147000, episode_reward=184.40 +/- 57.52
Episode length: 46.46 +/- 14.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.736    |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 146      |
|    time_elapsed     | 1006     |
|    total_timesteps  | 147105   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.723    |
|    n_updates        | 26776    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 146      |
|    time_elapsed     | 1006     |
|    total_timesteps  | 147354   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0621   |
|    n_updates        | 26838    |
----------------------------------
Eval num_timesteps=147500, episode_reward=228.12 +/- 108.69
Episode length: 57.50 +/- 27.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.5     |
|    mean_reward      | 228      |
| rollout/            |          |
|    exploration_rate | 0.734    |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.734    |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 146      |
|    time_elapsed     | 1009     |
|    total_timesteps  | 147546   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.624    |
|    n_updates        | 26886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.733    |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 146      |
|    time_elapsed     | 1010     |
|    total_timesteps  | 147799   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0983   |
|    n_updates        | 26949    |
----------------------------------
Eval num_timesteps=148000, episode_reward=263.40 +/- 124.79
Episode length: 66.22 +/- 31.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.2     |
|    mean_reward      | 263      |
| rollout/            |          |
|    exploration_rate | 0.733    |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.638    |
|    n_updates        | 26999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 146      |
|    time_elapsed     | 1014     |
|    total_timesteps  | 148228   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.21     |
|    n_updates        | 27056    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 146      |
|    time_elapsed     | 1014     |
|    total_timesteps  | 148451   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.18     |
|    n_updates        | 27112    |
----------------------------------
Eval num_timesteps=148500, episode_reward=312.84 +/- 171.11
Episode length: 78.58 +/- 42.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.6     |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.731    |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0921   |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.73     |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 145      |
|    time_elapsed     | 1019     |
|    total_timesteps  | 148742   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 27185    |
----------------------------------
Eval num_timesteps=149000, episode_reward=181.48 +/- 44.23
Episode length: 45.80 +/- 11.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.73     |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.729    |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 145      |
|    time_elapsed     | 1022     |
|    total_timesteps  | 149067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.733    |
|    n_updates        | 27266    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 146      |
|    time_elapsed     | 1023     |
|    total_timesteps  | 149437   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.168    |
|    n_updates        | 27359    |
----------------------------------
Eval num_timesteps=149500, episode_reward=201.86 +/- 116.04
Episode length: 50.88 +/- 29.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.9     |
|    mean_reward      | 202      |
| rollout/            |          |
|    exploration_rate | 0.727    |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0731   |
|    n_updates        | 27374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.726    |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 145      |
|    time_elapsed     | 1026     |
|    total_timesteps  | 149825   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.14     |
|    n_updates        | 27456    |
----------------------------------
Eval num_timesteps=150000, episode_reward=254.80 +/- 146.09
Episode length: 64.08 +/- 36.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.1     |
|    mean_reward      | 255      |
| rollout/            |          |
|    exploration_rate | 0.725    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0561   |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 145      |
|    time_elapsed     | 1031     |
|    total_timesteps  | 150169   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.186    |
|    n_updates        | 27542    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 145      |
|    time_elapsed     | 1031     |
|    total_timesteps  | 150445   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.85     |
|    n_updates        | 27611    |
----------------------------------
Eval num_timesteps=150500, episode_reward=180.26 +/- 47.31
Episode length: 45.40 +/- 11.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.723    |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.21     |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 145      |
|    time_elapsed     | 1034     |
|    total_timesteps  | 150861   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.596    |
|    n_updates        | 27715    |
----------------------------------
Eval num_timesteps=151000, episode_reward=291.80 +/- 136.92
Episode length: 73.40 +/- 34.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.4     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.721    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.716    |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.72     |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 145      |
|    time_elapsed     | 1039     |
|    total_timesteps  | 151298   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0716   |
|    n_updates        | 27824    |
----------------------------------
Eval num_timesteps=151500, episode_reward=287.54 +/- 146.18
Episode length: 72.22 +/- 36.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.2     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.719    |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.719    |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 145      |
|    time_elapsed     | 1044     |
|    total_timesteps  | 151676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.88     |
|    n_updates        | 27918    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.718    |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 145      |
|    time_elapsed     | 1044     |
|    total_timesteps  | 151991   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.623    |
|    n_updates        | 27997    |
----------------------------------
Eval num_timesteps=152000, episode_reward=274.40 +/- 135.34
Episode length: 68.94 +/- 33.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.9     |
|    mean_reward      | 274      |
| rollout/            |          |
|    exploration_rate | 0.718    |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.189    |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 145      |
|    time_elapsed     | 1049     |
|    total_timesteps  | 152436   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.72     |
|    n_updates        | 28108    |
----------------------------------
Eval num_timesteps=152500, episode_reward=185.56 +/- 58.57
Episode length: 46.76 +/- 14.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.716    |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.734    |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 145      |
|    time_elapsed     | 1052     |
|    total_timesteps  | 152755   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.23     |
|    n_updates        | 28188    |
----------------------------------
Eval num_timesteps=153000, episode_reward=204.34 +/- 51.27
Episode length: 51.48 +/- 12.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.5     |
|    mean_reward      | 204      |
| rollout/            |          |
|    exploration_rate | 0.714    |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 28249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 145      |
|    time_elapsed     | 1055     |
|    total_timesteps  | 153136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.743    |
|    n_updates        | 28283    |
----------------------------------
Eval num_timesteps=153500, episode_reward=176.96 +/- 52.41
Episode length: 44.62 +/- 13.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.712    |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.175    |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 144      |
|    time_elapsed     | 1059     |
|    total_timesteps  | 153532   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.733    |
|    n_updates        | 28382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.711    |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 145      |
|    time_elapsed     | 1059     |
|    total_timesteps  | 153841   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 28460    |
----------------------------------
Eval num_timesteps=154000, episode_reward=203.34 +/- 106.44
Episode length: 51.16 +/- 26.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | 203      |
| rollout/            |          |
|    exploration_rate | 0.71     |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.636    |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.71     |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 144      |
|    time_elapsed     | 1062     |
|    total_timesteps  | 154015   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0425   |
|    n_updates        | 28503    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 145      |
|    time_elapsed     | 1062     |
|    total_timesteps  | 154284   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.509    |
|    n_updates        | 28570    |
----------------------------------
Eval num_timesteps=154500, episode_reward=274.30 +/- 119.12
Episode length: 68.90 +/- 29.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.9     |
|    mean_reward      | 274      |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0466   |
|    n_updates        | 28624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 144      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 154595   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.612    |
|    n_updates        | 28648    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.707    |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 145      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 154838   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0878   |
|    n_updates        | 28709    |
----------------------------------
Eval num_timesteps=155000, episode_reward=184.76 +/- 42.44
Episode length: 46.50 +/- 10.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.706    |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.65     |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.706    |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 144      |
|    time_elapsed     | 1070     |
|    total_timesteps  | 155106   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.182    |
|    n_updates        | 28776    |
----------------------------------
Eval num_timesteps=155500, episode_reward=177.76 +/- 52.42
Episode length: 44.78 +/- 13.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.704    |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.262    |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.704    |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 144      |
|    time_elapsed     | 1073     |
|    total_timesteps  | 155574   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.164    |
|    n_updates        | 28893    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 145      |
|    time_elapsed     | 1074     |
|    total_timesteps  | 155860   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0478   |
|    n_updates        | 28964    |
----------------------------------
Eval num_timesteps=156000, episode_reward=178.08 +/- 54.31
Episode length: 44.92 +/- 13.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.702    |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 144      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 156038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0833   |
|    n_updates        | 29009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.701    |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 145      |
|    time_elapsed     | 1077     |
|    total_timesteps  | 156221   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.74     |
|    n_updates        | 29055    |
----------------------------------
Eval num_timesteps=156500, episode_reward=245.46 +/- 129.52
Episode length: 61.74 +/- 32.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.7     |
|    mean_reward      | 245      |
| rollout/            |          |
|    exploration_rate | 0.7      |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.178    |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.7      |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 144      |
|    time_elapsed     | 1081     |
|    total_timesteps  | 156631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 29157    |
----------------------------------
Eval num_timesteps=157000, episode_reward=188.28 +/- 47.37
Episode length: 47.44 +/- 11.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.4     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.698    |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.643    |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 144      |
|    time_elapsed     | 1084     |
|    total_timesteps  | 157000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 144      |
|    time_elapsed     | 1084     |
|    total_timesteps  | 157173   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.184    |
|    n_updates        | 29293    |
----------------------------------
Eval num_timesteps=157500, episode_reward=187.42 +/- 55.32
Episode length: 47.22 +/- 13.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.697    |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.234    |
|    n_updates        | 29374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.696    |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 144      |
|    time_elapsed     | 1087     |
|    total_timesteps  | 157522   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.69     |
|    n_updates        | 29380    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 145      |
|    time_elapsed     | 1088     |
|    total_timesteps  | 157815   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.387    |
|    n_updates        | 29453    |
----------------------------------
Eval num_timesteps=158000, episode_reward=253.18 +/- 89.40
Episode length: 63.62 +/- 22.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.6     |
|    mean_reward      | 253      |
| rollout/            |          |
|    exploration_rate | 0.695    |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.694    |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 144      |
|    time_elapsed     | 1092     |
|    total_timesteps  | 158227   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 29556    |
----------------------------------
Eval num_timesteps=158500, episode_reward=181.12 +/- 78.33
Episode length: 45.68 +/- 19.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.693    |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 144      |
|    time_elapsed     | 1095     |
|    total_timesteps  | 158629   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.22     |
|    n_updates        | 29657    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 145      |
|    time_elapsed     | 1095     |
|    total_timesteps  | 158987   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 29746    |
----------------------------------
Eval num_timesteps=159000, episode_reward=197.10 +/- 104.53
Episode length: 49.72 +/- 26.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.691    |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.1      |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.689    |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 144      |
|    time_elapsed     | 1099     |
|    total_timesteps  | 159320   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0862   |
|    n_updates        | 29829    |
----------------------------------
Eval num_timesteps=159500, episode_reward=190.48 +/- 97.28
Episode length: 47.94 +/- 24.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.168    |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 144      |
|    time_elapsed     | 1102     |
|    total_timesteps  | 159677   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0801   |
|    n_updates        | 29919    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 145      |
|    time_elapsed     | 1103     |
|    total_timesteps  | 159986   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 29996    |
----------------------------------
Eval num_timesteps=160000, episode_reward=167.88 +/- 79.87
Episode length: 42.32 +/- 20.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.687    |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0524   |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 144      |
|    time_elapsed     | 1106     |
|    total_timesteps  | 160292   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.32     |
|    n_updates        | 30072    |
----------------------------------
Eval num_timesteps=160500, episode_reward=179.40 +/- 53.85
Episode length: 45.22 +/- 13.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.685    |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.625    |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 144      |
|    time_elapsed     | 1109     |
|    total_timesteps  | 160605   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 30151    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.683    |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 145      |
|    time_elapsed     | 1109     |
|    total_timesteps  | 160949   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 30237    |
----------------------------------
Eval num_timesteps=161000, episode_reward=379.50 +/- 195.18
Episode length: 95.28 +/- 48.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 380      |
| rollout/            |          |
|    exploration_rate | 0.683    |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0927   |
|    n_updates        | 30249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 144      |
|    time_elapsed     | 1115     |
|    total_timesteps  | 161234   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 30308    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 144      |
|    time_elapsed     | 1115     |
|    total_timesteps  | 161439   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.71     |
|    n_updates        | 30359    |
----------------------------------
Eval num_timesteps=161500, episode_reward=259.66 +/- 111.85
Episode length: 65.30 +/- 27.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.3     |
|    mean_reward      | 260      |
| rollout/            |          |
|    exploration_rate | 0.681    |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.164    |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.68     |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 144      |
|    time_elapsed     | 1119     |
|    total_timesteps  | 161675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.822    |
|    n_updates        | 30418    |
----------------------------------
Eval num_timesteps=162000, episode_reward=171.84 +/- 44.26
Episode length: 43.32 +/- 11.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.679    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 144      |
|    time_elapsed     | 1122     |
|    total_timesteps  | 162038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.66     |
|    n_updates        | 30509    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 144      |
|    time_elapsed     | 1123     |
|    total_timesteps  | 162333   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 30583    |
----------------------------------
Eval num_timesteps=162500, episode_reward=182.22 +/- 55.44
Episode length: 45.86 +/- 13.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.677    |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.688    |
|    n_updates        | 30624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.676    |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 144      |
|    time_elapsed     | 1126     |
|    total_timesteps  | 162732   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.93     |
|    n_updates        | 30682    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.675    |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 144      |
|    time_elapsed     | 1127     |
|    total_timesteps  | 162992   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.764    |
|    n_updates        | 30747    |
----------------------------------
Eval num_timesteps=163000, episode_reward=172.98 +/- 39.87
Episode length: 43.64 +/- 9.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.675    |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.705    |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 144      |
|    time_elapsed     | 1130     |
|    total_timesteps  | 163338   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.738    |
|    n_updates        | 30834    |
----------------------------------
Eval num_timesteps=163500, episode_reward=195.66 +/- 115.35
Episode length: 49.24 +/- 28.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.2     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.625    |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.672    |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 144      |
|    time_elapsed     | 1133     |
|    total_timesteps  | 163675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 30918    |
----------------------------------
Eval num_timesteps=164000, episode_reward=175.32 +/- 46.35
Episode length: 44.20 +/- 11.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.671    |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.847    |
|    n_updates        | 30999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.67     |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 144      |
|    time_elapsed     | 1136     |
|    total_timesteps  | 164125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0991   |
|    n_updates        | 31031    |
----------------------------------
Eval num_timesteps=164500, episode_reward=264.50 +/- 140.01
Episode length: 66.48 +/- 34.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.5     |
|    mean_reward      | 264      |
| rollout/            |          |
|    exploration_rate | 0.669    |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.38     |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 144      |
|    time_elapsed     | 1140     |
|    total_timesteps  | 164515   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 31128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.668    |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 144      |
|    time_elapsed     | 1141     |
|    total_timesteps  | 164749   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.303    |
|    n_updates        | 31187    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.667    |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 144      |
|    time_elapsed     | 1141     |
|    total_timesteps  | 164970   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.731    |
|    n_updates        | 31242    |
----------------------------------
Eval num_timesteps=165000, episode_reward=291.02 +/- 177.46
Episode length: 73.14 +/- 44.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.1     |
|    mean_reward      | 291      |
| rollout/            |          |
|    exploration_rate | 0.667    |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.665    |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 144      |
|    time_elapsed     | 1146     |
|    total_timesteps  | 165480   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.214    |
|    n_updates        | 31369    |
----------------------------------
Eval num_timesteps=165500, episode_reward=206.92 +/- 49.20
Episode length: 52.10 +/- 12.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | 207      |
| rollout/            |          |
|    exploration_rate | 0.665    |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.748    |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.664    |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 144      |
|    time_elapsed     | 1149     |
|    total_timesteps  | 165857   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.983    |
|    n_updates        | 31464    |
----------------------------------
Eval num_timesteps=166000, episode_reward=170.60 +/- 37.34
Episode length: 43.02 +/- 9.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 31499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 144      |
|    time_elapsed     | 1152     |
|    total_timesteps  | 166197   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.873    |
|    n_updates        | 31549    |
----------------------------------
Eval num_timesteps=166500, episode_reward=174.02 +/- 41.55
Episode length: 43.90 +/- 10.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.661    |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0848   |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.661    |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 144      |
|    time_elapsed     | 1155     |
|    total_timesteps  | 166599   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 31649    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 144      |
|    time_elapsed     | 1156     |
|    total_timesteps  | 166874   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.273    |
|    n_updates        | 31718    |
----------------------------------
Eval num_timesteps=167000, episode_reward=179.56 +/- 40.89
Episode length: 45.24 +/- 10.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.659    |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.575    |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 144      |
|    time_elapsed     | 1159     |
|    total_timesteps  | 167142   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.871    |
|    n_updates        | 31785    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 144      |
|    time_elapsed     | 1159     |
|    total_timesteps  | 167343   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.53     |
|    n_updates        | 31835    |
----------------------------------
Eval num_timesteps=167500, episode_reward=365.28 +/- 182.95
Episode length: 91.80 +/- 45.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.8     |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 0.657    |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.694    |
|    n_updates        | 31874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.656    |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 143      |
|    time_elapsed     | 1165     |
|    total_timesteps  | 167637   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.821    |
|    n_updates        | 31909    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.655    |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 144      |
|    time_elapsed     | 1165     |
|    total_timesteps  | 167895   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 31973    |
----------------------------------
Eval num_timesteps=168000, episode_reward=183.02 +/- 48.52
Episode length: 46.12 +/- 12.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.655    |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.712    |
|    n_updates        | 31999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 143      |
|    time_elapsed     | 1168     |
|    total_timesteps  | 168153   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.608    |
|    n_updates        | 32038    |
----------------------------------
Eval num_timesteps=168500, episode_reward=187.34 +/- 44.14
Episode length: 47.26 +/- 11.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.42     |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.652    |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 143      |
|    time_elapsed     | 1171     |
|    total_timesteps  | 168618   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.296    |
|    n_updates        | 32154    |
----------------------------------
Eval num_timesteps=169000, episode_reward=339.48 +/- 182.63
Episode length: 85.26 +/- 45.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.3     |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 0.651    |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.24     |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 143      |
|    time_elapsed     | 1177     |
|    total_timesteps  | 169117   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.322    |
|    n_updates        | 32279    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 143      |
|    time_elapsed     | 1177     |
|    total_timesteps  | 169476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 32368    |
----------------------------------
Eval num_timesteps=169500, episode_reward=267.58 +/- 137.93
Episode length: 67.32 +/- 34.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.3     |
|    mean_reward      | 268      |
| rollout/            |          |
|    exploration_rate | 0.649    |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.886    |
|    n_updates        | 32374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.648    |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 143      |
|    time_elapsed     | 1181     |
|    total_timesteps  | 169801   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.189    |
|    n_updates        | 32450    |
----------------------------------
Eval num_timesteps=170000, episode_reward=181.24 +/- 80.13
Episode length: 45.68 +/- 20.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.647    |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.42     |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 143      |
|    time_elapsed     | 1184     |
|    total_timesteps  | 170049   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.16     |
|    n_updates        | 32512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 143      |
|    time_elapsed     | 1185     |
|    total_timesteps  | 170428   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.739    |
|    n_updates        | 32606    |
----------------------------------
Eval num_timesteps=170500, episode_reward=235.88 +/- 136.01
Episode length: 59.34 +/- 34.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.3     |
|    mean_reward      | 236      |
| rollout/            |          |
|    exploration_rate | 0.645    |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.643    |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 143      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 170819   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.769    |
|    n_updates        | 32704    |
----------------------------------
Eval num_timesteps=171000, episode_reward=289.38 +/- 153.36
Episode length: 72.76 +/- 38.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.8     |
|    mean_reward      | 289      |
| rollout/            |          |
|    exploration_rate | 0.643    |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.871    |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.8     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.642    |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 143      |
|    time_elapsed     | 1193     |
|    total_timesteps  | 171210   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.3      |
|    n_updates        | 32802    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 143      |
|    time_elapsed     | 1194     |
|    total_timesteps  | 171496   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.11     |
|    n_updates        | 32873    |
----------------------------------
Eval num_timesteps=171500, episode_reward=296.76 +/- 121.29
Episode length: 74.58 +/- 30.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.6     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.641    |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.726    |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.64     |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 143      |
|    time_elapsed     | 1199     |
|    total_timesteps  | 171765   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.723    |
|    n_updates        | 32941    |
----------------------------------
Eval num_timesteps=172000, episode_reward=163.40 +/- 42.16
Episode length: 41.28 +/- 10.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.639    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.839    |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 143      |
|    time_elapsed     | 1202     |
|    total_timesteps  | 172031   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.51     |
|    n_updates        | 33007    |
----------------------------------
Eval num_timesteps=172500, episode_reward=178.60 +/- 48.87
Episode length: 44.98 +/- 12.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.245    |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.636    |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 143      |
|    time_elapsed     | 1205     |
|    total_timesteps  | 172613   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.66     |
|    n_updates        | 33153    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.635    |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 143      |
|    time_elapsed     | 1205     |
|    total_timesteps  | 172775   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.89     |
|    n_updates        | 33193    |
----------------------------------
Eval num_timesteps=173000, episode_reward=366.76 +/- 213.27
Episode length: 92.08 +/- 53.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.1     |
|    mean_reward      | 367      |
| rollout/            |          |
|    exploration_rate | 0.635    |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.74     |
|    n_updates        | 33249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.634    |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 142      |
|    time_elapsed     | 1211     |
|    total_timesteps  | 173032   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 33257    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.633    |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 142      |
|    time_elapsed     | 1211     |
|    total_timesteps  | 173268   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.755    |
|    n_updates        | 33316    |
----------------------------------
Eval num_timesteps=173500, episode_reward=181.38 +/- 45.10
Episode length: 45.72 +/- 11.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.632    |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.65     |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.632    |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 142      |
|    time_elapsed     | 1215     |
|    total_timesteps  | 173617   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.938    |
|    n_updates        | 33404    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.631    |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 143      |
|    time_elapsed     | 1215     |
|    total_timesteps  | 173868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.42     |
|    n_updates        | 33466    |
----------------------------------
Eval num_timesteps=174000, episode_reward=254.60 +/- 80.53
Episode length: 64.04 +/- 20.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64       |
|    mean_reward      | 255      |
| rollout/            |          |
|    exploration_rate | 0.63     |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.778    |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.63     |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 142      |
|    time_elapsed     | 1219     |
|    total_timesteps  | 174085   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.681    |
|    n_updates        | 33521    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 142      |
|    time_elapsed     | 1219     |
|    total_timesteps  | 174298   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.907    |
|    n_updates        | 33574    |
----------------------------------
Eval num_timesteps=174500, episode_reward=264.36 +/- 120.93
Episode length: 66.54 +/- 30.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.5     |
|    mean_reward      | 264      |
| rollout/            |          |
|    exploration_rate | 0.628    |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0454   |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.628    |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 142      |
|    time_elapsed     | 1224     |
|    total_timesteps  | 174689   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.598    |
|    n_updates        | 33672    |
----------------------------------
Eval num_timesteps=175000, episode_reward=169.20 +/- 41.90
Episode length: 42.72 +/- 10.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.626    |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.25     |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.626    |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 142      |
|    time_elapsed     | 1227     |
|    total_timesteps  | 175112   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 33777    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.624    |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 142      |
|    time_elapsed     | 1228     |
|    total_timesteps  | 175441   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0956   |
|    n_updates        | 33860    |
----------------------------------
Eval num_timesteps=175500, episode_reward=177.48 +/- 46.20
Episode length: 44.70 +/- 11.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.624    |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.829    |
|    n_updates        | 33874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 142      |
|    time_elapsed     | 1231     |
|    total_timesteps  | 175952   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.86     |
|    n_updates        | 33987    |
----------------------------------
Eval num_timesteps=176000, episode_reward=181.46 +/- 46.21
Episode length: 45.72 +/- 11.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.622    |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.621    |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 142      |
|    time_elapsed     | 1235     |
|    total_timesteps  | 176255   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.36     |
|    n_updates        | 34063    |
----------------------------------
Eval num_timesteps=176500, episode_reward=264.74 +/- 103.49
Episode length: 66.56 +/- 25.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.6     |
|    mean_reward      | 265      |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.25     |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.619    |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 142      |
|    time_elapsed     | 1239     |
|    total_timesteps  | 176645   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0623   |
|    n_updates        | 34161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 142      |
|    time_elapsed     | 1240     |
|    total_timesteps  | 176879   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.959    |
|    n_updates        | 34219    |
----------------------------------
Eval num_timesteps=177000, episode_reward=262.44 +/- 106.71
Episode length: 65.96 +/- 26.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66       |
|    mean_reward      | 262      |
| rollout/            |          |
|    exploration_rate | 0.618    |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 142      |
|    time_elapsed     | 1244     |
|    total_timesteps  | 177156   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.57     |
|    n_updates        | 34288    |
----------------------------------
Eval num_timesteps=177500, episode_reward=174.16 +/- 36.05
Episode length: 43.90 +/- 9.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.616    |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.782    |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 142      |
|    time_elapsed     | 1247     |
|    total_timesteps  | 177563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.197    |
|    n_updates        | 34390    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.615    |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 142      |
|    time_elapsed     | 1248     |
|    total_timesteps  | 177761   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.314    |
|    n_updates        | 34440    |
----------------------------------
Eval num_timesteps=178000, episode_reward=173.10 +/- 43.88
Episode length: 43.62 +/- 10.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.614    |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 142      |
|    time_elapsed     | 1251     |
|    total_timesteps  | 178031   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.966    |
|    n_updates        | 34507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.613    |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 142      |
|    time_elapsed     | 1251     |
|    total_timesteps  | 178277   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.779    |
|    n_updates        | 34569    |
----------------------------------
Eval num_timesteps=178500, episode_reward=283.02 +/- 116.25
Episode length: 71.10 +/- 29.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.1     |
|    mean_reward      | 283      |
| rollout/            |          |
|    exploration_rate | 0.612    |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0856   |
|    n_updates        | 34624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 142      |
|    time_elapsed     | 1256     |
|    total_timesteps  | 178628   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 34656    |
----------------------------------
Eval num_timesteps=179000, episode_reward=188.80 +/- 51.64
Episode length: 47.56 +/- 12.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.609    |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0554   |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.609    |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 142      |
|    time_elapsed     | 1260     |
|    total_timesteps  | 179094   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.963    |
|    n_updates        | 34773    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 142      |
|    time_elapsed     | 1261     |
|    total_timesteps  | 179491   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.264    |
|    n_updates        | 34872    |
----------------------------------
Eval num_timesteps=179500, episode_reward=225.50 +/- 137.17
Episode length: 56.74 +/- 34.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.7     |
|    mean_reward      | 226      |
| rollout/            |          |
|    exploration_rate | 0.607    |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.606    |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 142      |
|    time_elapsed     | 1265     |
|    total_timesteps  | 179902   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 34975    |
----------------------------------
Eval num_timesteps=180000, episode_reward=173.22 +/- 45.14
Episode length: 43.64 +/- 11.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.605    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.824    |
|    n_updates        | 34999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.604    |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 142      |
|    time_elapsed     | 1268     |
|    total_timesteps  | 180238   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 35059    |
----------------------------------
Eval num_timesteps=180500, episode_reward=168.94 +/- 39.09
Episode length: 42.66 +/- 9.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.603    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0608   |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.603    |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 141      |
|    time_elapsed     | 1271     |
|    total_timesteps  | 180526   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.51     |
|    n_updates        | 35131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 142      |
|    time_elapsed     | 1271     |
|    total_timesteps  | 180741   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0713   |
|    n_updates        | 35185    |
----------------------------------
Eval num_timesteps=181000, episode_reward=179.52 +/- 50.84
Episode length: 45.30 +/- 12.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.601    |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.89     |
|    n_updates        | 35249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.601    |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 141      |
|    time_elapsed     | 1274     |
|    total_timesteps  | 181030   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.69     |
|    n_updates        | 35257    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.599    |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 142      |
|    time_elapsed     | 1275     |
|    total_timesteps  | 181496   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 35373    |
----------------------------------
Eval num_timesteps=181500, episode_reward=171.08 +/- 37.36
Episode length: 43.10 +/- 9.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.599    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.754    |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.597    |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 142      |
|    time_elapsed     | 1278     |
|    total_timesteps  | 181940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0801   |
|    n_updates        | 35484    |
----------------------------------
Eval num_timesteps=182000, episode_reward=177.60 +/- 47.86
Episode length: 44.78 +/- 11.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.597    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.22     |
|    n_updates        | 35499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 142      |
|    time_elapsed     | 1281     |
|    total_timesteps  | 182216   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.833    |
|    n_updates        | 35553    |
----------------------------------
Eval num_timesteps=182500, episode_reward=175.86 +/- 42.93
Episode length: 44.36 +/- 10.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.595    |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.888    |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 142      |
|    time_elapsed     | 1284     |
|    total_timesteps  | 182506   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.098    |
|    n_updates        | 35626    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.594    |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 142      |
|    time_elapsed     | 1285     |
|    total_timesteps  | 182765   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.849    |
|    n_updates        | 35691    |
----------------------------------
Eval num_timesteps=183000, episode_reward=168.12 +/- 44.57
Episode length: 42.42 +/- 11.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.593    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.592    |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 142      |
|    time_elapsed     | 1288     |
|    total_timesteps  | 183185   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.66     |
|    n_updates        | 35796    |
----------------------------------
Eval num_timesteps=183500, episode_reward=188.26 +/- 58.41
Episode length: 47.46 +/- 14.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.59     |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 35874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86       |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.589    |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 142      |
|    time_elapsed     | 1292     |
|    total_timesteps  | 183710   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.194    |
|    n_updates        | 35927    |
----------------------------------
Eval num_timesteps=184000, episode_reward=219.40 +/- 135.62
Episode length: 55.18 +/- 33.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.2     |
|    mean_reward      | 219      |
| rollout/            |          |
|    exploration_rate | 0.588    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0727   |
|    n_updates        | 35999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.587    |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 142      |
|    time_elapsed     | 1296     |
|    total_timesteps  | 184173   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.866    |
|    n_updates        | 36043    |
----------------------------------
Eval num_timesteps=184500, episode_reward=212.10 +/- 54.64
Episode length: 53.46 +/- 13.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.5     |
|    mean_reward      | 212      |
| rollout/            |          |
|    exploration_rate | 0.586    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0712   |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.585    |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 141      |
|    time_elapsed     | 1300     |
|    total_timesteps  | 184650   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.82     |
|    n_updates        | 36162    |
----------------------------------
Eval num_timesteps=185000, episode_reward=182.14 +/- 79.61
Episode length: 46.00 +/- 19.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.584    |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.79     |
|    n_updates        | 36249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 141      |
|    time_elapsed     | 1304     |
|    total_timesteps  | 185125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.822    |
|    n_updates        | 36281    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.582    |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 142      |
|    time_elapsed     | 1304     |
|    total_timesteps  | 185374   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0644   |
|    n_updates        | 36343    |
----------------------------------
Eval num_timesteps=185500, episode_reward=294.32 +/- 116.44
Episode length: 73.96 +/- 29.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74       |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 0.582    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.581    |
| time/               |          |
|    episodes         | 2456     |
|    fps              | 141      |
|    time_elapsed     | 1309     |
|    total_timesteps  | 185675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.811    |
|    n_updates        | 36418    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.2     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 2460     |
|    fps              | 141      |
|    time_elapsed     | 1310     |
|    total_timesteps  | 185975   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 36493    |
----------------------------------
Eval num_timesteps=186000, episode_reward=208.62 +/- 107.87
Episode length: 52.50 +/- 26.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.5     |
|    mean_reward      | 209      |
| rollout/            |          |
|    exploration_rate | 0.58     |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.32     |
|    n_updates        | 36499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.3     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.577    |
| time/               |          |
|    episodes         | 2464     |
|    fps              | 141      |
|    time_elapsed     | 1314     |
|    total_timesteps  | 186498   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 36624    |
----------------------------------
Eval num_timesteps=186500, episode_reward=197.54 +/- 104.46
Episode length: 49.74 +/- 26.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.577    |
| time/               |          |
|    total_timesteps  | 186500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 141      |
|    time_elapsed     | 1317     |
|    total_timesteps  | 186862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 36715    |
----------------------------------
Eval num_timesteps=187000, episode_reward=294.20 +/- 193.62
Episode length: 74.00 +/- 48.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74       |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.23     |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.575    |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 141      |
|    time_elapsed     | 1323     |
|    total_timesteps  | 187176   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.758    |
|    n_updates        | 36793    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 2476     |
|    fps              | 141      |
|    time_elapsed     | 1323     |
|    total_timesteps  | 187411   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.793    |
|    n_updates        | 36852    |
----------------------------------
Eval num_timesteps=187500, episode_reward=314.22 +/- 176.99
Episode length: 78.88 +/- 44.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.9     |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.573    |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 36874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 2480     |
|    fps              | 141      |
|    time_elapsed     | 1328     |
|    total_timesteps  | 187704   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.198    |
|    n_updates        | 36925    |
----------------------------------
Eval num_timesteps=188000, episode_reward=305.44 +/- 169.99
Episode length: 76.78 +/- 42.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.8     |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.571    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0471   |
|    n_updates        | 36999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.571    |
| time/               |          |
|    episodes         | 2484     |
|    fps              | 140      |
|    time_elapsed     | 1333     |
|    total_timesteps  | 188033   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.53     |
|    n_updates        | 37008    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 2488     |
|    fps              | 141      |
|    time_elapsed     | 1334     |
|    total_timesteps  | 188317   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.896    |
|    n_updates        | 37079    |
----------------------------------
Eval num_timesteps=188500, episode_reward=278.94 +/- 161.21
Episode length: 70.10 +/- 40.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.1     |
|    mean_reward      | 279      |
| rollout/            |          |
|    exploration_rate | 0.569    |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0616   |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.568    |
| time/               |          |
|    episodes         | 2492     |
|    fps              | 140      |
|    time_elapsed     | 1339     |
|    total_timesteps  | 188579   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.815    |
|    n_updates        | 37144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.3     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.567    |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 140      |
|    time_elapsed     | 1339     |
|    total_timesteps  | 188870   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.349    |
|    n_updates        | 37217    |
----------------------------------
Eval num_timesteps=189000, episode_reward=174.44 +/- 46.80
Episode length: 44.00 +/- 11.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.567    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86       |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.566    |
| time/               |          |
|    episodes         | 2500     |
|    fps              | 140      |
|    time_elapsed     | 1342     |
|    total_timesteps  | 189122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0874   |
|    n_updates        | 37280    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.4     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 2504     |
|    fps              | 141      |
|    time_elapsed     | 1342     |
|    total_timesteps  | 189383   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.808    |
|    n_updates        | 37345    |
----------------------------------
Eval num_timesteps=189500, episode_reward=184.08 +/- 41.74
Episode length: 46.42 +/- 10.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.564    |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.943    |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 2508     |
|    fps              | 140      |
|    time_elapsed     | 1346     |
|    total_timesteps  | 189790   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 37447    |
----------------------------------
Eval num_timesteps=190000, episode_reward=161.60 +/- 39.81
Episode length: 40.74 +/- 9.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.562    |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.562    |
| time/               |          |
|    episodes         | 2512     |
|    fps              | 140      |
|    time_elapsed     | 1349     |
|    total_timesteps  | 190068   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.41     |
|    n_updates        | 37516    |
----------------------------------
Eval num_timesteps=190500, episode_reward=221.86 +/- 133.29
Episode length: 55.90 +/- 33.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.9     |
|    mean_reward      | 222      |
| rollout/            |          |
|    exploration_rate | 0.56     |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.9     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 2516     |
|    fps              | 140      |
|    time_elapsed     | 1353     |
|    total_timesteps  | 190529   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.44     |
|    n_updates        | 37632    |
----------------------------------
Eval num_timesteps=191000, episode_reward=173.76 +/- 46.34
Episode length: 43.82 +/- 11.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.558    |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.871    |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 2520     |
|    fps              | 140      |
|    time_elapsed     | 1356     |
|    total_timesteps  | 191014   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.213    |
|    n_updates        | 37753    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.557    |
| time/               |          |
|    episodes         | 2524     |
|    fps              | 140      |
|    time_elapsed     | 1356     |
|    total_timesteps  | 191206   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0835   |
|    n_updates        | 37801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 2528     |
|    fps              | 141      |
|    time_elapsed     | 1357     |
|    total_timesteps  | 191480   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 37869    |
----------------------------------
Eval num_timesteps=191500, episode_reward=175.30 +/- 47.35
Episode length: 44.22 +/- 11.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.556    |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0925   |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.6     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 2532     |
|    fps              | 140      |
|    time_elapsed     | 1360     |
|    total_timesteps  | 191745   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.776    |
|    n_updates        | 37936    |
----------------------------------
Eval num_timesteps=192000, episode_reward=230.98 +/- 109.11
Episode length: 58.16 +/- 27.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.2     |
|    mean_reward      | 231      |
| rollout/            |          |
|    exploration_rate | 0.553    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.171    |
|    n_updates        | 37999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.553    |
| time/               |          |
|    episodes         | 2536     |
|    fps              | 140      |
|    time_elapsed     | 1364     |
|    total_timesteps  | 192111   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0601   |
|    n_updates        | 38027    |
----------------------------------
Eval num_timesteps=192500, episode_reward=321.28 +/- 153.88
Episode length: 80.72 +/- 38.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.7     |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.551    |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 2540     |
|    fps              | 140      |
|    time_elapsed     | 1370     |
|    total_timesteps  | 192561   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.82     |
|    n_updates        | 38140    |
----------------------------------
Eval num_timesteps=193000, episode_reward=304.58 +/- 143.91
Episode length: 76.58 +/- 35.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.6     |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.549    |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.346    |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.549    |
| time/               |          |
|    episodes         | 2544     |
|    fps              | 140      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 193003   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.91     |
|    n_updates        | 38250    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.548    |
| time/               |          |
|    episodes         | 2548     |
|    fps              | 140      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 193280   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 38319    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.547    |
| time/               |          |
|    episodes         | 2552     |
|    fps              | 140      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 193476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.909    |
|    n_updates        | 38368    |
----------------------------------
Eval num_timesteps=193500, episode_reward=228.02 +/- 80.91
Episode length: 57.38 +/- 20.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.4     |
|    mean_reward      | 228      |
| rollout/            |          |
|    exploration_rate | 0.547    |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.812    |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 2556     |
|    fps              | 140      |
|    time_elapsed     | 1380     |
|    total_timesteps  | 193796   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.846    |
|    n_updates        | 38448    |
----------------------------------
Eval num_timesteps=194000, episode_reward=298.76 +/- 137.51
Episode length: 75.12 +/- 34.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.1     |
|    mean_reward      | 299      |
| rollout/            |          |
|    exploration_rate | 0.545    |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.961    |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.543    |
| time/               |          |
|    episodes         | 2560     |
|    fps              | 140      |
|    time_elapsed     | 1386     |
|    total_timesteps  | 194273   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 38568    |
----------------------------------
Eval num_timesteps=194500, episode_reward=204.50 +/- 53.43
Episode length: 51.50 +/- 13.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.5     |
|    mean_reward      | 204      |
| rollout/            |          |
|    exploration_rate | 0.542    |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.844    |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.542    |
| time/               |          |
|    episodes         | 2564     |
|    fps              | 140      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 194584   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.882    |
|    n_updates        | 38645    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.541    |
| time/               |          |
|    episodes         | 2568     |
|    fps              | 140      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 194917   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 38729    |
----------------------------------
Eval num_timesteps=195000, episode_reward=166.20 +/- 50.20
Episode length: 41.96 +/- 12.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.54     |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.867    |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.539    |
| time/               |          |
|    episodes         | 2572     |
|    fps              | 140      |
|    time_elapsed     | 1392     |
|    total_timesteps  | 195233   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.208    |
|    n_updates        | 38808    |
----------------------------------
Eval num_timesteps=195500, episode_reward=241.10 +/- 96.57
Episode length: 60.60 +/- 24.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.6     |
|    mean_reward      | 241      |
| rollout/            |          |
|    exploration_rate | 0.538    |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.771    |
|    n_updates        | 38874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.538    |
| time/               |          |
|    episodes         | 2576     |
|    fps              | 139      |
|    time_elapsed     | 1397     |
|    total_timesteps  | 195577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.859    |
|    n_updates        | 38894    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.536    |
| time/               |          |
|    episodes         | 2580     |
|    fps              | 140      |
|    time_elapsed     | 1397     |
|    total_timesteps  | 195955   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0781   |
|    n_updates        | 38988    |
----------------------------------
Eval num_timesteps=196000, episode_reward=212.90 +/- 114.05
Episode length: 53.64 +/- 28.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.6     |
|    mean_reward      | 213      |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.893    |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.534    |
| time/               |          |
|    episodes         | 2584     |
|    fps              | 140      |
|    time_elapsed     | 1401     |
|    total_timesteps  | 196351   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.939    |
|    n_updates        | 39087    |
----------------------------------
Eval num_timesteps=196500, episode_reward=193.44 +/- 49.07
Episode length: 48.68 +/- 12.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.533    |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.66     |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.533    |
| time/               |          |
|    episodes         | 2588     |
|    fps              | 139      |
|    time_elapsed     | 1405     |
|    total_timesteps  | 196700   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 39174    |
----------------------------------
Eval num_timesteps=197000, episode_reward=184.86 +/- 50.83
Episode length: 46.58 +/- 12.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.531    |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.53     |
| time/               |          |
|    episodes         | 2592     |
|    fps              | 139      |
|    time_elapsed     | 1409     |
|    total_timesteps  | 197258   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.969    |
|    n_updates        | 39314    |
----------------------------------
Eval num_timesteps=197500, episode_reward=178.96 +/- 48.65
Episode length: 45.06 +/- 12.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.529    |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.159    |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.4     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.529    |
| time/               |          |
|    episodes         | 2596     |
|    fps              | 139      |
|    time_elapsed     | 1412     |
|    total_timesteps  | 197507   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 39376    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 2600     |
|    fps              | 140      |
|    time_elapsed     | 1413     |
|    total_timesteps  | 197867   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0774   |
|    n_updates        | 39466    |
----------------------------------
Eval num_timesteps=198000, episode_reward=185.82 +/- 42.94
Episode length: 46.90 +/- 10.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.527    |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.828    |
|    n_updates        | 39499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.5     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.525    |
| time/               |          |
|    episodes         | 2604     |
|    fps              | 140      |
|    time_elapsed     | 1416     |
|    total_timesteps  | 198434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 39608    |
----------------------------------
Eval num_timesteps=198500, episode_reward=259.20 +/- 73.47
Episode length: 65.12 +/- 18.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.1     |
|    mean_reward      | 259      |
| rollout/            |          |
|    exploration_rate | 0.525    |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.449    |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.3     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 2608     |
|    fps              | 139      |
|    time_elapsed     | 1421     |
|    total_timesteps  | 198825   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.68     |
|    n_updates        | 39706    |
----------------------------------
Eval num_timesteps=199000, episode_reward=290.04 +/- 164.40
Episode length: 72.94 +/- 41.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.9     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.522    |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0363   |
|    n_updates        | 39749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.7     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.521    |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 139      |
|    time_elapsed     | 1426     |
|    total_timesteps  | 199238   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0741   |
|    n_updates        | 39809    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.6     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 139      |
|    time_elapsed     | 1426     |
|    total_timesteps  | 199490   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 39872    |
----------------------------------
Eval num_timesteps=199500, episode_reward=174.78 +/- 49.40
Episode length: 44.12 +/- 12.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.52     |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0728   |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.2     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.518    |
| time/               |          |
|    episodes         | 2620     |
|    fps              | 139      |
|    time_elapsed     | 1430     |
|    total_timesteps  | 199837   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0471   |
|    n_updates        | 39959    |
----------------------------------
Eval num_timesteps=200000, episode_reward=230.74 +/- 86.38
Episode length: 58.10 +/- 21.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | 231      |
| rollout/            |          |
|    exploration_rate | 0.518    |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89       |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.517    |
| time/               |          |
|    episodes         | 2624     |
|    fps              | 139      |
|    time_elapsed     | 1433     |
|    total_timesteps  | 200111   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.418    |
|    n_updates        | 40027    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.516    |
| time/               |          |
|    episodes         | 2628     |
|    fps              | 139      |
|    time_elapsed     | 1434     |
|    total_timesteps  | 200420   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.892    |
|    n_updates        | 40104    |
----------------------------------
Eval num_timesteps=200500, episode_reward=178.06 +/- 52.77
Episode length: 44.84 +/- 13.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.515    |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.1     |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 2632     |
|    fps              | 139      |
|    time_elapsed     | 1437     |
|    total_timesteps  | 200752   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 40187    |
----------------------------------
Eval num_timesteps=201000, episode_reward=258.24 +/- 111.05
Episode length: 64.94 +/- 27.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.9     |
|    mean_reward      | 258      |
| rollout/            |          |
|    exploration_rate | 0.513    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0631   |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.2     |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.513    |
| time/               |          |
|    episodes         | 2636     |
|    fps              | 139      |
|    time_elapsed     | 1442     |
|    total_timesteps  | 201132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 40282    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 2640     |
|    fps              | 139      |
|    time_elapsed     | 1442     |
|    total_timesteps  | 201430   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.906    |
|    n_updates        | 40357    |
----------------------------------
Eval num_timesteps=201500, episode_reward=272.98 +/- 108.56
Episode length: 68.62 +/- 27.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.6     |
|    mean_reward      | 273      |
| rollout/            |          |
|    exploration_rate | 0.511    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.49     |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.51     |
| time/               |          |
|    episodes         | 2644     |
|    fps              | 139      |
|    time_elapsed     | 1447     |
|    total_timesteps  | 201800   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.873    |
|    n_updates        | 40449    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 2648     |
|    fps              | 139      |
|    time_elapsed     | 1448     |
|    total_timesteps  | 201999   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.42     |
|    n_updates        | 40499    |
----------------------------------
Eval num_timesteps=202000, episode_reward=177.92 +/- 40.54
Episode length: 44.88 +/- 10.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.507    |
| time/               |          |
|    episodes         | 2652     |
|    fps              | 139      |
|    time_elapsed     | 1451     |
|    total_timesteps  | 202384   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.882    |
|    n_updates        | 40595    |
----------------------------------
Eval num_timesteps=202500, episode_reward=192.04 +/- 78.09
Episode length: 48.40 +/- 19.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.506    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.92     |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.6     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 2656     |
|    fps              | 139      |
|    time_elapsed     | 1455     |
|    total_timesteps  | 202860   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.879    |
|    n_updates        | 40714    |
----------------------------------
Eval num_timesteps=203000, episode_reward=185.32 +/- 50.16
Episode length: 46.66 +/- 12.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.504    |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0477   |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.6     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.503    |
| time/               |          |
|    episodes         | 2660     |
|    fps              | 139      |
|    time_elapsed     | 1458     |
|    total_timesteps  | 203230   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 40807    |
----------------------------------
Eval num_timesteps=203500, episode_reward=223.10 +/- 72.04
Episode length: 56.16 +/- 18.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.2     |
|    mean_reward      | 223      |
| rollout/            |          |
|    exploration_rate | 0.502    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 40874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.6     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.502    |
| time/               |          |
|    episodes         | 2664     |
|    fps              | 139      |
|    time_elapsed     | 1462     |
|    total_timesteps  | 203543   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 40885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.5      |
| time/               |          |
|    episodes         | 2668     |
|    fps              | 139      |
|    time_elapsed     | 1463     |
|    total_timesteps  | 203840   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 40959    |
----------------------------------
Eval num_timesteps=204000, episode_reward=219.48 +/- 131.28
Episode length: 55.24 +/- 32.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.2     |
|    mean_reward      | 219      |
| rollout/            |          |
|    exploration_rate | 0.5      |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.72     |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.8     |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.499    |
| time/               |          |
|    episodes         | 2672     |
|    fps              | 139      |
|    time_elapsed     | 1467     |
|    total_timesteps  | 204214   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.329    |
|    n_updates        | 41053    |
----------------------------------
Eval num_timesteps=204500, episode_reward=274.74 +/- 128.69
Episode length: 69.04 +/- 32.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69       |
|    mean_reward      | 275      |
| rollout/            |          |
|    exploration_rate | 0.497    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0535   |
|    n_updates        | 41124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.3     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.497    |
| time/               |          |
|    episodes         | 2676     |
|    fps              | 138      |
|    time_elapsed     | 1472     |
|    total_timesteps  | 204611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.939    |
|    n_updates        | 41152    |
----------------------------------
Eval num_timesteps=205000, episode_reward=176.62 +/- 47.90
Episode length: 44.56 +/- 11.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.495    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0923   |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.7     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.495    |
| time/               |          |
|    episodes         | 2680     |
|    fps              | 138      |
|    time_elapsed     | 1475     |
|    total_timesteps  | 205020   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 41254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.3     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.493    |
| time/               |          |
|    episodes         | 2684     |
|    fps              | 139      |
|    time_elapsed     | 1476     |
|    total_timesteps  | 205386   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.181    |
|    n_updates        | 41346    |
----------------------------------
Eval num_timesteps=205500, episode_reward=171.64 +/- 54.28
Episode length: 43.34 +/- 13.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.493    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0915   |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.492    |
| time/               |          |
|    episodes         | 2688     |
|    fps              | 138      |
|    time_elapsed     | 1479     |
|    total_timesteps  | 205644   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.73     |
|    n_updates        | 41410    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.49     |
| time/               |          |
|    episodes         | 2692     |
|    fps              | 139      |
|    time_elapsed     | 1480     |
|    total_timesteps  | 205983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 41495    |
----------------------------------
Eval num_timesteps=206000, episode_reward=175.58 +/- 46.84
Episode length: 44.28 +/- 11.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.49     |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.276    |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.489    |
| time/               |          |
|    episodes         | 2696     |
|    fps              | 139      |
|    time_elapsed     | 1483     |
|    total_timesteps  | 206267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.487    |
|    n_updates        | 41566    |
----------------------------------
Eval num_timesteps=206500, episode_reward=228.92 +/- 65.26
Episode length: 57.56 +/- 16.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | 229      |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.92     |
|    n_updates        | 41624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 2700     |
|    fps              | 138      |
|    time_elapsed     | 1487     |
|    total_timesteps  | 206595   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.862    |
|    n_updates        | 41648    |
----------------------------------
Eval num_timesteps=207000, episode_reward=181.72 +/- 56.36
Episode length: 45.82 +/- 14.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.486    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.1     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.486    |
| time/               |          |
|    episodes         | 2704     |
|    fps              | 138      |
|    time_elapsed     | 1490     |
|    total_timesteps  | 207045   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.331    |
|    n_updates        | 41761    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.484    |
| time/               |          |
|    episodes         | 2708     |
|    fps              | 139      |
|    time_elapsed     | 1491     |
|    total_timesteps  | 207392   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.854    |
|    n_updates        | 41847    |
----------------------------------
Eval num_timesteps=207500, episode_reward=231.56 +/- 53.78
Episode length: 58.22 +/- 13.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.2     |
|    mean_reward      | 232      |
| rollout/            |          |
|    exploration_rate | 0.483    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.33     |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.483    |
| time/               |          |
|    episodes         | 2712     |
|    fps              | 138      |
|    time_elapsed     | 1495     |
|    total_timesteps  | 207674   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.196    |
|    n_updates        | 41918    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.481    |
| time/               |          |
|    episodes         | 2716     |
|    fps              | 139      |
|    time_elapsed     | 1495     |
|    total_timesteps  | 207996   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.06     |
|    n_updates        | 41998    |
----------------------------------
Eval num_timesteps=208000, episode_reward=172.20 +/- 48.42
Episode length: 43.46 +/- 12.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.481    |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.388    |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.48     |
| time/               |          |
|    episodes         | 2720     |
|    fps              | 138      |
|    time_elapsed     | 1498     |
|    total_timesteps  | 208328   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.813    |
|    n_updates        | 42081    |
----------------------------------
Eval num_timesteps=208500, episode_reward=181.36 +/- 49.23
Episode length: 45.72 +/- 12.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.479    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.223    |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.6     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.478    |
| time/               |          |
|    episodes         | 2724     |
|    fps              | 138      |
|    time_elapsed     | 1502     |
|    total_timesteps  | 208675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0627   |
|    n_updates        | 42168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.477    |
| time/               |          |
|    episodes         | 2728     |
|    fps              | 138      |
|    time_elapsed     | 1502     |
|    total_timesteps  | 208851   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 42212    |
----------------------------------
Eval num_timesteps=209000, episode_reward=176.92 +/- 43.71
Episode length: 44.64 +/- 10.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.476    |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.289    |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.476    |
| time/               |          |
|    episodes         | 2732     |
|    fps              | 138      |
|    time_elapsed     | 1505     |
|    total_timesteps  | 209195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.398    |
|    n_updates        | 42298    |
----------------------------------
Eval num_timesteps=209500, episode_reward=277.64 +/- 149.55
Episode length: 69.84 +/- 37.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.8     |
|    mean_reward      | 278      |
| rollout/            |          |
|    exploration_rate | 0.474    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.88     |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 2736     |
|    fps              | 138      |
|    time_elapsed     | 1510     |
|    total_timesteps  | 209681   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 42420    |
----------------------------------
Eval num_timesteps=210000, episode_reward=186.12 +/- 43.27
Episode length: 46.96 +/- 10.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.472    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.471    |
| time/               |          |
|    episodes         | 2740     |
|    fps              | 138      |
|    time_elapsed     | 1514     |
|    total_timesteps  | 210113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.555    |
|    n_updates        | 42528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.47     |
| time/               |          |
|    episodes         | 2744     |
|    fps              | 138      |
|    time_elapsed     | 1514     |
|    total_timesteps  | 210300   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.57     |
|    n_updates        | 42574    |
----------------------------------
Eval num_timesteps=210500, episode_reward=341.84 +/- 176.30
Episode length: 85.82 +/- 44.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.8     |
|    mean_reward      | 342      |
| rollout/            |          |
|    exploration_rate | 0.469    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.552    |
|    n_updates        | 42624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.469    |
| time/               |          |
|    episodes         | 2748     |
|    fps              | 138      |
|    time_elapsed     | 1520     |
|    total_timesteps  | 210569   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 42642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 2752     |
|    fps              | 138      |
|    time_elapsed     | 1520     |
|    total_timesteps  | 210783   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 42695    |
----------------------------------
Eval num_timesteps=211000, episode_reward=178.30 +/- 48.30
Episode length: 44.94 +/- 12.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.467    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.933    |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.466    |
| time/               |          |
|    episodes         | 2756     |
|    fps              | 138      |
|    time_elapsed     | 1524     |
|    total_timesteps  | 211251   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.976    |
|    n_updates        | 42812    |
----------------------------------
Eval num_timesteps=211500, episode_reward=198.42 +/- 89.91
Episode length: 49.98 +/- 22.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50       |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.465    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.465    |
| time/               |          |
|    episodes         | 2760     |
|    fps              | 138      |
|    time_elapsed     | 1528     |
|    total_timesteps  | 211555   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.045    |
|    n_updates        | 42888    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.463    |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 138      |
|    time_elapsed     | 1528     |
|    total_timesteps  | 211789   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.06     |
|    n_updates        | 42947    |
----------------------------------
Eval num_timesteps=212000, episode_reward=292.16 +/- 109.26
Episode length: 73.44 +/- 27.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.4     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.462    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.462    |
| time/               |          |
|    episodes         | 2768     |
|    fps              | 138      |
|    time_elapsed     | 1533     |
|    total_timesteps  | 212083   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.58     |
|    n_updates        | 43020    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.461    |
| time/               |          |
|    episodes         | 2772     |
|    fps              | 138      |
|    time_elapsed     | 1534     |
|    total_timesteps  | 212369   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 43092    |
----------------------------------
Eval num_timesteps=212500, episode_reward=231.78 +/- 107.27
Episode length: 58.36 +/- 26.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.4     |
|    mean_reward      | 232      |
| rollout/            |          |
|    exploration_rate | 0.46     |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.14     |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.459    |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 138      |
|    time_elapsed     | 1538     |
|    total_timesteps  | 212670   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.67     |
|    n_updates        | 43167    |
----------------------------------
Eval num_timesteps=213000, episode_reward=184.22 +/- 43.34
Episode length: 46.38 +/- 10.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.458    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.177    |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 2780     |
|    fps              | 138      |
|    time_elapsed     | 1541     |
|    total_timesteps  | 213052   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 43262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.456    |
| time/               |          |
|    episodes         | 2784     |
|    fps              | 138      |
|    time_elapsed     | 1542     |
|    total_timesteps  | 213409   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 43352    |
----------------------------------
Eval num_timesteps=213500, episode_reward=272.94 +/- 113.46
Episode length: 68.64 +/- 28.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.6     |
|    mean_reward      | 273      |
| rollout/            |          |
|    exploration_rate | 0.455    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.953    |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.455    |
| time/               |          |
|    episodes         | 2788     |
|    fps              | 138      |
|    time_elapsed     | 1546     |
|    total_timesteps  | 213638   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1        |
|    n_updates        | 43409    |
----------------------------------
Eval num_timesteps=214000, episode_reward=172.54 +/- 46.41
Episode length: 43.46 +/- 11.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.453    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 2792     |
|    fps              | 138      |
|    time_elapsed     | 1550     |
|    total_timesteps  | 214207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.48     |
|    n_updates        | 43551    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.451    |
| time/               |          |
|    episodes         | 2796     |
|    fps              | 138      |
|    time_elapsed     | 1550     |
|    total_timesteps  | 214369   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.991    |
|    n_updates        | 43592    |
----------------------------------
Eval num_timesteps=214500, episode_reward=165.42 +/- 37.07
Episode length: 41.74 +/- 9.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.451    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0637   |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.449    |
| time/               |          |
|    episodes         | 2800     |
|    fps              | 138      |
|    time_elapsed     | 1554     |
|    total_timesteps  | 214883   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.517    |
|    n_updates        | 43720    |
----------------------------------
Eval num_timesteps=215000, episode_reward=168.72 +/- 37.59
Episode length: 42.52 +/- 9.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.448    |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.972    |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.447    |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 138      |
|    time_elapsed     | 1557     |
|    total_timesteps  | 215225   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.49     |
|    n_updates        | 43806    |
----------------------------------
Eval num_timesteps=215500, episode_reward=182.64 +/- 57.78
Episode length: 46.00 +/- 14.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.446    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.8      |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 2808     |
|    fps              | 138      |
|    time_elapsed     | 1560     |
|    total_timesteps  | 215608   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0783   |
|    n_updates        | 43901    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 2812     |
|    fps              | 138      |
|    time_elapsed     | 1561     |
|    total_timesteps  | 215797   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.815    |
|    n_updates        | 43949    |
----------------------------------
Eval num_timesteps=216000, episode_reward=278.08 +/- 139.79
Episode length: 69.92 +/- 34.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.9     |
|    mean_reward      | 278      |
| rollout/            |          |
|    exploration_rate | 0.444    |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 2816     |
|    fps              | 138      |
|    time_elapsed     | 1566     |
|    total_timesteps  | 216172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 44042    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.441    |
| time/               |          |
|    episodes         | 2820     |
|    fps              | 138      |
|    time_elapsed     | 1566     |
|    total_timesteps  | 216451   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.372    |
|    n_updates        | 44112    |
----------------------------------
Eval num_timesteps=216500, episode_reward=183.56 +/- 60.54
Episode length: 46.22 +/- 15.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.441    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.426    |
|    n_updates        | 44124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.44     |
| time/               |          |
|    episodes         | 2824     |
|    fps              | 138      |
|    time_elapsed     | 1570     |
|    total_timesteps  | 216849   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.204    |
|    n_updates        | 44212    |
----------------------------------
Eval num_timesteps=217000, episode_reward=315.24 +/- 177.40
Episode length: 79.20 +/- 44.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.2     |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.439    |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.14     |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.438    |
| time/               |          |
|    episodes         | 2828     |
|    fps              | 137      |
|    time_elapsed     | 1575     |
|    total_timesteps  | 217101   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.833    |
|    n_updates        | 44275    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 2832     |
|    fps              | 137      |
|    time_elapsed     | 1575     |
|    total_timesteps  | 217403   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.93     |
|    n_updates        | 44350    |
----------------------------------
Eval num_timesteps=217500, episode_reward=321.92 +/- 103.83
Episode length: 80.82 +/- 26.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.8     |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 0.436    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.435    |
| time/               |          |
|    episodes         | 2836     |
|    fps              | 137      |
|    time_elapsed     | 1581     |
|    total_timesteps  | 217863   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.264    |
|    n_updates        | 44465    |
----------------------------------
Eval num_timesteps=218000, episode_reward=182.64 +/- 46.78
Episode length: 46.06 +/- 11.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.434    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.34     |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.433    |
| time/               |          |
|    episodes         | 2840     |
|    fps              | 137      |
|    time_elapsed     | 1584     |
|    total_timesteps  | 218174   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.929    |
|    n_updates        | 44543    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.432    |
| time/               |          |
|    episodes         | 2844     |
|    fps              | 137      |
|    time_elapsed     | 1585     |
|    total_timesteps  | 218462   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 44615    |
----------------------------------
Eval num_timesteps=218500, episode_reward=180.90 +/- 44.21
Episode length: 45.62 +/- 11.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.432    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.61     |
|    n_updates        | 44624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 2848     |
|    fps              | 137      |
|    time_elapsed     | 1588     |
|    total_timesteps  | 218754   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.261    |
|    n_updates        | 44688    |
----------------------------------
Eval num_timesteps=219000, episode_reward=177.96 +/- 44.90
Episode length: 44.88 +/- 11.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.429    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.958    |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 2852     |
|    fps              | 137      |
|    time_elapsed     | 1591     |
|    total_timesteps  | 219072   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.63     |
|    n_updates        | 44767    |
----------------------------------
Eval num_timesteps=219500, episode_reward=177.84 +/- 43.51
Episode length: 44.78 +/- 10.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.427    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.927    |
|    n_updates        | 44874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 2856     |
|    fps              | 137      |
|    time_elapsed     | 1594     |
|    total_timesteps  | 219536   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 44883    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.425    |
| time/               |          |
|    episodes         | 2860     |
|    fps              | 137      |
|    time_elapsed     | 1595     |
|    total_timesteps  | 219828   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.495    |
|    n_updates        | 44956    |
----------------------------------
Eval num_timesteps=220000, episode_reward=179.28 +/- 43.56
Episode length: 45.20 +/- 10.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.425    |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.883    |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.1     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.424    |
| time/               |          |
|    episodes         | 2864     |
|    fps              | 137      |
|    time_elapsed     | 1598     |
|    total_timesteps  | 220202   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 45050    |
----------------------------------
Eval num_timesteps=220500, episode_reward=164.30 +/- 44.74
Episode length: 41.52 +/- 11.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.422    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.63     |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 2868     |
|    fps              | 137      |
|    time_elapsed     | 1601     |
|    total_timesteps  | 220520   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 45129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.421    |
| time/               |          |
|    episodes         | 2872     |
|    fps              | 137      |
|    time_elapsed     | 1601     |
|    total_timesteps  | 220824   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0624   |
|    n_updates        | 45205    |
----------------------------------
Eval num_timesteps=221000, episode_reward=169.06 +/- 42.12
Episode length: 42.56 +/- 10.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.42     |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.419    |
| time/               |          |
|    episodes         | 2876     |
|    fps              | 137      |
|    time_elapsed     | 1604     |
|    total_timesteps  | 221105   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 45276    |
----------------------------------
Eval num_timesteps=221500, episode_reward=269.76 +/- 114.87
Episode length: 67.80 +/- 28.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.8     |
|    mean_reward      | 270      |
| rollout/            |          |
|    exploration_rate | 0.417    |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0598   |
|    n_updates        | 45374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.417    |
| time/               |          |
|    episodes         | 2880     |
|    fps              | 137      |
|    time_elapsed     | 1609     |
|    total_timesteps  | 221601   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.963    |
|    n_updates        | 45400    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.415    |
| time/               |          |
|    episodes         | 2884     |
|    fps              | 137      |
|    time_elapsed     | 1610     |
|    total_timesteps  | 221914   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.61     |
|    n_updates        | 45478    |
----------------------------------
Eval num_timesteps=222000, episode_reward=327.50 +/- 188.94
Episode length: 82.26 +/- 47.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.3     |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.415    |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.236    |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.6     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.414    |
| time/               |          |
|    episodes         | 2888     |
|    fps              | 137      |
|    time_elapsed     | 1616     |
|    total_timesteps  | 222295   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.383    |
|    n_updates        | 45573    |
----------------------------------
Eval num_timesteps=222500, episode_reward=364.92 +/- 156.83
Episode length: 91.62 +/- 39.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.6     |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 0.413    |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.374    |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.411    |
| time/               |          |
|    episodes         | 2892     |
|    fps              | 137      |
|    time_elapsed     | 1622     |
|    total_timesteps  | 222729   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 45682    |
----------------------------------
Eval num_timesteps=223000, episode_reward=180.84 +/- 38.55
Episode length: 45.54 +/- 9.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.41     |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.793    |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 2896     |
|    fps              | 137      |
|    time_elapsed     | 1625     |
|    total_timesteps  | 223021   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0983   |
|    n_updates        | 45755    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.408    |
| time/               |          |
|    episodes         | 2900     |
|    fps              | 137      |
|    time_elapsed     | 1626     |
|    total_timesteps  | 223424   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 45855    |
----------------------------------
Eval num_timesteps=223500, episode_reward=172.18 +/- 45.11
Episode length: 43.42 +/- 11.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.408    |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.257    |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 2904     |
|    fps              | 137      |
|    time_elapsed     | 1629     |
|    total_timesteps  | 223982   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.987    |
|    n_updates        | 45995    |
----------------------------------
Eval num_timesteps=224000, episode_reward=166.12 +/- 48.76
Episode length: 41.92 +/- 12.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.405    |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0914   |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.403    |
| time/               |          |
|    episodes         | 2908     |
|    fps              | 137      |
|    time_elapsed     | 1633     |
|    total_timesteps  | 224363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.268    |
|    n_updates        | 46090    |
----------------------------------
Eval num_timesteps=224500, episode_reward=201.08 +/- 114.11
Episode length: 50.64 +/- 28.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.6     |
|    mean_reward      | 201      |
| rollout/            |          |
|    exploration_rate | 0.403    |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 46124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.402    |
| time/               |          |
|    episodes         | 2912     |
|    fps              | 137      |
|    time_elapsed     | 1636     |
|    total_timesteps  | 224630   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 46157    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 2916     |
|    fps              | 137      |
|    time_elapsed     | 1637     |
|    total_timesteps  | 224825   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.4      |
|    n_updates        | 46206    |
----------------------------------
Eval num_timesteps=225000, episode_reward=185.36 +/- 56.44
Episode length: 46.72 +/- 14.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.4      |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.335    |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.4      |
| time/               |          |
|    episodes         | 2920     |
|    fps              | 137      |
|    time_elapsed     | 1640     |
|    total_timesteps  | 225182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.1      |
|    n_updates        | 46295    |
----------------------------------
Eval num_timesteps=225500, episode_reward=275.84 +/- 91.86
Episode length: 69.26 +/- 22.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.3     |
|    mean_reward      | 276      |
| rollout/            |          |
|    exploration_rate | 0.398    |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.74     |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.1     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 2924     |
|    fps              | 137      |
|    time_elapsed     | 1645     |
|    total_timesteps  | 225563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.437    |
|    n_updates        | 46390    |
----------------------------------
Eval num_timesteps=226000, episode_reward=184.78 +/- 48.58
Episode length: 46.52 +/- 12.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.396    |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.5     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.395    |
| time/               |          |
|    episodes         | 2928     |
|    fps              | 137      |
|    time_elapsed     | 1649     |
|    total_timesteps  | 226054   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.233    |
|    n_updates        | 46513    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.9     |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 2932     |
|    fps              | 137      |
|    time_elapsed     | 1649     |
|    total_timesteps  | 226395   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.214    |
|    n_updates        | 46598    |
----------------------------------
Eval num_timesteps=226500, episode_reward=277.42 +/- 154.54
Episode length: 69.74 +/- 38.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.7     |
|    mean_reward      | 277      |
| rollout/            |          |
|    exploration_rate | 0.393    |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.988    |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89       |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.392    |
| time/               |          |
|    episodes         | 2936     |
|    fps              | 137      |
|    time_elapsed     | 1654     |
|    total_timesteps  | 226758   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.966    |
|    n_updates        | 46689    |
----------------------------------
Eval num_timesteps=227000, episode_reward=174.38 +/- 52.24
Episode length: 43.96 +/- 13.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.391    |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.9      |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90       |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.39     |
| time/               |          |
|    episodes         | 2940     |
|    fps              | 137      |
|    time_elapsed     | 1657     |
|    total_timesteps  | 227170   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.18     |
|    n_updates        | 46792    |
----------------------------------
Eval num_timesteps=227500, episode_reward=191.02 +/- 58.07
Episode length: 48.12 +/- 14.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.388    |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.02     |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.7     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.388    |
| time/               |          |
|    episodes         | 2944     |
|    fps              | 137      |
|    time_elapsed     | 1661     |
|    total_timesteps  | 227629   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.668    |
|    n_updates        | 46907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.386    |
| time/               |          |
|    episodes         | 2948     |
|    fps              | 137      |
|    time_elapsed     | 1661     |
|    total_timesteps  | 227901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.69     |
|    n_updates        | 46975    |
----------------------------------
Eval num_timesteps=228000, episode_reward=169.76 +/- 48.62
Episode length: 42.86 +/- 12.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.386    |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.182    |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.9     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.384    |
| time/               |          |
|    episodes         | 2952     |
|    fps              | 137      |
|    time_elapsed     | 1665     |
|    total_timesteps  | 228363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 47090    |
----------------------------------
Eval num_timesteps=228500, episode_reward=179.50 +/- 45.47
Episode length: 45.28 +/- 11.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.382    |
| time/               |          |
|    episodes         | 2956     |
|    fps              | 137      |
|    time_elapsed     | 1668     |
|    total_timesteps  | 228732   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.375    |
|    n_updates        | 47182    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.4     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 2960     |
|    fps              | 137      |
|    time_elapsed     | 1669     |
|    total_timesteps  | 228970   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0669   |
|    n_updates        | 47242    |
----------------------------------
Eval num_timesteps=229000, episode_reward=178.04 +/- 45.25
Episode length: 44.86 +/- 11.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.381    |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.9     |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.38     |
| time/               |          |
|    episodes         | 2964     |
|    fps              | 137      |
|    time_elapsed     | 1672     |
|    total_timesteps  | 229188   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.965    |
|    n_updates        | 47296    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 2968     |
|    fps              | 137      |
|    time_elapsed     | 1673     |
|    total_timesteps  | 229486   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.03     |
|    n_updates        | 47371    |
----------------------------------
Eval num_timesteps=229500, episode_reward=178.62 +/- 46.46
Episode length: 45.08 +/- 11.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.378    |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2        |
|    n_updates        | 47374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.377    |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 137      |
|    time_elapsed     | 1676     |
|    total_timesteps  | 229762   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.203    |
|    n_updates        | 47440    |
----------------------------------
Eval num_timesteps=230000, episode_reward=175.98 +/- 54.95
Episode length: 44.34 +/- 13.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.376    |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.168    |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.1     |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.375    |
| time/               |          |
|    episodes         | 2976     |
|    fps              | 137      |
|    time_elapsed     | 1679     |
|    total_timesteps  | 230113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.377    |
|    n_updates        | 47528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.374    |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 137      |
|    time_elapsed     | 1679     |
|    total_timesteps  | 230461   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 47615    |
----------------------------------
Eval num_timesteps=230500, episode_reward=184.62 +/- 56.97
Episode length: 46.48 +/- 14.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.373    |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 47624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.372    |
| time/               |          |
|    episodes         | 2984     |
|    fps              | 137      |
|    time_elapsed     | 1682     |
|    total_timesteps  | 230711   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0707   |
|    n_updates        | 47677    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 137      |
|    time_elapsed     | 1683     |
|    total_timesteps  | 230994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.22     |
|    n_updates        | 47748    |
----------------------------------
Eval num_timesteps=231000, episode_reward=166.18 +/- 44.80
Episode length: 41.84 +/- 11.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.371    |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.37     |
| time/               |          |
|    episodes         | 2992     |
|    fps              | 137      |
|    time_elapsed     | 1686     |
|    total_timesteps  | 231194   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.907    |
|    n_updates        | 47798    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.369    |
| time/               |          |
|    episodes         | 2996     |
|    fps              | 137      |
|    time_elapsed     | 1686     |
|    total_timesteps  | 231426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 47856    |
----------------------------------
Eval num_timesteps=231500, episode_reward=351.84 +/- 215.32
Episode length: 88.38 +/- 53.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.4     |
|    mean_reward      | 352      |
| rollout/            |          |
|    exploration_rate | 0.369    |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 47874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.367    |
| time/               |          |
|    episodes         | 3000     |
|    fps              | 136      |
|    time_elapsed     | 1692     |
|    total_timesteps  | 231750   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.88     |
|    n_updates        | 47937    |
----------------------------------
Eval num_timesteps=232000, episode_reward=168.10 +/- 40.00
Episode length: 42.38 +/- 10.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.366    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.272    |
|    n_updates        | 47999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 3004     |
|    fps              | 136      |
|    time_elapsed     | 1695     |
|    total_timesteps  | 232121   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.087    |
|    n_updates        | 48030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.364    |
| time/               |          |
|    episodes         | 3008     |
|    fps              | 137      |
|    time_elapsed     | 1695     |
|    total_timesteps  | 232367   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.388    |
|    n_updates        | 48091    |
----------------------------------
Eval num_timesteps=232500, episode_reward=331.68 +/- 155.29
Episode length: 83.30 +/- 38.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.3     |
|    mean_reward      | 332      |
| rollout/            |          |
|    exploration_rate | 0.364    |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.342    |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.363    |
| time/               |          |
|    episodes         | 3012     |
|    fps              | 136      |
|    time_elapsed     | 1702     |
|    total_timesteps  | 232654   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.05     |
|    n_updates        | 48163    |
----------------------------------
Eval num_timesteps=233000, episode_reward=179.56 +/- 54.42
Episode length: 45.30 +/- 13.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.361    |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 3016     |
|    fps              | 136      |
|    time_elapsed     | 1705     |
|    total_timesteps  | 233217   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.405    |
|    n_updates        | 48304    |
----------------------------------
Eval num_timesteps=233500, episode_reward=244.40 +/- 76.79
Episode length: 61.50 +/- 19.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.5     |
|    mean_reward      | 244      |
| rollout/            |          |
|    exploration_rate | 0.359    |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.16     |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.359    |
| time/               |          |
|    episodes         | 3020     |
|    fps              | 136      |
|    time_elapsed     | 1710     |
|    total_timesteps  | 233512   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.18     |
|    n_updates        | 48377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.357    |
| time/               |          |
|    episodes         | 3024     |
|    fps              | 136      |
|    time_elapsed     | 1710     |
|    total_timesteps  | 233915   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.17     |
|    n_updates        | 48478    |
----------------------------------
Eval num_timesteps=234000, episode_reward=175.72 +/- 46.08
Episode length: 44.34 +/- 11.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.356    |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.02     |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.356    |
| time/               |          |
|    episodes         | 3028     |
|    fps              | 136      |
|    time_elapsed     | 1713     |
|    total_timesteps  | 234098   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 48524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.354    |
| time/               |          |
|    episodes         | 3032     |
|    fps              | 136      |
|    time_elapsed     | 1713     |
|    total_timesteps  | 234376   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.877    |
|    n_updates        | 48593    |
----------------------------------
Eval num_timesteps=234500, episode_reward=299.88 +/- 149.45
Episode length: 75.32 +/- 37.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.3     |
|    mean_reward      | 300      |
| rollout/            |          |
|    exploration_rate | 0.354    |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.29     |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.353    |
| time/               |          |
|    episodes         | 3036     |
|    fps              | 136      |
|    time_elapsed     | 1719     |
|    total_timesteps  | 234631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.1      |
|    n_updates        | 48657    |
----------------------------------
Eval num_timesteps=235000, episode_reward=170.60 +/- 41.50
Episode length: 43.06 +/- 10.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.351    |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.62     |
|    n_updates        | 48749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.351    |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 136      |
|    time_elapsed     | 1722     |
|    total_timesteps  | 235033   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.87     |
|    n_updates        | 48758    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.35     |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 136      |
|    time_elapsed     | 1722     |
|    total_timesteps  | 235232   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 48807    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.349    |
| time/               |          |
|    episodes         | 3048     |
|    fps              | 136      |
|    time_elapsed     | 1723     |
|    total_timesteps  | 235486   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0787   |
|    n_updates        | 48871    |
----------------------------------
Eval num_timesteps=235500, episode_reward=184.80 +/- 74.36
Episode length: 46.56 +/- 18.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.349    |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.347    |
| time/               |          |
|    episodes         | 3052     |
|    fps              | 136      |
|    time_elapsed     | 1726     |
|    total_timesteps  | 235876   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.1      |
|    n_updates        | 48968    |
----------------------------------
Eval num_timesteps=236000, episode_reward=169.44 +/- 41.02
Episode length: 42.72 +/- 10.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.346    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.27     |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.345    |
| time/               |          |
|    episodes         | 3056     |
|    fps              | 136      |
|    time_elapsed     | 1729     |
|    total_timesteps  | 236183   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 49045    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.344    |
| time/               |          |
|    episodes         | 3060     |
|    fps              | 136      |
|    time_elapsed     | 1730     |
|    total_timesteps  | 236428   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.7      |
|    n_updates        | 49106    |
----------------------------------
Eval num_timesteps=236500, episode_reward=198.82 +/- 58.17
Episode length: 50.16 +/- 14.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.2     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.344    |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.18     |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.343    |
| time/               |          |
|    episodes         | 3064     |
|    fps              | 136      |
|    time_elapsed     | 1733     |
|    total_timesteps  | 236669   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 49167    |
----------------------------------
Eval num_timesteps=237000, episode_reward=172.56 +/- 47.77
Episode length: 43.52 +/- 11.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.341    |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 49249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.341    |
| time/               |          |
|    episodes         | 3068     |
|    fps              | 136      |
|    time_elapsed     | 1737     |
|    total_timesteps  | 237086   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.154    |
|    n_updates        | 49271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.339    |
| time/               |          |
|    episodes         | 3072     |
|    fps              | 136      |
|    time_elapsed     | 1737     |
|    total_timesteps  | 237472   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.324    |
|    n_updates        | 49367    |
----------------------------------
Eval num_timesteps=237500, episode_reward=171.34 +/- 45.03
Episode length: 43.20 +/- 11.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.339    |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.337    |
| time/               |          |
|    episodes         | 3076     |
|    fps              | 136      |
|    time_elapsed     | 1740     |
|    total_timesteps  | 237842   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.538    |
|    n_updates        | 49460    |
----------------------------------
Eval num_timesteps=238000, episode_reward=172.14 +/- 45.08
Episode length: 43.42 +/- 11.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.336    |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.593    |
|    n_updates        | 49499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.335    |
| time/               |          |
|    episodes         | 3080     |
|    fps              | 136      |
|    time_elapsed     | 1743     |
|    total_timesteps  | 238128   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.24     |
|    n_updates        | 49531    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.334    |
| time/               |          |
|    episodes         | 3084     |
|    fps              | 136      |
|    time_elapsed     | 1744     |
|    total_timesteps  | 238365   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.281    |
|    n_updates        | 49591    |
----------------------------------
Eval num_timesteps=238500, episode_reward=285.46 +/- 139.82
Episode length: 71.68 +/- 34.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.7     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.334    |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.216    |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.333    |
| time/               |          |
|    episodes         | 3088     |
|    fps              | 136      |
|    time_elapsed     | 1749     |
|    total_timesteps  | 238689   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 49672    |
----------------------------------
Eval num_timesteps=239000, episode_reward=179.84 +/- 38.05
Episode length: 45.36 +/- 9.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0912   |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.331    |
| time/               |          |
|    episodes         | 3092     |
|    fps              | 136      |
|    time_elapsed     | 1752     |
|    total_timesteps  | 239030   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.336    |
|    n_updates        | 49757    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.329    |
| time/               |          |
|    episodes         | 3096     |
|    fps              | 136      |
|    time_elapsed     | 1753     |
|    total_timesteps  | 239357   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.36     |
|    n_updates        | 49839    |
----------------------------------
Eval num_timesteps=239500, episode_reward=193.24 +/- 46.36
Episode length: 48.68 +/- 11.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.329    |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.92     |
|    n_updates        | 49874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.328    |
| time/               |          |
|    episodes         | 3100     |
|    fps              | 136      |
|    time_elapsed     | 1756     |
|    total_timesteps  | 239603   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.248    |
|    n_updates        | 49900    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.327    |
| time/               |          |
|    episodes         | 3104     |
|    fps              | 136      |
|    time_elapsed     | 1756     |
|    total_timesteps  | 239828   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.11     |
|    n_updates        | 49956    |
----------------------------------
Eval num_timesteps=240000, episode_reward=180.38 +/- 45.09
Episode length: 45.44 +/- 11.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.326    |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.324    |
| time/               |          |
|    episodes         | 3108     |
|    fps              | 136      |
|    time_elapsed     | 1760     |
|    total_timesteps  | 240302   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.3      |
|    n_updates        | 50075    |
----------------------------------
Eval num_timesteps=240500, episode_reward=186.46 +/- 47.66
Episode length: 47.02 +/- 11.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.323    |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.339    |
|    n_updates        | 50124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.323    |
| time/               |          |
|    episodes         | 3112     |
|    fps              | 136      |
|    time_elapsed     | 1763     |
|    total_timesteps  | 240586   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.34     |
|    n_updates        | 50146    |
----------------------------------
Eval num_timesteps=241000, episode_reward=179.52 +/- 52.73
Episode length: 45.32 +/- 13.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.321    |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.297    |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 3116     |
|    fps              | 136      |
|    time_elapsed     | 1767     |
|    total_timesteps  | 241089   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.36     |
|    n_updates        | 50272    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.319    |
| time/               |          |
|    episodes         | 3120     |
|    fps              | 136      |
|    time_elapsed     | 1767     |
|    total_timesteps  | 241379   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.162    |
|    n_updates        | 50344    |
----------------------------------
Eval num_timesteps=241500, episode_reward=279.34 +/- 188.01
Episode length: 70.22 +/- 47.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.2     |
|    mean_reward      | 279      |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.163    |
|    n_updates        | 50374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.316    |
| time/               |          |
|    episodes         | 3124     |
|    fps              | 136      |
|    time_elapsed     | 1773     |
|    total_timesteps  | 241891   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0751   |
|    n_updates        | 50472    |
----------------------------------
Eval num_timesteps=242000, episode_reward=179.20 +/- 47.96
Episode length: 45.22 +/- 11.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.316    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.344    |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.315    |
| time/               |          |
|    episodes         | 3128     |
|    fps              | 136      |
|    time_elapsed     | 1776     |
|    total_timesteps  | 242099   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.484    |
|    n_updates        | 50524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.313    |
| time/               |          |
|    episodes         | 3132     |
|    fps              | 136      |
|    time_elapsed     | 1777     |
|    total_timesteps  | 242497   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.2      |
|    n_updates        | 50624    |
----------------------------------
Eval num_timesteps=242500, episode_reward=165.68 +/- 42.89
Episode length: 41.80 +/- 10.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.313    |
| time/               |          |
|    total_timesteps  | 242500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.311    |
| time/               |          |
|    episodes         | 3136     |
|    fps              | 136      |
|    time_elapsed     | 1780     |
|    total_timesteps  | 242901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 50725    |
----------------------------------
Eval num_timesteps=243000, episode_reward=169.86 +/- 44.92
Episode length: 42.88 +/- 11.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.311    |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.486    |
|    n_updates        | 50749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.31     |
| time/               |          |
|    episodes         | 3140     |
|    fps              | 136      |
|    time_elapsed     | 1783     |
|    total_timesteps  | 243242   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.85     |
|    n_updates        | 50810    |
----------------------------------
Eval num_timesteps=243500, episode_reward=173.36 +/- 37.45
Episode length: 43.68 +/- 9.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.308    |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.307    |
| time/               |          |
|    episodes         | 3144     |
|    fps              | 136      |
|    time_elapsed     | 1786     |
|    total_timesteps  | 243657   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 50914    |
----------------------------------
Eval num_timesteps=244000, episode_reward=373.54 +/- 187.66
Episode length: 93.82 +/- 46.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 374      |
| rollout/            |          |
|    exploration_rate | 0.306    |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.906    |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.306    |
| time/               |          |
|    episodes         | 3148     |
|    fps              | 136      |
|    time_elapsed     | 1793     |
|    total_timesteps  | 244011   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.656    |
|    n_updates        | 51002    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.304    |
| time/               |          |
|    episodes         | 3152     |
|    fps              | 136      |
|    time_elapsed     | 1793     |
|    total_timesteps  | 244425   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.08     |
|    n_updates        | 51106    |
----------------------------------
Eval num_timesteps=244500, episode_reward=168.56 +/- 41.08
Episode length: 42.48 +/- 10.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.303    |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.3     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.302    |
| time/               |          |
|    episodes         | 3156     |
|    fps              | 136      |
|    time_elapsed     | 1797     |
|    total_timesteps  | 244809   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.9      |
|    n_updates        | 51202    |
----------------------------------
Eval num_timesteps=245000, episode_reward=310.08 +/- 188.45
Episode length: 77.86 +/- 47.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.9     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.301    |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.986    |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.6     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.3      |
| time/               |          |
|    episodes         | 3160     |
|    fps              | 135      |
|    time_elapsed     | 1802     |
|    total_timesteps  | 245087   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.16     |
|    n_updates        | 51271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.299    |
| time/               |          |
|    episodes         | 3164     |
|    fps              | 136      |
|    time_elapsed     | 1802     |
|    total_timesteps  | 245289   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.988    |
|    n_updates        | 51322    |
----------------------------------
Eval num_timesteps=245500, episode_reward=298.24 +/- 174.10
Episode length: 75.02 +/- 43.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | 298      |
| rollout/            |          |
|    exploration_rate | 0.298    |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.24     |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.298    |
| time/               |          |
|    episodes         | 3168     |
|    fps              | 135      |
|    time_elapsed     | 1807     |
|    total_timesteps  | 245590   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 51397    |
----------------------------------
Eval num_timesteps=246000, episode_reward=194.28 +/- 53.11
Episode length: 48.98 +/- 13.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49       |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.295    |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.04     |
|    n_updates        | 51499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.295    |
| time/               |          |
|    episodes         | 3172     |
|    fps              | 135      |
|    time_elapsed     | 1811     |
|    total_timesteps  | 246090   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.18     |
|    n_updates        | 51522    |
----------------------------------
Eval num_timesteps=246500, episode_reward=339.96 +/- 176.56
Episode length: 85.36 +/- 44.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.4     |
|    mean_reward      | 340      |
| rollout/            |          |
|    exploration_rate | 0.293    |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.46     |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.293    |
| time/               |          |
|    episodes         | 3176     |
|    fps              | 135      |
|    time_elapsed     | 1818     |
|    total_timesteps  | 246514   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.887    |
|    n_updates        | 51628    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.291    |
| time/               |          |
|    episodes         | 3180     |
|    fps              | 135      |
|    time_elapsed     | 1818     |
|    total_timesteps  | 246794   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0971   |
|    n_updates        | 51698    |
----------------------------------
Eval num_timesteps=247000, episode_reward=190.82 +/- 60.51
Episode length: 48.08 +/- 15.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.29     |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.19     |
|    n_updates        | 51749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.4     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.29     |
| time/               |          |
|    episodes         | 3184     |
|    fps              | 135      |
|    time_elapsed     | 1822     |
|    total_timesteps  | 247006   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 51751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.1     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.289    |
| time/               |          |
|    episodes         | 3188     |
|    fps              | 135      |
|    time_elapsed     | 1822     |
|    total_timesteps  | 247298   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 51824    |
----------------------------------
Eval num_timesteps=247500, episode_reward=194.12 +/- 54.72
Episode length: 48.88 +/- 13.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.288    |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.199    |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.287    |
| time/               |          |
|    episodes         | 3192     |
|    fps              | 135      |
|    time_elapsed     | 1826     |
|    total_timesteps  | 247654   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.267    |
|    n_updates        | 51913    |
----------------------------------
Eval num_timesteps=248000, episode_reward=162.56 +/- 35.50
Episode length: 41.00 +/- 8.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.38     |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.9     |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.284    |
| time/               |          |
|    episodes         | 3196     |
|    fps              | 135      |
|    time_elapsed     | 1829     |
|    total_timesteps  | 248149   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.396    |
|    n_updates        | 52037    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.2     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.283    |
| time/               |          |
|    episodes         | 3200     |
|    fps              | 135      |
|    time_elapsed     | 1830     |
|    total_timesteps  | 248425   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 52106    |
----------------------------------
Eval num_timesteps=248500, episode_reward=297.48 +/- 94.22
Episode length: 74.76 +/- 23.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.8     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.283    |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.28     |
|    n_updates        | 52124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.282    |
| time/               |          |
|    episodes         | 3204     |
|    fps              | 135      |
|    time_elapsed     | 1835     |
|    total_timesteps  | 248627   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.1      |
|    n_updates        | 52156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86       |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.281    |
| time/               |          |
|    episodes         | 3208     |
|    fps              | 135      |
|    time_elapsed     | 1836     |
|    total_timesteps  | 248901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.1      |
|    n_updates        | 52225    |
----------------------------------
Eval num_timesteps=249000, episode_reward=337.20 +/- 175.36
Episode length: 84.70 +/- 43.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.7     |
|    mean_reward      | 337      |
| rollout/            |          |
|    exploration_rate | 0.28     |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.43     |
|    n_updates        | 52249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.279    |
| time/               |          |
|    episodes         | 3212     |
|    fps              | 135      |
|    time_elapsed     | 1841     |
|    total_timesteps  | 249160   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.12     |
|    n_updates        | 52289    |
----------------------------------
Eval num_timesteps=249500, episode_reward=180.62 +/- 49.53
Episode length: 45.50 +/- 12.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.277    |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.6     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.277    |
| time/               |          |
|    episodes         | 3216     |
|    fps              | 135      |
|    time_elapsed     | 1845     |
|    total_timesteps  | 249547   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.212    |
|    n_updates        | 52386    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 3220     |
|    fps              | 135      |
|    time_elapsed     | 1846     |
|    total_timesteps  | 249901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.374    |
|    n_updates        | 52475    |
----------------------------------
Eval num_timesteps=250000, episode_reward=315.46 +/- 182.64
Episode length: 79.28 +/- 45.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.3     |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.275    |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.17     |
|    n_updates        | 52499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.274    |
| time/               |          |
|    episodes         | 3224     |
|    fps              | 135      |
|    time_elapsed     | 1851     |
|    total_timesteps  | 250241   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.179    |
|    n_updates        | 52560    |
----------------------------------
Eval num_timesteps=250500, episode_reward=317.44 +/- 177.11
Episode length: 79.70 +/- 44.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.7     |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.272    |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.272    |
| time/               |          |
|    episodes         | 3228     |
|    fps              | 134      |
|    time_elapsed     | 1857     |
|    total_timesteps  | 250504   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.429    |
|    n_updates        | 52625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.271    |
| time/               |          |
|    episodes         | 3232     |
|    fps              | 134      |
|    time_elapsed     | 1857     |
|    total_timesteps  | 250698   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.248    |
|    n_updates        | 52674    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.27     |
| time/               |          |
|    episodes         | 3236     |
|    fps              | 135      |
|    time_elapsed     | 1857     |
|    total_timesteps  | 250950   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 52737    |
----------------------------------
Eval num_timesteps=251000, episode_reward=254.00 +/- 59.32
Episode length: 63.86 +/- 14.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.9     |
|    mean_reward      | 254      |
| rollout/            |          |
|    exploration_rate | 0.27     |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0741   |
|    n_updates        | 52749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 3240     |
|    fps              | 134      |
|    time_elapsed     | 1861     |
|    total_timesteps  | 251269   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.179    |
|    n_updates        | 52817    |
----------------------------------
Eval num_timesteps=251500, episode_reward=184.96 +/- 47.70
Episode length: 46.60 +/- 11.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.267    |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.909    |
|    n_updates        | 52874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.267    |
| time/               |          |
|    episodes         | 3244     |
|    fps              | 134      |
|    time_elapsed     | 1865     |
|    total_timesteps  | 251560   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.48     |
|    n_updates        | 52889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.265    |
| time/               |          |
|    episodes         | 3248     |
|    fps              | 135      |
|    time_elapsed     | 1865     |
|    total_timesteps  | 251983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.91     |
|    n_updates        | 52995    |
----------------------------------
Eval num_timesteps=252000, episode_reward=188.90 +/- 59.15
Episode length: 47.54 +/- 14.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.264    |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.576    |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.263    |
| time/               |          |
|    episodes         | 3252     |
|    fps              | 134      |
|    time_elapsed     | 1869     |
|    total_timesteps  | 252213   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.303    |
|    n_updates        | 53053    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 3256     |
|    fps              | 135      |
|    time_elapsed     | 1870     |
|    total_timesteps  | 252460   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 53114    |
----------------------------------
Eval num_timesteps=252500, episode_reward=326.36 +/- 153.77
Episode length: 81.96 +/- 38.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82       |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.262    |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.416    |
|    n_updates        | 53124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.26     |
| time/               |          |
|    episodes         | 3260     |
|    fps              | 134      |
|    time_elapsed     | 1875     |
|    total_timesteps  | 252782   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.959    |
|    n_updates        | 53195    |
----------------------------------
Eval num_timesteps=253000, episode_reward=180.10 +/- 49.94
Episode length: 45.40 +/- 12.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.259    |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.56     |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.259    |
| time/               |          |
|    episodes         | 3264     |
|    fps              | 134      |
|    time_elapsed     | 1879     |
|    total_timesteps  | 253054   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.06     |
|    n_updates        | 53263    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.258    |
| time/               |          |
|    episodes         | 3268     |
|    fps              | 134      |
|    time_elapsed     | 1879     |
|    total_timesteps  | 253302   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.198    |
|    n_updates        | 53325    |
----------------------------------
Eval num_timesteps=253500, episode_reward=182.92 +/- 46.67
Episode length: 46.02 +/- 11.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.257    |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.34     |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.256    |
| time/               |          |
|    episodes         | 3272     |
|    fps              | 134      |
|    time_elapsed     | 1882     |
|    total_timesteps  | 253617   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.632    |
|    n_updates        | 53404    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 3276     |
|    fps              | 134      |
|    time_elapsed     | 1883     |
|    total_timesteps  | 253950   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.06     |
|    n_updates        | 53487    |
----------------------------------
Eval num_timesteps=254000, episode_reward=201.80 +/- 53.08
Episode length: 50.90 +/- 13.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.9     |
|    mean_reward      | 202      |
| rollout/            |          |
|    exploration_rate | 0.254    |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.43     |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.8     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.253    |
| time/               |          |
|    episodes         | 3280     |
|    fps              | 134      |
|    time_elapsed     | 1887     |
|    total_timesteps  | 254178   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.38     |
|    n_updates        | 53544    |
----------------------------------
Eval num_timesteps=254500, episode_reward=173.42 +/- 42.90
Episode length: 43.70 +/- 10.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.466    |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.251    |
| time/               |          |
|    episodes         | 3284     |
|    fps              | 134      |
|    time_elapsed     | 1890     |
|    total_timesteps  | 254577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.754    |
|    n_updates        | 53644    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 3288     |
|    fps              | 134      |
|    time_elapsed     | 1890     |
|    total_timesteps  | 254799   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.16     |
|    n_updates        | 53699    |
----------------------------------
Eval num_timesteps=255000, episode_reward=307.30 +/- 173.04
Episode length: 77.18 +/- 43.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.2     |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.249    |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 53749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.248    |
| time/               |          |
|    episodes         | 3292     |
|    fps              | 134      |
|    time_elapsed     | 1896     |
|    total_timesteps  | 255138   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0742   |
|    n_updates        | 53784    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 3296     |
|    fps              | 134      |
|    time_elapsed     | 1896     |
|    total_timesteps  | 255476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.234    |
|    n_updates        | 53868    |
----------------------------------
Eval num_timesteps=255500, episode_reward=179.64 +/- 59.77
Episode length: 45.32 +/- 14.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.246    |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.23     |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.244    |
| time/               |          |
|    episodes         | 3300     |
|    fps              | 134      |
|    time_elapsed     | 1900     |
|    total_timesteps  | 255935   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.7      |
|    n_updates        | 53983    |
----------------------------------
Eval num_timesteps=256000, episode_reward=400.96 +/- 178.93
Episode length: 100.56 +/- 44.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 401      |
| rollout/            |          |
|    exploration_rate | 0.244    |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.18     |
|    n_updates        | 53999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.243    |
| time/               |          |
|    episodes         | 3304     |
|    fps              | 134      |
|    time_elapsed     | 1907     |
|    total_timesteps  | 256179   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.381    |
|    n_updates        | 54044    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.241    |
| time/               |          |
|    episodes         | 3308     |
|    fps              | 134      |
|    time_elapsed     | 1907     |
|    total_timesteps  | 256471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.712    |
|    n_updates        | 54117    |
----------------------------------
Eval num_timesteps=256500, episode_reward=197.86 +/- 54.21
Episode length: 49.86 +/- 13.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.9     |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.241    |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.24     |
| time/               |          |
|    episodes         | 3312     |
|    fps              | 134      |
|    time_elapsed     | 1910     |
|    total_timesteps  | 256665   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0967   |
|    n_updates        | 54166    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.238    |
| time/               |          |
|    episodes         | 3316     |
|    fps              | 134      |
|    time_elapsed     | 1911     |
|    total_timesteps  | 256976   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.617    |
|    n_updates        | 54243    |
----------------------------------
Eval num_timesteps=257000, episode_reward=304.30 +/- 153.07
Episode length: 76.48 +/- 38.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.5     |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.238    |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.6      |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.237    |
| time/               |          |
|    episodes         | 3320     |
|    fps              | 134      |
|    time_elapsed     | 1916     |
|    total_timesteps  | 257236   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.15     |
|    n_updates        | 54308    |
----------------------------------
Eval num_timesteps=257500, episode_reward=175.74 +/- 48.17
Episode length: 44.30 +/- 12.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.236    |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.188    |
|    n_updates        | 54374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.235    |
| time/               |          |
|    episodes         | 3324     |
|    fps              | 134      |
|    time_elapsed     | 1919     |
|    total_timesteps  | 257529   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.62     |
|    n_updates        | 54382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.234    |
| time/               |          |
|    episodes         | 3328     |
|    fps              | 134      |
|    time_elapsed     | 1920     |
|    total_timesteps  | 257801   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.33     |
|    n_updates        | 54450    |
----------------------------------
Eval num_timesteps=258000, episode_reward=287.02 +/- 147.12
Episode length: 72.12 +/- 36.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.1     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.233    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.986    |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.6     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.233    |
| time/               |          |
|    episodes         | 3332     |
|    fps              | 134      |
|    time_elapsed     | 1925     |
|    total_timesteps  | 258061   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.98     |
|    n_updates        | 54515    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 3336     |
|    fps              | 134      |
|    time_elapsed     | 1925     |
|    total_timesteps  | 258491   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 54622    |
----------------------------------
Eval num_timesteps=258500, episode_reward=189.76 +/- 45.36
Episode length: 47.82 +/- 11.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.8     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.454    |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.229    |
| time/               |          |
|    episodes         | 3340     |
|    fps              | 134      |
|    time_elapsed     | 1929     |
|    total_timesteps  | 258774   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.583    |
|    n_updates        | 54693    |
----------------------------------
Eval num_timesteps=259000, episode_reward=208.56 +/- 56.80
Episode length: 52.52 +/- 14.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.5     |
|    mean_reward      | 209      |
| rollout/            |          |
|    exploration_rate | 0.228    |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.505    |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.227    |
| time/               |          |
|    episodes         | 3344     |
|    fps              | 134      |
|    time_elapsed     | 1933     |
|    total_timesteps  | 259077   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.81     |
|    n_updates        | 54769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.225    |
| time/               |          |
|    episodes         | 3348     |
|    fps              | 134      |
|    time_elapsed     | 1933     |
|    total_timesteps  | 259473   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.297    |
|    n_updates        | 54868    |
----------------------------------
Eval num_timesteps=259500, episode_reward=177.68 +/- 42.74
Episode length: 44.80 +/- 10.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.225    |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.241    |
|    n_updates        | 54874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.224    |
| time/               |          |
|    episodes         | 3352     |
|    fps              | 134      |
|    time_elapsed     | 1937     |
|    total_timesteps  | 259735   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 54933    |
----------------------------------
Eval num_timesteps=260000, episode_reward=301.78 +/- 191.88
Episode length: 75.78 +/- 47.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.8     |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 0.222    |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.889    |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.222    |
| time/               |          |
|    episodes         | 3356     |
|    fps              | 133      |
|    time_elapsed     | 1942     |
|    total_timesteps  | 260013   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.68     |
|    n_updates        | 55003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.22     |
| time/               |          |
|    episodes         | 3360     |
|    fps              | 134      |
|    time_elapsed     | 1942     |
|    total_timesteps  | 260440   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.297    |
|    n_updates        | 55109    |
----------------------------------
Eval num_timesteps=260500, episode_reward=250.58 +/- 139.54
Episode length: 63.02 +/- 34.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63       |
|    mean_reward      | 251      |
| rollout/            |          |
|    exploration_rate | 0.22     |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.365    |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.219    |
| time/               |          |
|    episodes         | 3364     |
|    fps              | 133      |
|    time_elapsed     | 1947     |
|    total_timesteps  | 260737   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.3      |
|    n_updates        | 55184    |
----------------------------------
Eval num_timesteps=261000, episode_reward=298.92 +/- 131.00
Episode length: 75.10 +/- 32.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.1     |
|    mean_reward      | 299      |
| rollout/            |          |
|    exploration_rate | 0.217    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.217    |
| time/               |          |
|    episodes         | 3368     |
|    fps              | 133      |
|    time_elapsed     | 1952     |
|    total_timesteps  | 261073   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.531    |
|    n_updates        | 55268    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.215    |
| time/               |          |
|    episodes         | 3372     |
|    fps              | 133      |
|    time_elapsed     | 1953     |
|    total_timesteps  | 261323   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.942    |
|    n_updates        | 55330    |
----------------------------------
Eval num_timesteps=261500, episode_reward=167.72 +/- 39.36
Episode length: 42.34 +/- 9.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.214    |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.492    |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.214    |
| time/               |          |
|    episodes         | 3376     |
|    fps              | 133      |
|    time_elapsed     | 1956     |
|    total_timesteps  | 261656   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 55413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.212    |
| time/               |          |
|    episodes         | 3380     |
|    fps              | 133      |
|    time_elapsed     | 1956     |
|    total_timesteps  | 261925   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.2      |
|    n_updates        | 55481    |
----------------------------------
Eval num_timesteps=262000, episode_reward=189.14 +/- 71.60
Episode length: 47.62 +/- 17.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.212    |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.211    |
| time/               |          |
|    episodes         | 3384     |
|    fps              | 133      |
|    time_elapsed     | 1959     |
|    total_timesteps  | 262207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 55551    |
----------------------------------
Eval num_timesteps=262500, episode_reward=167.52 +/- 49.16
Episode length: 42.34 +/- 12.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.209    |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.209    |
| time/               |          |
|    episodes         | 3388     |
|    fps              | 133      |
|    time_elapsed     | 1962     |
|    total_timesteps  | 262529   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.75     |
|    n_updates        | 55632    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.208    |
| time/               |          |
|    episodes         | 3392     |
|    fps              | 133      |
|    time_elapsed     | 1963     |
|    total_timesteps  | 262793   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.657    |
|    n_updates        | 55698    |
----------------------------------
Eval num_timesteps=263000, episode_reward=390.42 +/- 172.38
Episode length: 97.94 +/- 43.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 390      |
| rollout/            |          |
|    exploration_rate | 0.206    |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.73     |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.206    |
| time/               |          |
|    episodes         | 3396     |
|    fps              | 133      |
|    time_elapsed     | 1969     |
|    total_timesteps  | 263152   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.44     |
|    n_updates        | 55787    |
----------------------------------
Eval num_timesteps=263500, episode_reward=189.52 +/- 43.32
Episode length: 47.72 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.204    |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.79     |
|    n_updates        | 55874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 3400     |
|    fps              | 133      |
|    time_elapsed     | 1973     |
|    total_timesteps  | 263596   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.232    |
|    n_updates        | 55898    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.202    |
| time/               |          |
|    episodes         | 3404     |
|    fps              | 133      |
|    time_elapsed     | 1973     |
|    total_timesteps  | 263868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.197    |
|    n_updates        | 55966    |
----------------------------------
Eval num_timesteps=264000, episode_reward=246.14 +/- 72.14
Episode length: 61.94 +/- 18.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.9     |
|    mean_reward      | 246      |
| rollout/            |          |
|    exploration_rate | 0.201    |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.53     |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 3408     |
|    fps              | 133      |
|    time_elapsed     | 1978     |
|    total_timesteps  | 264252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.452    |
|    n_updates        | 56062    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.199    |
| time/               |          |
|    episodes         | 3412     |
|    fps              | 133      |
|    time_elapsed     | 1978     |
|    total_timesteps  | 264490   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.1      |
|    n_updates        | 56122    |
----------------------------------
Eval num_timesteps=264500, episode_reward=192.18 +/- 47.36
Episode length: 48.44 +/- 11.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.198    |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.69     |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.196    |
| time/               |          |
|    episodes         | 3416     |
|    fps              | 133      |
|    time_elapsed     | 1982     |
|    total_timesteps  | 264909   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.98     |
|    n_updates        | 56227    |
----------------------------------
Eval num_timesteps=265000, episode_reward=178.94 +/- 51.81
Episode length: 45.10 +/- 12.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.196    |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 56249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.195    |
| time/               |          |
|    episodes         | 3420     |
|    fps              | 133      |
|    time_elapsed     | 1985     |
|    total_timesteps  | 265174   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.58     |
|    n_updates        | 56293    |
----------------------------------
Eval num_timesteps=265500, episode_reward=211.62 +/- 44.51
Episode length: 53.22 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.2     |
|    mean_reward      | 212      |
| rollout/            |          |
|    exploration_rate | 0.193    |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.159    |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.192    |
| time/               |          |
|    episodes         | 3424     |
|    fps              | 133      |
|    time_elapsed     | 1989     |
|    total_timesteps  | 265743   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.354    |
|    n_updates        | 56435    |
----------------------------------
Eval num_timesteps=266000, episode_reward=349.42 +/- 146.92
Episode length: 87.70 +/- 36.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.7     |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.19     |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.857    |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.1     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.189    |
| time/               |          |
|    episodes         | 3428     |
|    fps              | 133      |
|    time_elapsed     | 1996     |
|    total_timesteps  | 266215   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.88     |
|    n_updates        | 56553    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.188    |
| time/               |          |
|    episodes         | 3432     |
|    fps              | 133      |
|    time_elapsed     | 1996     |
|    total_timesteps  | 266392   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.33     |
|    n_updates        | 56597    |
----------------------------------
Eval num_timesteps=266500, episode_reward=200.66 +/- 57.46
Episode length: 50.52 +/- 14.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.5     |
|    mean_reward      | 201      |
| rollout/            |          |
|    exploration_rate | 0.188    |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.186    |
| time/               |          |
|    episodes         | 3436     |
|    fps              | 133      |
|    time_elapsed     | 1999     |
|    total_timesteps  | 266728   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.52     |
|    n_updates        | 56681    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.185    |
| time/               |          |
|    episodes         | 3440     |
|    fps              | 133      |
|    time_elapsed     | 2000     |
|    total_timesteps  | 266992   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.177    |
|    n_updates        | 56747    |
----------------------------------
Eval num_timesteps=267000, episode_reward=205.98 +/- 59.12
Episode length: 51.86 +/- 14.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.9     |
|    mean_reward      | 206      |
| rollout/            |          |
|    exploration_rate | 0.185    |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.54     |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.184    |
| time/               |          |
|    episodes         | 3444     |
|    fps              | 133      |
|    time_elapsed     | 2004     |
|    total_timesteps  | 267267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.05     |
|    n_updates        | 56816    |
----------------------------------
Eval num_timesteps=267500, episode_reward=179.60 +/- 48.44
Episode length: 45.26 +/- 12.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.182    |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.3      |
|    n_updates        | 56874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.182    |
| time/               |          |
|    episodes         | 3448     |
|    fps              | 133      |
|    time_elapsed     | 2007     |
|    total_timesteps  | 267610   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 56902    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.181    |
| time/               |          |
|    episodes         | 3452     |
|    fps              | 133      |
|    time_elapsed     | 2007     |
|    total_timesteps  | 267820   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.4      |
|    n_updates        | 56954    |
----------------------------------
Eval num_timesteps=268000, episode_reward=292.92 +/- 129.61
Episode length: 73.68 +/- 32.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.7     |
|    mean_reward      | 293      |
| rollout/            |          |
|    exploration_rate | 0.18     |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.59     |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.178    |
| time/               |          |
|    episodes         | 3456     |
|    fps              | 133      |
|    time_elapsed     | 2012     |
|    total_timesteps  | 268240   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.3      |
|    n_updates        | 57059    |
----------------------------------
Eval num_timesteps=268500, episode_reward=171.30 +/- 41.80
Episode length: 43.16 +/- 10.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.177    |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 57124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.177    |
| time/               |          |
|    episodes         | 3460     |
|    fps              | 133      |
|    time_elapsed     | 2015     |
|    total_timesteps  | 268517   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.03     |
|    n_updates        | 57129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.175    |
| time/               |          |
|    episodes         | 3464     |
|    fps              | 133      |
|    time_elapsed     | 2016     |
|    total_timesteps  | 268776   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.36     |
|    n_updates        | 57193    |
----------------------------------
Eval num_timesteps=269000, episode_reward=167.98 +/- 38.56
Episode length: 42.34 +/- 9.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.58     |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.174    |
| time/               |          |
|    episodes         | 3468     |
|    fps              | 133      |
|    time_elapsed     | 2019     |
|    total_timesteps  | 269005   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.22     |
|    n_updates        | 57251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.172    |
| time/               |          |
|    episodes         | 3472     |
|    fps              | 133      |
|    time_elapsed     | 2020     |
|    total_timesteps  | 269400   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.4      |
|    n_updates        | 57349    |
----------------------------------
Eval num_timesteps=269500, episode_reward=180.50 +/- 45.87
Episode length: 45.52 +/- 11.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.172    |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.773    |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.17     |
| time/               |          |
|    episodes         | 3476     |
|    fps              | 133      |
|    time_elapsed     | 2023     |
|    total_timesteps  | 269838   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.51     |
|    n_updates        | 57459    |
----------------------------------
Eval num_timesteps=270000, episode_reward=195.76 +/- 50.00
Episode length: 49.34 +/- 12.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.169    |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.312    |
|    n_updates        | 57499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.168    |
| time/               |          |
|    episodes         | 3480     |
|    fps              | 133      |
|    time_elapsed     | 2027     |
|    total_timesteps  | 270068   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.572    |
|    n_updates        | 57516    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.167    |
| time/               |          |
|    episodes         | 3484     |
|    fps              | 133      |
|    time_elapsed     | 2027     |
|    total_timesteps  | 270381   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 57595    |
----------------------------------
Eval num_timesteps=270500, episode_reward=284.74 +/- 81.98
Episode length: 71.60 +/- 20.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.166    |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.08     |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.166    |
| time/               |          |
|    episodes         | 3488     |
|    fps              | 133      |
|    time_elapsed     | 2032     |
|    total_timesteps  | 270584   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.759    |
|    n_updates        | 57645    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.164    |
| time/               |          |
|    episodes         | 3492     |
|    fps              | 133      |
|    time_elapsed     | 2032     |
|    total_timesteps  | 270921   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.783    |
|    n_updates        | 57730    |
----------------------------------
Eval num_timesteps=271000, episode_reward=293.32 +/- 87.22
Episode length: 73.72 +/- 21.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.7     |
|    mean_reward      | 293      |
| rollout/            |          |
|    exploration_rate | 0.163    |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.56     |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.162    |
| time/               |          |
|    episodes         | 3496     |
|    fps              | 133      |
|    time_elapsed     | 2037     |
|    total_timesteps  | 271274   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.968    |
|    n_updates        | 57818    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.161    |
| time/               |          |
|    episodes         | 3500     |
|    fps              | 133      |
|    time_elapsed     | 2037     |
|    total_timesteps  | 271419   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.976    |
|    n_updates        | 57854    |
----------------------------------
Eval num_timesteps=271500, episode_reward=175.08 +/- 47.08
Episode length: 44.16 +/- 11.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.161    |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.64     |
|    n_updates        | 57874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.159    |
| time/               |          |
|    episodes         | 3504     |
|    fps              | 133      |
|    time_elapsed     | 2041     |
|    total_timesteps  | 271821   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 57955    |
----------------------------------
Eval num_timesteps=272000, episode_reward=294.64 +/- 187.36
Episode length: 74.00 +/- 46.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74       |
|    mean_reward      | 295      |
| rollout/            |          |
|    exploration_rate | 0.158    |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.09     |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.156    |
| time/               |          |
|    episodes         | 3508     |
|    fps              | 133      |
|    time_elapsed     | 2046     |
|    total_timesteps  | 272270   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.58     |
|    n_updates        | 58067    |
----------------------------------
Eval num_timesteps=272500, episode_reward=164.80 +/- 45.37
Episode length: 41.56 +/- 11.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.155    |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.3      |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.154    |
| time/               |          |
|    episodes         | 3512     |
|    fps              | 133      |
|    time_elapsed     | 2049     |
|    total_timesteps  | 272660   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.34     |
|    n_updates        | 58164    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.153    |
| time/               |          |
|    episodes         | 3516     |
|    fps              | 133      |
|    time_elapsed     | 2050     |
|    total_timesteps  | 272845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.6      |
|    n_updates        | 58211    |
----------------------------------
Eval num_timesteps=273000, episode_reward=426.66 +/- 177.45
Episode length: 107.08 +/- 44.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 427      |
| rollout/            |          |
|    exploration_rate | 0.153    |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.321    |
|    n_updates        | 58249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.152    |
| time/               |          |
|    episodes         | 3520     |
|    fps              | 132      |
|    time_elapsed     | 2056     |
|    total_timesteps  | 273169   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.04     |
|    n_updates        | 58292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.2     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.15     |
| time/               |          |
|    episodes         | 3524     |
|    fps              | 132      |
|    time_elapsed     | 2057     |
|    total_timesteps  | 273461   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.649    |
|    n_updates        | 58365    |
----------------------------------
Eval num_timesteps=273500, episode_reward=342.62 +/- 168.17
Episode length: 86.00 +/- 42.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86       |
|    mean_reward      | 343      |
| rollout/            |          |
|    exploration_rate | 0.15     |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.444    |
|    n_updates        | 58374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.148    |
| time/               |          |
|    episodes         | 3528     |
|    fps              | 132      |
|    time_elapsed     | 2063     |
|    total_timesteps  | 273820   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.64     |
|    n_updates        | 58454    |
----------------------------------
Eval num_timesteps=274000, episode_reward=198.50 +/- 41.68
Episode length: 50.02 +/- 10.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50       |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.147    |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.65     |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.147    |
| time/               |          |
|    episodes         | 3532     |
|    fps              | 132      |
|    time_elapsed     | 2066     |
|    total_timesteps  | 274052   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.23     |
|    n_updates        | 58512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.145    |
| time/               |          |
|    episodes         | 3536     |
|    fps              | 132      |
|    time_elapsed     | 2067     |
|    total_timesteps  | 274331   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.74     |
|    n_updates        | 58582    |
----------------------------------
Eval num_timesteps=274500, episode_reward=338.20 +/- 122.79
Episode length: 84.94 +/- 30.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.9     |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 0.144    |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.58     |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.144    |
| time/               |          |
|    episodes         | 3540     |
|    fps              | 132      |
|    time_elapsed     | 2072     |
|    total_timesteps  | 274539   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.197    |
|    n_updates        | 58634    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.143    |
| time/               |          |
|    episodes         | 3544     |
|    fps              | 132      |
|    time_elapsed     | 2073     |
|    total_timesteps  | 274816   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.13     |
|    n_updates        | 58703    |
----------------------------------
Eval num_timesteps=275000, episode_reward=182.84 +/- 47.89
Episode length: 46.08 +/- 12.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.142    |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.94     |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.141    |
| time/               |          |
|    episodes         | 3548     |
|    fps              | 132      |
|    time_elapsed     | 2076     |
|    total_timesteps  | 275157   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.35     |
|    n_updates        | 58789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.139    |
| time/               |          |
|    episodes         | 3552     |
|    fps              | 132      |
|    time_elapsed     | 2077     |
|    total_timesteps  | 275455   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.228    |
|    n_updates        | 58863    |
----------------------------------
Eval num_timesteps=275500, episode_reward=224.18 +/- 58.37
Episode length: 56.40 +/- 14.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.4     |
|    mean_reward      | 224      |
| rollout/            |          |
|    exploration_rate | 0.139    |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.46     |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.138    |
| time/               |          |
|    episodes         | 3556     |
|    fps              | 132      |
|    time_elapsed     | 2081     |
|    total_timesteps  | 275664   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.62     |
|    n_updates        | 58915    |
----------------------------------
Eval num_timesteps=276000, episode_reward=280.90 +/- 145.99
Episode length: 70.52 +/- 36.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.5     |
|    mean_reward      | 281      |
| rollout/            |          |
|    exploration_rate | 0.136    |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.135    |
| time/               |          |
|    episodes         | 3560     |
|    fps              | 132      |
|    time_elapsed     | 2086     |
|    total_timesteps  | 276175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.73     |
|    n_updates        | 59043    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.134    |
| time/               |          |
|    episodes         | 3564     |
|    fps              | 132      |
|    time_elapsed     | 2086     |
|    total_timesteps  | 276407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 59101    |
----------------------------------
Eval num_timesteps=276500, episode_reward=309.36 +/- 129.74
Episode length: 77.74 +/- 32.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 0.133    |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.28     |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.133    |
| time/               |          |
|    episodes         | 3568     |
|    fps              | 132      |
|    time_elapsed     | 2091     |
|    total_timesteps  | 276623   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 59155    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 3572     |
|    fps              | 132      |
|    time_elapsed     | 2092     |
|    total_timesteps  | 276806   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 59201    |
----------------------------------
Eval num_timesteps=277000, episode_reward=178.92 +/- 42.43
Episode length: 45.04 +/- 10.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.131    |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.13     |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.129    |
| time/               |          |
|    episodes         | 3576     |
|    fps              | 132      |
|    time_elapsed     | 2095     |
|    total_timesteps  | 277280   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.574    |
|    n_updates        | 59319    |
----------------------------------
Eval num_timesteps=277500, episode_reward=206.24 +/- 70.71
Episode length: 51.90 +/- 17.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.9     |
|    mean_reward      | 206      |
| rollout/            |          |
|    exploration_rate | 0.128    |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.32     |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.127    |
| time/               |          |
|    episodes         | 3580     |
|    fps              | 132      |
|    time_elapsed     | 2099     |
|    total_timesteps  | 277676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.548    |
|    n_updates        | 59418    |
----------------------------------
Eval num_timesteps=278000, episode_reward=209.78 +/- 52.07
Episode length: 52.90 +/- 13.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.125    |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.71     |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.124    |
| time/               |          |
|    episodes         | 3584     |
|    fps              | 132      |
|    time_elapsed     | 2102     |
|    total_timesteps  | 278142   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.11     |
|    n_updates        | 59535    |
----------------------------------
Eval num_timesteps=278500, episode_reward=190.04 +/- 64.35
Episode length: 47.92 +/- 16.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.122    |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.33     |
|    n_updates        | 59624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.122    |
| time/               |          |
|    episodes         | 3588     |
|    fps              | 132      |
|    time_elapsed     | 2106     |
|    total_timesteps  | 278524   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.264    |
|    n_updates        | 59630    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.12     |
| time/               |          |
|    episodes         | 3592     |
|    fps              | 132      |
|    time_elapsed     | 2106     |
|    total_timesteps  | 278965   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.472    |
|    n_updates        | 59741    |
----------------------------------
Eval num_timesteps=279000, episode_reward=209.60 +/- 68.29
Episode length: 52.72 +/- 17.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.7     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.12     |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.95     |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.118    |
| time/               |          |
|    episodes         | 3596     |
|    fps              | 132      |
|    time_elapsed     | 2110     |
|    total_timesteps  | 279244   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.343    |
|    n_updates        | 59810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.117    |
| time/               |          |
|    episodes         | 3600     |
|    fps              | 132      |
|    time_elapsed     | 2110     |
|    total_timesteps  | 279420   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.431    |
|    n_updates        | 59854    |
----------------------------------
Eval num_timesteps=279500, episode_reward=179.94 +/- 50.03
Episode length: 45.34 +/- 12.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.117    |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.427    |
|    n_updates        | 59874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.116    |
| time/               |          |
|    episodes         | 3604     |
|    fps              | 132      |
|    time_elapsed     | 2113     |
|    total_timesteps  | 279660   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.941    |
|    n_updates        | 59914    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.114    |
| time/               |          |
|    episodes         | 3608     |
|    fps              | 132      |
|    time_elapsed     | 2114     |
|    total_timesteps  | 279965   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.18     |
|    n_updates        | 59991    |
----------------------------------
Eval num_timesteps=280000, episode_reward=287.48 +/- 134.44
Episode length: 72.18 +/- 33.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.2     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.114    |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.62     |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.112    |
| time/               |          |
|    episodes         | 3612     |
|    fps              | 132      |
|    time_elapsed     | 2118     |
|    total_timesteps  | 280288   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.98     |
|    n_updates        | 60071    |
----------------------------------
Eval num_timesteps=280500, episode_reward=174.16 +/- 50.62
Episode length: 43.88 +/- 12.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.111    |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.85     |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.2     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.111    |
| time/               |          |
|    episodes         | 3616     |
|    fps              | 132      |
|    time_elapsed     | 2121     |
|    total_timesteps  | 280569   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 60142    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.11     |
| time/               |          |
|    episodes         | 3620     |
|    fps              | 132      |
|    time_elapsed     | 2122     |
|    total_timesteps  | 280805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.47     |
|    n_updates        | 60201    |
----------------------------------
Eval num_timesteps=281000, episode_reward=169.34 +/- 48.22
Episode length: 42.70 +/- 12.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.108    |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.84     |
|    n_updates        | 60249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 3624     |
|    fps              | 132      |
|    time_elapsed     | 2125     |
|    total_timesteps  | 281088   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.85     |
|    n_updates        | 60271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.107    |
| time/               |          |
|    episodes         | 3628     |
|    fps              | 132      |
|    time_elapsed     | 2125     |
|    total_timesteps  | 281354   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 60338    |
----------------------------------
Eval num_timesteps=281500, episode_reward=179.06 +/- 56.39
Episode length: 45.12 +/- 14.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.106    |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.462    |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.105    |
| time/               |          |
|    episodes         | 3632     |
|    fps              | 132      |
|    time_elapsed     | 2128     |
|    total_timesteps  | 281593   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.892    |
|    n_updates        | 60398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.103    |
| time/               |          |
|    episodes         | 3636     |
|    fps              | 132      |
|    time_elapsed     | 2129     |
|    total_timesteps  | 281947   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.768    |
|    n_updates        | 60486    |
----------------------------------
Eval num_timesteps=282000, episode_reward=184.58 +/- 53.96
Episode length: 46.48 +/- 13.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.103    |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.49     |
|    n_updates        | 60499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.101    |
| time/               |          |
|    episodes         | 3640     |
|    fps              | 132      |
|    time_elapsed     | 2132     |
|    total_timesteps  | 282358   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.44     |
|    n_updates        | 60589    |
----------------------------------
Eval num_timesteps=282500, episode_reward=174.46 +/- 38.72
Episode length: 44.02 +/- 9.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.1      |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.622    |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 3644     |
|    fps              | 132      |
|    time_elapsed     | 2135     |
|    total_timesteps  | 282631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.728    |
|    n_updates        | 60657    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.0983   |
| time/               |          |
|    episodes         | 3648     |
|    fps              | 132      |
|    time_elapsed     | 2136     |
|    total_timesteps  | 282836   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.71     |
|    n_updates        | 60708    |
----------------------------------
Eval num_timesteps=283000, episode_reward=182.94 +/- 64.32
Episode length: 46.08 +/- 16.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.0974   |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.18     |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.097    |
| time/               |          |
|    episodes         | 3652     |
|    fps              | 132      |
|    time_elapsed     | 2139     |
|    total_timesteps  | 283069   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.721    |
|    n_updates        | 60767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.0954   |
| time/               |          |
|    episodes         | 3656     |
|    fps              | 132      |
|    time_elapsed     | 2139     |
|    total_timesteps  | 283358   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.79     |
|    n_updates        | 60839    |
----------------------------------
Eval num_timesteps=283500, episode_reward=227.56 +/- 78.95
Episode length: 57.28 +/- 19.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.3     |
|    mean_reward      | 228      |
| rollout/            |          |
|    exploration_rate | 0.0946   |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.58     |
|    n_updates        | 60874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.0934   |
| time/               |          |
|    episodes         | 3660     |
|    fps              | 132      |
|    time_elapsed     | 2143     |
|    total_timesteps  | 283703   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 60925    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.0921   |
| time/               |          |
|    episodes         | 3664     |
|    fps              | 132      |
|    time_elapsed     | 2144     |
|    total_timesteps  | 283945   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.12     |
|    n_updates        | 60986    |
----------------------------------
Eval num_timesteps=284000, episode_reward=183.72 +/- 54.59
Episode length: 46.22 +/- 13.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.0918   |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.4      |
|    n_updates        | 60999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.0904   |
| time/               |          |
|    episodes         | 3668     |
|    fps              | 132      |
|    time_elapsed     | 2147     |
|    total_timesteps  | 284249   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.865    |
|    n_updates        | 61062    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.0891   |
| time/               |          |
|    episodes         | 3672     |
|    fps              | 132      |
|    time_elapsed     | 2147     |
|    total_timesteps  | 284475   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.863    |
|    n_updates        | 61118    |
----------------------------------
Eval num_timesteps=284500, episode_reward=177.46 +/- 55.71
Episode length: 44.70 +/- 13.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.34     |
|    n_updates        | 61124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.0881   |
| time/               |          |
|    episodes         | 3676     |
|    fps              | 132      |
|    time_elapsed     | 2150     |
|    total_timesteps  | 284667   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 61166    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.0869   |
| time/               |          |
|    episodes         | 3680     |
|    fps              | 132      |
|    time_elapsed     | 2151     |
|    total_timesteps  | 284876   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 61218    |
----------------------------------
Eval num_timesteps=285000, episode_reward=207.92 +/- 70.34
Episode length: 52.28 +/- 17.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.3     |
|    mean_reward      | 208      |
| rollout/            |          |
|    exploration_rate | 0.0862   |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.522    |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.2     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.0858   |
| time/               |          |
|    episodes         | 3684     |
|    fps              | 132      |
|    time_elapsed     | 2154     |
|    total_timesteps  | 285062   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.451    |
|    n_updates        | 61265    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.9     |
|    ep_rew_mean      | 270      |
|    exploration_rate | 0.0844   |
| time/               |          |
|    episodes         | 3688     |
|    fps              | 132      |
|    time_elapsed     | 2155     |
|    total_timesteps  | 285317   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.76     |
|    n_updates        | 61329    |
----------------------------------
Eval num_timesteps=285500, episode_reward=425.18 +/- 215.69
Episode length: 106.64 +/- 53.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 425      |
| rollout/            |          |
|    exploration_rate | 0.0834   |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | 265      |
|    exploration_rate | 0.0826   |
| time/               |          |
|    episodes         | 3692     |
|    fps              | 132      |
|    time_elapsed     | 2161     |
|    total_timesteps  | 285639   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.99     |
|    n_updates        | 61409    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.9     |
|    ep_rew_mean      | 266      |
|    exploration_rate | 0.081    |
| time/               |          |
|    episodes         | 3696     |
|    fps              | 132      |
|    time_elapsed     | 2161     |
|    total_timesteps  | 285931   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.42     |
|    n_updates        | 61482    |
----------------------------------
Eval num_timesteps=286000, episode_reward=195.90 +/- 64.46
Episode length: 49.36 +/- 16.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.4     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.0806   |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.14     |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.9     |
|    ep_rew_mean      | 270      |
|    exploration_rate | 0.0794   |
| time/               |          |
|    episodes         | 3700     |
|    fps              | 132      |
|    time_elapsed     | 2165     |
|    total_timesteps  | 286207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.81     |
|    n_updates        | 61551    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 0.0783   |
| time/               |          |
|    episodes         | 3704     |
|    fps              | 132      |
|    time_elapsed     | 2165     |
|    total_timesteps  | 286406   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.72     |
|    n_updates        | 61601    |
----------------------------------
Eval num_timesteps=286500, episode_reward=389.22 +/- 207.61
Episode length: 97.64 +/- 51.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 389      |
| rollout/            |          |
|    exploration_rate | 0.0778   |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 0.0766   |
| time/               |          |
|    episodes         | 3708     |
|    fps              | 132      |
|    time_elapsed     | 2171     |
|    total_timesteps  | 286718   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.65     |
|    n_updates        | 61679    |
----------------------------------
Eval num_timesteps=287000, episode_reward=396.96 +/- 178.24
Episode length: 99.56 +/- 44.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.6     |
|    mean_reward      | 397      |
| rollout/            |          |
|    exploration_rate | 0.075    |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.69     |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 0.0746   |
| time/               |          |
|    episodes         | 3712     |
|    fps              | 131      |
|    time_elapsed     | 2178     |
|    total_timesteps  | 287059   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.468    |
|    n_updates        | 61764    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 0.0733   |
| time/               |          |
|    episodes         | 3716     |
|    fps              | 131      |
|    time_elapsed     | 2178     |
|    total_timesteps  | 287306   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.355    |
|    n_updates        | 61826    |
----------------------------------
Eval num_timesteps=287500, episode_reward=196.44 +/- 61.51
Episode length: 49.52 +/- 15.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.5     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.0722   |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.12     |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 0.0718   |
| time/               |          |
|    episodes         | 3720     |
|    fps              | 131      |
|    time_elapsed     | 2182     |
|    total_timesteps  | 287572   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.277    |
|    n_updates        | 61892    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | 267      |
|    exploration_rate | 0.0704   |
| time/               |          |
|    episodes         | 3724     |
|    fps              | 131      |
|    time_elapsed     | 2182     |
|    total_timesteps  | 287812   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 61952    |
----------------------------------
Eval num_timesteps=288000, episode_reward=199.22 +/- 72.35
Episode length: 50.22 +/- 18.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.2     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.0694   |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.28     |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.0674   |
| time/               |          |
|    episodes         | 3728     |
|    fps              | 131      |
|    time_elapsed     | 2186     |
|    total_timesteps  | 288356   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.01     |
|    n_updates        | 62088    |
----------------------------------
Eval num_timesteps=288500, episode_reward=365.26 +/- 151.32
Episode length: 91.70 +/- 37.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.7     |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 0.0665   |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.02     |
|    n_updates        | 62124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.3     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.0653   |
| time/               |          |
|    episodes         | 3732     |
|    fps              | 131      |
|    time_elapsed     | 2192     |
|    total_timesteps  | 288726   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.41     |
|    n_updates        | 62181    |
----------------------------------
Eval num_timesteps=289000, episode_reward=283.24 +/- 80.89
Episode length: 71.26 +/- 20.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.3     |
|    mean_reward      | 283      |
| rollout/            |          |
|    exploration_rate | 0.0637   |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.564    |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.1     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.0629   |
| time/               |          |
|    episodes         | 3736     |
|    fps              | 131      |
|    time_elapsed     | 2197     |
|    total_timesteps  | 289153   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.226    |
|    n_updates        | 62288    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.6     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.0613   |
| time/               |          |
|    episodes         | 3740     |
|    fps              | 131      |
|    time_elapsed     | 2197     |
|    total_timesteps  | 289421   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 62355    |
----------------------------------
Eval num_timesteps=289500, episode_reward=343.48 +/- 116.82
Episode length: 86.28 +/- 29.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.3     |
|    mean_reward      | 343      |
| rollout/            |          |
|    exploration_rate | 0.0609   |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.32     |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.0601   |
| time/               |          |
|    episodes         | 3744     |
|    fps              | 131      |
|    time_elapsed     | 2202     |
|    total_timesteps  | 289640   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.54     |
|    n_updates        | 62409    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.0583   |
| time/               |          |
|    episodes         | 3748     |
|    fps              | 131      |
|    time_elapsed     | 2203     |
|    total_timesteps  | 289955   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 62488    |
----------------------------------
Eval num_timesteps=290000, episode_reward=181.04 +/- 48.97
Episode length: 45.60 +/- 12.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.0581   |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.29     |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.8     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.0555   |
| time/               |          |
|    episodes         | 3752     |
|    fps              | 131      |
|    time_elapsed     | 2207     |
|    total_timesteps  | 290451   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.21     |
|    n_updates        | 62612    |
----------------------------------
Eval num_timesteps=290500, episode_reward=369.80 +/- 154.98
Episode length: 92.80 +/- 38.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 370      |
| rollout/            |          |
|    exploration_rate | 0.0553   |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 62624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.7     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.054    |
| time/               |          |
|    episodes         | 3756     |
|    fps              | 131      |
|    time_elapsed     | 2213     |
|    total_timesteps  | 290729   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.06     |
|    n_updates        | 62682    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.0529   |
| time/               |          |
|    episodes         | 3760     |
|    fps              | 131      |
|    time_elapsed     | 2213     |
|    total_timesteps  | 290922   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.76     |
|    n_updates        | 62730    |
----------------------------------
Eval num_timesteps=291000, episode_reward=301.80 +/- 110.36
Episode length: 75.88 +/- 27.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.9     |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 0.0524   |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.24     |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.051    |
| time/               |          |
|    episodes         | 3764     |
|    fps              | 131      |
|    time_elapsed     | 2218     |
|    total_timesteps  | 291248   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.58     |
|    n_updates        | 62811    |
----------------------------------
Eval num_timesteps=291500, episode_reward=190.82 +/- 44.73
Episode length: 48.12 +/- 11.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.0496   |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.37     |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.0482   |
| time/               |          |
|    episodes         | 3768     |
|    fps              | 131      |
|    time_elapsed     | 2222     |
|    total_timesteps  | 291747   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.651    |
|    n_updates        | 62936    |
----------------------------------
Eval num_timesteps=292000, episode_reward=158.30 +/- 35.77
Episode length: 40.00 +/- 8.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40       |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.0468   |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.2     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.0456   |
| time/               |          |
|    episodes         | 3772     |
|    fps              | 131      |
|    time_elapsed     | 2225     |
|    total_timesteps  | 292195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.91     |
|    n_updates        | 63048    |
----------------------------------
Eval num_timesteps=292500, episode_reward=205.52 +/- 75.48
Episode length: 51.76 +/- 18.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.8     |
|    mean_reward      | 206      |
| rollout/            |          |
|    exploration_rate | 0.0439   |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.23     |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.0437   |
| time/               |          |
|    episodes         | 3776     |
|    fps              | 131      |
|    time_elapsed     | 2229     |
|    total_timesteps  | 292533   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.98     |
|    n_updates        | 63133    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.0421   |
| time/               |          |
|    episodes         | 3780     |
|    fps              | 131      |
|    time_elapsed     | 2229     |
|    total_timesteps  | 292824   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 63205    |
----------------------------------
Eval num_timesteps=293000, episode_reward=210.10 +/- 54.58
Episode length: 52.92 +/- 13.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.0411   |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.255    |
|    n_updates        | 63249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 3784     |
|    fps              | 131      |
|    time_elapsed     | 2233     |
|    total_timesteps  | 293192   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.84     |
|    n_updates        | 63297    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.0387   |
| time/               |          |
|    episodes         | 3788     |
|    fps              | 131      |
|    time_elapsed     | 2233     |
|    total_timesteps  | 293409   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.858    |
|    n_updates        | 63352    |
----------------------------------
Eval num_timesteps=293500, episode_reward=214.62 +/- 74.00
Episode length: 54.00 +/- 18.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54       |
|    mean_reward      | 215      |
| rollout/            |          |
|    exploration_rate | 0.0382   |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.854    |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.0369   |
| time/               |          |
|    episodes         | 3792     |
|    fps              | 131      |
|    time_elapsed     | 2237     |
|    total_timesteps  | 293740   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.33     |
|    n_updates        | 63434    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.0356   |
| time/               |          |
|    episodes         | 3796     |
|    fps              | 131      |
|    time_elapsed     | 2238     |
|    total_timesteps  | 293957   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.395    |
|    n_updates        | 63489    |
----------------------------------
Eval num_timesteps=294000, episode_reward=335.16 +/- 141.74
Episode length: 84.18 +/- 35.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.2     |
|    mean_reward      | 335      |
| rollout/            |          |
|    exploration_rate | 0.0354   |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.1      |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.0333   |
| time/               |          |
|    episodes         | 3800     |
|    fps              | 131      |
|    time_elapsed     | 2243     |
|    total_timesteps  | 294363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.462    |
|    n_updates        | 63590    |
----------------------------------
Eval num_timesteps=294500, episode_reward=202.70 +/- 57.80
Episode length: 51.00 +/- 14.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51       |
|    mean_reward      | 203      |
| rollout/            |          |
|    exploration_rate | 0.0325   |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.346    |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.0316   |
| time/               |          |
|    episodes         | 3804     |
|    fps              | 131      |
|    time_elapsed     | 2247     |
|    total_timesteps  | 294655   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.28     |
|    n_updates        | 63663    |
----------------------------------
Eval num_timesteps=295000, episode_reward=228.86 +/- 83.08
Episode length: 57.64 +/- 20.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | 229      |
| rollout/            |          |
|    exploration_rate | 0.0297   |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.72     |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.0294   |
| time/               |          |
|    episodes         | 3808     |
|    fps              | 131      |
|    time_elapsed     | 2251     |
|    total_timesteps  | 295051   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 63762    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.0271   |
| time/               |          |
|    episodes         | 3812     |
|    fps              | 131      |
|    time_elapsed     | 2251     |
|    total_timesteps  | 295446   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.487    |
|    n_updates        | 63861    |
----------------------------------
Eval num_timesteps=295500, episode_reward=170.40 +/- 46.79
Episode length: 42.96 +/- 11.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.0268   |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.245    |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.026    |
| time/               |          |
|    episodes         | 3816     |
|    fps              | 131      |
|    time_elapsed     | 2254     |
|    total_timesteps  | 295650   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.11     |
|    n_updates        | 63912    |
----------------------------------
Eval num_timesteps=296000, episode_reward=209.18 +/- 63.12
Episode length: 52.70 +/- 15.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.7     |
|    mean_reward      | 209      |
| rollout/            |          |
|    exploration_rate | 0.024    |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.419    |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.6     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.0238   |
| time/               |          |
|    episodes         | 3820     |
|    fps              | 131      |
|    time_elapsed     | 2258     |
|    total_timesteps  | 296028   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.31     |
|    n_updates        | 64006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.0224   |
| time/               |          |
|    episodes         | 3824     |
|    fps              | 131      |
|    time_elapsed     | 2258     |
|    total_timesteps  | 296267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.879    |
|    n_updates        | 64066    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.0212   |
| time/               |          |
|    episodes         | 3828     |
|    fps              | 131      |
|    time_elapsed     | 2259     |
|    total_timesteps  | 296481   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.4      |
|    n_updates        | 64120    |
----------------------------------
Eval num_timesteps=296500, episode_reward=282.90 +/- 131.26
Episode length: 71.12 +/- 32.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.1     |
|    mean_reward      | 283      |
| rollout/            |          |
|    exploration_rate | 0.0211   |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.2      |
|    n_updates        | 64124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.0189   |
| time/               |          |
|    episodes         | 3832     |
|    fps              | 131      |
|    time_elapsed     | 2264     |
|    total_timesteps  | 296890   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.28     |
|    n_updates        | 64222    |
----------------------------------
Eval num_timesteps=297000, episode_reward=156.28 +/- 34.87
Episode length: 39.42 +/- 8.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.0182   |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.0172   |
| time/               |          |
|    episodes         | 3836     |
|    fps              | 131      |
|    time_elapsed     | 2267     |
|    total_timesteps  | 297181   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 64295    |
----------------------------------
Eval num_timesteps=297500, episode_reward=268.78 +/- 105.73
Episode length: 67.52 +/- 26.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.5     |
|    mean_reward      | 269      |
| rollout/            |          |
|    exploration_rate | 0.0154   |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 64374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.0147   |
| time/               |          |
|    episodes         | 3840     |
|    fps              | 131      |
|    time_elapsed     | 2271     |
|    total_timesteps  | 297615   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.38     |
|    n_updates        | 64403    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.0135   |
| time/               |          |
|    episodes         | 3844     |
|    fps              | 131      |
|    time_elapsed     | 2271     |
|    total_timesteps  | 297826   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.444    |
|    n_updates        | 64456    |
----------------------------------
Eval num_timesteps=298000, episode_reward=215.82 +/- 64.01
Episode length: 54.30 +/- 16.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.3     |
|    mean_reward      | 216      |
| rollout/            |          |
|    exploration_rate | 0.0125   |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.548    |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.0115   |
| time/               |          |
|    episodes         | 3848     |
|    fps              | 131      |
|    time_elapsed     | 2275     |
|    total_timesteps  | 298179   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.47     |
|    n_updates        | 64544    |
----------------------------------
Eval num_timesteps=298500, episode_reward=192.54 +/- 49.54
Episode length: 48.50 +/- 12.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.00964  |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.84     |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.00894  |
| time/               |          |
|    episodes         | 3852     |
|    fps              | 131      |
|    time_elapsed     | 2279     |
|    total_timesteps  | 298621   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.22     |
|    n_updates        | 64655    |
----------------------------------
Eval num_timesteps=299000, episode_reward=278.38 +/- 103.05
Episode length: 69.96 +/- 25.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70       |
|    mean_reward      | 278      |
| rollout/            |          |
|    exploration_rate | 0.00676  |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 64749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.0066   |
| time/               |          |
|    episodes         | 3856     |
|    fps              | 130      |
|    time_elapsed     | 2284     |
|    total_timesteps  | 299027   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 64756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.0045   |
| time/               |          |
|    episodes         | 3860     |
|    fps              | 131      |
|    time_elapsed     | 2284     |
|    total_timesteps  | 299393   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.97     |
|    n_updates        | 64848    |
----------------------------------
Eval num_timesteps=299500, episode_reward=353.76 +/- 160.09
Episode length: 88.80 +/- 40.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.8     |
|    mean_reward      | 354      |
| rollout/            |          |
|    exploration_rate | 0.00389  |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.37     |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.0025   |
| time/               |          |
|    episodes         | 3864     |
|    fps              | 130      |
|    time_elapsed     | 2290     |
|    total_timesteps  | 299739   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.849    |
|    n_updates        | 64934    |
----------------------------------
Eval num_timesteps=300000, episode_reward=212.30 +/- 61.90
Episode length: 53.50 +/- 15.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.5     |
|    mean_reward      | 212      |
| rollout/            |          |
|    exploration_rate | 0.00101  |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.51     |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3868     |
|    fps              | 130      |
|    time_elapsed     | 2293     |
|    total_timesteps  | 300116   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.34     |
|    n_updates        | 65028    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3872     |
|    fps              | 130      |
|    time_elapsed     | 2294     |
|    total_timesteps  | 300382   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 65095    |
----------------------------------
Eval num_timesteps=300500, episode_reward=243.86 +/- 89.67
Episode length: 61.36 +/- 22.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.4     |
|    mean_reward      | 244      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.27     |
|    n_updates        | 65124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3876     |
|    fps              | 130      |
|    time_elapsed     | 2298     |
|    total_timesteps  | 300634   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.47     |
|    n_updates        | 65158    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3880     |
|    fps              | 130      |
|    time_elapsed     | 2299     |
|    total_timesteps  | 300905   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.89     |
|    n_updates        | 65226    |
----------------------------------
Eval num_timesteps=301000, episode_reward=174.02 +/- 47.54
Episode length: 43.90 +/- 11.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 301000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.337    |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3884     |
|    fps              | 130      |
|    time_elapsed     | 2302     |
|    total_timesteps  | 301224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.62     |
|    n_updates        | 65305    |
----------------------------------
Eval num_timesteps=301500, episode_reward=161.64 +/- 41.27
Episode length: 40.78 +/- 10.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.8     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 301500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.43     |
|    n_updates        | 65374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3888     |
|    fps              | 130      |
|    time_elapsed     | 2305     |
|    total_timesteps  | 301623   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.18     |
|    n_updates        | 65405    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3892     |
|    fps              | 130      |
|    time_elapsed     | 2306     |
|    total_timesteps  | 301963   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.38     |
|    n_updates        | 65490    |
----------------------------------
Eval num_timesteps=302000, episode_reward=207.12 +/- 52.02
Episode length: 52.22 +/- 12.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | 207      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 302000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.46     |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3896     |
|    fps              | 130      |
|    time_elapsed     | 2309     |
|    total_timesteps  | 302381   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.21     |
|    n_updates        | 65595    |
----------------------------------
Eval num_timesteps=302500, episode_reward=332.68 +/- 152.35
Episode length: 83.58 +/- 38.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.6     |
|    mean_reward      | 333      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 302500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3900     |
|    fps              | 130      |
|    time_elapsed     | 2315     |
|    total_timesteps  | 302715   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.47     |
|    n_updates        | 65678    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3904     |
|    fps              | 130      |
|    time_elapsed     | 2315     |
|    total_timesteps  | 302913   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.95     |
|    n_updates        | 65728    |
----------------------------------
Eval num_timesteps=303000, episode_reward=299.62 +/- 113.04
Episode length: 75.26 +/- 28.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.3     |
|    mean_reward      | 300      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.86     |
|    n_updates        | 65749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3908     |
|    fps              | 130      |
|    time_elapsed     | 2320     |
|    total_timesteps  | 303175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.399    |
|    n_updates        | 65793    |
----------------------------------
Eval num_timesteps=303500, episode_reward=175.94 +/- 53.99
Episode length: 44.42 +/- 13.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 303500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.486    |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3912     |
|    fps              | 130      |
|    time_elapsed     | 2323     |
|    total_timesteps  | 303620   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 65904    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3916     |
|    fps              | 130      |
|    time_elapsed     | 2324     |
|    total_timesteps  | 303903   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 65975    |
----------------------------------
Eval num_timesteps=304000, episode_reward=333.68 +/- 192.84
Episode length: 83.80 +/- 48.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.8     |
|    mean_reward      | 334      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 304000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 65999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3920     |
|    fps              | 130      |
|    time_elapsed     | 2329     |
|    total_timesteps  | 304221   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.782    |
|    n_updates        | 66055    |
----------------------------------
Eval num_timesteps=304500, episode_reward=221.76 +/- 75.12
Episode length: 55.90 +/- 18.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.9     |
|    mean_reward      | 222      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 304500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.87     |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3924     |
|    fps              | 130      |
|    time_elapsed     | 2333     |
|    total_timesteps  | 304574   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.18     |
|    n_updates        | 66143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3928     |
|    fps              | 130      |
|    time_elapsed     | 2334     |
|    total_timesteps  | 304925   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.33     |
|    n_updates        | 66231    |
----------------------------------
Eval num_timesteps=305000, episode_reward=213.98 +/- 66.87
Episode length: 53.92 +/- 16.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.9     |
|    mean_reward      | 214      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 305000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3932     |
|    fps              | 130      |
|    time_elapsed     | 2338     |
|    total_timesteps  | 305426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.19     |
|    n_updates        | 66356    |
----------------------------------
Eval num_timesteps=305500, episode_reward=172.50 +/- 44.73
Episode length: 43.50 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 305500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.59     |
|    n_updates        | 66374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3936     |
|    fps              | 130      |
|    time_elapsed     | 2341     |
|    total_timesteps  | 305702   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.81     |
|    n_updates        | 66425    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3940     |
|    fps              | 130      |
|    time_elapsed     | 2341     |
|    total_timesteps  | 305946   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.45     |
|    n_updates        | 66486    |
----------------------------------
Eval num_timesteps=306000, episode_reward=264.88 +/- 81.48
Episode length: 66.62 +/- 20.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.6     |
|    mean_reward      | 265      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.3      |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.9     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3944     |
|    fps              | 130      |
|    time_elapsed     | 2346     |
|    total_timesteps  | 306415   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.935    |
|    n_updates        | 66603    |
----------------------------------
Eval num_timesteps=306500, episode_reward=172.98 +/- 35.71
Episode length: 43.66 +/- 8.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 306500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.592    |
|    n_updates        | 66624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3948     |
|    fps              | 130      |
|    time_elapsed     | 2349     |
|    total_timesteps  | 306799   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.59     |
|    n_updates        | 66699    |
----------------------------------
Eval num_timesteps=307000, episode_reward=207.36 +/- 81.87
Episode length: 52.24 +/- 20.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | 207      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 307000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 66749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3952     |
|    fps              | 130      |
|    time_elapsed     | 2353     |
|    total_timesteps  | 307045   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.28     |
|    n_updates        | 66761    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3956     |
|    fps              | 130      |
|    time_elapsed     | 2354     |
|    total_timesteps  | 307476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.41     |
|    n_updates        | 66868    |
----------------------------------
Eval num_timesteps=307500, episode_reward=321.06 +/- 156.02
Episode length: 80.60 +/- 38.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.6     |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 307500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.91     |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.4     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3960     |
|    fps              | 130      |
|    time_elapsed     | 2359     |
|    total_timesteps  | 307836   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.95     |
|    n_updates        | 66958    |
----------------------------------
Eval num_timesteps=308000, episode_reward=309.64 +/- 101.32
Episode length: 77.80 +/- 25.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.8     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.27     |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3964     |
|    fps              | 130      |
|    time_elapsed     | 2364     |
|    total_timesteps  | 308074   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.67     |
|    n_updates        | 67018    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3968     |
|    fps              | 130      |
|    time_elapsed     | 2364     |
|    total_timesteps  | 308263   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.46     |
|    n_updates        | 67065    |
----------------------------------
Eval num_timesteps=308500, episode_reward=217.62 +/- 56.34
Episode length: 54.74 +/- 14.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.7     |
|    mean_reward      | 218      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 308500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.487    |
|    n_updates        | 67124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3972     |
|    fps              | 130      |
|    time_elapsed     | 2368     |
|    total_timesteps  | 308685   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.71     |
|    n_updates        | 67171    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3976     |
|    fps              | 130      |
|    time_elapsed     | 2368     |
|    total_timesteps  | 308818   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.53     |
|    n_updates        | 67204    |
----------------------------------
Eval num_timesteps=309000, episode_reward=221.26 +/- 89.39
Episode length: 55.74 +/- 22.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.7     |
|    mean_reward      | 221      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 309000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.16     |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3980     |
|    fps              | 130      |
|    time_elapsed     | 2372     |
|    total_timesteps  | 309119   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 67279    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3984     |
|    fps              | 130      |
|    time_elapsed     | 2373     |
|    total_timesteps  | 309452   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.379    |
|    n_updates        | 67362    |
----------------------------------
Eval num_timesteps=309500, episode_reward=206.46 +/- 64.50
Episode length: 51.98 +/- 16.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52       |
|    mean_reward      | 206      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 309500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.643    |
|    n_updates        | 67374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3988     |
|    fps              | 130      |
|    time_elapsed     | 2375     |
|    total_timesteps  | 309712   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.14     |
|    n_updates        | 67427    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3992     |
|    fps              | 130      |
|    time_elapsed     | 2375     |
|    total_timesteps  | 309978   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.59     |
|    n_updates        | 67494    |
----------------------------------
Eval num_timesteps=310000, episode_reward=429.62 +/- 187.80
Episode length: 107.76 +/- 46.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 430      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 67499    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3996     |
|    fps              | 130      |
|    time_elapsed     | 2382     |
|    total_timesteps  | 310267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.486    |
|    n_updates        | 67566    |
----------------------------------
Eval num_timesteps=310500, episode_reward=248.34 +/- 86.61
Episode length: 62.46 +/- 21.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.5     |
|    mean_reward      | 248      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.74     |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4000     |
|    fps              | 130      |
|    time_elapsed     | 2386     |
|    total_timesteps  | 310620   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.461    |
|    n_updates        | 67654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4004     |
|    fps              | 130      |
|    time_elapsed     | 2386     |
|    total_timesteps  | 310868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.33     |
|    n_updates        | 67716    |
----------------------------------
Eval num_timesteps=311000, episode_reward=310.50 +/- 176.01
Episode length: 78.06 +/- 43.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.1     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 311000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 67749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4008     |
|    fps              | 130      |
|    time_elapsed     | 2392     |
|    total_timesteps  | 311461   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.95     |
|    n_updates        | 67865    |
----------------------------------
Eval num_timesteps=311500, episode_reward=206.08 +/- 68.20
Episode length: 51.94 +/- 17.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.9     |
|    mean_reward      | 206      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 311500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4012     |
|    fps              | 130      |
|    time_elapsed     | 2396     |
|    total_timesteps  | 311766   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.78     |
|    n_updates        | 67941    |
----------------------------------
Eval num_timesteps=312000, episode_reward=337.48 +/- 121.12
Episode length: 84.72 +/- 30.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.7     |
|    mean_reward      | 337      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 312000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.531    |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4016     |
|    fps              | 129      |
|    time_elapsed     | 2402     |
|    total_timesteps  | 312069   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.34     |
|    n_updates        | 68017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4020     |
|    fps              | 129      |
|    time_elapsed     | 2403     |
|    total_timesteps  | 312375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 68093    |
----------------------------------
Eval num_timesteps=312500, episode_reward=213.30 +/- 68.39
Episode length: 53.74 +/- 17.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.7     |
|    mean_reward      | 213      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.28     |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4024     |
|    fps              | 129      |
|    time_elapsed     | 2407     |
|    total_timesteps  | 312625   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.91     |
|    n_updates        | 68156    |
----------------------------------
Eval num_timesteps=313000, episode_reward=256.70 +/- 88.04
Episode length: 64.58 +/- 21.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.6     |
|    mean_reward      | 257      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 313000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.25     |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4028     |
|    fps              | 129      |
|    time_elapsed     | 2411     |
|    total_timesteps  | 313137   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.522    |
|    n_updates        | 68284    |
----------------------------------
Eval num_timesteps=313500, episode_reward=291.16 +/- 96.45
Episode length: 73.20 +/- 24.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.2     |
|    mean_reward      | 291      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 313500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.53     |
|    n_updates        | 68374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4032     |
|    fps              | 129      |
|    time_elapsed     | 2417     |
|    total_timesteps  | 313604   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 68400    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4036     |
|    fps              | 129      |
|    time_elapsed     | 2417     |
|    total_timesteps  | 313901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.22     |
|    n_updates        | 68475    |
----------------------------------
Eval num_timesteps=314000, episode_reward=273.52 +/- 139.92
Episode length: 68.76 +/- 34.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.8     |
|    mean_reward      | 274      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 314000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.16     |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4040     |
|    fps              | 129      |
|    time_elapsed     | 2422     |
|    total_timesteps  | 314271   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.835    |
|    n_updates        | 68567    |
----------------------------------
Eval num_timesteps=314500, episode_reward=333.20 +/- 112.36
Episode length: 83.66 +/- 28.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.7     |
|    mean_reward      | 333      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 314500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.57     |
|    n_updates        | 68624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4044     |
|    fps              | 129      |
|    time_elapsed     | 2428     |
|    total_timesteps  | 314614   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.67     |
|    n_updates        | 68653    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4048     |
|    fps              | 129      |
|    time_elapsed     | 2428     |
|    total_timesteps  | 314807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.66     |
|    n_updates        | 68701    |
----------------------------------
Eval num_timesteps=315000, episode_reward=212.76 +/- 78.80
Episode length: 53.56 +/- 19.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.6     |
|    mean_reward      | 213      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.46     |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4052     |
|    fps              | 129      |
|    time_elapsed     | 2432     |
|    total_timesteps  | 315208   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.43     |
|    n_updates        | 68801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4056     |
|    fps              | 129      |
|    time_elapsed     | 2432     |
|    total_timesteps  | 315407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.81     |
|    n_updates        | 68851    |
----------------------------------
Eval num_timesteps=315500, episode_reward=343.28 +/- 151.74
Episode length: 86.12 +/- 37.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.1     |
|    mean_reward      | 343      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 315500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4060     |
|    fps              | 129      |
|    time_elapsed     | 2438     |
|    total_timesteps  | 315684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.01     |
|    n_updates        | 68920    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4064     |
|    fps              | 129      |
|    time_elapsed     | 2438     |
|    total_timesteps  | 315920   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.46     |
|    n_updates        | 68979    |
----------------------------------
Eval num_timesteps=316000, episode_reward=259.70 +/- 103.10
Episode length: 65.24 +/- 25.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.2     |
|    mean_reward      | 260      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 316000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.35     |
|    n_updates        | 68999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4068     |
|    fps              | 129      |
|    time_elapsed     | 2443     |
|    total_timesteps  | 316264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.17     |
|    n_updates        | 69065    |
----------------------------------
Eval num_timesteps=316500, episode_reward=318.30 +/- 193.19
Episode length: 79.92 +/- 48.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.9     |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 316500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.816    |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4072     |
|    fps              | 129      |
|    time_elapsed     | 2448     |
|    total_timesteps  | 316501   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.61     |
|    n_updates        | 69125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4076     |
|    fps              | 129      |
|    time_elapsed     | 2449     |
|    total_timesteps  | 316816   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.21     |
|    n_updates        | 69203    |
----------------------------------
Eval num_timesteps=317000, episode_reward=257.76 +/- 83.96
Episode length: 64.80 +/- 20.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.8     |
|    mean_reward      | 258      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 317000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.08     |
|    n_updates        | 69249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4080     |
|    fps              | 129      |
|    time_elapsed     | 2453     |
|    total_timesteps  | 317022   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.62     |
|    n_updates        | 69255    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4084     |
|    fps              | 129      |
|    time_elapsed     | 2454     |
|    total_timesteps  | 317391   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.01     |
|    n_updates        | 69347    |
----------------------------------
Eval num_timesteps=317500, episode_reward=214.42 +/- 69.78
Episode length: 53.98 +/- 17.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54       |
|    mean_reward      | 214      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 317500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4088     |
|    fps              | 129      |
|    time_elapsed     | 2457     |
|    total_timesteps  | 317606   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.4      |
|    n_updates        | 69401    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4092     |
|    fps              | 129      |
|    time_elapsed     | 2458     |
|    total_timesteps  | 317897   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.73     |
|    n_updates        | 69474    |
----------------------------------
Eval num_timesteps=318000, episode_reward=346.60 +/- 144.29
Episode length: 86.98 +/- 36.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 347      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 318000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.252    |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4096     |
|    fps              | 129      |
|    time_elapsed     | 2464     |
|    total_timesteps  | 318339   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.905    |
|    n_updates        | 69584    |
----------------------------------
Eval num_timesteps=318500, episode_reward=265.64 +/- 111.25
Episode length: 66.76 +/- 27.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.8     |
|    mean_reward      | 266      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 318500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 69624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4100     |
|    fps              | 129      |
|    time_elapsed     | 2468     |
|    total_timesteps  | 318590   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.73     |
|    n_updates        | 69647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4104     |
|    fps              | 129      |
|    time_elapsed     | 2469     |
|    total_timesteps  | 318787   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.1      |
|    n_updates        | 69696    |
----------------------------------
Eval num_timesteps=319000, episode_reward=249.80 +/- 105.26
Episode length: 62.76 +/- 26.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.8     |
|    mean_reward      | 250      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 319000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.74     |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4108     |
|    fps              | 128      |
|    time_elapsed     | 2473     |
|    total_timesteps  | 319077   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.14     |
|    n_updates        | 69769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4112     |
|    fps              | 129      |
|    time_elapsed     | 2474     |
|    total_timesteps  | 319347   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.53     |
|    n_updates        | 69836    |
----------------------------------
Eval num_timesteps=319500, episode_reward=347.90 +/- 146.21
Episode length: 87.34 +/- 36.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.3     |
|    mean_reward      | 348      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 319500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.56     |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4116     |
|    fps              | 128      |
|    time_elapsed     | 2480     |
|    total_timesteps  | 319646   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.34     |
|    n_updates        | 69911    |
----------------------------------
Eval num_timesteps=320000, episode_reward=350.28 +/- 171.63
Episode length: 88.00 +/- 42.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 350      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 320000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.24     |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4120     |
|    fps              | 128      |
|    time_elapsed     | 2485     |
|    total_timesteps  | 320004   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.11     |
|    n_updates        | 70000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4124     |
|    fps              | 128      |
|    time_elapsed     | 2486     |
|    total_timesteps  | 320365   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 70091    |
----------------------------------
Eval num_timesteps=320500, episode_reward=392.74 +/- 143.30
Episode length: 98.50 +/- 35.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.5     |
|    mean_reward      | 393      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.97     |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4128     |
|    fps              | 128      |
|    time_elapsed     | 2492     |
|    total_timesteps  | 320675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.965    |
|    n_updates        | 70168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4132     |
|    fps              | 128      |
|    time_elapsed     | 2493     |
|    total_timesteps  | 320956   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.12     |
|    n_updates        | 70238    |
----------------------------------
Eval num_timesteps=321000, episode_reward=414.20 +/- 182.98
Episode length: 103.90 +/- 45.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 414      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 70249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4136     |
|    fps              | 128      |
|    time_elapsed     | 2499     |
|    total_timesteps  | 321199   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 70299    |
----------------------------------
Eval num_timesteps=321500, episode_reward=249.38 +/- 80.80
Episode length: 62.76 +/- 20.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.8     |
|    mean_reward      | 249      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 321500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.14     |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4140     |
|    fps              | 128      |
|    time_elapsed     | 2504     |
|    total_timesteps  | 321559   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 70389    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4144     |
|    fps              | 128      |
|    time_elapsed     | 2504     |
|    total_timesteps  | 321945   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 70486    |
----------------------------------
Eval num_timesteps=322000, episode_reward=320.46 +/- 156.02
Episode length: 80.44 +/- 38.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.4     |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 322000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 70499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4148     |
|    fps              | 128      |
|    time_elapsed     | 2510     |
|    total_timesteps  | 322364   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.74     |
|    n_updates        | 70590    |
----------------------------------
Eval num_timesteps=322500, episode_reward=271.58 +/- 100.49
Episode length: 68.32 +/- 25.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.3     |
|    mean_reward      | 272      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 322500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.21     |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4152     |
|    fps              | 128      |
|    time_elapsed     | 2515     |
|    total_timesteps  | 322744   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.37     |
|    n_updates        | 70685    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4156     |
|    fps              | 128      |
|    time_elapsed     | 2516     |
|    total_timesteps  | 322974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 70743    |
----------------------------------
Eval num_timesteps=323000, episode_reward=347.12 +/- 180.12
Episode length: 87.14 +/- 44.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.1     |
|    mean_reward      | 347      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 323000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.402    |
|    n_updates        | 70749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4160     |
|    fps              | 128      |
|    time_elapsed     | 2521     |
|    total_timesteps  | 323358   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.97     |
|    n_updates        | 70839    |
----------------------------------
Eval num_timesteps=323500, episode_reward=307.92 +/- 145.19
Episode length: 77.44 +/- 36.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.4     |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 323500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.73     |
|    n_updates        | 70874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4164     |
|    fps              | 128      |
|    time_elapsed     | 2527     |
|    total_timesteps  | 323715   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.6      |
|    n_updates        | 70928    |
----------------------------------
Eval num_timesteps=324000, episode_reward=223.88 +/- 83.66
Episode length: 56.36 +/- 20.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.4     |
|    mean_reward      | 224      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.21     |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4168     |
|    fps              | 128      |
|    time_elapsed     | 2531     |
|    total_timesteps  | 324067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 71016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4172     |
|    fps              | 128      |
|    time_elapsed     | 2532     |
|    total_timesteps  | 324399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.7      |
|    n_updates        | 71099    |
----------------------------------
Eval num_timesteps=324500, episode_reward=339.44 +/- 207.01
Episode length: 85.22 +/- 51.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.2     |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 324500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.25     |
|    n_updates        | 71124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4176     |
|    fps              | 127      |
|    time_elapsed     | 2538     |
|    total_timesteps  | 324833   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.34     |
|    n_updates        | 71208    |
----------------------------------
Eval num_timesteps=325000, episode_reward=188.38 +/- 52.77
Episode length: 47.42 +/- 13.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.4     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 325000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.36     |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4180     |
|    fps              | 127      |
|    time_elapsed     | 2541     |
|    total_timesteps  | 325110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.25     |
|    n_updates        | 71277    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4184     |
|    fps              | 128      |
|    time_elapsed     | 2542     |
|    total_timesteps  | 325499   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.5      |
|    n_updates        | 71374    |
----------------------------------
Eval num_timesteps=325500, episode_reward=226.64 +/- 96.18
Episode length: 57.04 +/- 24.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57       |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4188     |
|    fps              | 127      |
|    time_elapsed     | 2546     |
|    total_timesteps  | 325766   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.32     |
|    n_updates        | 71441    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4192     |
|    fps              | 127      |
|    time_elapsed     | 2547     |
|    total_timesteps  | 325921   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 71480    |
----------------------------------
Eval num_timesteps=326000, episode_reward=232.90 +/- 85.82
Episode length: 58.54 +/- 21.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.5     |
|    mean_reward      | 233      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4196     |
|    fps              | 127      |
|    time_elapsed     | 2550     |
|    total_timesteps  | 326167   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.58     |
|    n_updates        | 71541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4200     |
|    fps              | 127      |
|    time_elapsed     | 2551     |
|    total_timesteps  | 326440   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.1      |
|    n_updates        | 71609    |
----------------------------------
Eval num_timesteps=326500, episode_reward=219.26 +/- 64.26
Episode length: 55.22 +/- 16.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.2     |
|    mean_reward      | 219      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 326500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.35     |
|    n_updates        | 71624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4204     |
|    fps              | 127      |
|    time_elapsed     | 2555     |
|    total_timesteps  | 326868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.39     |
|    n_updates        | 71716    |
----------------------------------
Eval num_timesteps=327000, episode_reward=232.30 +/- 78.34
Episode length: 58.40 +/- 19.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.4     |
|    mean_reward      | 232      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 327000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.51     |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4208     |
|    fps              | 127      |
|    time_elapsed     | 2559     |
|    total_timesteps  | 327167   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.64     |
|    n_updates        | 71791    |
----------------------------------
Eval num_timesteps=327500, episode_reward=282.26 +/- 158.96
Episode length: 70.86 +/- 39.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.9     |
|    mean_reward      | 282      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 327500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.646    |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4212     |
|    fps              | 127      |
|    time_elapsed     | 2564     |
|    total_timesteps  | 327532   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.85     |
|    n_updates        | 71882    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4216     |
|    fps              | 127      |
|    time_elapsed     | 2564     |
|    total_timesteps  | 327691   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.5      |
|    n_updates        | 71922    |
----------------------------------
Eval num_timesteps=328000, episode_reward=285.82 +/- 87.36
Episode length: 71.80 +/- 21.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.8     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 328000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.91     |
|    n_updates        | 71999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4220     |
|    fps              | 127      |
|    time_elapsed     | 2569     |
|    total_timesteps  | 328202   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.43     |
|    n_updates        | 72050    |
----------------------------------
Eval num_timesteps=328500, episode_reward=233.64 +/- 78.80
Episode length: 58.80 +/- 19.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.8     |
|    mean_reward      | 234      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 328500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.18     |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4224     |
|    fps              | 127      |
|    time_elapsed     | 2574     |
|    total_timesteps  | 328547   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.941    |
|    n_updates        | 72136    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4228     |
|    fps              | 127      |
|    time_elapsed     | 2574     |
|    total_timesteps  | 328901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.81     |
|    n_updates        | 72225    |
----------------------------------
Eval num_timesteps=329000, episode_reward=363.58 +/- 162.51
Episode length: 91.32 +/- 40.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 364      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 329000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.65     |
|    n_updates        | 72249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4232     |
|    fps              | 127      |
|    time_elapsed     | 2580     |
|    total_timesteps  | 329341   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.18     |
|    n_updates        | 72335    |
----------------------------------
Eval num_timesteps=329500, episode_reward=315.44 +/- 162.74
Episode length: 79.14 +/- 40.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.1     |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 329500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.333    |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4236     |
|    fps              | 127      |
|    time_elapsed     | 2586     |
|    total_timesteps  | 329671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.12     |
|    n_updates        | 72417    |
----------------------------------
Eval num_timesteps=330000, episode_reward=235.66 +/- 73.60
Episode length: 59.24 +/- 18.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.2     |
|    mean_reward      | 236      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 330000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.997    |
|    n_updates        | 72499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.8     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4240     |
|    fps              | 127      |
|    time_elapsed     | 2590     |
|    total_timesteps  | 330035   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.71     |
|    n_updates        | 72508    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4244     |
|    fps              | 127      |
|    time_elapsed     | 2591     |
|    total_timesteps  | 330272   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.64     |
|    n_updates        | 72567    |
----------------------------------
Eval num_timesteps=330500, episode_reward=199.70 +/- 77.40
Episode length: 50.28 +/- 19.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.3     |
|    mean_reward      | 200      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 330500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 72624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4248     |
|    fps              | 127      |
|    time_elapsed     | 2595     |
|    total_timesteps  | 330510   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.24     |
|    n_updates        | 72627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4252     |
|    fps              | 127      |
|    time_elapsed     | 2595     |
|    total_timesteps  | 330702   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.86     |
|    n_updates        | 72675    |
----------------------------------
Eval num_timesteps=331000, episode_reward=230.50 +/- 111.98
Episode length: 57.98 +/- 27.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58       |
|    mean_reward      | 230      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.75     |
|    n_updates        | 72749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4256     |
|    fps              | 127      |
|    time_elapsed     | 2600     |
|    total_timesteps  | 331052   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.46     |
|    n_updates        | 72762    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4260     |
|    fps              | 127      |
|    time_elapsed     | 2600     |
|    total_timesteps  | 331314   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.907    |
|    n_updates        | 72828    |
----------------------------------
Eval num_timesteps=331500, episode_reward=307.82 +/- 164.64
Episode length: 77.32 +/- 41.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.3     |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.26     |
|    n_updates        | 72874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4264     |
|    fps              | 127      |
|    time_elapsed     | 2606     |
|    total_timesteps  | 331676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.8      |
|    n_updates        | 72918    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4268     |
|    fps              | 127      |
|    time_elapsed     | 2607     |
|    total_timesteps  | 331980   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.22     |
|    n_updates        | 72994    |
----------------------------------
Eval num_timesteps=332000, episode_reward=187.34 +/- 59.83
Episode length: 47.32 +/- 15.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 332000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.831    |
|    n_updates        | 72999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4272     |
|    fps              | 127      |
|    time_elapsed     | 2610     |
|    total_timesteps  | 332182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.604    |
|    n_updates        | 73045    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4276     |
|    fps              | 127      |
|    time_elapsed     | 2611     |
|    total_timesteps  | 332471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.03     |
|    n_updates        | 73117    |
----------------------------------
Eval num_timesteps=332500, episode_reward=210.42 +/- 58.86
Episode length: 52.94 +/- 14.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 332500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.962    |
|    n_updates        | 73124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4280     |
|    fps              | 127      |
|    time_elapsed     | 2614     |
|    total_timesteps  | 332767   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.32     |
|    n_updates        | 73191    |
----------------------------------
Eval num_timesteps=333000, episode_reward=203.92 +/- 84.69
Episode length: 51.36 +/- 21.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | 204      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.14     |
|    n_updates        | 73249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4284     |
|    fps              | 127      |
|    time_elapsed     | 2618     |
|    total_timesteps  | 333105   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.44     |
|    n_updates        | 73276    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4288     |
|    fps              | 127      |
|    time_elapsed     | 2619     |
|    total_timesteps  | 333415   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.23     |
|    n_updates        | 73353    |
----------------------------------
Eval num_timesteps=333500, episode_reward=313.32 +/- 150.25
Episode length: 78.64 +/- 37.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.6     |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 333500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.31     |
|    n_updates        | 73374    |
----------------------------------
Eval num_timesteps=334000, episode_reward=227.92 +/- 85.24
Episode length: 57.26 +/- 21.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.3     |
|    mean_reward      | 228      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 73499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4292     |
|    fps              | 127      |
|    time_elapsed     | 2629     |
|    total_timesteps  | 334023   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 73505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4296     |
|    fps              | 127      |
|    time_elapsed     | 2629     |
|    total_timesteps  | 334304   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.595    |
|    n_updates        | 73575    |
----------------------------------
Eval num_timesteps=334500, episode_reward=282.90 +/- 106.15
Episode length: 71.06 +/- 26.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.1     |
|    mean_reward      | 283      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 334500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.16     |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4300     |
|    fps              | 126      |
|    time_elapsed     | 2634     |
|    total_timesteps  | 334561   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.64     |
|    n_updates        | 73640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4304     |
|    fps              | 127      |
|    time_elapsed     | 2635     |
|    total_timesteps  | 334934   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.44     |
|    n_updates        | 73733    |
----------------------------------
Eval num_timesteps=335000, episode_reward=201.28 +/- 68.45
Episode length: 50.72 +/- 17.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.7     |
|    mean_reward      | 201      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 335000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 73749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4308     |
|    fps              | 126      |
|    time_elapsed     | 2638     |
|    total_timesteps  | 335103   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.73     |
|    n_updates        | 73775    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4312     |
|    fps              | 127      |
|    time_elapsed     | 2639     |
|    total_timesteps  | 335323   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.669    |
|    n_updates        | 73830    |
----------------------------------
Eval num_timesteps=335500, episode_reward=318.20 +/- 137.36
Episode length: 79.88 +/- 34.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.9     |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 335500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 73874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4316     |
|    fps              | 126      |
|    time_elapsed     | 2645     |
|    total_timesteps  | 335723   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.38     |
|    n_updates        | 73930    |
----------------------------------
Eval num_timesteps=336000, episode_reward=223.32 +/- 67.67
Episode length: 56.20 +/- 16.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.2     |
|    mean_reward      | 223      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.76     |
|    n_updates        | 73999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4320     |
|    fps              | 126      |
|    time_elapsed     | 2649     |
|    total_timesteps  | 336034   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 74008    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4324     |
|    fps              | 126      |
|    time_elapsed     | 2650     |
|    total_timesteps  | 336478   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.6      |
|    n_updates        | 74119    |
----------------------------------
Eval num_timesteps=336500, episode_reward=253.98 +/- 95.82
Episode length: 63.86 +/- 23.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.9     |
|    mean_reward      | 254      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 336500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.928    |
|    n_updates        | 74124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4328     |
|    fps              | 126      |
|    time_elapsed     | 2654     |
|    total_timesteps  | 336658   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.2      |
|    n_updates        | 74164    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4332     |
|    fps              | 126      |
|    time_elapsed     | 2655     |
|    total_timesteps  | 336889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.28     |
|    n_updates        | 74222    |
----------------------------------
Eval num_timesteps=337000, episode_reward=212.46 +/- 80.04
Episode length: 53.50 +/- 20.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.5     |
|    mean_reward      | 212      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 337000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.83     |
|    n_updates        | 74249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4336     |
|    fps              | 126      |
|    time_elapsed     | 2659     |
|    total_timesteps  | 337114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.4      |
|    n_updates        | 74278    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4340     |
|    fps              | 126      |
|    time_elapsed     | 2659     |
|    total_timesteps  | 337336   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.703    |
|    n_updates        | 74333    |
----------------------------------
Eval num_timesteps=337500, episode_reward=222.44 +/- 77.93
Episode length: 55.92 +/- 19.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.9     |
|    mean_reward      | 222      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 337500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.81     |
|    n_updates        | 74374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4344     |
|    fps              | 126      |
|    time_elapsed     | 2663     |
|    total_timesteps  | 337602   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 74400    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.6     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4348     |
|    fps              | 126      |
|    time_elapsed     | 2663     |
|    total_timesteps  | 337868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.833    |
|    n_updates        | 74466    |
----------------------------------
Eval num_timesteps=338000, episode_reward=225.40 +/- 63.68
Episode length: 56.78 +/- 15.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.8     |
|    mean_reward      | 225      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 338000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.87     |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4352     |
|    fps              | 126      |
|    time_elapsed     | 2668     |
|    total_timesteps  | 338237   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.636    |
|    n_updates        | 74559    |
----------------------------------
Eval num_timesteps=338500, episode_reward=223.66 +/- 66.45
Episode length: 56.24 +/- 16.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.2     |
|    mean_reward      | 224      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 338500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 74624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4356     |
|    fps              | 126      |
|    time_elapsed     | 2672     |
|    total_timesteps  | 338509   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 74627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4360     |
|    fps              | 126      |
|    time_elapsed     | 2672     |
|    total_timesteps  | 338734   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.64     |
|    n_updates        | 74683    |
----------------------------------
Eval num_timesteps=339000, episode_reward=308.14 +/- 168.04
Episode length: 77.46 +/- 42.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.5     |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 339000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.39     |
|    n_updates        | 74749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4364     |
|    fps              | 126      |
|    time_elapsed     | 2678     |
|    total_timesteps  | 339133   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.49     |
|    n_updates        | 74783    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4368     |
|    fps              | 126      |
|    time_elapsed     | 2679     |
|    total_timesteps  | 339484   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.54     |
|    n_updates        | 74870    |
----------------------------------
Eval num_timesteps=339500, episode_reward=345.40 +/- 142.58
Episode length: 86.70 +/- 35.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.7     |
|    mean_reward      | 345      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 339500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.59     |
|    n_updates        | 74874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4372     |
|    fps              | 126      |
|    time_elapsed     | 2685     |
|    total_timesteps  | 339874   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.47     |
|    n_updates        | 74968    |
----------------------------------
Eval num_timesteps=340000, episode_reward=322.26 +/- 162.86
Episode length: 80.96 +/- 40.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81       |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 340000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.557    |
|    n_updates        | 74999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4376     |
|    fps              | 126      |
|    time_elapsed     | 2691     |
|    total_timesteps  | 340113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 75028    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4380     |
|    fps              | 126      |
|    time_elapsed     | 2691     |
|    total_timesteps  | 340339   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.15     |
|    n_updates        | 75084    |
----------------------------------
Eval num_timesteps=340500, episode_reward=290.66 +/- 103.48
Episode length: 73.02 +/- 25.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73       |
|    mean_reward      | 291      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 340500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.09     |
|    n_updates        | 75124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4384     |
|    fps              | 126      |
|    time_elapsed     | 2696     |
|    total_timesteps  | 340667   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 75166    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4388     |
|    fps              | 126      |
|    time_elapsed     | 2697     |
|    total_timesteps  | 340957   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.702    |
|    n_updates        | 75239    |
----------------------------------
Eval num_timesteps=341000, episode_reward=337.82 +/- 192.40
Episode length: 84.76 +/- 48.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 341000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.69     |
|    n_updates        | 75249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4392     |
|    fps              | 126      |
|    time_elapsed     | 2703     |
|    total_timesteps  | 341144   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.19     |
|    n_updates        | 75285    |
----------------------------------
Eval num_timesteps=341500, episode_reward=262.86 +/- 101.02
Episode length: 66.04 +/- 25.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66       |
|    mean_reward      | 263      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 341500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.36     |
|    n_updates        | 75374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4396     |
|    fps              | 126      |
|    time_elapsed     | 2708     |
|    total_timesteps  | 341528   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.06     |
|    n_updates        | 75381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4400     |
|    fps              | 126      |
|    time_elapsed     | 2708     |
|    total_timesteps  | 341678   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.17     |
|    n_updates        | 75419    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.2     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4404     |
|    fps              | 126      |
|    time_elapsed     | 2709     |
|    total_timesteps  | 341954   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.87     |
|    n_updates        | 75488    |
----------------------------------
Eval num_timesteps=342000, episode_reward=284.94 +/- 123.86
Episode length: 71.64 +/- 30.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.98     |
|    n_updates        | 75499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4408     |
|    fps              | 126      |
|    time_elapsed     | 2714     |
|    total_timesteps  | 342302   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 75575    |
----------------------------------
Eval num_timesteps=342500, episode_reward=386.52 +/- 198.01
Episode length: 96.98 +/- 49.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 387      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 342500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.93     |
|    n_updates        | 75624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4412     |
|    fps              | 125      |
|    time_elapsed     | 2720     |
|    total_timesteps  | 342555   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.47     |
|    n_updates        | 75638    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.5     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4416     |
|    fps              | 125      |
|    time_elapsed     | 2721     |
|    total_timesteps  | 342875   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.944    |
|    n_updates        | 75718    |
----------------------------------
Eval num_timesteps=343000, episode_reward=288.18 +/- 160.96
Episode length: 72.48 +/- 40.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.5     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 343000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.91     |
|    n_updates        | 75749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.4     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4420     |
|    fps              | 125      |
|    time_elapsed     | 2726     |
|    total_timesteps  | 343176   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.44     |
|    n_updates        | 75793    |
----------------------------------
Eval num_timesteps=343500, episode_reward=318.48 +/- 120.84
Episode length: 80.02 +/- 30.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80       |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 343500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.96     |
|    n_updates        | 75874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.3     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4424     |
|    fps              | 125      |
|    time_elapsed     | 2732     |
|    total_timesteps  | 343510   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.11     |
|    n_updates        | 75877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4428     |
|    fps              | 125      |
|    time_elapsed     | 2733     |
|    total_timesteps  | 343848   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 75961    |
----------------------------------
Eval num_timesteps=344000, episode_reward=230.56 +/- 81.69
Episode length: 58.04 +/- 20.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58       |
|    mean_reward      | 231      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.55     |
|    n_updates        | 75999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.7     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4432     |
|    fps              | 125      |
|    time_elapsed     | 2737     |
|    total_timesteps  | 344154   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.02     |
|    n_updates        | 76038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.7     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4436     |
|    fps              | 125      |
|    time_elapsed     | 2738     |
|    total_timesteps  | 344479   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.03     |
|    n_updates        | 76119    |
----------------------------------
Eval num_timesteps=344500, episode_reward=234.44 +/- 85.08
Episode length: 58.96 +/- 21.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | 234      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 344500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.876    |
|    n_updates        | 76124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4440     |
|    fps              | 125      |
|    time_elapsed     | 2742     |
|    total_timesteps  | 344780   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 76194    |
----------------------------------
Eval num_timesteps=345000, episode_reward=367.12 +/- 179.32
Episode length: 92.12 +/- 44.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.1     |
|    mean_reward      | 367      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 345000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.35     |
|    n_updates        | 76249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4444     |
|    fps              | 125      |
|    time_elapsed     | 2748     |
|    total_timesteps  | 345067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.46     |
|    n_updates        | 76266    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4448     |
|    fps              | 125      |
|    time_elapsed     | 2749     |
|    total_timesteps  | 345415   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.47     |
|    n_updates        | 76353    |
----------------------------------
Eval num_timesteps=345500, episode_reward=243.62 +/- 88.35
Episode length: 61.26 +/- 22.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.3     |
|    mean_reward      | 244      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.42     |
|    n_updates        | 76374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4452     |
|    fps              | 125      |
|    time_elapsed     | 2753     |
|    total_timesteps  | 345705   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.72     |
|    n_updates        | 76426    |
----------------------------------
Eval num_timesteps=346000, episode_reward=256.82 +/- 107.96
Episode length: 64.58 +/- 26.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.6     |
|    mean_reward      | 257      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 346000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.65     |
|    n_updates        | 76499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4456     |
|    fps              | 125      |
|    time_elapsed     | 2757     |
|    total_timesteps  | 346038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.287    |
|    n_updates        | 76509    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4460     |
|    fps              | 125      |
|    time_elapsed     | 2758     |
|    total_timesteps  | 346350   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.65     |
|    n_updates        | 76587    |
----------------------------------
Eval num_timesteps=346500, episode_reward=335.82 +/- 188.18
Episode length: 84.30 +/- 47.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.3     |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 346500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.873    |
|    n_updates        | 76624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4464     |
|    fps              | 125      |
|    time_elapsed     | 2763     |
|    total_timesteps  | 346706   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 76676    |
----------------------------------
Eval num_timesteps=347000, episode_reward=220.78 +/- 82.19
Episode length: 55.58 +/- 20.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.6     |
|    mean_reward      | 221      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 347000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.67     |
|    n_updates        | 76749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4468     |
|    fps              | 125      |
|    time_elapsed     | 2767     |
|    total_timesteps  | 347084   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.67     |
|    n_updates        | 76770    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4472     |
|    fps              | 125      |
|    time_elapsed     | 2768     |
|    total_timesteps  | 347406   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.72     |
|    n_updates        | 76851    |
----------------------------------
Eval num_timesteps=347500, episode_reward=352.92 +/- 178.83
Episode length: 88.52 +/- 44.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.5     |
|    mean_reward      | 353      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 347500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 76874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4476     |
|    fps              | 125      |
|    time_elapsed     | 2774     |
|    total_timesteps  | 347690   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.893    |
|    n_updates        | 76922    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4480     |
|    fps              | 125      |
|    time_elapsed     | 2774     |
|    total_timesteps  | 347967   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.911    |
|    n_updates        | 76991    |
----------------------------------
Eval num_timesteps=348000, episode_reward=294.02 +/- 159.93
Episode length: 73.90 +/- 40.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.9     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 348000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.28     |
|    n_updates        | 76999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4484     |
|    fps              | 125      |
|    time_elapsed     | 2779     |
|    total_timesteps  | 348180   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.35     |
|    n_updates        | 77044    |
----------------------------------
Eval num_timesteps=348500, episode_reward=292.14 +/- 115.59
Episode length: 73.44 +/- 28.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.4     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 348500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.83     |
|    n_updates        | 77124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4488     |
|    fps              | 125      |
|    time_elapsed     | 2784     |
|    total_timesteps  | 348524   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.74     |
|    n_updates        | 77130    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4492     |
|    fps              | 125      |
|    time_elapsed     | 2784     |
|    total_timesteps  | 348706   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.67     |
|    n_updates        | 77176    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4496     |
|    fps              | 125      |
|    time_elapsed     | 2785     |
|    total_timesteps  | 348941   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.21     |
|    n_updates        | 77235    |
----------------------------------
Eval num_timesteps=349000, episode_reward=297.80 +/- 110.50
Episode length: 74.78 +/- 27.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.8     |
|    mean_reward      | 298      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 349000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.72     |
|    n_updates        | 77249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4500     |
|    fps              | 125      |
|    time_elapsed     | 2790     |
|    total_timesteps  | 349178   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.81     |
|    n_updates        | 77294    |
----------------------------------
Eval num_timesteps=349500, episode_reward=264.12 +/- 88.93
Episode length: 66.36 +/- 22.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.4     |
|    mean_reward      | 264      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 349500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.48     |
|    n_updates        | 77374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4504     |
|    fps              | 125      |
|    time_elapsed     | 2795     |
|    total_timesteps  | 349622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.26     |
|    n_updates        | 77405    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4508     |
|    fps              | 125      |
|    time_elapsed     | 2795     |
|    total_timesteps  | 349894   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.12     |
|    n_updates        | 77473    |
----------------------------------
Eval num_timesteps=350000, episode_reward=329.50 +/- 149.21
Episode length: 82.68 +/- 37.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.7     |
|    mean_reward      | 330      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 350000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.615    |
|    n_updates        | 77499    |
----------------------------------
Eval num_timesteps=350500, episode_reward=264.98 +/- 81.35
Episode length: 66.66 +/- 20.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.7     |
|    mean_reward      | 265      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 350500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.846    |
|    n_updates        | 77624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4512     |
|    fps              | 124      |
|    time_elapsed     | 2805     |
|    total_timesteps  | 350563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.1      |
|    n_updates        | 77640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4516     |
|    fps              | 125      |
|    time_elapsed     | 2806     |
|    total_timesteps  | 350815   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 77703    |
----------------------------------
Eval num_timesteps=351000, episode_reward=192.00 +/- 73.05
Episode length: 48.28 +/- 18.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 77749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4520     |
|    fps              | 124      |
|    time_elapsed     | 2810     |
|    total_timesteps  | 351216   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.89     |
|    n_updates        | 77803    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4524     |
|    fps              | 125      |
|    time_elapsed     | 2810     |
|    total_timesteps  | 351484   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.829    |
|    n_updates        | 77870    |
----------------------------------
Eval num_timesteps=351500, episode_reward=307.44 +/- 151.35
Episode length: 77.30 +/- 37.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.3     |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 351500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.66     |
|    n_updates        | 77874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4528     |
|    fps              | 124      |
|    time_elapsed     | 2815     |
|    total_timesteps  | 351791   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.98     |
|    n_updates        | 77947    |
----------------------------------
Eval num_timesteps=352000, episode_reward=184.04 +/- 70.60
Episode length: 46.40 +/- 17.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 352000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.9      |
|    n_updates        | 77999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4532     |
|    fps              | 124      |
|    time_elapsed     | 2819     |
|    total_timesteps  | 352252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.6      |
|    n_updates        | 78062    |
----------------------------------
Eval num_timesteps=352500, episode_reward=334.02 +/- 144.21
Episode length: 83.82 +/- 36.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.8     |
|    mean_reward      | 334      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 352500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.344    |
|    n_updates        | 78124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4536     |
|    fps              | 124      |
|    time_elapsed     | 2825     |
|    total_timesteps  | 352526   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.611    |
|    n_updates        | 78131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4540     |
|    fps              | 124      |
|    time_elapsed     | 2825     |
|    total_timesteps  | 352838   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.27     |
|    n_updates        | 78209    |
----------------------------------
Eval num_timesteps=353000, episode_reward=260.66 +/- 153.46
Episode length: 65.56 +/- 38.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.6     |
|    mean_reward      | 261      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 353000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.29     |
|    n_updates        | 78249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4544     |
|    fps              | 124      |
|    time_elapsed     | 2830     |
|    total_timesteps  | 353362   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.24     |
|    n_updates        | 78340    |
----------------------------------
Eval num_timesteps=353500, episode_reward=225.52 +/- 77.62
Episode length: 56.76 +/- 19.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.8     |
|    mean_reward      | 226      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 353500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.93     |
|    n_updates        | 78374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4548     |
|    fps              | 124      |
|    time_elapsed     | 2834     |
|    total_timesteps  | 353613   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.94     |
|    n_updates        | 78403    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4552     |
|    fps              | 124      |
|    time_elapsed     | 2835     |
|    total_timesteps  | 353889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2        |
|    n_updates        | 78472    |
----------------------------------
Eval num_timesteps=354000, episode_reward=309.50 +/- 176.36
Episode length: 77.74 +/- 44.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 354000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.05     |
|    n_updates        | 78499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4556     |
|    fps              | 124      |
|    time_elapsed     | 2840     |
|    total_timesteps  | 354268   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.533    |
|    n_updates        | 78566    |
----------------------------------
Eval num_timesteps=354500, episode_reward=244.16 +/- 91.68
Episode length: 61.44 +/- 23.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.4     |
|    mean_reward      | 244      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 354500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.71     |
|    n_updates        | 78624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4560     |
|    fps              | 124      |
|    time_elapsed     | 2845     |
|    total_timesteps  | 354557   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.62     |
|    n_updates        | 78639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4564     |
|    fps              | 124      |
|    time_elapsed     | 2845     |
|    total_timesteps  | 354974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.11     |
|    n_updates        | 78743    |
----------------------------------
Eval num_timesteps=355000, episode_reward=322.58 +/- 172.09
Episode length: 81.00 +/- 42.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81       |
|    mean_reward      | 323      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 355000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.16     |
|    n_updates        | 78749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4568     |
|    fps              | 124      |
|    time_elapsed     | 2851     |
|    total_timesteps  | 355266   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.09     |
|    n_updates        | 78816    |
----------------------------------
Eval num_timesteps=355500, episode_reward=313.40 +/- 158.98
Episode length: 78.68 +/- 39.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.7     |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 355500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.724    |
|    n_updates        | 78874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4572     |
|    fps              | 124      |
|    time_elapsed     | 2856     |
|    total_timesteps  | 355586   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.664    |
|    n_updates        | 78896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4576     |
|    fps              | 124      |
|    time_elapsed     | 2856     |
|    total_timesteps  | 355876   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.95     |
|    n_updates        | 78968    |
----------------------------------
Eval num_timesteps=356000, episode_reward=206.50 +/- 79.56
Episode length: 51.96 +/- 19.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52       |
|    mean_reward      | 206      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 356000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.06     |
|    n_updates        | 78999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4580     |
|    fps              | 124      |
|    time_elapsed     | 2860     |
|    total_timesteps  | 356091   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.32     |
|    n_updates        | 79022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4584     |
|    fps              | 124      |
|    time_elapsed     | 2861     |
|    total_timesteps  | 356426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.66     |
|    n_updates        | 79106    |
----------------------------------
Eval num_timesteps=356500, episode_reward=195.68 +/- 65.04
Episode length: 49.28 +/- 16.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 356500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.65     |
|    n_updates        | 79124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4588     |
|    fps              | 124      |
|    time_elapsed     | 2864     |
|    total_timesteps  | 356688   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.36     |
|    n_updates        | 79171    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4592     |
|    fps              | 124      |
|    time_elapsed     | 2865     |
|    total_timesteps  | 356897   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.59     |
|    n_updates        | 79224    |
----------------------------------
Eval num_timesteps=357000, episode_reward=254.84 +/- 89.86
Episode length: 64.04 +/- 22.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64       |
|    mean_reward      | 255      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 357000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.71     |
|    n_updates        | 79249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4596     |
|    fps              | 124      |
|    time_elapsed     | 2869     |
|    total_timesteps  | 357194   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.7      |
|    n_updates        | 79298    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4600     |
|    fps              | 124      |
|    time_elapsed     | 2870     |
|    total_timesteps  | 357470   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.16     |
|    n_updates        | 79367    |
----------------------------------
Eval num_timesteps=357500, episode_reward=371.82 +/- 213.26
Episode length: 93.36 +/- 53.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.4     |
|    mean_reward      | 372      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 357500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 79374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4604     |
|    fps              | 124      |
|    time_elapsed     | 2876     |
|    total_timesteps  | 357765   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.84     |
|    n_updates        | 79441    |
----------------------------------
Eval num_timesteps=358000, episode_reward=316.80 +/- 149.88
Episode length: 79.54 +/- 37.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.5     |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 358000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.8      |
|    n_updates        | 79499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4608     |
|    fps              | 124      |
|    time_elapsed     | 2882     |
|    total_timesteps  | 358281   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5        |
|    n_updates        | 79570    |
----------------------------------
Eval num_timesteps=358500, episode_reward=339.38 +/- 151.01
Episode length: 85.22 +/- 37.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.2     |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.53     |
|    n_updates        | 79624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4612     |
|    fps              | 124      |
|    time_elapsed     | 2887     |
|    total_timesteps  | 358570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.37     |
|    n_updates        | 79642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4616     |
|    fps              | 124      |
|    time_elapsed     | 2887     |
|    total_timesteps  | 358817   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.665    |
|    n_updates        | 79704    |
----------------------------------
Eval num_timesteps=359000, episode_reward=269.58 +/- 89.77
Episode length: 67.72 +/- 22.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.7     |
|    mean_reward      | 270      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 359000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.419    |
|    n_updates        | 79749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4620     |
|    fps              | 124      |
|    time_elapsed     | 2892     |
|    total_timesteps  | 359072   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.79     |
|    n_updates        | 79767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4624     |
|    fps              | 124      |
|    time_elapsed     | 2893     |
|    total_timesteps  | 359383   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.12     |
|    n_updates        | 79845    |
----------------------------------
Eval num_timesteps=359500, episode_reward=244.30 +/- 117.59
Episode length: 61.48 +/- 29.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.5     |
|    mean_reward      | 244      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 359500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.54     |
|    n_updates        | 79874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4628     |
|    fps              | 124      |
|    time_elapsed     | 2897     |
|    total_timesteps  | 359552   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 79887    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4632     |
|    fps              | 124      |
|    time_elapsed     | 2897     |
|    total_timesteps  | 359824   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.69     |
|    n_updates        | 79955    |
----------------------------------
Eval num_timesteps=360000, episode_reward=247.82 +/- 93.21
Episode length: 62.32 +/- 23.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.3     |
|    mean_reward      | 248      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.25     |
|    n_updates        | 79999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4636     |
|    fps              | 124      |
|    time_elapsed     | 2901     |
|    total_timesteps  | 360037   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.12     |
|    n_updates        | 80009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4640     |
|    fps              | 124      |
|    time_elapsed     | 2902     |
|    total_timesteps  | 360269   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.7      |
|    n_updates        | 80067    |
----------------------------------
Eval num_timesteps=360500, episode_reward=241.22 +/- 104.83
Episode length: 60.76 +/- 26.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.8     |
|    mean_reward      | 241      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 360500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.63     |
|    n_updates        | 80124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4644     |
|    fps              | 124      |
|    time_elapsed     | 2906     |
|    total_timesteps  | 360595   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.63     |
|    n_updates        | 80148    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.6     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4648     |
|    fps              | 124      |
|    time_elapsed     | 2907     |
|    total_timesteps  | 360972   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.14     |
|    n_updates        | 80242    |
----------------------------------
Eval num_timesteps=361000, episode_reward=236.82 +/- 104.24
Episode length: 59.60 +/- 26.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.6     |
|    mean_reward      | 237      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 361000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.19     |
|    n_updates        | 80249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4652     |
|    fps              | 124      |
|    time_elapsed     | 2910     |
|    total_timesteps  | 361352   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.78     |
|    n_updates        | 80337    |
----------------------------------
Eval num_timesteps=361500, episode_reward=237.36 +/- 102.77
Episode length: 59.76 +/- 25.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.8     |
|    mean_reward      | 237      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 361500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.864    |
|    n_updates        | 80374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4656     |
|    fps              | 124      |
|    time_elapsed     | 2914     |
|    total_timesteps  | 361614   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.66     |
|    n_updates        | 80403    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4660     |
|    fps              | 124      |
|    time_elapsed     | 2915     |
|    total_timesteps  | 361799   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.67     |
|    n_updates        | 80449    |
----------------------------------
Eval num_timesteps=362000, episode_reward=253.70 +/- 83.20
Episode length: 63.78 +/- 20.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.8     |
|    mean_reward      | 254      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 362000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.81     |
|    n_updates        | 80499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.5     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4664     |
|    fps              | 124      |
|    time_elapsed     | 2919     |
|    total_timesteps  | 362125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.64     |
|    n_updates        | 80531    |
----------------------------------
Eval num_timesteps=362500, episode_reward=225.80 +/- 103.20
Episode length: 56.78 +/- 25.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.8     |
|    mean_reward      | 226      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 362500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.31     |
|    n_updates        | 80624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4668     |
|    fps              | 123      |
|    time_elapsed     | 2924     |
|    total_timesteps  | 362520   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.53     |
|    n_updates        | 80629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4672     |
|    fps              | 124      |
|    time_elapsed     | 2924     |
|    total_timesteps  | 362808   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.08     |
|    n_updates        | 80701    |
----------------------------------
Eval num_timesteps=363000, episode_reward=347.28 +/- 169.94
Episode length: 87.18 +/- 42.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.2     |
|    mean_reward      | 347      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 363000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 80749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4676     |
|    fps              | 123      |
|    time_elapsed     | 2930     |
|    total_timesteps  | 363111   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.57     |
|    n_updates        | 80777    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4680     |
|    fps              | 123      |
|    time_elapsed     | 2930     |
|    total_timesteps  | 363324   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.622    |
|    n_updates        | 80830    |
----------------------------------
Eval num_timesteps=363500, episode_reward=216.64 +/- 79.79
Episode length: 54.58 +/- 19.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.6     |
|    mean_reward      | 217      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 363500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.27     |
|    n_updates        | 80874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4684     |
|    fps              | 123      |
|    time_elapsed     | 2934     |
|    total_timesteps  | 363661   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.06     |
|    n_updates        | 80915    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4688     |
|    fps              | 123      |
|    time_elapsed     | 2935     |
|    total_timesteps  | 363970   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 80992    |
----------------------------------
Eval num_timesteps=364000, episode_reward=346.46 +/- 147.43
Episode length: 86.98 +/- 36.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 346      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 364000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.8      |
|    n_updates        | 80999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4692     |
|    fps              | 123      |
|    time_elapsed     | 2941     |
|    total_timesteps  | 364331   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.614    |
|    n_updates        | 81082    |
----------------------------------
Eval num_timesteps=364500, episode_reward=252.02 +/- 95.48
Episode length: 63.38 +/- 23.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.4     |
|    mean_reward      | 252      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 364500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.04     |
|    n_updates        | 81124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4696     |
|    fps              | 123      |
|    time_elapsed     | 2945     |
|    total_timesteps  | 364580   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.747    |
|    n_updates        | 81144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.8     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4700     |
|    fps              | 123      |
|    time_elapsed     | 2946     |
|    total_timesteps  | 364855   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.725    |
|    n_updates        | 81213    |
----------------------------------
Eval num_timesteps=365000, episode_reward=343.52 +/- 171.69
Episode length: 86.24 +/- 43.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.2     |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 365000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.43     |
|    n_updates        | 81249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4704     |
|    fps              | 123      |
|    time_elapsed     | 2952     |
|    total_timesteps  | 365256   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.89     |
|    n_updates        | 81313    |
----------------------------------
Eval num_timesteps=365500, episode_reward=277.08 +/- 151.55
Episode length: 69.62 +/- 37.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.6     |
|    mean_reward      | 277      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 365500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.72     |
|    n_updates        | 81374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.2     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4708     |
|    fps              | 123      |
|    time_elapsed     | 2957     |
|    total_timesteps  | 365601   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 81400    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4712     |
|    fps              | 123      |
|    time_elapsed     | 2958     |
|    total_timesteps  | 365876   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.12     |
|    n_updates        | 81468    |
----------------------------------
Eval num_timesteps=366000, episode_reward=273.64 +/- 140.40
Episode length: 68.78 +/- 35.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.8     |
|    mean_reward      | 274      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 366000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.69     |
|    n_updates        | 81499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4716     |
|    fps              | 123      |
|    time_elapsed     | 2962     |
|    total_timesteps  | 366218   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.251    |
|    n_updates        | 81554    |
----------------------------------
Eval num_timesteps=366500, episode_reward=204.92 +/- 66.38
Episode length: 51.48 +/- 16.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.5     |
|    mean_reward      | 205      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 366500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.69     |
|    n_updates        | 81624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4720     |
|    fps              | 123      |
|    time_elapsed     | 2966     |
|    total_timesteps  | 366552   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.89     |
|    n_updates        | 81637    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4724     |
|    fps              | 123      |
|    time_elapsed     | 2967     |
|    total_timesteps  | 366894   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.03     |
|    n_updates        | 81723    |
----------------------------------
Eval num_timesteps=367000, episode_reward=284.90 +/- 123.80
Episode length: 71.60 +/- 30.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 367000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.05     |
|    n_updates        | 81749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4728     |
|    fps              | 123      |
|    time_elapsed     | 2972     |
|    total_timesteps  | 367334   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.537    |
|    n_updates        | 81833    |
----------------------------------
Eval num_timesteps=367500, episode_reward=291.06 +/- 106.45
Episode length: 73.18 +/- 26.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.2     |
|    mean_reward      | 291      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 367500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.86     |
|    n_updates        | 81874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4732     |
|    fps              | 123      |
|    time_elapsed     | 2977     |
|    total_timesteps  | 367563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 81890    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4736     |
|    fps              | 123      |
|    time_elapsed     | 2977     |
|    total_timesteps  | 367916   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.737    |
|    n_updates        | 81978    |
----------------------------------
Eval num_timesteps=368000, episode_reward=302.00 +/- 142.06
Episode length: 75.86 +/- 35.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.9     |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 368000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.67     |
|    n_updates        | 81999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4740     |
|    fps              | 123      |
|    time_elapsed     | 2983     |
|    total_timesteps  | 368178   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.13     |
|    n_updates        | 82044    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4744     |
|    fps              | 123      |
|    time_elapsed     | 2983     |
|    total_timesteps  | 368385   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 82096    |
----------------------------------
Eval num_timesteps=368500, episode_reward=220.76 +/- 63.64
Episode length: 55.60 +/- 15.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.6     |
|    mean_reward      | 221      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 368500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.07     |
|    n_updates        | 82124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4748     |
|    fps              | 123      |
|    time_elapsed     | 2987     |
|    total_timesteps  | 368860   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.45     |
|    n_updates        | 82214    |
----------------------------------
Eval num_timesteps=369000, episode_reward=341.38 +/- 147.90
Episode length: 85.80 +/- 37.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.8     |
|    mean_reward      | 341      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.24     |
|    n_updates        | 82249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4752     |
|    fps              | 123      |
|    time_elapsed     | 2993     |
|    total_timesteps  | 369296   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.704    |
|    n_updates        | 82323    |
----------------------------------
Eval num_timesteps=369500, episode_reward=309.68 +/- 111.62
Episode length: 77.76 +/- 27.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.8     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 369500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.54     |
|    n_updates        | 82374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4756     |
|    fps              | 123      |
|    time_elapsed     | 2998     |
|    total_timesteps  | 369503   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.959    |
|    n_updates        | 82375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4760     |
|    fps              | 123      |
|    time_elapsed     | 2999     |
|    total_timesteps  | 369801   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.73     |
|    n_updates        | 82450    |
----------------------------------
Eval num_timesteps=370000, episode_reward=230.82 +/- 74.56
Episode length: 58.10 +/- 18.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | 231      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 370000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.88     |
|    n_updates        | 82499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4764     |
|    fps              | 123      |
|    time_elapsed     | 3003     |
|    total_timesteps  | 370102   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.59     |
|    n_updates        | 82525    |
----------------------------------
Eval num_timesteps=370500, episode_reward=284.24 +/- 106.56
Episode length: 71.42 +/- 26.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.4     |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 370500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.83     |
|    n_updates        | 82624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4768     |
|    fps              | 123      |
|    time_elapsed     | 3009     |
|    total_timesteps  | 370552   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.51     |
|    n_updates        | 82637    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4772     |
|    fps              | 123      |
|    time_elapsed     | 3009     |
|    total_timesteps  | 370786   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.17     |
|    n_updates        | 82696    |
----------------------------------
Eval num_timesteps=371000, episode_reward=194.44 +/- 73.64
Episode length: 48.98 +/- 18.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49       |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 371000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.74     |
|    n_updates        | 82749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4776     |
|    fps              | 123      |
|    time_elapsed     | 3013     |
|    total_timesteps  | 371098   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.89     |
|    n_updates        | 82774    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4780     |
|    fps              | 123      |
|    time_elapsed     | 3013     |
|    total_timesteps  | 371338   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.04     |
|    n_updates        | 82834    |
----------------------------------
Eval num_timesteps=371500, episode_reward=418.42 +/- 187.20
Episode length: 104.96 +/- 46.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 418      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 371500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.98     |
|    n_updates        | 82874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4784     |
|    fps              | 123      |
|    time_elapsed     | 3020     |
|    total_timesteps  | 371689   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.27     |
|    n_updates        | 82922    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4788     |
|    fps              | 123      |
|    time_elapsed     | 3021     |
|    total_timesteps  | 371935   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.41     |
|    n_updates        | 82983    |
----------------------------------
Eval num_timesteps=372000, episode_reward=209.88 +/- 85.23
Episode length: 52.86 +/- 21.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 372000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.2      |
|    n_updates        | 82999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4792     |
|    fps              | 123      |
|    time_elapsed     | 3025     |
|    total_timesteps  | 372216   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.18     |
|    n_updates        | 83053    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4796     |
|    fps              | 123      |
|    time_elapsed     | 3025     |
|    total_timesteps  | 372469   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.949    |
|    n_updates        | 83117    |
----------------------------------
Eval num_timesteps=372500, episode_reward=240.36 +/- 108.83
Episode length: 60.34 +/- 27.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.3     |
|    mean_reward      | 240      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 372500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 83124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4800     |
|    fps              | 123      |
|    time_elapsed     | 3029     |
|    total_timesteps  | 372753   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.06     |
|    n_updates        | 83188    |
----------------------------------
Eval num_timesteps=373000, episode_reward=353.52 +/- 189.92
Episode length: 88.76 +/- 47.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.8     |
|    mean_reward      | 354      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 373000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.415    |
|    n_updates        | 83249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4804     |
|    fps              | 122      |
|    time_elapsed     | 3035     |
|    total_timesteps  | 373147   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.29     |
|    n_updates        | 83286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4808     |
|    fps              | 122      |
|    time_elapsed     | 3036     |
|    total_timesteps  | 373445   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.593    |
|    n_updates        | 83361    |
----------------------------------
Eval num_timesteps=373500, episode_reward=267.70 +/- 103.95
Episode length: 67.30 +/- 26.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.3     |
|    mean_reward      | 268      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 373500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 83374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4812     |
|    fps              | 122      |
|    time_elapsed     | 3041     |
|    total_timesteps  | 373726   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 83431    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4816     |
|    fps              | 122      |
|    time_elapsed     | 3041     |
|    total_timesteps  | 373989   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 83497    |
----------------------------------
Eval num_timesteps=374000, episode_reward=307.14 +/- 123.64
Episode length: 77.16 +/- 30.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.2     |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 374000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.88     |
|    n_updates        | 83499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4820     |
|    fps              | 122      |
|    time_elapsed     | 3046     |
|    total_timesteps  | 374182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.75     |
|    n_updates        | 83545    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4824     |
|    fps              | 122      |
|    time_elapsed     | 3046     |
|    total_timesteps  | 374390   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.568    |
|    n_updates        | 83597    |
----------------------------------
Eval num_timesteps=374500, episode_reward=244.38 +/- 107.95
Episode length: 61.50 +/- 26.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.5     |
|    mean_reward      | 244      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 374500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.69     |
|    n_updates        | 83624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4828     |
|    fps              | 122      |
|    time_elapsed     | 3050     |
|    total_timesteps  | 374619   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.25     |
|    n_updates        | 83654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4832     |
|    fps              | 122      |
|    time_elapsed     | 3051     |
|    total_timesteps  | 374803   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.528    |
|    n_updates        | 83700    |
----------------------------------
Eval num_timesteps=375000, episode_reward=365.28 +/- 189.99
Episode length: 91.76 +/- 47.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.8     |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 375000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.44     |
|    n_updates        | 83749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4836     |
|    fps              | 122      |
|    time_elapsed     | 3057     |
|    total_timesteps  | 375222   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.15     |
|    n_updates        | 83805    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4840     |
|    fps              | 122      |
|    time_elapsed     | 3058     |
|    total_timesteps  | 375458   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.977    |
|    n_updates        | 83864    |
----------------------------------
Eval num_timesteps=375500, episode_reward=347.10 +/- 166.54
Episode length: 87.08 +/- 41.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.1     |
|    mean_reward      | 347      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 375500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.974    |
|    n_updates        | 83874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4844     |
|    fps              | 122      |
|    time_elapsed     | 3064     |
|    total_timesteps  | 375820   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.37     |
|    n_updates        | 83954    |
----------------------------------
Eval num_timesteps=376000, episode_reward=227.58 +/- 73.65
Episode length: 57.32 +/- 18.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.3     |
|    mean_reward      | 228      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 376000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.918    |
|    n_updates        | 83999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4848     |
|    fps              | 122      |
|    time_elapsed     | 3068     |
|    total_timesteps  | 376100   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.16     |
|    n_updates        | 84024    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.3     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4852     |
|    fps              | 122      |
|    time_elapsed     | 3068     |
|    total_timesteps  | 376329   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.87     |
|    n_updates        | 84082    |
----------------------------------
Eval num_timesteps=376500, episode_reward=252.42 +/- 91.08
Episode length: 63.50 +/- 22.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.5     |
|    mean_reward      | 252      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 376500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.814    |
|    n_updates        | 84124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4856     |
|    fps              | 122      |
|    time_elapsed     | 3073     |
|    total_timesteps  | 376724   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.823    |
|    n_updates        | 84180    |
----------------------------------
Eval num_timesteps=377000, episode_reward=379.80 +/- 180.67
Episode length: 95.34 +/- 45.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 380      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.14     |
|    n_updates        | 84249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4860     |
|    fps              | 122      |
|    time_elapsed     | 3079     |
|    total_timesteps  | 377042   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.99     |
|    n_updates        | 84260    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4864     |
|    fps              | 122      |
|    time_elapsed     | 3080     |
|    total_timesteps  | 377318   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.17     |
|    n_updates        | 84329    |
----------------------------------
Eval num_timesteps=377500, episode_reward=236.88 +/- 90.66
Episode length: 59.58 +/- 22.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.6     |
|    mean_reward      | 237      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 377500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.72     |
|    n_updates        | 84374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4868     |
|    fps              | 122      |
|    time_elapsed     | 3084     |
|    total_timesteps  | 377782   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.562    |
|    n_updates        | 84445    |
----------------------------------
Eval num_timesteps=378000, episode_reward=218.74 +/- 70.94
Episode length: 55.10 +/- 17.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.1     |
|    mean_reward      | 219      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.85     |
|    n_updates        | 84499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4872     |
|    fps              | 122      |
|    time_elapsed     | 3088     |
|    total_timesteps  | 378180   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.11     |
|    n_updates        | 84544    |
----------------------------------
Eval num_timesteps=378500, episode_reward=221.84 +/- 98.31
Episode length: 55.76 +/- 24.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.8     |
|    mean_reward      | 222      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 378500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 84624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4876     |
|    fps              | 122      |
|    time_elapsed     | 3092     |
|    total_timesteps  | 378591   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.25     |
|    n_updates        | 84647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4880     |
|    fps              | 122      |
|    time_elapsed     | 3093     |
|    total_timesteps  | 378849   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.747    |
|    n_updates        | 84712    |
----------------------------------
Eval num_timesteps=379000, episode_reward=233.54 +/- 114.48
Episode length: 58.80 +/- 28.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.8     |
|    mean_reward      | 234      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 379000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.692    |
|    n_updates        | 84749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4884     |
|    fps              | 122      |
|    time_elapsed     | 3097     |
|    total_timesteps  | 379096   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.31     |
|    n_updates        | 84773    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4888     |
|    fps              | 122      |
|    time_elapsed     | 3098     |
|    total_timesteps  | 379371   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.96     |
|    n_updates        | 84842    |
----------------------------------
Eval num_timesteps=379500, episode_reward=218.78 +/- 63.61
Episode length: 55.08 +/- 15.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.1     |
|    mean_reward      | 219      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 379500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.6      |
|    n_updates        | 84874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4892     |
|    fps              | 122      |
|    time_elapsed     | 3101     |
|    total_timesteps  | 379735   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.92     |
|    n_updates        | 84933    |
----------------------------------
Eval num_timesteps=380000, episode_reward=191.04 +/- 56.71
Episode length: 48.12 +/- 14.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 380000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.85     |
|    n_updates        | 84999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4896     |
|    fps              | 122      |
|    time_elapsed     | 3105     |
|    total_timesteps  | 380035   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.68     |
|    n_updates        | 85008    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4900     |
|    fps              | 122      |
|    time_elapsed     | 3106     |
|    total_timesteps  | 380459   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 85114    |
----------------------------------
Eval num_timesteps=380500, episode_reward=233.36 +/- 101.45
Episode length: 58.74 +/- 25.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.7     |
|    mean_reward      | 233      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 380500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.52     |
|    n_updates        | 85124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4904     |
|    fps              | 122      |
|    time_elapsed     | 3110     |
|    total_timesteps  | 380659   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.9      |
|    n_updates        | 85164    |
----------------------------------
Eval num_timesteps=381000, episode_reward=210.36 +/- 63.02
Episode length: 52.92 +/- 15.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 381000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.46     |
|    n_updates        | 85249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4908     |
|    fps              | 122      |
|    time_elapsed     | 3114     |
|    total_timesteps  | 381125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.62     |
|    n_updates        | 85281    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4912     |
|    fps              | 122      |
|    time_elapsed     | 3115     |
|    total_timesteps  | 381323   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.6      |
|    n_updates        | 85330    |
----------------------------------
Eval num_timesteps=381500, episode_reward=223.44 +/- 78.59
Episode length: 56.22 +/- 19.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.2     |
|    mean_reward      | 223      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 381500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.54     |
|    n_updates        | 85374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4916     |
|    fps              | 122      |
|    time_elapsed     | 3119     |
|    total_timesteps  | 381590   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.654    |
|    n_updates        | 85397    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4920     |
|    fps              | 122      |
|    time_elapsed     | 3119     |
|    total_timesteps  | 381862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.598    |
|    n_updates        | 85465    |
----------------------------------
Eval num_timesteps=382000, episode_reward=331.00 +/- 167.86
Episode length: 83.04 +/- 41.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83       |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 382000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.999    |
|    n_updates        | 85499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4924     |
|    fps              | 122      |
|    time_elapsed     | 3125     |
|    total_timesteps  | 382180   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.99     |
|    n_updates        | 85544    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4928     |
|    fps              | 122      |
|    time_elapsed     | 3126     |
|    total_timesteps  | 382467   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.4      |
|    n_updates        | 85616    |
----------------------------------
Eval num_timesteps=382500, episode_reward=255.74 +/- 124.43
Episode length: 64.30 +/- 31.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.3     |
|    mean_reward      | 256      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 382500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.13     |
|    n_updates        | 85624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4932     |
|    fps              | 122      |
|    time_elapsed     | 3130     |
|    total_timesteps  | 382862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.23     |
|    n_updates        | 85715    |
----------------------------------
Eval num_timesteps=383000, episode_reward=382.90 +/- 194.85
Episode length: 96.12 +/- 48.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.1     |
|    mean_reward      | 383      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 383000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.73     |
|    n_updates        | 85749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4936     |
|    fps              | 122      |
|    time_elapsed     | 3137     |
|    total_timesteps  | 383218   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.445    |
|    n_updates        | 85804    |
----------------------------------
Eval num_timesteps=383500, episode_reward=253.32 +/- 88.89
Episode length: 63.66 +/- 22.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.7     |
|    mean_reward      | 253      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 383500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.33     |
|    n_updates        | 85874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4940     |
|    fps              | 122      |
|    time_elapsed     | 3142     |
|    total_timesteps  | 383646   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 85911    |
----------------------------------
Eval num_timesteps=384000, episode_reward=239.40 +/- 87.04
Episode length: 60.18 +/- 21.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.2     |
|    mean_reward      | 239      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 384000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.33     |
|    n_updates        | 85999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4944     |
|    fps              | 122      |
|    time_elapsed     | 3146     |
|    total_timesteps  | 384021   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.48     |
|    n_updates        | 86005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4948     |
|    fps              | 122      |
|    time_elapsed     | 3147     |
|    total_timesteps  | 384201   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.97     |
|    n_updates        | 86050    |
----------------------------------
Eval num_timesteps=384500, episode_reward=274.38 +/- 133.96
Episode length: 68.96 +/- 33.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69       |
|    mean_reward      | 274      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 384500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.722    |
|    n_updates        | 86124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4952     |
|    fps              | 122      |
|    time_elapsed     | 3151     |
|    total_timesteps  | 384657   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.64     |
|    n_updates        | 86164    |
----------------------------------
Eval num_timesteps=385000, episode_reward=292.46 +/- 123.39
Episode length: 73.56 +/- 30.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 385000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.46     |
|    n_updates        | 86249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4956     |
|    fps              | 121      |
|    time_elapsed     | 3156     |
|    total_timesteps  | 385028   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.68     |
|    n_updates        | 86256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4960     |
|    fps              | 122      |
|    time_elapsed     | 3156     |
|    total_timesteps  | 385298   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.58     |
|    n_updates        | 86324    |
----------------------------------
Eval num_timesteps=385500, episode_reward=324.48 +/- 141.74
Episode length: 81.52 +/- 35.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.5     |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 385500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.06     |
|    n_updates        | 86374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4964     |
|    fps              | 121      |
|    time_elapsed     | 3161     |
|    total_timesteps  | 385525   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.06     |
|    n_updates        | 86381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4968     |
|    fps              | 122      |
|    time_elapsed     | 3162     |
|    total_timesteps  | 385811   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.56     |
|    n_updates        | 86452    |
----------------------------------
Eval num_timesteps=386000, episode_reward=295.82 +/- 146.22
Episode length: 74.38 +/- 36.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.4     |
|    mean_reward      | 296      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.86     |
|    n_updates        | 86499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4972     |
|    fps              | 121      |
|    time_elapsed     | 3166     |
|    total_timesteps  | 386162   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.63     |
|    n_updates        | 86540    |
----------------------------------
Eval num_timesteps=386500, episode_reward=269.56 +/- 148.84
Episode length: 67.70 +/- 37.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.7     |
|    mean_reward      | 270      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 386500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 86624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4976     |
|    fps              | 121      |
|    time_elapsed     | 3171     |
|    total_timesteps  | 386519   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.46     |
|    n_updates        | 86629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4980     |
|    fps              | 121      |
|    time_elapsed     | 3172     |
|    total_timesteps  | 386845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.84     |
|    n_updates        | 86711    |
----------------------------------
Eval num_timesteps=387000, episode_reward=314.06 +/- 169.46
Episode length: 78.82 +/- 42.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.8     |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.53     |
|    n_updates        | 86749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4984     |
|    fps              | 121      |
|    time_elapsed     | 3177     |
|    total_timesteps  | 387052   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.62     |
|    n_updates        | 86762    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4988     |
|    fps              | 121      |
|    time_elapsed     | 3177     |
|    total_timesteps  | 387341   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.99     |
|    n_updates        | 86835    |
----------------------------------
Eval num_timesteps=387500, episode_reward=193.48 +/- 63.06
Episode length: 48.74 +/- 15.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 387500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.82     |
|    n_updates        | 86874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4992     |
|    fps              | 121      |
|    time_elapsed     | 3181     |
|    total_timesteps  | 387630   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 86907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4996     |
|    fps              | 121      |
|    time_elapsed     | 3181     |
|    total_timesteps  | 387911   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 86977    |
----------------------------------
Eval num_timesteps=388000, episode_reward=248.08 +/- 93.36
Episode length: 62.42 +/- 23.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.4     |
|    mean_reward      | 248      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.818    |
|    n_updates        | 86999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5000     |
|    fps              | 121      |
|    time_elapsed     | 3185     |
|    total_timesteps  | 388128   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.502    |
|    n_updates        | 87031    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5004     |
|    fps              | 121      |
|    time_elapsed     | 3186     |
|    total_timesteps  | 388456   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.08     |
|    n_updates        | 87113    |
----------------------------------
Eval num_timesteps=388500, episode_reward=215.98 +/- 86.67
Episode length: 54.38 +/- 21.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.4     |
|    mean_reward      | 216      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 388500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.26     |
|    n_updates        | 87124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5008     |
|    fps              | 121      |
|    time_elapsed     | 3190     |
|    total_timesteps  | 388819   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.93     |
|    n_updates        | 87204    |
----------------------------------
Eval num_timesteps=389000, episode_reward=242.50 +/- 91.73
Episode length: 61.00 +/- 22.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61       |
|    mean_reward      | 242      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 389000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.4      |
|    n_updates        | 87249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5012     |
|    fps              | 121      |
|    time_elapsed     | 3194     |
|    total_timesteps  | 389171   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.87     |
|    n_updates        | 87292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5016     |
|    fps              | 121      |
|    time_elapsed     | 3194     |
|    total_timesteps  | 389440   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.04     |
|    n_updates        | 87359    |
----------------------------------
Eval num_timesteps=389500, episode_reward=371.90 +/- 182.30
Episode length: 93.36 +/- 45.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.4     |
|    mean_reward      | 372      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 389500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.19     |
|    n_updates        | 87374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5020     |
|    fps              | 121      |
|    time_elapsed     | 3200     |
|    total_timesteps  | 389767   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.591    |
|    n_updates        | 87441    |
----------------------------------
Eval num_timesteps=390000, episode_reward=335.96 +/- 132.31
Episode length: 84.38 +/- 33.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.4     |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 390000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.496    |
|    n_updates        | 87499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5024     |
|    fps              | 121      |
|    time_elapsed     | 3206     |
|    total_timesteps  | 390205   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.2      |
|    n_updates        | 87551    |
----------------------------------
Eval num_timesteps=390500, episode_reward=331.14 +/- 126.44
Episode length: 83.18 +/- 31.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.2     |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 390500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.28     |
|    n_updates        | 87624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5028     |
|    fps              | 121      |
|    time_elapsed     | 3212     |
|    total_timesteps  | 390649   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.31     |
|    n_updates        | 87662    |
----------------------------------
Eval num_timesteps=391000, episode_reward=228.92 +/- 95.77
Episode length: 57.68 +/- 23.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.7     |
|    mean_reward      | 229      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.924    |
|    n_updates        | 87749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5032     |
|    fps              | 121      |
|    time_elapsed     | 3216     |
|    total_timesteps  | 391079   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.81     |
|    n_updates        | 87769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5036     |
|    fps              | 121      |
|    time_elapsed     | 3216     |
|    total_timesteps  | 391298   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.86     |
|    n_updates        | 87824    |
----------------------------------
Eval num_timesteps=391500, episode_reward=372.14 +/- 160.16
Episode length: 93.46 +/- 40.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.5     |
|    mean_reward      | 372      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 391500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.25     |
|    n_updates        | 87874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5040     |
|    fps              | 121      |
|    time_elapsed     | 3222     |
|    total_timesteps  | 391652   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.72     |
|    n_updates        | 87912    |
----------------------------------
Eval num_timesteps=392000, episode_reward=230.42 +/- 90.33
Episode length: 57.98 +/- 22.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58       |
|    mean_reward      | 230      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 392000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.59     |
|    n_updates        | 87999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5044     |
|    fps              | 121      |
|    time_elapsed     | 3226     |
|    total_timesteps  | 392049   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.653    |
|    n_updates        | 88012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5048     |
|    fps              | 121      |
|    time_elapsed     | 3227     |
|    total_timesteps  | 392433   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.25     |
|    n_updates        | 88108    |
----------------------------------
Eval num_timesteps=392500, episode_reward=260.82 +/- 108.13
Episode length: 65.62 +/- 27.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.6     |
|    mean_reward      | 261      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 392500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.963    |
|    n_updates        | 88124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5052     |
|    fps              | 121      |
|    time_elapsed     | 3232     |
|    total_timesteps  | 392798   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.7      |
|    n_updates        | 88199    |
----------------------------------
Eval num_timesteps=393000, episode_reward=231.64 +/- 117.59
Episode length: 58.28 +/- 29.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.3     |
|    mean_reward      | 232      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 393000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.988    |
|    n_updates        | 88249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5056     |
|    fps              | 121      |
|    time_elapsed     | 3235     |
|    total_timesteps  | 393017   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.85     |
|    n_updates        | 88254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5060     |
|    fps              | 121      |
|    time_elapsed     | 3236     |
|    total_timesteps  | 393418   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.439    |
|    n_updates        | 88354    |
----------------------------------
Eval num_timesteps=393500, episode_reward=294.50 +/- 127.58
Episode length: 74.06 +/- 31.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.1     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 393500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.88     |
|    n_updates        | 88374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5064     |
|    fps              | 121      |
|    time_elapsed     | 3241     |
|    total_timesteps  | 393703   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.28     |
|    n_updates        | 88425    |
----------------------------------
Eval num_timesteps=394000, episode_reward=304.52 +/- 132.04
Episode length: 76.48 +/- 32.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.5     |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 394000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.747    |
|    n_updates        | 88499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5068     |
|    fps              | 121      |
|    time_elapsed     | 3246     |
|    total_timesteps  | 394139   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.64     |
|    n_updates        | 88534    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5072     |
|    fps              | 121      |
|    time_elapsed     | 3246     |
|    total_timesteps  | 394373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.67     |
|    n_updates        | 88593    |
----------------------------------
Eval num_timesteps=394500, episode_reward=259.60 +/- 126.05
Episode length: 65.20 +/- 31.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.2     |
|    mean_reward      | 260      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 394500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.18     |
|    n_updates        | 88624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5076     |
|    fps              | 121      |
|    time_elapsed     | 3251     |
|    total_timesteps  | 394575   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.999    |
|    n_updates        | 88643    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5080     |
|    fps              | 121      |
|    time_elapsed     | 3251     |
|    total_timesteps  | 394901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.76     |
|    n_updates        | 88725    |
----------------------------------
Eval num_timesteps=395000, episode_reward=330.56 +/- 125.89
Episode length: 82.98 +/- 31.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83       |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 395000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.742    |
|    n_updates        | 88749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5084     |
|    fps              | 121      |
|    time_elapsed     | 3257     |
|    total_timesteps  | 395352   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 88837    |
----------------------------------
Eval num_timesteps=395500, episode_reward=241.34 +/- 88.50
Episode length: 60.74 +/- 22.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.7     |
|    mean_reward      | 241      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 395500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.4      |
|    n_updates        | 88874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5088     |
|    fps              | 121      |
|    time_elapsed     | 3261     |
|    total_timesteps  | 395717   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.07     |
|    n_updates        | 88929    |
----------------------------------
Eval num_timesteps=396000, episode_reward=221.38 +/- 91.38
Episode length: 55.70 +/- 22.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.7     |
|    mean_reward      | 221      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.59     |
|    n_updates        | 88999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5092     |
|    fps              | 121      |
|    time_elapsed     | 3265     |
|    total_timesteps  | 396063   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.51     |
|    n_updates        | 89015    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5096     |
|    fps              | 121      |
|    time_elapsed     | 3266     |
|    total_timesteps  | 396428   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.12     |
|    n_updates        | 89106    |
----------------------------------
Eval num_timesteps=396500, episode_reward=353.32 +/- 185.34
Episode length: 88.70 +/- 46.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.7     |
|    mean_reward      | 353      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 396500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.03     |
|    n_updates        | 89124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5100     |
|    fps              | 121      |
|    time_elapsed     | 3272     |
|    total_timesteps  | 396795   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.522    |
|    n_updates        | 89198    |
----------------------------------
Eval num_timesteps=397000, episode_reward=334.38 +/- 139.58
Episode length: 84.02 +/- 34.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84       |
|    mean_reward      | 334      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 397000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.796    |
|    n_updates        | 89249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.9     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5104     |
|    fps              | 121      |
|    time_elapsed     | 3277     |
|    total_timesteps  | 397046   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.22     |
|    n_updates        | 89261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5108     |
|    fps              | 121      |
|    time_elapsed     | 3277     |
|    total_timesteps  | 397363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.24     |
|    n_updates        | 89340    |
----------------------------------
Eval num_timesteps=397500, episode_reward=268.36 +/- 147.45
Episode length: 67.50 +/- 36.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.5     |
|    mean_reward      | 268      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 397500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.446    |
|    n_updates        | 89374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5112     |
|    fps              | 121      |
|    time_elapsed     | 3282     |
|    total_timesteps  | 397684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.22     |
|    n_updates        | 89420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5116     |
|    fps              | 121      |
|    time_elapsed     | 3282     |
|    total_timesteps  | 397871   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.427    |
|    n_updates        | 89467    |
----------------------------------
Eval num_timesteps=398000, episode_reward=321.20 +/- 153.95
Episode length: 80.66 +/- 38.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.7     |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 398000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.18     |
|    n_updates        | 89499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5120     |
|    fps              | 121      |
|    time_elapsed     | 3287     |
|    total_timesteps  | 398313   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.51     |
|    n_updates        | 89578    |
----------------------------------
Eval num_timesteps=398500, episode_reward=233.62 +/- 86.50
Episode length: 58.76 +/- 21.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.8     |
|    mean_reward      | 234      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 398500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.09     |
|    n_updates        | 89624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5124     |
|    fps              | 121      |
|    time_elapsed     | 3292     |
|    total_timesteps  | 398823   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.73     |
|    n_updates        | 89705    |
----------------------------------
Eval num_timesteps=399000, episode_reward=262.02 +/- 107.12
Episode length: 65.88 +/- 26.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.9     |
|    mean_reward      | 262      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 399000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.09     |
|    n_updates        | 89749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5128     |
|    fps              | 121      |
|    time_elapsed     | 3296     |
|    total_timesteps  | 399199   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.39     |
|    n_updates        | 89799    |
----------------------------------
Eval num_timesteps=399500, episode_reward=316.66 +/- 125.55
Episode length: 79.58 +/- 31.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.6     |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 399500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.56     |
|    n_updates        | 89874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5132     |
|    fps              | 121      |
|    time_elapsed     | 3302     |
|    total_timesteps  | 399617   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.46     |
|    n_updates        | 89904    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5136     |
|    fps              | 121      |
|    time_elapsed     | 3303     |
|    total_timesteps  | 399976   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.625    |
|    n_updates        | 89993    |
----------------------------------
Eval num_timesteps=400000, episode_reward=344.16 +/- 157.44
Episode length: 86.46 +/- 39.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.5     |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 400000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.4      |
|    n_updates        | 89999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5140     |
|    fps              | 120      |
|    time_elapsed     | 3308     |
|    total_timesteps  | 400268   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 90066    |
----------------------------------
Eval num_timesteps=400500, episode_reward=296.04 +/- 101.12
Episode length: 74.38 +/- 25.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.4     |
|    mean_reward      | 296      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 400500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.98     |
|    n_updates        | 90124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5144     |
|    fps              | 120      |
|    time_elapsed     | 3313     |
|    total_timesteps  | 400559   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.58     |
|    n_updates        | 90139    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5148     |
|    fps              | 120      |
|    time_elapsed     | 3313     |
|    total_timesteps  | 400905   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.08     |
|    n_updates        | 90226    |
----------------------------------
Eval num_timesteps=401000, episode_reward=250.86 +/- 102.91
Episode length: 63.06 +/- 25.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.1     |
|    mean_reward      | 251      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 401000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.59     |
|    n_updates        | 90249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5152     |
|    fps              | 120      |
|    time_elapsed     | 3318     |
|    total_timesteps  | 401267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.08     |
|    n_updates        | 90316    |
----------------------------------
Eval num_timesteps=401500, episode_reward=245.28 +/- 94.45
Episode length: 61.70 +/- 23.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.7     |
|    mean_reward      | 245      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 401500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 90374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.3     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5156     |
|    fps              | 120      |
|    time_elapsed     | 3322     |
|    total_timesteps  | 401549   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.02     |
|    n_updates        | 90387    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5160     |
|    fps              | 120      |
|    time_elapsed     | 3322     |
|    total_timesteps  | 401929   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 90482    |
----------------------------------
Eval num_timesteps=402000, episode_reward=258.76 +/- 123.64
Episode length: 65.08 +/- 30.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.1     |
|    mean_reward      | 259      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 402000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.95     |
|    n_updates        | 90499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5164     |
|    fps              | 120      |
|    time_elapsed     | 3327     |
|    total_timesteps  | 402204   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.68     |
|    n_updates        | 90550    |
----------------------------------
Eval num_timesteps=402500, episode_reward=309.26 +/- 145.91
Episode length: 77.70 +/- 36.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 402500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.878    |
|    n_updates        | 90624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5168     |
|    fps              | 120      |
|    time_elapsed     | 3332     |
|    total_timesteps  | 402607   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1        |
|    n_updates        | 90651    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5172     |
|    fps              | 120      |
|    time_elapsed     | 3333     |
|    total_timesteps  | 402926   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.9      |
|    n_updates        | 90731    |
----------------------------------
Eval num_timesteps=403000, episode_reward=381.38 +/- 144.74
Episode length: 95.64 +/- 36.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.6     |
|    mean_reward      | 381      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 403000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.873    |
|    n_updates        | 90749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5176     |
|    fps              | 120      |
|    time_elapsed     | 3339     |
|    total_timesteps  | 403300   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.561    |
|    n_updates        | 90824    |
----------------------------------
Eval num_timesteps=403500, episode_reward=409.84 +/- 237.10
Episode length: 102.86 +/- 59.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 410      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 403500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.86     |
|    n_updates        | 90874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.3     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5180     |
|    fps              | 120      |
|    time_elapsed     | 3345     |
|    total_timesteps  | 403530   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.99     |
|    n_updates        | 90882    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.6     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5184     |
|    fps              | 120      |
|    time_elapsed     | 3346     |
|    total_timesteps  | 403812   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.97     |
|    n_updates        | 90952    |
----------------------------------
Eval num_timesteps=404000, episode_reward=221.16 +/- 67.50
Episode length: 55.68 +/- 16.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.7     |
|    mean_reward      | 221      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 404000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.418    |
|    n_updates        | 90999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.8     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5188     |
|    fps              | 120      |
|    time_elapsed     | 3350     |
|    total_timesteps  | 404195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.42     |
|    n_updates        | 91048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5192     |
|    fps              | 120      |
|    time_elapsed     | 3350     |
|    total_timesteps  | 404440   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.566    |
|    n_updates        | 91109    |
----------------------------------
Eval num_timesteps=404500, episode_reward=336.72 +/- 152.08
Episode length: 84.58 +/- 38.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.6     |
|    mean_reward      | 337      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 404500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.64     |
|    n_updates        | 91124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5196     |
|    fps              | 120      |
|    time_elapsed     | 3356     |
|    total_timesteps  | 404822   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 91205    |
----------------------------------
Eval num_timesteps=405000, episode_reward=383.08 +/- 158.68
Episode length: 96.18 +/- 39.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 383      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 405000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.555    |
|    n_updates        | 91249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5200     |
|    fps              | 120      |
|    time_elapsed     | 3362     |
|    total_timesteps  | 405132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.37     |
|    n_updates        | 91282    |
----------------------------------
Eval num_timesteps=405500, episode_reward=295.86 +/- 177.74
Episode length: 74.38 +/- 44.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.4     |
|    mean_reward      | 296      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 405500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.61     |
|    n_updates        | 91374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.6     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5204     |
|    fps              | 120      |
|    time_elapsed     | 3367     |
|    total_timesteps  | 405510   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.536    |
|    n_updates        | 91377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5208     |
|    fps              | 120      |
|    time_elapsed     | 3367     |
|    total_timesteps  | 405720   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.99     |
|    n_updates        | 91429    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5212     |
|    fps              | 120      |
|    time_elapsed     | 3368     |
|    total_timesteps  | 405902   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.398    |
|    n_updates        | 91475    |
----------------------------------
Eval num_timesteps=406000, episode_reward=240.12 +/- 81.31
Episode length: 60.38 +/- 20.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.4     |
|    mean_reward      | 240      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 406000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.63     |
|    n_updates        | 91499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5216     |
|    fps              | 120      |
|    time_elapsed     | 3372     |
|    total_timesteps  | 406091   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.11     |
|    n_updates        | 91522    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5220     |
|    fps              | 120      |
|    time_elapsed     | 3372     |
|    total_timesteps  | 406269   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.307    |
|    n_updates        | 91567    |
----------------------------------
Eval num_timesteps=406500, episode_reward=249.06 +/- 106.75
Episode length: 62.62 +/- 26.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.6     |
|    mean_reward      | 249      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 406500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.42     |
|    n_updates        | 91624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5224     |
|    fps              | 120      |
|    time_elapsed     | 3377     |
|    total_timesteps  | 406759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.96     |
|    n_updates        | 91689    |
----------------------------------
Eval num_timesteps=407000, episode_reward=337.74 +/- 164.40
Episode length: 84.74 +/- 41.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.7     |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 407000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.962    |
|    n_updates        | 91749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5228     |
|    fps              | 120      |
|    time_elapsed     | 3382     |
|    total_timesteps  | 407095   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.44     |
|    n_updates        | 91773    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5232     |
|    fps              | 120      |
|    time_elapsed     | 3383     |
|    total_timesteps  | 407372   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.58     |
|    n_updates        | 91842    |
----------------------------------
Eval num_timesteps=407500, episode_reward=217.66 +/- 100.20
Episode length: 54.82 +/- 25.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | 218      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 407500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.14     |
|    n_updates        | 91874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5236     |
|    fps              | 120      |
|    time_elapsed     | 3386     |
|    total_timesteps  | 407600   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 91899    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5240     |
|    fps              | 120      |
|    time_elapsed     | 3387     |
|    total_timesteps  | 407804   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.907    |
|    n_updates        | 91950    |
----------------------------------
Eval num_timesteps=408000, episode_reward=282.90 +/- 120.33
Episode length: 71.06 +/- 30.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.1     |
|    mean_reward      | 283      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 408000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.838    |
|    n_updates        | 91999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5244     |
|    fps              | 120      |
|    time_elapsed     | 3391     |
|    total_timesteps  | 408039   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.557    |
|    n_updates        | 92009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5248     |
|    fps              | 120      |
|    time_elapsed     | 3392     |
|    total_timesteps  | 408398   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 92099    |
----------------------------------
Eval num_timesteps=408500, episode_reward=270.26 +/- 148.34
Episode length: 68.00 +/- 37.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | 270      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 408500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.31     |
|    n_updates        | 92124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5252     |
|    fps              | 120      |
|    time_elapsed     | 3397     |
|    total_timesteps  | 408815   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.51     |
|    n_updates        | 92203    |
----------------------------------
Eval num_timesteps=409000, episode_reward=210.60 +/- 74.40
Episode length: 53.08 +/- 18.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.1     |
|    mean_reward      | 211      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 409000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.9      |
|    n_updates        | 92249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5256     |
|    fps              | 120      |
|    time_elapsed     | 3401     |
|    total_timesteps  | 409202   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 92300    |
----------------------------------
Eval num_timesteps=409500, episode_reward=342.32 +/- 132.01
Episode length: 86.02 +/- 33.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86       |
|    mean_reward      | 342      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 409500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.542    |
|    n_updates        | 92374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5260     |
|    fps              | 120      |
|    time_elapsed     | 3406     |
|    total_timesteps  | 409620   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.12     |
|    n_updates        | 92404    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5264     |
|    fps              | 120      |
|    time_elapsed     | 3407     |
|    total_timesteps  | 409884   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.92     |
|    n_updates        | 92470    |
----------------------------------
Eval num_timesteps=410000, episode_reward=352.60 +/- 180.10
Episode length: 88.56 +/- 45.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.6     |
|    mean_reward      | 353      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 410000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.463    |
|    n_updates        | 92499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5268     |
|    fps              | 120      |
|    time_elapsed     | 3413     |
|    total_timesteps  | 410250   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.38     |
|    n_updates        | 92562    |
----------------------------------
Eval num_timesteps=410500, episode_reward=351.64 +/- 152.54
Episode length: 88.20 +/- 38.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.2     |
|    mean_reward      | 352      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 410500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.14     |
|    n_updates        | 92624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5272     |
|    fps              | 120      |
|    time_elapsed     | 3418     |
|    total_timesteps  | 410505   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.92     |
|    n_updates        | 92626    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5276     |
|    fps              | 120      |
|    time_elapsed     | 3419     |
|    total_timesteps  | 410875   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.72     |
|    n_updates        | 92718    |
----------------------------------
Eval num_timesteps=411000, episode_reward=305.38 +/- 104.05
Episode length: 76.76 +/- 26.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.8     |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 411000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.49     |
|    n_updates        | 92749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5280     |
|    fps              | 120      |
|    time_elapsed     | 3424     |
|    total_timesteps  | 411286   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.83     |
|    n_updates        | 92821    |
----------------------------------
Eval num_timesteps=411500, episode_reward=234.98 +/- 112.68
Episode length: 59.18 +/- 28.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.2     |
|    mean_reward      | 235      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 411500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.4      |
|    n_updates        | 92874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5284     |
|    fps              | 120      |
|    time_elapsed     | 3428     |
|    total_timesteps  | 411593   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.771    |
|    n_updates        | 92898    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5288     |
|    fps              | 120      |
|    time_elapsed     | 3429     |
|    total_timesteps  | 411827   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.346    |
|    n_updates        | 92956    |
----------------------------------
Eval num_timesteps=412000, episode_reward=364.70 +/- 156.03
Episode length: 91.58 +/- 39.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.6     |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 412000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.456    |
|    n_updates        | 92999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5292     |
|    fps              | 119      |
|    time_elapsed     | 3435     |
|    total_timesteps  | 412122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.687    |
|    n_updates        | 93030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5296     |
|    fps              | 120      |
|    time_elapsed     | 3435     |
|    total_timesteps  | 412387   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.389    |
|    n_updates        | 93096    |
----------------------------------
Eval num_timesteps=412500, episode_reward=255.88 +/- 101.08
Episode length: 64.30 +/- 25.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.3     |
|    mean_reward      | 256      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 412500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.05     |
|    n_updates        | 93124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5300     |
|    fps              | 119      |
|    time_elapsed     | 3440     |
|    total_timesteps  | 412664   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.467    |
|    n_updates        | 93165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5304     |
|    fps              | 120      |
|    time_elapsed     | 3441     |
|    total_timesteps  | 412983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.3      |
|    n_updates        | 93245    |
----------------------------------
Eval num_timesteps=413000, episode_reward=288.22 +/- 139.63
Episode length: 72.42 +/- 34.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.4     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 413000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.421    |
|    n_updates        | 93249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5308     |
|    fps              | 119      |
|    time_elapsed     | 3445     |
|    total_timesteps  | 413210   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.67     |
|    n_updates        | 93302    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5312     |
|    fps              | 119      |
|    time_elapsed     | 3446     |
|    total_timesteps  | 413447   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.74     |
|    n_updates        | 93361    |
----------------------------------
Eval num_timesteps=413500, episode_reward=277.50 +/- 101.61
Episode length: 69.76 +/- 25.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.8     |
|    mean_reward      | 278      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 413500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.87     |
|    n_updates        | 93374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5316     |
|    fps              | 119      |
|    time_elapsed     | 3451     |
|    total_timesteps  | 413820   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.5      |
|    n_updates        | 93454    |
----------------------------------
Eval num_timesteps=414000, episode_reward=221.36 +/- 100.81
Episode length: 55.74 +/- 25.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.7     |
|    mean_reward      | 221      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 414000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.29     |
|    n_updates        | 93499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5320     |
|    fps              | 119      |
|    time_elapsed     | 3455     |
|    total_timesteps  | 414078   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.62     |
|    n_updates        | 93519    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5324     |
|    fps              | 119      |
|    time_elapsed     | 3455     |
|    total_timesteps  | 414490   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.73     |
|    n_updates        | 93622    |
----------------------------------
Eval num_timesteps=414500, episode_reward=275.00 +/- 116.57
Episode length: 69.14 +/- 29.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.1     |
|    mean_reward      | 275      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 414500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.238    |
|    n_updates        | 93624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5328     |
|    fps              | 119      |
|    time_elapsed     | 3460     |
|    total_timesteps  | 414742   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.75     |
|    n_updates        | 93685    |
----------------------------------
Eval num_timesteps=415000, episode_reward=326.78 +/- 102.44
Episode length: 82.02 +/- 25.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82       |
|    mean_reward      | 327      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 415000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 93749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5332     |
|    fps              | 119      |
|    time_elapsed     | 3466     |
|    total_timesteps  | 415044   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.894    |
|    n_updates        | 93760    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5336     |
|    fps              | 119      |
|    time_elapsed     | 3466     |
|    total_timesteps  | 415329   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.82     |
|    n_updates        | 93832    |
----------------------------------
Eval num_timesteps=415500, episode_reward=313.96 +/- 130.46
Episode length: 78.88 +/- 32.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.9     |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 415500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.18     |
|    n_updates        | 93874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5340     |
|    fps              | 119      |
|    time_elapsed     | 3472     |
|    total_timesteps  | 415728   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.856    |
|    n_updates        | 93931    |
----------------------------------
Eval num_timesteps=416000, episode_reward=263.84 +/- 94.65
Episode length: 66.32 +/- 23.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.3     |
|    mean_reward      | 264      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 416000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.819    |
|    n_updates        | 93999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5344     |
|    fps              | 119      |
|    time_elapsed     | 3476     |
|    total_timesteps  | 416027   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.53     |
|    n_updates        | 94006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5348     |
|    fps              | 119      |
|    time_elapsed     | 3477     |
|    total_timesteps  | 416294   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.26     |
|    n_updates        | 94073    |
----------------------------------
Eval num_timesteps=416500, episode_reward=358.16 +/- 190.10
Episode length: 89.92 +/- 47.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.9     |
|    mean_reward      | 358      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 416500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.939    |
|    n_updates        | 94124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5352     |
|    fps              | 119      |
|    time_elapsed     | 3483     |
|    total_timesteps  | 416579   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.05     |
|    n_updates        | 94144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5356     |
|    fps              | 119      |
|    time_elapsed     | 3484     |
|    total_timesteps  | 416988   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.241    |
|    n_updates        | 94246    |
----------------------------------
Eval num_timesteps=417000, episode_reward=312.42 +/- 115.74
Episode length: 78.50 +/- 28.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.5     |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 417000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 94249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5360     |
|    fps              | 119      |
|    time_elapsed     | 3489     |
|    total_timesteps  | 417392   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.89     |
|    n_updates        | 94347    |
----------------------------------
Eval num_timesteps=417500, episode_reward=331.24 +/- 137.73
Episode length: 83.18 +/- 34.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.2     |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 417500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.54     |
|    n_updates        | 94374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5364     |
|    fps              | 119      |
|    time_elapsed     | 3495     |
|    total_timesteps  | 417716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.97     |
|    n_updates        | 94428    |
----------------------------------
Eval num_timesteps=418000, episode_reward=256.42 +/- 88.81
Episode length: 64.54 +/- 22.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.5     |
|    mean_reward      | 256      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 418000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.02     |
|    n_updates        | 94499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5368     |
|    fps              | 119      |
|    time_elapsed     | 3500     |
|    total_timesteps  | 418079   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.78     |
|    n_updates        | 94519    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5372     |
|    fps              | 119      |
|    time_elapsed     | 3501     |
|    total_timesteps  | 418351   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.42     |
|    n_updates        | 94587    |
----------------------------------
Eval num_timesteps=418500, episode_reward=277.24 +/- 142.52
Episode length: 69.64 +/- 35.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.6     |
|    mean_reward      | 277      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 418500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.12     |
|    n_updates        | 94624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5376     |
|    fps              | 119      |
|    time_elapsed     | 3506     |
|    total_timesteps  | 418737   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.4      |
|    n_updates        | 94684    |
----------------------------------
Eval num_timesteps=419000, episode_reward=234.40 +/- 110.15
Episode length: 59.00 +/- 27.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | 234      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 419000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.1      |
|    n_updates        | 94749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5380     |
|    fps              | 119      |
|    time_elapsed     | 3510     |
|    total_timesteps  | 419121   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.32     |
|    n_updates        | 94780    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5384     |
|    fps              | 119      |
|    time_elapsed     | 3510     |
|    total_timesteps  | 419394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.917    |
|    n_updates        | 94848    |
----------------------------------
Eval num_timesteps=419500, episode_reward=411.50 +/- 223.79
Episode length: 103.18 +/- 55.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 412      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 419500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.53     |
|    n_updates        | 94874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5388     |
|    fps              | 119      |
|    time_elapsed     | 3517     |
|    total_timesteps  | 419750   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.261    |
|    n_updates        | 94937    |
----------------------------------
Eval num_timesteps=420000, episode_reward=299.20 +/- 140.47
Episode length: 75.16 +/- 35.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.2     |
|    mean_reward      | 299      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 420000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.64     |
|    n_updates        | 94999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5392     |
|    fps              | 119      |
|    time_elapsed     | 3523     |
|    total_timesteps  | 420058   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 95014    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5396     |
|    fps              | 119      |
|    time_elapsed     | 3523     |
|    total_timesteps  | 420401   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.56     |
|    n_updates        | 95100    |
----------------------------------
Eval num_timesteps=420500, episode_reward=341.58 +/- 174.65
Episode length: 85.74 +/- 43.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.7     |
|    mean_reward      | 342      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 420500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.79     |
|    n_updates        | 95124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5400     |
|    fps              | 119      |
|    time_elapsed     | 3529     |
|    total_timesteps  | 420767   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.07     |
|    n_updates        | 95191    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5404     |
|    fps              | 119      |
|    time_elapsed     | 3530     |
|    total_timesteps  | 420986   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.58     |
|    n_updates        | 95246    |
----------------------------------
Eval num_timesteps=421000, episode_reward=290.30 +/- 148.02
Episode length: 72.86 +/- 37.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.9     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 421000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 95249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5408     |
|    fps              | 119      |
|    time_elapsed     | 3535     |
|    total_timesteps  | 421290   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.904    |
|    n_updates        | 95322    |
----------------------------------
Eval num_timesteps=421500, episode_reward=320.34 +/- 131.17
Episode length: 80.42 +/- 32.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.4     |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 421500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.903    |
|    n_updates        | 95374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5412     |
|    fps              | 119      |
|    time_elapsed     | 3540     |
|    total_timesteps  | 421649   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.34     |
|    n_updates        | 95412    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5416     |
|    fps              | 119      |
|    time_elapsed     | 3541     |
|    total_timesteps  | 421999   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.614    |
|    n_updates        | 95499    |
----------------------------------
Eval num_timesteps=422000, episode_reward=287.36 +/- 126.51
Episode length: 72.16 +/- 31.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.2     |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 422000   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5420     |
|    fps              | 119      |
|    time_elapsed     | 3546     |
|    total_timesteps  | 422396   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.612    |
|    n_updates        | 95598    |
----------------------------------
Eval num_timesteps=422500, episode_reward=273.98 +/- 127.90
Episode length: 68.94 +/- 31.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.9     |
|    mean_reward      | 274      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 422500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.12     |
|    n_updates        | 95624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5424     |
|    fps              | 119      |
|    time_elapsed     | 3550     |
|    total_timesteps  | 422766   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.351    |
|    n_updates        | 95691    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5428     |
|    fps              | 119      |
|    time_elapsed     | 3551     |
|    total_timesteps  | 422972   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.24     |
|    n_updates        | 95742    |
----------------------------------
Eval num_timesteps=423000, episode_reward=257.64 +/- 91.27
Episode length: 64.84 +/- 22.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.8     |
|    mean_reward      | 258      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 423000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.418    |
|    n_updates        | 95749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5432     |
|    fps              | 119      |
|    time_elapsed     | 3555     |
|    total_timesteps  | 423432   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.468    |
|    n_updates        | 95857    |
----------------------------------
Eval num_timesteps=423500, episode_reward=353.16 +/- 180.54
Episode length: 88.70 +/- 45.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.7     |
|    mean_reward      | 353      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 423500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.71     |
|    n_updates        | 95874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5436     |
|    fps              | 118      |
|    time_elapsed     | 3561     |
|    total_timesteps  | 423779   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.94     |
|    n_updates        | 95944    |
----------------------------------
Eval num_timesteps=424000, episode_reward=344.48 +/- 165.70
Episode length: 86.48 +/- 41.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.5     |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 424000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.23     |
|    n_updates        | 95999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5440     |
|    fps              | 118      |
|    time_elapsed     | 3568     |
|    total_timesteps  | 424108   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.86     |
|    n_updates        | 96026    |
----------------------------------
Eval num_timesteps=424500, episode_reward=284.22 +/- 125.32
Episode length: 71.46 +/- 31.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.5     |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 424500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.93     |
|    n_updates        | 96124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5444     |
|    fps              | 118      |
|    time_elapsed     | 3573     |
|    total_timesteps  | 424542   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 96135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5448     |
|    fps              | 118      |
|    time_elapsed     | 3574     |
|    total_timesteps  | 424973   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.24     |
|    n_updates        | 96243    |
----------------------------------
Eval num_timesteps=425000, episode_reward=257.38 +/- 124.77
Episode length: 64.70 +/- 31.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.7     |
|    mean_reward      | 257      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 425000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.87     |
|    n_updates        | 96249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.8     |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5452     |
|    fps              | 118      |
|    time_elapsed     | 3579     |
|    total_timesteps  | 425364   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.557    |
|    n_updates        | 96340    |
----------------------------------
Eval num_timesteps=425500, episode_reward=213.86 +/- 70.08
Episode length: 53.84 +/- 17.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.8     |
|    mean_reward      | 214      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 425500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.59     |
|    n_updates        | 96374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5456     |
|    fps              | 118      |
|    time_elapsed     | 3583     |
|    total_timesteps  | 425720   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.95     |
|    n_updates        | 96429    |
----------------------------------
Eval num_timesteps=426000, episode_reward=377.02 +/- 171.62
Episode length: 94.62 +/- 42.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 377      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 426000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.04     |
|    n_updates        | 96499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.1     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5460     |
|    fps              | 118      |
|    time_elapsed     | 3590     |
|    total_timesteps  | 426098   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.43     |
|    n_updates        | 96524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5464     |
|    fps              | 118      |
|    time_elapsed     | 3590     |
|    total_timesteps  | 426420   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.3      |
|    n_updates        | 96604    |
----------------------------------
Eval num_timesteps=426500, episode_reward=304.50 +/- 162.45
Episode length: 76.46 +/- 40.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.5     |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 426500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.918    |
|    n_updates        | 96624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.6     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5468     |
|    fps              | 118      |
|    time_elapsed     | 3596     |
|    total_timesteps  | 426739   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.633    |
|    n_updates        | 96684    |
----------------------------------
Eval num_timesteps=427000, episode_reward=293.84 +/- 126.05
Episode length: 73.86 +/- 31.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.9     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 427000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.37     |
|    n_updates        | 96749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5472     |
|    fps              | 118      |
|    time_elapsed     | 3601     |
|    total_timesteps  | 427081   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.8      |
|    n_updates        | 96770    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5476     |
|    fps              | 118      |
|    time_elapsed     | 3602     |
|    total_timesteps  | 427408   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.96     |
|    n_updates        | 96851    |
----------------------------------
Eval num_timesteps=427500, episode_reward=228.22 +/- 82.04
Episode length: 57.50 +/- 20.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.5     |
|    mean_reward      | 228      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 427500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.472    |
|    n_updates        | 96874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5480     |
|    fps              | 118      |
|    time_elapsed     | 3606     |
|    total_timesteps  | 427637   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.323    |
|    n_updates        | 96909    |
----------------------------------
Eval num_timesteps=428000, episode_reward=304.74 +/- 149.26
Episode length: 76.62 +/- 37.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.6     |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 428000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.81     |
|    n_updates        | 96999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.6     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5484     |
|    fps              | 118      |
|    time_elapsed     | 3611     |
|    total_timesteps  | 428051   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.784    |
|    n_updates        | 97012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.6     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5488     |
|    fps              | 118      |
|    time_elapsed     | 3612     |
|    total_timesteps  | 428407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.36     |
|    n_updates        | 97101    |
----------------------------------
Eval num_timesteps=428500, episode_reward=407.92 +/- 179.29
Episode length: 102.24 +/- 44.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 408      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 428500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.365    |
|    n_updates        | 97124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.9     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5492     |
|    fps              | 118      |
|    time_elapsed     | 3619     |
|    total_timesteps  | 428750   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.28     |
|    n_updates        | 97187    |
----------------------------------
Eval num_timesteps=429000, episode_reward=208.92 +/- 81.98
Episode length: 52.62 +/- 20.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.6     |
|    mean_reward      | 209      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 429000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.78     |
|    n_updates        | 97249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5496     |
|    fps              | 118      |
|    time_elapsed     | 3623     |
|    total_timesteps  | 429232   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.278    |
|    n_updates        | 97307    |
----------------------------------
Eval num_timesteps=429500, episode_reward=309.94 +/- 145.52
Episode length: 77.92 +/- 36.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.9     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 429500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.55     |
|    n_updates        | 97374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5500     |
|    fps              | 118      |
|    time_elapsed     | 3628     |
|    total_timesteps  | 429531   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.48     |
|    n_updates        | 97382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.6     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5504     |
|    fps              | 118      |
|    time_elapsed     | 3629     |
|    total_timesteps  | 429943   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.289    |
|    n_updates        | 97485    |
----------------------------------
Eval num_timesteps=430000, episode_reward=370.48 +/- 152.48
Episode length: 93.02 +/- 38.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93       |
|    mean_reward      | 370      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 430000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.63     |
|    n_updates        | 97499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.2     |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5508     |
|    fps              | 118      |
|    time_elapsed     | 3636     |
|    total_timesteps  | 430315   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2        |
|    n_updates        | 97578    |
----------------------------------
Eval num_timesteps=430500, episode_reward=270.84 +/- 104.02
Episode length: 68.02 +/- 26.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | 271      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 430500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.18     |
|    n_updates        | 97624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5512     |
|    fps              | 118      |
|    time_elapsed     | 3640     |
|    total_timesteps  | 430585   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.93     |
|    n_updates        | 97646    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89       |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5516     |
|    fps              | 118      |
|    time_elapsed     | 3641     |
|    total_timesteps  | 430898   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.547    |
|    n_updates        | 97724    |
----------------------------------
Eval num_timesteps=431000, episode_reward=274.74 +/- 138.56
Episode length: 69.06 +/- 34.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.1     |
|    mean_reward      | 275      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 431000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.46     |
|    n_updates        | 97749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.9     |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5520     |
|    fps              | 118      |
|    time_elapsed     | 3646     |
|    total_timesteps  | 431185   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.72     |
|    n_updates        | 97796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5524     |
|    fps              | 118      |
|    time_elapsed     | 3647     |
|    total_timesteps  | 431441   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.39     |
|    n_updates        | 97860    |
----------------------------------
Eval num_timesteps=431500, episode_reward=347.62 +/- 163.84
Episode length: 87.24 +/- 40.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.2     |
|    mean_reward      | 348      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 431500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 97874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.1     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5528     |
|    fps              | 118      |
|    time_elapsed     | 3653     |
|    total_timesteps  | 431786   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.15     |
|    n_updates        | 97946    |
----------------------------------
Eval num_timesteps=432000, episode_reward=317.60 +/- 163.44
Episode length: 79.74 +/- 40.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.7     |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 432000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.21     |
|    n_updates        | 97999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.9     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5532     |
|    fps              | 118      |
|    time_elapsed     | 3658     |
|    total_timesteps  | 432023   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 98005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5536     |
|    fps              | 118      |
|    time_elapsed     | 3658     |
|    total_timesteps  | 432285   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.533    |
|    n_updates        | 98071    |
----------------------------------
Eval num_timesteps=432500, episode_reward=292.90 +/- 104.02
Episode length: 73.58 +/- 25.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | 293      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 432500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.28     |
|    n_updates        | 98124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5540     |
|    fps              | 118      |
|    time_elapsed     | 3664     |
|    total_timesteps  | 432628   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.81     |
|    n_updates        | 98156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5544     |
|    fps              | 118      |
|    time_elapsed     | 3664     |
|    total_timesteps  | 432830   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.38     |
|    n_updates        | 98207    |
----------------------------------
Eval num_timesteps=433000, episode_reward=254.02 +/- 111.41
Episode length: 63.88 +/- 27.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.9     |
|    mean_reward      | 254      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 433000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.06     |
|    n_updates        | 98249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5548     |
|    fps              | 118      |
|    time_elapsed     | 3668     |
|    total_timesteps  | 433148   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.13     |
|    n_updates        | 98286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5552     |
|    fps              | 118      |
|    time_elapsed     | 3669     |
|    total_timesteps  | 433449   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 98362    |
----------------------------------
Eval num_timesteps=433500, episode_reward=309.74 +/- 139.71
Episode length: 77.78 +/- 34.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.8     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 433500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.11     |
|    n_updates        | 98374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5556     |
|    fps              | 118      |
|    time_elapsed     | 3674     |
|    total_timesteps  | 433818   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.13     |
|    n_updates        | 98454    |
----------------------------------
Eval num_timesteps=434000, episode_reward=390.34 +/- 201.99
Episode length: 98.00 +/- 50.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98       |
|    mean_reward      | 390      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 434000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.73     |
|    n_updates        | 98499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5560     |
|    fps              | 117      |
|    time_elapsed     | 3681     |
|    total_timesteps  | 434186   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.47     |
|    n_updates        | 98546    |
----------------------------------
Eval num_timesteps=434500, episode_reward=302.74 +/- 172.62
Episode length: 76.10 +/- 43.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.1     |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 434500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.628    |
|    n_updates        | 98624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5564     |
|    fps              | 117      |
|    time_elapsed     | 3687     |
|    total_timesteps  | 434562   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.258    |
|    n_updates        | 98640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5568     |
|    fps              | 117      |
|    time_elapsed     | 3687     |
|    total_timesteps  | 434985   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.99     |
|    n_updates        | 98746    |
----------------------------------
Eval num_timesteps=435000, episode_reward=212.54 +/- 74.41
Episode length: 53.52 +/- 18.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.5     |
|    mean_reward      | 213      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 435000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.491    |
|    n_updates        | 98749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5572     |
|    fps              | 117      |
|    time_elapsed     | 3691     |
|    total_timesteps  | 435235   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.93     |
|    n_updates        | 98808    |
----------------------------------
Eval num_timesteps=435500, episode_reward=325.32 +/- 138.44
Episode length: 81.68 +/- 34.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.7     |
|    mean_reward      | 325      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 435500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.67     |
|    n_updates        | 98874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5576     |
|    fps              | 117      |
|    time_elapsed     | 3697     |
|    total_timesteps  | 435727   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 98931    |
----------------------------------
Eval num_timesteps=436000, episode_reward=304.70 +/- 155.60
Episode length: 76.54 +/- 38.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.5     |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 436000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.92     |
|    n_updates        | 98999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5580     |
|    fps              | 117      |
|    time_elapsed     | 3702     |
|    total_timesteps  | 436035   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.31     |
|    n_updates        | 99008    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5584     |
|    fps              | 117      |
|    time_elapsed     | 3703     |
|    total_timesteps  | 436291   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.779    |
|    n_updates        | 99072    |
----------------------------------
Eval num_timesteps=436500, episode_reward=219.66 +/- 89.11
Episode length: 55.28 +/- 22.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.3     |
|    mean_reward      | 220      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 436500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.18     |
|    n_updates        | 99124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5588     |
|    fps              | 117      |
|    time_elapsed     | 3707     |
|    total_timesteps  | 436738   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.52     |
|    n_updates        | 99184    |
----------------------------------
Eval num_timesteps=437000, episode_reward=220.78 +/- 96.18
Episode length: 55.60 +/- 24.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.6     |
|    mean_reward      | 221      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 437000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.603    |
|    n_updates        | 99249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5592     |
|    fps              | 117      |
|    time_elapsed     | 3711     |
|    total_timesteps  | 437106   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.14     |
|    n_updates        | 99276    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5596     |
|    fps              | 117      |
|    time_elapsed     | 3712     |
|    total_timesteps  | 437343   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.663    |
|    n_updates        | 99335    |
----------------------------------
Eval num_timesteps=437500, episode_reward=261.08 +/- 91.94
Episode length: 65.66 +/- 22.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.7     |
|    mean_reward      | 261      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 437500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.3      |
|    n_updates        | 99374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5600     |
|    fps              | 117      |
|    time_elapsed     | 3717     |
|    total_timesteps  | 437782   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.09     |
|    n_updates        | 99445    |
----------------------------------
Eval num_timesteps=438000, episode_reward=237.68 +/- 98.30
Episode length: 59.74 +/- 24.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.7     |
|    mean_reward      | 238      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 438000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.23     |
|    n_updates        | 99499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5604     |
|    fps              | 117      |
|    time_elapsed     | 3721     |
|    total_timesteps  | 438057   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.64     |
|    n_updates        | 99514    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5608     |
|    fps              | 117      |
|    time_elapsed     | 3722     |
|    total_timesteps  | 438321   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.43     |
|    n_updates        | 99580    |
----------------------------------
Eval num_timesteps=438500, episode_reward=303.38 +/- 152.93
Episode length: 76.22 +/- 38.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.2     |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 438500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.08     |
|    n_updates        | 99624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5612     |
|    fps              | 117      |
|    time_elapsed     | 3727     |
|    total_timesteps  | 438682   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.39     |
|    n_updates        | 99670    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5616     |
|    fps              | 117      |
|    time_elapsed     | 3727     |
|    total_timesteps  | 438940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.61     |
|    n_updates        | 99734    |
----------------------------------
Eval num_timesteps=439000, episode_reward=211.92 +/- 67.58
Episode length: 53.34 +/- 16.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.3     |
|    mean_reward      | 212      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 439000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.28     |
|    n_updates        | 99749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5620     |
|    fps              | 117      |
|    time_elapsed     | 3731     |
|    total_timesteps  | 439308   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.06     |
|    n_updates        | 99826    |
----------------------------------
Eval num_timesteps=439500, episode_reward=369.76 +/- 146.27
Episode length: 92.84 +/- 36.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 370      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 439500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.59     |
|    n_updates        | 99874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5624     |
|    fps              | 117      |
|    time_elapsed     | 3737     |
|    total_timesteps  | 439646   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.38     |
|    n_updates        | 99911    |
----------------------------------
Eval num_timesteps=440000, episode_reward=396.78 +/- 167.86
Episode length: 99.60 +/- 42.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.6     |
|    mean_reward      | 397      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 440000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.13     |
|    n_updates        | 99999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5628     |
|    fps              | 117      |
|    time_elapsed     | 3743     |
|    total_timesteps  | 440007   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.74     |
|    n_updates        | 100001   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5632     |
|    fps              | 117      |
|    time_elapsed     | 3744     |
|    total_timesteps  | 440330   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.44     |
|    n_updates        | 100082   |
----------------------------------
Eval num_timesteps=440500, episode_reward=328.76 +/- 142.62
Episode length: 82.54 +/- 35.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.5     |
|    mean_reward      | 329      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 440500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.69     |
|    n_updates        | 100124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5636     |
|    fps              | 117      |
|    time_elapsed     | 3750     |
|    total_timesteps  | 440660   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.99     |
|    n_updates        | 100164   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5640     |
|    fps              | 117      |
|    time_elapsed     | 3750     |
|    total_timesteps  | 440981   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.4      |
|    n_updates        | 100245   |
----------------------------------
Eval num_timesteps=441000, episode_reward=336.76 +/- 151.23
Episode length: 84.62 +/- 37.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.6     |
|    mean_reward      | 337      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 441000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.86     |
|    n_updates        | 100249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5644     |
|    fps              | 117      |
|    time_elapsed     | 3756     |
|    total_timesteps  | 441159   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 100289   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5648     |
|    fps              | 117      |
|    time_elapsed     | 3756     |
|    total_timesteps  | 441390   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.7      |
|    n_updates        | 100347   |
----------------------------------
Eval num_timesteps=441500, episode_reward=241.50 +/- 76.59
Episode length: 60.82 +/- 19.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.8     |
|    mean_reward      | 242      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 441500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.35     |
|    n_updates        | 100374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5652     |
|    fps              | 117      |
|    time_elapsed     | 3761     |
|    total_timesteps  | 441868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.06     |
|    n_updates        | 100466   |
----------------------------------
Eval num_timesteps=442000, episode_reward=354.80 +/- 171.29
Episode length: 89.08 +/- 42.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.1     |
|    mean_reward      | 355      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 442000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.07     |
|    n_updates        | 100499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5656     |
|    fps              | 117      |
|    time_elapsed     | 3767     |
|    total_timesteps  | 442217   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.694    |
|    n_updates        | 100554   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5660     |
|    fps              | 117      |
|    time_elapsed     | 3767     |
|    total_timesteps  | 442453   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.522    |
|    n_updates        | 100613   |
----------------------------------
Eval num_timesteps=442500, episode_reward=299.78 +/- 180.21
Episode length: 75.36 +/- 45.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.4     |
|    mean_reward      | 300      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 442500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.925    |
|    n_updates        | 100624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5664     |
|    fps              | 117      |
|    time_elapsed     | 3772     |
|    total_timesteps  | 442806   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.48     |
|    n_updates        | 100701   |
----------------------------------
Eval num_timesteps=443000, episode_reward=404.22 +/- 206.81
Episode length: 101.46 +/- 51.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 404      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 443000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.72     |
|    n_updates        | 100749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5668     |
|    fps              | 117      |
|    time_elapsed     | 3779     |
|    total_timesteps  | 443041   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.76     |
|    n_updates        | 100760   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5672     |
|    fps              | 117      |
|    time_elapsed     | 3780     |
|    total_timesteps  | 443399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.86     |
|    n_updates        | 100849   |
----------------------------------
Eval num_timesteps=443500, episode_reward=250.60 +/- 111.85
Episode length: 63.04 +/- 27.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63       |
|    mean_reward      | 251      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 443500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 100874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5676     |
|    fps              | 117      |
|    time_elapsed     | 3784     |
|    total_timesteps  | 443759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.43     |
|    n_updates        | 100939   |
----------------------------------
Eval num_timesteps=444000, episode_reward=388.28 +/- 160.55
Episode length: 97.44 +/- 40.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.4     |
|    mean_reward      | 388      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 444000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.266    |
|    n_updates        | 100999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5680     |
|    fps              | 117      |
|    time_elapsed     | 3791     |
|    total_timesteps  | 444086   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.55     |
|    n_updates        | 101021   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5684     |
|    fps              | 117      |
|    time_elapsed     | 3791     |
|    total_timesteps  | 444463   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.5      |
|    n_updates        | 101115   |
----------------------------------
Eval num_timesteps=444500, episode_reward=283.48 +/- 78.66
Episode length: 71.26 +/- 19.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.3     |
|    mean_reward      | 283      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 444500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.936    |
|    n_updates        | 101124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5688     |
|    fps              | 117      |
|    time_elapsed     | 3796     |
|    total_timesteps  | 444649   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.549    |
|    n_updates        | 101162   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5692     |
|    fps              | 117      |
|    time_elapsed     | 3797     |
|    total_timesteps  | 444889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.822    |
|    n_updates        | 101222   |
----------------------------------
Eval num_timesteps=445000, episode_reward=281.06 +/- 102.19
Episode length: 70.68 +/- 25.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.7     |
|    mean_reward      | 281      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 445000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.57     |
|    n_updates        | 101249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5696     |
|    fps              | 117      |
|    time_elapsed     | 3802     |
|    total_timesteps  | 445096   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.02     |
|    n_updates        | 101273   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5700     |
|    fps              | 117      |
|    time_elapsed     | 3802     |
|    total_timesteps  | 445438   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.42     |
|    n_updates        | 101359   |
----------------------------------
Eval num_timesteps=445500, episode_reward=273.86 +/- 105.40
Episode length: 68.82 +/- 26.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.8     |
|    mean_reward      | 274      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 445500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.08     |
|    n_updates        | 101374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5704     |
|    fps              | 117      |
|    time_elapsed     | 3807     |
|    total_timesteps  | 445684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.24     |
|    n_updates        | 101420   |
----------------------------------
Eval num_timesteps=446000, episode_reward=298.00 +/- 156.84
Episode length: 74.98 +/- 39.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | 298      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 446000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.53     |
|    n_updates        | 101499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5708     |
|    fps              | 116      |
|    time_elapsed     | 3812     |
|    total_timesteps  | 446032   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.4      |
|    n_updates        | 101507   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5712     |
|    fps              | 117      |
|    time_elapsed     | 3812     |
|    total_timesteps  | 446366   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.92     |
|    n_updates        | 101591   |
----------------------------------
Eval num_timesteps=446500, episode_reward=329.46 +/- 128.12
Episode length: 82.76 +/- 32.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.8     |
|    mean_reward      | 329      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 446500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.801    |
|    n_updates        | 101624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5716     |
|    fps              | 116      |
|    time_elapsed     | 3818     |
|    total_timesteps  | 446750   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.61     |
|    n_updates        | 101687   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5720     |
|    fps              | 117      |
|    time_elapsed     | 3819     |
|    total_timesteps  | 446939   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.267    |
|    n_updates        | 101734   |
----------------------------------
Eval num_timesteps=447000, episode_reward=355.36 +/- 172.47
Episode length: 89.32 +/- 43.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.3     |
|    mean_reward      | 355      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 447000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.479    |
|    n_updates        | 101749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5724     |
|    fps              | 116      |
|    time_elapsed     | 3824     |
|    total_timesteps  | 447160   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.44     |
|    n_updates        | 101789   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5728     |
|    fps              | 116      |
|    time_elapsed     | 3825     |
|    total_timesteps  | 447431   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.79     |
|    n_updates        | 101857   |
----------------------------------
Eval num_timesteps=447500, episode_reward=247.38 +/- 80.02
Episode length: 62.24 +/- 20.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.2     |
|    mean_reward      | 247      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 447500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.11     |
|    n_updates        | 101874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5732     |
|    fps              | 116      |
|    time_elapsed     | 3829     |
|    total_timesteps  | 447640   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.581    |
|    n_updates        | 101909   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5736     |
|    fps              | 116      |
|    time_elapsed     | 3830     |
|    total_timesteps  | 447893   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.84     |
|    n_updates        | 101973   |
----------------------------------
Eval num_timesteps=448000, episode_reward=378.70 +/- 218.09
Episode length: 95.02 +/- 54.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 379      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 448000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.57     |
|    n_updates        | 101999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5740     |
|    fps              | 116      |
|    time_elapsed     | 3836     |
|    total_timesteps  | 448175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 102043   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5744     |
|    fps              | 116      |
|    time_elapsed     | 3836     |
|    total_timesteps  | 448407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.57     |
|    n_updates        | 102101   |
----------------------------------
Eval num_timesteps=448500, episode_reward=360.20 +/- 150.05
Episode length: 90.40 +/- 37.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.4     |
|    mean_reward      | 360      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 448500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.63     |
|    n_updates        | 102124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5748     |
|    fps              | 116      |
|    time_elapsed     | 3842     |
|    total_timesteps  | 448744   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.259    |
|    n_updates        | 102185   |
----------------------------------
Eval num_timesteps=449000, episode_reward=237.62 +/- 102.59
Episode length: 59.80 +/- 25.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.8     |
|    mean_reward      | 238      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 449000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.91     |
|    n_updates        | 102249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5752     |
|    fps              | 116      |
|    time_elapsed     | 3846     |
|    total_timesteps  | 449113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.04     |
|    n_updates        | 102278   |
----------------------------------
Eval num_timesteps=449500, episode_reward=240.46 +/- 97.40
Episode length: 60.42 +/- 24.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.4     |
|    mean_reward      | 240      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 449500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 102374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5756     |
|    fps              | 116      |
|    time_elapsed     | 3851     |
|    total_timesteps  | 449710   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.16     |
|    n_updates        | 102427   |
----------------------------------
Eval num_timesteps=450000, episode_reward=325.74 +/- 164.37
Episode length: 81.84 +/- 41.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.8     |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 450000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.805    |
|    n_updates        | 102499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5760     |
|    fps              | 116      |
|    time_elapsed     | 3857     |
|    total_timesteps  | 450269   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1        |
|    n_updates        | 102567   |
----------------------------------
Eval num_timesteps=450500, episode_reward=333.48 +/- 156.39
Episode length: 83.70 +/- 39.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.7     |
|    mean_reward      | 333      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 450500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.383    |
|    n_updates        | 102624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5764     |
|    fps              | 116      |
|    time_elapsed     | 3862     |
|    total_timesteps  | 450627   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.78     |
|    n_updates        | 102656   |
----------------------------------
Eval num_timesteps=451000, episode_reward=311.38 +/- 144.36
Episode length: 78.22 +/- 36.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.2     |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 451000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.96     |
|    n_updates        | 102749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5768     |
|    fps              | 116      |
|    time_elapsed     | 3868     |
|    total_timesteps  | 451004   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.834    |
|    n_updates        | 102750   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5772     |
|    fps              | 116      |
|    time_elapsed     | 3868     |
|    total_timesteps  | 451390   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.09     |
|    n_updates        | 102847   |
----------------------------------
Eval num_timesteps=451500, episode_reward=414.76 +/- 191.41
Episode length: 104.08 +/- 47.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 415      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 451500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.529    |
|    n_updates        | 102874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5776     |
|    fps              | 116      |
|    time_elapsed     | 3876     |
|    total_timesteps  | 451676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.73     |
|    n_updates        | 102918   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5780     |
|    fps              | 116      |
|    time_elapsed     | 3876     |
|    total_timesteps  | 451974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.456    |
|    n_updates        | 102993   |
----------------------------------
Eval num_timesteps=452000, episode_reward=304.24 +/- 114.29
Episode length: 76.42 +/- 28.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.4     |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 452000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 102999   |
----------------------------------
Eval num_timesteps=452500, episode_reward=404.46 +/- 159.84
Episode length: 101.48 +/- 39.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 404      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 452500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.86     |
|    n_updates        | 103124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5784     |
|    fps              | 116      |
|    time_elapsed     | 3888     |
|    total_timesteps  | 452525   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.79     |
|    n_updates        | 103131   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5788     |
|    fps              | 116      |
|    time_elapsed     | 3889     |
|    total_timesteps  | 452845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.92     |
|    n_updates        | 103211   |
----------------------------------
Eval num_timesteps=453000, episode_reward=306.02 +/- 150.84
Episode length: 76.92 +/- 37.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.9     |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 453000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.28     |
|    n_updates        | 103249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5792     |
|    fps              | 116      |
|    time_elapsed     | 3894     |
|    total_timesteps  | 453132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.07     |
|    n_updates        | 103282   |
----------------------------------
Eval num_timesteps=453500, episode_reward=291.28 +/- 128.71
Episode length: 73.24 +/- 32.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.2     |
|    mean_reward      | 291      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 453500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.266    |
|    n_updates        | 103374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5796     |
|    fps              | 116      |
|    time_elapsed     | 3899     |
|    total_timesteps  | 453562   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.64     |
|    n_updates        | 103390   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5800     |
|    fps              | 116      |
|    time_elapsed     | 3899     |
|    total_timesteps  | 453904   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.428    |
|    n_updates        | 103475   |
----------------------------------
Eval num_timesteps=454000, episode_reward=204.34 +/- 71.95
Episode length: 51.48 +/- 17.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.5     |
|    mean_reward      | 204      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 454000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.799    |
|    n_updates        | 103499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5804     |
|    fps              | 116      |
|    time_elapsed     | 3903     |
|    total_timesteps  | 454252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.235    |
|    n_updates        | 103562   |
----------------------------------
Eval num_timesteps=454500, episode_reward=260.94 +/- 109.16
Episode length: 65.58 +/- 27.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.6     |
|    mean_reward      | 261      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 454500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.11     |
|    n_updates        | 103624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5808     |
|    fps              | 116      |
|    time_elapsed     | 3908     |
|    total_timesteps  | 454546   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.668    |
|    n_updates        | 103636   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86       |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5812     |
|    fps              | 116      |
|    time_elapsed     | 3909     |
|    total_timesteps  | 454964   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.97     |
|    n_updates        | 103740   |
----------------------------------
Eval num_timesteps=455000, episode_reward=297.44 +/- 131.23
Episode length: 74.68 +/- 32.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.7     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 455000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.824    |
|    n_updates        | 103749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5816     |
|    fps              | 116      |
|    time_elapsed     | 3914     |
|    total_timesteps  | 455249   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.733    |
|    n_updates        | 103812   |
----------------------------------
Eval num_timesteps=455500, episode_reward=243.46 +/- 99.40
Episode length: 61.26 +/- 24.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.3     |
|    mean_reward      | 243      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 455500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.43     |
|    n_updates        | 103874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.9     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5820     |
|    fps              | 116      |
|    time_elapsed     | 3918     |
|    total_timesteps  | 455525   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.57     |
|    n_updates        | 103881   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5824     |
|    fps              | 116      |
|    time_elapsed     | 3918     |
|    total_timesteps  | 455680   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.513    |
|    n_updates        | 103919   |
----------------------------------
Eval num_timesteps=456000, episode_reward=328.64 +/- 148.38
Episode length: 82.54 +/- 37.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.5     |
|    mean_reward      | 329      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 456000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.695    |
|    n_updates        | 103999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.9     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5828     |
|    fps              | 116      |
|    time_elapsed     | 3924     |
|    total_timesteps  | 456020   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.54     |
|    n_updates        | 104004   |
----------------------------------
Eval num_timesteps=456500, episode_reward=351.26 +/- 167.28
Episode length: 88.26 +/- 41.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.3     |
|    mean_reward      | 351      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 456500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.87     |
|    n_updates        | 104124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.7     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5832     |
|    fps              | 116      |
|    time_elapsed     | 3930     |
|    total_timesteps  | 456706   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.02     |
|    n_updates        | 104176   |
----------------------------------
Eval num_timesteps=457000, episode_reward=385.10 +/- 186.69
Episode length: 96.62 +/- 46.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 385      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 457000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.81     |
|    n_updates        | 104249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.1     |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5836     |
|    fps              | 116      |
|    time_elapsed     | 3937     |
|    total_timesteps  | 457003   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.324    |
|    n_updates        | 104250   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5840     |
|    fps              | 116      |
|    time_elapsed     | 3937     |
|    total_timesteps  | 457374   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.96     |
|    n_updates        | 104343   |
----------------------------------
Eval num_timesteps=457500, episode_reward=355.90 +/- 135.93
Episode length: 89.30 +/- 33.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.3     |
|    mean_reward      | 356      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 457500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.35     |
|    n_updates        | 104374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5844     |
|    fps              | 116      |
|    time_elapsed     | 3944     |
|    total_timesteps  | 457732   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.281    |
|    n_updates        | 104432   |
----------------------------------
Eval num_timesteps=458000, episode_reward=248.26 +/- 111.01
Episode length: 62.42 +/- 27.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.4     |
|    mean_reward      | 248      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 458000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.64     |
|    n_updates        | 104499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5848     |
|    fps              | 116      |
|    time_elapsed     | 3948     |
|    total_timesteps  | 458132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.47     |
|    n_updates        | 104532   |
----------------------------------
Eval num_timesteps=458500, episode_reward=348.80 +/- 154.09
Episode length: 87.64 +/- 38.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 458500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 104624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.5     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5852     |
|    fps              | 115      |
|    time_elapsed     | 3954     |
|    total_timesteps  | 458560   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.553    |
|    n_updates        | 104639   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.1     |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5856     |
|    fps              | 115      |
|    time_elapsed     | 3955     |
|    total_timesteps  | 458818   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.892    |
|    n_updates        | 104704   |
----------------------------------
Eval num_timesteps=459000, episode_reward=312.34 +/- 200.33
Episode length: 78.46 +/- 50.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.5     |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 459000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.86     |
|    n_updates        | 104749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.4     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5860     |
|    fps              | 115      |
|    time_elapsed     | 3960     |
|    total_timesteps  | 459110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.27     |
|    n_updates        | 104777   |
----------------------------------
Eval num_timesteps=459500, episode_reward=345.80 +/- 140.09
Episode length: 86.82 +/- 34.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.8     |
|    mean_reward      | 346      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 459500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 104874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5864     |
|    fps              | 115      |
|    time_elapsed     | 3966     |
|    total_timesteps  | 459511   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.14     |
|    n_updates        | 104877   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5868     |
|    fps              | 115      |
|    time_elapsed     | 3967     |
|    total_timesteps  | 459898   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.07     |
|    n_updates        | 104974   |
----------------------------------
Eval num_timesteps=460000, episode_reward=319.52 +/- 127.42
Episode length: 80.26 +/- 31.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.3     |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 460000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.319    |
|    n_updates        | 104999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.4     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5872     |
|    fps              | 115      |
|    time_elapsed     | 3972     |
|    total_timesteps  | 460134   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.244    |
|    n_updates        | 105033   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5876     |
|    fps              | 115      |
|    time_elapsed     | 3973     |
|    total_timesteps  | 460476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.871    |
|    n_updates        | 105118   |
----------------------------------
Eval num_timesteps=460500, episode_reward=318.30 +/- 162.58
Episode length: 79.94 +/- 40.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.9     |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 460500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.513    |
|    n_updates        | 105124   |
----------------------------------
Eval num_timesteps=461000, episode_reward=286.54 +/- 121.68
Episode length: 72.00 +/- 30.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72       |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 461000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.6      |
|    n_updates        | 105249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5880     |
|    fps              | 115      |
|    time_elapsed     | 3983     |
|    total_timesteps  | 461107   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.49     |
|    n_updates        | 105276   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5884     |
|    fps              | 115      |
|    time_elapsed     | 3984     |
|    total_timesteps  | 461391   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.61     |
|    n_updates        | 105347   |
----------------------------------
Eval num_timesteps=461500, episode_reward=279.60 +/- 137.37
Episode length: 70.30 +/- 34.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.3     |
|    mean_reward      | 280      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 461500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.21     |
|    n_updates        | 105374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.6     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5888     |
|    fps              | 115      |
|    time_elapsed     | 3989     |
|    total_timesteps  | 461809   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.358    |
|    n_updates        | 105452   |
----------------------------------
Eval num_timesteps=462000, episode_reward=239.58 +/- 75.70
Episode length: 60.28 +/- 18.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.3     |
|    mean_reward      | 240      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 462000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.345    |
|    n_updates        | 105499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5892     |
|    fps              | 115      |
|    time_elapsed     | 3993     |
|    total_timesteps  | 462290   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.81     |
|    n_updates        | 105572   |
----------------------------------
Eval num_timesteps=462500, episode_reward=253.98 +/- 96.22
Episode length: 63.88 +/- 24.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.9     |
|    mean_reward      | 254      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 462500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.854    |
|    n_updates        | 105624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5896     |
|    fps              | 115      |
|    time_elapsed     | 3998     |
|    total_timesteps  | 462536   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.46     |
|    n_updates        | 105633   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5900     |
|    fps              | 115      |
|    time_elapsed     | 3998     |
|    total_timesteps  | 462732   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.24     |
|    n_updates        | 105682   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.9     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5904     |
|    fps              | 115      |
|    time_elapsed     | 3998     |
|    total_timesteps  | 462940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.22     |
|    n_updates        | 105734   |
----------------------------------
Eval num_timesteps=463000, episode_reward=355.08 +/- 194.65
Episode length: 89.10 +/- 48.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.1     |
|    mean_reward      | 355      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 463000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.5      |
|    n_updates        | 105749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5908     |
|    fps              | 115      |
|    time_elapsed     | 4004     |
|    total_timesteps  | 463264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.28     |
|    n_updates        | 105815   |
----------------------------------
Eval num_timesteps=463500, episode_reward=355.76 +/- 153.46
Episode length: 89.26 +/- 38.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.3     |
|    mean_reward      | 356      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 463500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.03     |
|    n_updates        | 105874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5912     |
|    fps              | 115      |
|    time_elapsed     | 4010     |
|    total_timesteps  | 463589   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.979    |
|    n_updates        | 105897   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5916     |
|    fps              | 115      |
|    time_elapsed     | 4011     |
|    total_timesteps  | 463826   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.4      |
|    n_updates        | 105956   |
----------------------------------
Eval num_timesteps=464000, episode_reward=289.32 +/- 111.83
Episode length: 72.74 +/- 27.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.7     |
|    mean_reward      | 289      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 464000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 105999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5920     |
|    fps              | 115      |
|    time_elapsed     | 4016     |
|    total_timesteps  | 464106   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.1      |
|    n_updates        | 106026   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5924     |
|    fps              | 115      |
|    time_elapsed     | 4016     |
|    total_timesteps  | 464362   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.34     |
|    n_updates        | 106090   |
----------------------------------
Eval num_timesteps=464500, episode_reward=261.76 +/- 131.30
Episode length: 65.80 +/- 32.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.8     |
|    mean_reward      | 262      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 464500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.32     |
|    n_updates        | 106124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5928     |
|    fps              | 115      |
|    time_elapsed     | 4021     |
|    total_timesteps  | 464695   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.17     |
|    n_updates        | 106173   |
----------------------------------
Eval num_timesteps=465000, episode_reward=328.12 +/- 114.60
Episode length: 82.42 +/- 28.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.4     |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 465000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.39     |
|    n_updates        | 106249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5932     |
|    fps              | 115      |
|    time_elapsed     | 4027     |
|    total_timesteps  | 465032   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.71     |
|    n_updates        | 106257   |
----------------------------------
Eval num_timesteps=465500, episode_reward=288.34 +/- 117.40
Episode length: 72.50 +/- 29.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.5     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 465500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.41     |
|    n_updates        | 106374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5936     |
|    fps              | 115      |
|    time_elapsed     | 4033     |
|    total_timesteps  | 465512   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.03     |
|    n_updates        | 106377   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5940     |
|    fps              | 115      |
|    time_elapsed     | 4034     |
|    total_timesteps  | 465918   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.29     |
|    n_updates        | 106479   |
----------------------------------
Eval num_timesteps=466000, episode_reward=225.54 +/- 81.93
Episode length: 56.82 +/- 20.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.8     |
|    mean_reward      | 226      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 466000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.1      |
|    n_updates        | 106499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5944     |
|    fps              | 115      |
|    time_elapsed     | 4038     |
|    total_timesteps  | 466230   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.96     |
|    n_updates        | 106557   |
----------------------------------
Eval num_timesteps=466500, episode_reward=335.84 +/- 168.38
Episode length: 84.34 +/- 42.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.3     |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 466500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.31     |
|    n_updates        | 106624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.6     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5948     |
|    fps              | 115      |
|    time_elapsed     | 4044     |
|    total_timesteps  | 466793   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.28     |
|    n_updates        | 106698   |
----------------------------------
Eval num_timesteps=467000, episode_reward=255.64 +/- 114.74
Episode length: 64.30 +/- 28.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.3     |
|    mean_reward      | 256      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 467000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.77     |
|    n_updates        | 106749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5952     |
|    fps              | 115      |
|    time_elapsed     | 4048     |
|    total_timesteps  | 467078   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.95     |
|    n_updates        | 106769   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5956     |
|    fps              | 115      |
|    time_elapsed     | 4049     |
|    total_timesteps  | 467393   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 106848   |
----------------------------------
Eval num_timesteps=467500, episode_reward=278.98 +/- 143.43
Episode length: 70.14 +/- 35.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.1     |
|    mean_reward      | 279      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 467500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.748    |
|    n_updates        | 106874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.1     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5960     |
|    fps              | 115      |
|    time_elapsed     | 4054     |
|    total_timesteps  | 467724   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.39     |
|    n_updates        | 106930   |
----------------------------------
Eval num_timesteps=468000, episode_reward=258.42 +/- 129.51
Episode length: 64.92 +/- 32.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.9     |
|    mean_reward      | 258      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 468000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 106999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.6     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5964     |
|    fps              | 115      |
|    time_elapsed     | 4059     |
|    total_timesteps  | 468068   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.13     |
|    n_updates        | 107016   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5968     |
|    fps              | 115      |
|    time_elapsed     | 4060     |
|    total_timesteps  | 468450   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.76     |
|    n_updates        | 107112   |
----------------------------------
Eval num_timesteps=468500, episode_reward=331.26 +/- 176.56
Episode length: 83.16 +/- 44.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.2     |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 468500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.994    |
|    n_updates        | 107124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5972     |
|    fps              | 115      |
|    time_elapsed     | 4065     |
|    total_timesteps  | 468890   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.54     |
|    n_updates        | 107222   |
----------------------------------
Eval num_timesteps=469000, episode_reward=269.46 +/- 126.55
Episode length: 67.66 +/- 31.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.7     |
|    mean_reward      | 269      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 469000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.466    |
|    n_updates        | 107249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.1     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5976     |
|    fps              | 115      |
|    time_elapsed     | 4070     |
|    total_timesteps  | 469185   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.74     |
|    n_updates        | 107296   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5980     |
|    fps              | 115      |
|    time_elapsed     | 4071     |
|    total_timesteps  | 469406   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.401    |
|    n_updates        | 107351   |
----------------------------------
Eval num_timesteps=469500, episode_reward=258.10 +/- 109.38
Episode length: 64.90 +/- 27.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.9     |
|    mean_reward      | 258      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 469500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.36     |
|    n_updates        | 107374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5984     |
|    fps              | 115      |
|    time_elapsed     | 4075     |
|    total_timesteps  | 469682   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.7      |
|    n_updates        | 107420   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5988     |
|    fps              | 115      |
|    time_elapsed     | 4075     |
|    total_timesteps  | 469917   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.48     |
|    n_updates        | 107479   |
----------------------------------
Eval num_timesteps=470000, episode_reward=236.54 +/- 111.03
Episode length: 59.56 +/- 27.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.6     |
|    mean_reward      | 237      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 470000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.251    |
|    n_updates        | 107499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5992     |
|    fps              | 115      |
|    time_elapsed     | 4079     |
|    total_timesteps  | 470163   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.56     |
|    n_updates        | 107540   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5996     |
|    fps              | 115      |
|    time_elapsed     | 4079     |
|    total_timesteps  | 470383   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.373    |
|    n_updates        | 107595   |
----------------------------------
Eval num_timesteps=470500, episode_reward=313.90 +/- 151.38
Episode length: 78.84 +/- 37.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.8     |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 470500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.551    |
|    n_updates        | 107624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6000     |
|    fps              | 115      |
|    time_elapsed     | 4084     |
|    total_timesteps  | 470690   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.14     |
|    n_updates        | 107672   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6004     |
|    fps              | 115      |
|    time_elapsed     | 4085     |
|    total_timesteps  | 470989   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.36     |
|    n_updates        | 107747   |
----------------------------------
Eval num_timesteps=471000, episode_reward=352.72 +/- 167.83
Episode length: 88.60 +/- 41.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.6     |
|    mean_reward      | 353      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 471000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.76     |
|    n_updates        | 107749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6008     |
|    fps              | 115      |
|    time_elapsed     | 4091     |
|    total_timesteps  | 471380   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.12     |
|    n_updates        | 107844   |
----------------------------------
Eval num_timesteps=471500, episode_reward=316.60 +/- 155.85
Episode length: 79.56 +/- 38.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.6     |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 471500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.87     |
|    n_updates        | 107874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6012     |
|    fps              | 115      |
|    time_elapsed     | 4096     |
|    total_timesteps  | 471629   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.347    |
|    n_updates        | 107907   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.8     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6016     |
|    fps              | 115      |
|    time_elapsed     | 4096     |
|    total_timesteps  | 471905   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.1      |
|    n_updates        | 107976   |
----------------------------------
Eval num_timesteps=472000, episode_reward=364.40 +/- 124.74
Episode length: 91.52 +/- 31.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 364      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 472000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.98     |
|    n_updates        | 107999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6020     |
|    fps              | 115      |
|    time_elapsed     | 4102     |
|    total_timesteps  | 472224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.27     |
|    n_updates        | 108055   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6024     |
|    fps              | 115      |
|    time_elapsed     | 4103     |
|    total_timesteps  | 472468   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 108116   |
----------------------------------
Eval num_timesteps=472500, episode_reward=244.78 +/- 131.69
Episode length: 61.54 +/- 32.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.5     |
|    mean_reward      | 245      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 472500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.06     |
|    n_updates        | 108124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6028     |
|    fps              | 115      |
|    time_elapsed     | 4107     |
|    total_timesteps  | 472849   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.59     |
|    n_updates        | 108212   |
----------------------------------
Eval num_timesteps=473000, episode_reward=283.38 +/- 131.62
Episode length: 71.22 +/- 32.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.2     |
|    mean_reward      | 283      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 473000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.12     |
|    n_updates        | 108249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6032     |
|    fps              | 115      |
|    time_elapsed     | 4111     |
|    total_timesteps  | 473036   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.855    |
|    n_updates        | 108258   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6036     |
|    fps              | 115      |
|    time_elapsed     | 4112     |
|    total_timesteps  | 473297   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.88     |
|    n_updates        | 108324   |
----------------------------------
Eval num_timesteps=473500, episode_reward=250.82 +/- 90.36
Episode length: 63.08 +/- 22.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.1     |
|    mean_reward      | 251      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 473500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.22     |
|    n_updates        | 108374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6040     |
|    fps              | 115      |
|    time_elapsed     | 4116     |
|    total_timesteps  | 473517   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.23     |
|    n_updates        | 108379   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6044     |
|    fps              | 115      |
|    time_elapsed     | 4116     |
|    total_timesteps  | 473805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.01     |
|    n_updates        | 108451   |
----------------------------------
Eval num_timesteps=474000, episode_reward=322.94 +/- 150.87
Episode length: 81.10 +/- 37.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.1     |
|    mean_reward      | 323      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 474000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.84     |
|    n_updates        | 108499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6048     |
|    fps              | 115      |
|    time_elapsed     | 4122     |
|    total_timesteps  | 474089   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.6      |
|    n_updates        | 108522   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6052     |
|    fps              | 115      |
|    time_elapsed     | 4122     |
|    total_timesteps  | 474317   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.606    |
|    n_updates        | 108579   |
----------------------------------
Eval num_timesteps=474500, episode_reward=337.02 +/- 175.71
Episode length: 84.62 +/- 43.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.6     |
|    mean_reward      | 337      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 474500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.684    |
|    n_updates        | 108624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6056     |
|    fps              | 114      |
|    time_elapsed     | 4128     |
|    total_timesteps  | 474741   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.26     |
|    n_updates        | 108685   |
----------------------------------
Eval num_timesteps=475000, episode_reward=266.14 +/- 117.65
Episode length: 66.88 +/- 29.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.9     |
|    mean_reward      | 266      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 475000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.14     |
|    n_updates        | 108749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6060     |
|    fps              | 114      |
|    time_elapsed     | 4133     |
|    total_timesteps  | 475169   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.34     |
|    n_updates        | 108792   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.4     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6064     |
|    fps              | 114      |
|    time_elapsed     | 4133     |
|    total_timesteps  | 475406   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.47     |
|    n_updates        | 108851   |
----------------------------------
Eval num_timesteps=475500, episode_reward=314.88 +/- 167.84
Episode length: 79.08 +/- 41.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.1     |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 475500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.6      |
|    n_updates        | 108874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6068     |
|    fps              | 114      |
|    time_elapsed     | 4139     |
|    total_timesteps  | 475744   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.02     |
|    n_updates        | 108935   |
----------------------------------
Eval num_timesteps=476000, episode_reward=297.34 +/- 140.60
Episode length: 74.72 +/- 35.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.7     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 476000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 108999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.4     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6072     |
|    fps              | 114      |
|    time_elapsed     | 4144     |
|    total_timesteps  | 476029   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.29     |
|    n_updates        | 109007   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.9     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6076     |
|    fps              | 114      |
|    time_elapsed     | 4144     |
|    total_timesteps  | 476272   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.2      |
|    n_updates        | 109067   |
----------------------------------
Eval num_timesteps=476500, episode_reward=346.78 +/- 200.25
Episode length: 87.10 +/- 50.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.1     |
|    mean_reward      | 347      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 476500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.93     |
|    n_updates        | 109124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.7     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6080     |
|    fps              | 114      |
|    time_elapsed     | 4150     |
|    total_timesteps  | 476677   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.49     |
|    n_updates        | 109169   |
----------------------------------
Eval num_timesteps=477000, episode_reward=277.16 +/- 106.08
Episode length: 69.68 +/- 26.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.7     |
|    mean_reward      | 277      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 477000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.02     |
|    n_updates        | 109249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6084     |
|    fps              | 114      |
|    time_elapsed     | 4155     |
|    total_timesteps  | 477028   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.48     |
|    n_updates        | 109256   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6088     |
|    fps              | 114      |
|    time_elapsed     | 4156     |
|    total_timesteps  | 477400   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.36     |
|    n_updates        | 109349   |
----------------------------------
Eval num_timesteps=477500, episode_reward=239.78 +/- 123.83
Episode length: 60.36 +/- 30.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.4     |
|    mean_reward      | 240      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 477500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.8      |
|    n_updates        | 109374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6092     |
|    fps              | 114      |
|    time_elapsed     | 4159     |
|    total_timesteps  | 477622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.67     |
|    n_updates        | 109405   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6096     |
|    fps              | 114      |
|    time_elapsed     | 4160     |
|    total_timesteps  | 477976   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.32     |
|    n_updates        | 109493   |
----------------------------------
Eval num_timesteps=478000, episode_reward=245.98 +/- 112.39
Episode length: 61.92 +/- 28.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.9     |
|    mean_reward      | 246      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 478000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.2      |
|    n_updates        | 109499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6100     |
|    fps              | 114      |
|    time_elapsed     | 4164     |
|    total_timesteps  | 478243   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.56     |
|    n_updates        | 109560   |
----------------------------------
Eval num_timesteps=478500, episode_reward=318.60 +/- 161.62
Episode length: 79.98 +/- 40.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80       |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 478500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.9      |
|    n_updates        | 109624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6104     |
|    fps              | 114      |
|    time_elapsed     | 4170     |
|    total_timesteps  | 478586   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.696    |
|    n_updates        | 109646   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6108     |
|    fps              | 114      |
|    time_elapsed     | 4170     |
|    total_timesteps  | 478900   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.69     |
|    n_updates        | 109724   |
----------------------------------
Eval num_timesteps=479000, episode_reward=367.50 +/- 150.66
Episode length: 92.24 +/- 37.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.2     |
|    mean_reward      | 368      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 479000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.15     |
|    n_updates        | 109749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6112     |
|    fps              | 114      |
|    time_elapsed     | 4176     |
|    total_timesteps  | 479135   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.15     |
|    n_updates        | 109783   |
----------------------------------
Eval num_timesteps=479500, episode_reward=347.38 +/- 159.27
Episode length: 87.24 +/- 39.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.2     |
|    mean_reward      | 347      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 479500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.807    |
|    n_updates        | 109874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6116     |
|    fps              | 114      |
|    time_elapsed     | 4182     |
|    total_timesteps  | 479648   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.294    |
|    n_updates        | 109911   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6120     |
|    fps              | 114      |
|    time_elapsed     | 4183     |
|    total_timesteps  | 479998   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.94     |
|    n_updates        | 109999   |
----------------------------------
Eval num_timesteps=480000, episode_reward=374.50 +/- 205.22
Episode length: 94.02 +/- 51.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 374      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 480000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6124     |
|    fps              | 114      |
|    time_elapsed     | 4189     |
|    total_timesteps  | 480385   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.11     |
|    n_updates        | 110096   |
----------------------------------
Eval num_timesteps=480500, episode_reward=226.64 +/- 96.95
Episode length: 57.10 +/- 24.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.1     |
|    mean_reward      | 227      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 480500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.98     |
|    n_updates        | 110124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6128     |
|    fps              | 114      |
|    time_elapsed     | 4192     |
|    total_timesteps  | 480798   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.891    |
|    n_updates        | 110199   |
----------------------------------
Eval num_timesteps=481000, episode_reward=251.84 +/- 113.43
Episode length: 63.32 +/- 28.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.3     |
|    mean_reward      | 252      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 481000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 110249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6132     |
|    fps              | 114      |
|    time_elapsed     | 4195     |
|    total_timesteps  | 481208   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.54     |
|    n_updates        | 110301   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6136     |
|    fps              | 114      |
|    time_elapsed     | 4196     |
|    total_timesteps  | 481468   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.558    |
|    n_updates        | 110366   |
----------------------------------
Eval num_timesteps=481500, episode_reward=284.82 +/- 117.32
Episode length: 71.52 +/- 29.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.5     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 481500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.643    |
|    n_updates        | 110374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6140     |
|    fps              | 114      |
|    time_elapsed     | 4201     |
|    total_timesteps  | 481725   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.42     |
|    n_updates        | 110431   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6144     |
|    fps              | 114      |
|    time_elapsed     | 4201     |
|    total_timesteps  | 481955   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.76     |
|    n_updates        | 110488   |
----------------------------------
Eval num_timesteps=482000, episode_reward=297.64 +/- 127.54
Episode length: 74.82 +/- 31.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.8     |
|    mean_reward      | 298      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 482000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.19     |
|    n_updates        | 110499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6148     |
|    fps              | 114      |
|    time_elapsed     | 4206     |
|    total_timesteps  | 482231   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.834    |
|    n_updates        | 110557   |
----------------------------------
Eval num_timesteps=482500, episode_reward=362.62 +/- 174.04
Episode length: 91.10 +/- 43.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.1     |
|    mean_reward      | 363      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 482500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.89     |
|    n_updates        | 110624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6152     |
|    fps              | 114      |
|    time_elapsed     | 4213     |
|    total_timesteps  | 482537   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.36     |
|    n_updates        | 110634   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6156     |
|    fps              | 114      |
|    time_elapsed     | 4213     |
|    total_timesteps  | 482766   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.836    |
|    n_updates        | 110691   |
----------------------------------
Eval num_timesteps=483000, episode_reward=357.22 +/- 201.81
Episode length: 89.66 +/- 50.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.7     |
|    mean_reward      | 357      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 483000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.09     |
|    n_updates        | 110749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6160     |
|    fps              | 114      |
|    time_elapsed     | 4219     |
|    total_timesteps  | 483134   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.863    |
|    n_updates        | 110783   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6164     |
|    fps              | 114      |
|    time_elapsed     | 4220     |
|    total_timesteps  | 483363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 110840   |
----------------------------------
Eval num_timesteps=483500, episode_reward=219.18 +/- 90.53
Episode length: 55.10 +/- 22.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.1     |
|    mean_reward      | 219      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 483500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.95     |
|    n_updates        | 110874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6168     |
|    fps              | 114      |
|    time_elapsed     | 4224     |
|    total_timesteps  | 483700   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.14     |
|    n_updates        | 110924   |
----------------------------------
Eval num_timesteps=484000, episode_reward=266.78 +/- 93.13
Episode length: 67.12 +/- 23.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.1     |
|    mean_reward      | 267      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 484000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.94     |
|    n_updates        | 110999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6172     |
|    fps              | 114      |
|    time_elapsed     | 4229     |
|    total_timesteps  | 484097   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.73     |
|    n_updates        | 111024   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6176     |
|    fps              | 114      |
|    time_elapsed     | 4230     |
|    total_timesteps  | 484413   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.44     |
|    n_updates        | 111103   |
----------------------------------
Eval num_timesteps=484500, episode_reward=238.10 +/- 96.30
Episode length: 59.96 +/- 24.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60       |
|    mean_reward      | 238      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 484500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.638    |
|    n_updates        | 111124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6180     |
|    fps              | 114      |
|    time_elapsed     | 4234     |
|    total_timesteps  | 484713   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.586    |
|    n_updates        | 111178   |
----------------------------------
Eval num_timesteps=485000, episode_reward=393.28 +/- 153.89
Episode length: 98.70 +/- 38.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 393      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 485000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.2      |
|    n_updates        | 111249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6184     |
|    fps              | 114      |
|    time_elapsed     | 4241     |
|    total_timesteps  | 485090   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.992    |
|    n_updates        | 111272   |
----------------------------------
Eval num_timesteps=485500, episode_reward=291.76 +/- 121.08
Episode length: 73.30 +/- 30.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.3     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 485500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.414    |
|    n_updates        | 111374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6188     |
|    fps              | 114      |
|    time_elapsed     | 4246     |
|    total_timesteps  | 485505   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.13     |
|    n_updates        | 111376   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6192     |
|    fps              | 114      |
|    time_elapsed     | 4247     |
|    total_timesteps  | 485797   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.93     |
|    n_updates        | 111449   |
----------------------------------
Eval num_timesteps=486000, episode_reward=369.70 +/- 192.40
Episode length: 92.80 +/- 48.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 370      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 486000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.6      |
|    n_updates        | 111499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6196     |
|    fps              | 114      |
|    time_elapsed     | 4253     |
|    total_timesteps  | 486115   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.382    |
|    n_updates        | 111528   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6200     |
|    fps              | 114      |
|    time_elapsed     | 4253     |
|    total_timesteps  | 486458   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.34     |
|    n_updates        | 111614   |
----------------------------------
Eval num_timesteps=486500, episode_reward=298.12 +/- 119.38
Episode length: 74.86 +/- 29.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.9     |
|    mean_reward      | 298      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 486500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.504    |
|    n_updates        | 111624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.1     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6204     |
|    fps              | 114      |
|    time_elapsed     | 4259     |
|    total_timesteps  | 486997   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.461    |
|    n_updates        | 111749   |
----------------------------------
Eval num_timesteps=487000, episode_reward=270.62 +/- 125.88
Episode length: 68.02 +/- 31.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | 271      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 487000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6208     |
|    fps              | 114      |
|    time_elapsed     | 4264     |
|    total_timesteps  | 487374   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.72     |
|    n_updates        | 111843   |
----------------------------------
Eval num_timesteps=487500, episode_reward=264.28 +/- 106.99
Episode length: 66.44 +/- 26.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.4     |
|    mean_reward      | 264      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 487500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.32     |
|    n_updates        | 111874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6212     |
|    fps              | 114      |
|    time_elapsed     | 4268     |
|    total_timesteps  | 487673   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.58     |
|    n_updates        | 111918   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6216     |
|    fps              | 114      |
|    time_elapsed     | 4269     |
|    total_timesteps  | 487964   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.22     |
|    n_updates        | 111990   |
----------------------------------
Eval num_timesteps=488000, episode_reward=268.68 +/- 115.10
Episode length: 67.58 +/- 28.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.6     |
|    mean_reward      | 269      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 488000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.82     |
|    n_updates        | 111999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6220     |
|    fps              | 114      |
|    time_elapsed     | 4274     |
|    total_timesteps  | 488205   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.07     |
|    n_updates        | 112051   |
----------------------------------
Eval num_timesteps=488500, episode_reward=423.84 +/- 152.53
Episode length: 106.36 +/- 38.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 424      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 488500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.713    |
|    n_updates        | 112124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6224     |
|    fps              | 114      |
|    time_elapsed     | 4281     |
|    total_timesteps  | 488589   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.499    |
|    n_updates        | 112147   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6228     |
|    fps              | 114      |
|    time_elapsed     | 4282     |
|    total_timesteps  | 488925   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.08     |
|    n_updates        | 112231   |
----------------------------------
Eval num_timesteps=489000, episode_reward=354.72 +/- 168.48
Episode length: 89.08 +/- 42.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.1     |
|    mean_reward      | 355      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 489000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.99     |
|    n_updates        | 112249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6232     |
|    fps              | 114      |
|    time_elapsed     | 4287     |
|    total_timesteps  | 489265   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.25     |
|    n_updates        | 112316   |
----------------------------------
Eval num_timesteps=489500, episode_reward=375.18 +/- 159.20
Episode length: 94.24 +/- 39.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.2     |
|    mean_reward      | 375      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 489500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.373    |
|    n_updates        | 112374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6236     |
|    fps              | 114      |
|    time_elapsed     | 4294     |
|    total_timesteps  | 489619   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.96     |
|    n_updates        | 112404   |
----------------------------------
Eval num_timesteps=490000, episode_reward=359.02 +/- 122.59
Episode length: 90.14 +/- 30.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.1     |
|    mean_reward      | 359      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 490000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.89     |
|    n_updates        | 112499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6240     |
|    fps              | 113      |
|    time_elapsed     | 4300     |
|    total_timesteps  | 490050   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.759    |
|    n_updates        | 112512   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6244     |
|    fps              | 113      |
|    time_elapsed     | 4301     |
|    total_timesteps  | 490373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.418    |
|    n_updates        | 112593   |
----------------------------------
Eval num_timesteps=490500, episode_reward=303.74 +/- 136.51
Episode length: 76.30 +/- 34.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.3     |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 490500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.52     |
|    n_updates        | 112624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6248     |
|    fps              | 113      |
|    time_elapsed     | 4306     |
|    total_timesteps  | 490630   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.32     |
|    n_updates        | 112657   |
----------------------------------
Eval num_timesteps=491000, episode_reward=368.40 +/- 169.51
Episode length: 92.42 +/- 42.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.4     |
|    mean_reward      | 368      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 491000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.924    |
|    n_updates        | 112749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6252     |
|    fps              | 113      |
|    time_elapsed     | 4313     |
|    total_timesteps  | 491007   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.816    |
|    n_updates        | 112751   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6256     |
|    fps              | 113      |
|    time_elapsed     | 4313     |
|    total_timesteps  | 491196   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.753    |
|    n_updates        | 112798   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6260     |
|    fps              | 113      |
|    time_elapsed     | 4313     |
|    total_timesteps  | 491476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.22     |
|    n_updates        | 112868   |
----------------------------------
Eval num_timesteps=491500, episode_reward=308.96 +/- 122.69
Episode length: 77.62 +/- 30.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.6     |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 491500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.929    |
|    n_updates        | 112874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6264     |
|    fps              | 113      |
|    time_elapsed     | 4319     |
|    total_timesteps  | 491880   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.574    |
|    n_updates        | 112969   |
----------------------------------
Eval num_timesteps=492000, episode_reward=258.34 +/- 99.86
Episode length: 64.96 +/- 24.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65       |
|    mean_reward      | 258      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 492000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.41     |
|    n_updates        | 112999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6268     |
|    fps              | 113      |
|    time_elapsed     | 4324     |
|    total_timesteps  | 492191   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.94     |
|    n_updates        | 113047   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6272     |
|    fps              | 113      |
|    time_elapsed     | 4324     |
|    total_timesteps  | 492412   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.99     |
|    n_updates        | 113102   |
----------------------------------
Eval num_timesteps=492500, episode_reward=318.32 +/- 166.83
Episode length: 79.94 +/- 41.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.9     |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 492500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.745    |
|    n_updates        | 113124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6276     |
|    fps              | 113      |
|    time_elapsed     | 4330     |
|    total_timesteps  | 492838   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.51     |
|    n_updates        | 113209   |
----------------------------------
Eval num_timesteps=493000, episode_reward=319.92 +/- 175.58
Episode length: 80.34 +/- 43.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.3     |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 493000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.63     |
|    n_updates        | 113249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.6     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6280     |
|    fps              | 113      |
|    time_elapsed     | 4336     |
|    total_timesteps  | 493170   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.23     |
|    n_updates        | 113292   |
----------------------------------
Eval num_timesteps=493500, episode_reward=228.84 +/- 111.85
Episode length: 57.62 +/- 27.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | 229      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 493500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.37     |
|    n_updates        | 113374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6284     |
|    fps              | 113      |
|    time_elapsed     | 4340     |
|    total_timesteps  | 493543   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.22     |
|    n_updates        | 113385   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6288     |
|    fps              | 113      |
|    time_elapsed     | 4341     |
|    total_timesteps  | 493928   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.15     |
|    n_updates        | 113481   |
----------------------------------
Eval num_timesteps=494000, episode_reward=318.96 +/- 158.64
Episode length: 80.14 +/- 39.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.1     |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 494000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.05     |
|    n_updates        | 113499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6292     |
|    fps              | 113      |
|    time_elapsed     | 4346     |
|    total_timesteps  | 494292   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.13     |
|    n_updates        | 113572   |
----------------------------------
Eval num_timesteps=494500, episode_reward=309.60 +/- 158.62
Episode length: 77.72 +/- 39.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 494500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.42     |
|    n_updates        | 113624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.3     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6296     |
|    fps              | 113      |
|    time_elapsed     | 4352     |
|    total_timesteps  | 494641   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.94     |
|    n_updates        | 113660   |
----------------------------------
Eval num_timesteps=495000, episode_reward=191.42 +/- 51.19
Episode length: 48.28 +/- 12.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 495000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.338    |
|    n_updates        | 113749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6300     |
|    fps              | 113      |
|    time_elapsed     | 4355     |
|    total_timesteps  | 495031   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.442    |
|    n_updates        | 113757   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6304     |
|    fps              | 113      |
|    time_elapsed     | 4356     |
|    total_timesteps  | 495310   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.19     |
|    n_updates        | 113827   |
----------------------------------
Eval num_timesteps=495500, episode_reward=239.86 +/- 135.40
Episode length: 60.38 +/- 33.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.4     |
|    mean_reward      | 240      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 495500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 6.46     |
|    n_updates        | 113874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6308     |
|    fps              | 113      |
|    time_elapsed     | 4360     |
|    total_timesteps  | 495622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 5.41     |
|    n_updates        | 113905   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6312     |
|    fps              | 113      |
|    time_elapsed     | 4361     |
|    total_timesteps  | 495868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.54     |
|    n_updates        | 113966   |
----------------------------------
Eval num_timesteps=496000, episode_reward=210.26 +/- 60.92
Episode length: 52.94 +/- 15.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | 210      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 496000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.01     |
|    n_updates        | 113999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6316     |
|    fps              | 113      |
|    time_elapsed     | 4365     |
|    total_timesteps  | 496246   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.18     |
|    n_updates        | 114061   |
----------------------------------
Eval num_timesteps=496500, episode_reward=359.34 +/- 148.10
Episode length: 90.24 +/- 36.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.2     |
|    mean_reward      | 359      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 496500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.33     |
|    n_updates        | 114124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6320     |
|    fps              | 113      |
|    time_elapsed     | 4371     |
|    total_timesteps  | 496637   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.233    |
|    n_updates        | 114159   |
----------------------------------
Eval num_timesteps=497000, episode_reward=303.76 +/- 162.39
Episode length: 76.34 +/- 40.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.3     |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 497000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.664    |
|    n_updates        | 114249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6324     |
|    fps              | 113      |
|    time_elapsed     | 4376     |
|    total_timesteps  | 497019   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.282    |
|    n_updates        | 114254   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 334      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6328     |
|    fps              | 113      |
|    time_elapsed     | 4377     |
|    total_timesteps  | 497313   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.4      |
|    n_updates        | 114328   |
----------------------------------
Eval num_timesteps=497500, episode_reward=353.94 +/- 162.30
Episode length: 88.86 +/- 40.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.9     |
|    mean_reward      | 354      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 497500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.61     |
|    n_updates        | 114374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6332     |
|    fps              | 113      |
|    time_elapsed     | 4383     |
|    total_timesteps  | 497572   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.15     |
|    n_updates        | 114392   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6336     |
|    fps              | 113      |
|    time_elapsed     | 4383     |
|    total_timesteps  | 497883   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.04     |
|    n_updates        | 114470   |
----------------------------------
Eval num_timesteps=498000, episode_reward=347.20 +/- 163.34
Episode length: 87.20 +/- 40.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.2     |
|    mean_reward      | 347      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 498000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.26     |
|    n_updates        | 114499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6340     |
|    fps              | 113      |
|    time_elapsed     | 4390     |
|    total_timesteps  | 498137   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.16     |
|    n_updates        | 114534   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6344     |
|    fps              | 113      |
|    time_elapsed     | 4390     |
|    total_timesteps  | 498336   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.53     |
|    n_updates        | 114583   |
----------------------------------
Eval num_timesteps=498500, episode_reward=357.98 +/- 174.84
Episode length: 89.86 +/- 43.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.9     |
|    mean_reward      | 358      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 498500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.72     |
|    n_updates        | 114624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6348     |
|    fps              | 113      |
|    time_elapsed     | 4396     |
|    total_timesteps  | 498659   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.724    |
|    n_updates        | 114664   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6352     |
|    fps              | 113      |
|    time_elapsed     | 4397     |
|    total_timesteps  | 498875   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.384    |
|    n_updates        | 114718   |
----------------------------------
Eval num_timesteps=499000, episode_reward=356.66 +/- 209.39
Episode length: 89.50 +/- 52.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 357      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 499000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.17     |
|    n_updates        | 114749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6356     |
|    fps              | 113      |
|    time_elapsed     | 4403     |
|    total_timesteps  | 499269   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.2      |
|    n_updates        | 114817   |
----------------------------------
Eval num_timesteps=499500, episode_reward=316.84 +/- 161.51
Episode length: 79.60 +/- 40.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.6     |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 499500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.649    |
|    n_updates        | 114874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6360     |
|    fps              | 113      |
|    time_elapsed     | 4409     |
|    total_timesteps  | 499592   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.17     |
|    n_updates        | 114897   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6364     |
|    fps              | 113      |
|    time_elapsed     | 4410     |
|    total_timesteps  | 499856   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.97     |
|    n_updates        | 114963   |
----------------------------------
Eval num_timesteps=500000, episode_reward=333.08 +/- 167.01
Episode length: 83.68 +/- 41.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.7     |
|    mean_reward      | 333      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 500000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 2.34     |
|    n_updates        | 114999   |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/take-cover/dqn-6/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=164.60 +/- 44.54
Episode length: 41.48 +/- 11.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=164.74 +/- 45.48
Episode length: 41.54 +/- 11.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=160.60 +/- 39.03
Episode length: 40.46 +/- 9.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.5     |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=179.34 +/- 41.36
Episode length: 45.16 +/- 10.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.5     |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 95       |
|    iterations      | 1        |
|    time_elapsed    | 21       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=250.80 +/- 117.99
Episode length: 63.12 +/- 29.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 63.1         |
|    mean_reward          | 251          |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0076859808 |
|    clip_fraction        | 0.223        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.000318     |
|    learning_rate        | 0.0001       |
|    loss                 | 136          |
|    n_updates            | 10           |
|    policy_gradient_loss | 0.00428      |
|    value_loss           | 587          |
------------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=247.44 +/- 103.27
Episode length: 62.28 +/- 25.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | 247      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=225.76 +/- 109.82
Episode length: 56.88 +/- 27.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.9     |
|    mean_reward     | 226      |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=250.86 +/- 93.71
Episode length: 63.12 +/- 23.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.1     |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 82       |
|    iterations      | 2        |
|    time_elapsed    | 49       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=181.04 +/- 47.80
Episode length: 45.62 +/- 11.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.6        |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.003240291 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.0579      |
|    learning_rate        | 0.0001      |
|    loss                 | 281         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0022      |
|    value_loss           | 712         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=168.36 +/- 33.34
Episode length: 42.50 +/- 8.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=174.92 +/- 43.11
Episode length: 44.12 +/- 10.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=174.28 +/- 40.10
Episode length: 43.88 +/- 10.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.5     |
|    ep_rew_mean     | 289      |
| time/              |          |
|    fps             | 83       |
|    iterations      | 3        |
|    time_elapsed    | 74       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=330.54 +/- 232.02
Episode length: 83.04 +/- 58.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83          |
|    mean_reward          | 331         |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.010796776 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.0001      |
|    loss                 | 380         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.000277   |
|    value_loss           | 965         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=7000, episode_reward=271.26 +/- 134.25
Episode length: 68.20 +/- 33.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=370.66 +/- 209.13
Episode length: 93.04 +/- 52.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
New best mean reward!
Eval num_timesteps=8000, episode_reward=368.32 +/- 230.89
Episode length: 92.44 +/- 57.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 309      |
| time/              |          |
|    fps             | 73       |
|    iterations      | 4        |
|    time_elapsed    | 111      |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=197.42 +/- 69.18
Episode length: 49.70 +/- 17.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.7        |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.009230059 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.193       |
|    learning_rate        | 0.0001      |
|    loss                 | 515         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00216     |
|    value_loss           | 1.26e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=194.02 +/- 65.87
Episode length: 48.94 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=197.12 +/- 73.89
Episode length: 49.68 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=193.44 +/- 56.72
Episode length: 48.70 +/- 14.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.2     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 5        |
|    time_elapsed    | 137      |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=183.32 +/- 54.87
Episode length: 46.20 +/- 13.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.2        |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.012988598 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.11e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | 5.34e-05    |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=185.86 +/- 47.65
Episode length: 46.82 +/- 11.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=195.86 +/- 78.06
Episode length: 49.36 +/- 19.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=184.50 +/- 59.08
Episode length: 46.54 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 75       |
|    iterations      | 6        |
|    time_elapsed    | 162      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=183.76 +/- 76.92
Episode length: 46.34 +/- 19.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.3        |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.014947895 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0001      |
|    loss                 | 475         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=169.88 +/- 60.58
Episode length: 42.84 +/- 15.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=190.54 +/- 64.88
Episode length: 48.04 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=181.32 +/- 72.58
Episode length: 45.66 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 7        |
|    time_elapsed    | 187      |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=218.00 +/- 109.07
Episode length: 54.84 +/- 27.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.8        |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.006212441 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.0001      |
|    loss                 | 982         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00276     |
|    value_loss           | 2.11e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=204.02 +/- 95.71
Episode length: 51.42 +/- 23.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=231.10 +/- 123.17
Episode length: 58.16 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.2     |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=217.32 +/- 115.39
Episode length: 54.68 +/- 28.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 8        |
|    time_elapsed    | 214      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=179.24 +/- 57.13
Episode length: 45.22 +/- 14.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.2        |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.009764973 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.22e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00179     |
|    value_loss           | 2.25e+03    |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=177.56 +/- 52.37
Episode length: 44.70 +/- 13.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=165.72 +/- 43.66
Episode length: 41.80 +/- 10.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=183.18 +/- 51.49
Episode length: 46.12 +/- 12.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76       |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 9        |
|    time_elapsed    | 238      |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=176.34 +/- 48.24
Episode length: 44.46 +/- 12.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.5        |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.008677826 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0001      |
|    loss                 | 953         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.0074      |
|    value_loss           | 1.98e+03    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=177.80 +/- 50.25
Episode length: 44.84 +/- 12.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=174.66 +/- 46.03
Episode length: 43.96 +/- 11.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44       |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=165.08 +/- 43.05
Episode length: 41.72 +/- 10.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | 278      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 10       |
|    time_elapsed    | 262      |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=181.28 +/- 67.34
Episode length: 45.66 +/- 16.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.7         |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0101557905 |
|    clip_fraction        | 0.213        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.0001       |
|    loss                 | 937          |
|    n_updates            | 100          |
|    policy_gradient_loss | 0.00559      |
|    value_loss           | 1.57e+03     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=182.46 +/- 69.40
Episode length: 45.96 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=182.94 +/- 68.60
Episode length: 46.08 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=165.20 +/- 48.47
Episode length: 41.62 +/- 12.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=174.24 +/- 53.80
Episode length: 43.92 +/- 13.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 11       |
|    time_elapsed    | 290      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=187.64 +/- 52.25
Episode length: 47.24 +/- 13.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.009271625 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0001      |
|    loss                 | 690         |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00483     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=184.34 +/- 53.89
Episode length: 46.48 +/- 13.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=185.96 +/- 70.08
Episode length: 46.84 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=190.06 +/- 61.45
Episode length: 48.00 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | 283      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 12       |
|    time_elapsed    | 316      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=279.20 +/- 85.28
Episode length: 70.18 +/- 21.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.2        |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009848643 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.37        |
|    learning_rate        | 0.0001      |
|    loss                 | 843         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=257.88 +/- 59.52
Episode length: 64.88 +/- 14.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.9     |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=254.22 +/- 74.56
Episode length: 63.94 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.9     |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=254.90 +/- 66.64
Episode length: 64.06 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.1     |
|    mean_reward     | 255      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.8     |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 13       |
|    time_elapsed    | 347      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=194.68 +/- 67.28
Episode length: 49.02 +/- 16.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.021528475 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0001      |
|    loss                 | 756         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.00711     |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=193.32 +/- 58.31
Episode length: 48.70 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=181.36 +/- 58.22
Episode length: 45.74 +/- 14.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=190.16 +/- 71.79
Episode length: 47.88 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.3     |
|    ep_rew_mean     | 292      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 14       |
|    time_elapsed    | 372      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=186.34 +/- 78.13
Episode length: 46.98 +/- 19.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47          |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.010254841 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.0001      |
|    loss                 | 604         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.00528     |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=165.20 +/- 60.93
Episode length: 41.68 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=187.18 +/- 80.15
Episode length: 47.16 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=170.08 +/- 63.47
Episode length: 42.90 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.9     |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.8     |
|    ep_rew_mean     | 294      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 15       |
|    time_elapsed    | 395      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=174.70 +/- 61.35
Episode length: 44.08 +/- 15.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44.1         |
|    mean_reward          | 175          |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0039379643 |
|    clip_fraction        | 0.287        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.429        |
|    learning_rate        | 0.0001       |
|    loss                 | 621          |
|    n_updates            | 150          |
|    policy_gradient_loss | 0.01         |
|    value_loss           | 1.35e+03     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=166.58 +/- 62.76
Episode length: 42.02 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=165.08 +/- 61.06
Episode length: 41.76 +/- 15.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=165.22 +/- 56.60
Episode length: 41.70 +/- 14.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.5     |
|    ep_rew_mean     | 289      |
| time/              |          |
|    fps             | 78       |
|    iterations      | 16       |
|    time_elapsed    | 419      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=183.20 +/- 74.51
Episode length: 46.14 +/- 18.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.1        |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.019398201 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.306       |
|    learning_rate        | 0.0001      |
|    loss                 | 740         |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00205     |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=171.50 +/- 65.46
Episode length: 43.26 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.3     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=183.10 +/- 69.48
Episode length: 46.18 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=184.94 +/- 77.01
Episode length: 46.70 +/- 19.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.4     |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 78       |
|    iterations      | 17       |
|    time_elapsed    | 443      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=216.56 +/- 111.93
Episode length: 54.54 +/- 28.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.5        |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009498247 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.429       |
|    learning_rate        | 0.0001      |
|    loss                 | 978         |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.0152      |
|    value_loss           | 1.73e+03    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=196.24 +/- 88.77
Episode length: 49.44 +/- 22.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=183.60 +/- 87.73
Episode length: 46.26 +/- 21.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=192.24 +/- 79.83
Episode length: 48.40 +/- 19.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 78       |
|    iterations      | 18       |
|    time_elapsed    | 468      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=284.98 +/- 70.44
Episode length: 71.60 +/- 17.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.6        |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.019065514 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.97       |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0001      |
|    loss                 | 534         |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.0233      |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=293.76 +/- 64.60
Episode length: 73.86 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=289.94 +/- 52.76
Episode length: 72.88 +/- 13.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=277.58 +/- 76.02
Episode length: 69.76 +/- 19.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | 279      |
| time/              |          |
|    fps             | 77       |
|    iterations      | 19       |
|    time_elapsed    | 501      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=280.52 +/- 66.44
Episode length: 70.46 +/- 16.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.5        |
|    mean_reward          | 281         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.010473892 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0001      |
|    loss                 | 493         |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.0239      |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=310.10 +/- 59.72
Episode length: 77.94 +/- 14.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=281.34 +/- 63.96
Episode length: 70.74 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=269.18 +/- 63.88
Episode length: 67.68 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.7     |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.5     |
|    ep_rew_mean     | 272      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 20       |
|    time_elapsed    | 533      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=173.68 +/- 40.23
Episode length: 43.86 +/- 10.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.9        |
|    mean_reward          | 174         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.015318504 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0001      |
|    loss                 | 335         |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.0094      |
|    value_loss           | 715         |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=172.96 +/- 49.46
Episode length: 43.64 +/- 12.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=181.40 +/- 43.20
Episode length: 45.78 +/- 10.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=175.46 +/- 42.98
Episode length: 44.26 +/- 10.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=170.46 +/- 40.93
Episode length: 43.06 +/- 10.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.1     |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61.4     |
|    ep_rew_mean     | 244      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 21       |
|    time_elapsed    | 560      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=285.72 +/- 119.21
Episode length: 71.80 +/- 29.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.8        |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.023077182 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0001      |
|    loss                 | 181         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00192    |
|    value_loss           | 427         |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=277.16 +/- 107.07
Episode length: 69.60 +/- 26.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.6     |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=319.54 +/- 112.22
Episode length: 80.34 +/- 28.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=282.98 +/- 109.79
Episode length: 71.10 +/- 27.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.1     |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 63.3     |
|    ep_rew_mean     | 252      |
| time/              |          |
|    fps             | 76       |
|    iterations      | 22       |
|    time_elapsed    | 592      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=300.44 +/- 119.97
Episode length: 75.52 +/- 29.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.5        |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.018416319 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0001      |
|    loss                 | 786         |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.0109      |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=257.22 +/- 123.26
Episode length: 64.70 +/- 30.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.7     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=287.90 +/- 117.85
Episode length: 72.28 +/- 29.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=279.94 +/- 112.10
Episode length: 70.26 +/- 28.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.4     |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 75       |
|    iterations      | 23       |
|    time_elapsed    | 625      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=258.52 +/- 80.47
Episode length: 64.98 +/- 20.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65          |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.018846706 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0001      |
|    loss                 | 416         |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.0118      |
|    value_loss           | 1.62e+03    |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=273.42 +/- 82.18
Episode length: 68.78 +/- 20.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=263.16 +/- 81.21
Episode length: 66.18 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.2     |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=271.34 +/- 80.08
Episode length: 68.20 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.1     |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 24       |
|    time_elapsed    | 657      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=273.80 +/- 67.25
Episode length: 68.82 +/- 16.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.8        |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.024089158 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.958      |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0001      |
|    loss                 | 268         |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.0101      |
|    value_loss           | 543         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=273.42 +/- 77.63
Episode length: 68.74 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=280.86 +/- 78.63
Episode length: 70.60 +/- 19.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=290.54 +/- 67.71
Episode length: 72.98 +/- 16.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.5     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 25       |
|    time_elapsed    | 689      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=206.56 +/- 40.29
Episode length: 52.02 +/- 9.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.052502934 |
|    clip_fraction        | 0.398       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.989      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0001      |
|    loss                 | 532         |
|    n_updates            | 250         |
|    policy_gradient_loss | 0.0196      |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=194.96 +/- 37.61
Episode length: 49.08 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=202.26 +/- 48.03
Episode length: 51.00 +/- 11.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=185.18 +/- 35.51
Episode length: 46.64 +/- 8.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.8     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 26       |
|    time_elapsed    | 714      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=324.88 +/- 86.85
Episode length: 81.58 +/- 21.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.6        |
|    mean_reward          | 325         |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.025346812 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0001      |
|    loss                 | 318         |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.0195      |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=354.24 +/- 94.11
Episode length: 88.96 +/- 23.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89       |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=329.16 +/- 79.44
Episode length: 82.64 +/- 19.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.6     |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=346.12 +/- 90.50
Episode length: 86.98 +/- 22.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87       |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.6     |
|    ep_rew_mean     | 293      |
| time/              |          |
|    fps             | 73       |
|    iterations      | 27       |
|    time_elapsed    | 751      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=237.62 +/- 135.73
Episode length: 59.74 +/- 33.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.7        |
|    mean_reward          | 238         |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.031448502 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0001      |
|    loss                 | 447         |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00778     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=212.16 +/- 92.11
Episode length: 53.36 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=225.02 +/- 93.71
Episode length: 56.62 +/- 23.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=233.00 +/- 120.01
Episode length: 58.58 +/- 30.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.6     |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.1     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 73       |
|    iterations      | 28       |
|    time_elapsed    | 778      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=250.24 +/- 119.22
Episode length: 62.94 +/- 29.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62.9        |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.012139459 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.926      |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0001      |
|    loss                 | 802         |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.0198      |
|    value_loss           | 1.73e+03    |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=222.58 +/- 107.04
Episode length: 56.06 +/- 26.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=233.86 +/- 92.48
Episode length: 58.82 +/- 23.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=228.70 +/- 87.87
Episode length: 57.52 +/- 22.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.5     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | 279      |
| time/              |          |
|    fps             | 73       |
|    iterations      | 29       |
|    time_elapsed    | 805      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=194.64 +/- 67.23
Episode length: 49.16 +/- 16.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.014736496 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0.358       |
|    learning_rate        | 0.0001      |
|    loss                 | 972         |
|    n_updates            | 290         |
|    policy_gradient_loss | 0.0131      |
|    value_loss           | 1.82e+03    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=202.70 +/- 64.39
Episode length: 51.06 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=179.82 +/- 59.43
Episode length: 45.34 +/- 14.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=188.76 +/- 54.54
Episode length: 47.58 +/- 13.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.7     |
|    ep_rew_mean     | 241      |
| time/              |          |
|    fps             | 74       |
|    iterations      | 30       |
|    time_elapsed    | 830      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=236.46 +/- 132.01
Episode length: 59.42 +/- 33.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.4        |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.008775005 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.811      |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0001      |
|    loss                 | 556         |
|    n_updates            | 300         |
|    policy_gradient_loss | 0.00619     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=250.06 +/- 143.33
Episode length: 62.86 +/- 35.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.9     |
|    mean_reward     | 250      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=241.96 +/- 136.24
Episode length: 60.86 +/- 34.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.9     |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=220.70 +/- 91.91
Episode length: 55.56 +/- 22.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.2     |
|    ep_rew_mean     | 227      |
| time/              |          |
|    fps             | 73       |
|    iterations      | 31       |
|    time_elapsed    | 858      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=404.54 +/- 247.19
Episode length: 101.50 +/- 61.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 405         |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.031375445 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.884      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0001      |
|    loss                 | 291         |
|    n_updates            | 310         |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 955         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=64000, episode_reward=312.56 +/- 190.14
Episode length: 78.50 +/- 47.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=340.46 +/- 188.28
Episode length: 85.48 +/- 47.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.5     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=350.48 +/- 203.57
Episode length: 87.98 +/- 50.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88       |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=345.84 +/- 209.10
Episode length: 86.84 +/- 52.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.8     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61.9     |
|    ep_rew_mean     | 246      |
| time/              |          |
|    fps             | 72       |
|    iterations      | 32       |
|    time_elapsed    | 901      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=350.22 +/- 171.85
Episode length: 87.96 +/- 43.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 88         |
|    mean_reward          | 350        |
| time/                   |            |
|    total_timesteps      | 66000      |
| train/                  |            |
|    approx_kl            | 0.02965871 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.984     |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.0001     |
|    loss                 | 680        |
|    n_updates            | 320        |
|    policy_gradient_loss | 0.0117     |
|    value_loss           | 1.29e+03   |
----------------------------------------
Eval num_timesteps=66500, episode_reward=459.28 +/- 277.00
Episode length: 115.22 +/- 69.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
New best mean reward!
Eval num_timesteps=67000, episode_reward=431.38 +/- 255.77
Episode length: 108.26 +/- 63.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=455.90 +/- 256.67
Episode length: 114.34 +/- 64.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.4     |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 71       |
|    iterations      | 33       |
|    time_elapsed    | 945      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=381.86 +/- 113.64
Episode length: 95.80 +/- 28.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 95.8       |
|    mean_reward          | 382        |
| time/                   |            |
|    total_timesteps      | 68000      |
| train/                  |            |
|    approx_kl            | 0.03306645 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.999     |
|    explained_variance   | 0.421      |
|    learning_rate        | 0.0001     |
|    loss                 | 755        |
|    n_updates            | 330        |
|    policy_gradient_loss | 0.0087     |
|    value_loss           | 1.39e+03   |
----------------------------------------
Eval num_timesteps=68500, episode_reward=384.12 +/- 117.85
Episode length: 96.40 +/- 29.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=359.90 +/- 129.76
Episode length: 90.42 +/- 32.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=367.68 +/- 120.21
Episode length: 92.40 +/- 30.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.7     |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 70       |
|    iterations      | 34       |
|    time_elapsed    | 987      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=440.82 +/- 184.57
Episode length: 110.64 +/- 46.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 441         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.020934274 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.0001      |
|    loss                 | 684         |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.0114      |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=384.60 +/- 173.71
Episode length: 96.56 +/- 43.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=437.32 +/- 188.60
Episode length: 109.70 +/- 47.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=453.36 +/- 186.22
Episode length: 113.70 +/- 46.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.9     |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 69       |
|    iterations      | 35       |
|    time_elapsed    | 1031     |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=451.96 +/- 182.55
Episode length: 113.38 +/- 45.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 113          |
|    mean_reward          | 452          |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0099346135 |
|    clip_fraction        | 0.248        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.444        |
|    learning_rate        | 0.0001       |
|    loss                 | 867          |
|    n_updates            | 350          |
|    policy_gradient_loss | 0.00607      |
|    value_loss           | 1.43e+03     |
------------------------------------------
Eval num_timesteps=72500, episode_reward=463.58 +/- 184.06
Episode length: 116.16 +/- 45.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
New best mean reward!
Eval num_timesteps=73000, episode_reward=440.80 +/- 214.04
Episode length: 110.60 +/- 53.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=436.52 +/- 169.71
Episode length: 109.56 +/- 42.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 68       |
|    iterations      | 36       |
|    time_elapsed    | 1078     |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=290.30 +/- 68.93
Episode length: 72.92 +/- 17.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.9        |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.010307276 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0001      |
|    loss                 | 617         |
|    n_updates            | 360         |
|    policy_gradient_loss | 0.0157      |
|    value_loss           | 1.63e+03    |
-----------------------------------------
Eval num_timesteps=74500, episode_reward=292.40 +/- 53.51
Episode length: 73.48 +/- 13.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=294.96 +/- 50.00
Episode length: 74.06 +/- 12.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.1     |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=297.40 +/- 55.60
Episode length: 74.76 +/- 13.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.8     |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 68       |
|    iterations      | 37       |
|    time_elapsed    | 1111     |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=292.62 +/- 60.45
Episode length: 73.50 +/- 15.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.5        |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.013614949 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.956      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0001      |
|    loss                 | 596         |
|    n_updates            | 370         |
|    policy_gradient_loss | 0.0114      |
|    value_loss           | 988         |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=295.86 +/- 54.49
Episode length: 74.32 +/- 13.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=297.78 +/- 61.22
Episode length: 74.86 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=298.28 +/- 62.14
Episode length: 74.94 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 38       |
|    time_elapsed    | 1144     |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=390.60 +/- 117.85
Episode length: 98.08 +/- 29.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.1       |
|    mean_reward          | 391        |
| time/                   |            |
|    total_timesteps      | 78000      |
| train/                  |            |
|    approx_kl            | 0.01687967 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.897     |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0001     |
|    loss                 | 402        |
|    n_updates            | 380        |
|    policy_gradient_loss | 0.00356    |
|    value_loss           | 862        |
----------------------------------------
Eval num_timesteps=78500, episode_reward=395.92 +/- 108.16
Episode length: 99.36 +/- 27.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=414.86 +/- 142.32
Episode length: 104.14 +/- 35.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=419.22 +/- 117.98
Episode length: 105.20 +/- 29.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 39       |
|    time_elapsed    | 1186     |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=376.70 +/- 99.08
Episode length: 94.52 +/- 24.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.5        |
|    mean_reward          | 377         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.020570751 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.914      |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.03e+03    |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.01        |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=380.98 +/- 94.95
Episode length: 95.60 +/- 23.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=369.58 +/- 89.66
Episode length: 92.78 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=348.90 +/- 96.55
Episode length: 87.54 +/- 24.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.5     |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 40       |
|    time_elapsed    | 1225     |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=270.92 +/- 51.78
Episode length: 68.12 +/- 12.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.1        |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.018297605 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.891      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0001      |
|    loss                 | 431         |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=264.62 +/- 50.32
Episode length: 66.54 +/- 12.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=266.78 +/- 47.56
Episode length: 67.08 +/- 11.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.1     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=264.46 +/- 49.93
Episode length: 66.44 +/- 12.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.4     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 41       |
|    time_elapsed    | 1257     |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=252.42 +/- 57.03
Episode length: 63.44 +/- 14.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 63.4        |
|    mean_reward          | 252         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.009124681 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0001      |
|    loss                 | 340         |
|    n_updates            | 410         |
|    policy_gradient_loss | 0.00288     |
|    value_loss           | 623         |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=251.60 +/- 42.50
Episode length: 63.28 +/- 10.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.3     |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=257.78 +/- 44.16
Episode length: 64.88 +/- 11.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.9     |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=245.78 +/- 49.42
Episode length: 61.84 +/- 12.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=248.98 +/- 45.40
Episode length: 62.60 +/- 11.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 249      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 42       |
|    time_elapsed    | 1292     |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=286.44 +/- 49.31
Episode length: 72.02 +/- 12.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 72         |
|    mean_reward          | 286        |
| time/                   |            |
|    total_timesteps      | 86500      |
| train/                  |            |
|    approx_kl            | 0.03924805 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0001     |
|    loss                 | 452        |
|    n_updates            | 420        |
|    policy_gradient_loss | 0.00567    |
|    value_loss           | 564        |
----------------------------------------
Eval num_timesteps=87000, episode_reward=305.84 +/- 54.93
Episode length: 76.82 +/- 13.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=292.24 +/- 47.52
Episode length: 73.42 +/- 11.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=289.52 +/- 68.44
Episode length: 72.70 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 333      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 43       |
|    time_elapsed    | 1325     |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=407.26 +/- 146.79
Episode length: 102.16 +/- 36.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 407         |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.027737554 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0001      |
|    loss                 | 290         |
|    n_updates            | 430         |
|    policy_gradient_loss | 0.00685     |
|    value_loss           | 805         |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=443.80 +/- 164.50
Episode length: 111.30 +/- 41.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=412.90 +/- 154.94
Episode length: 103.56 +/- 38.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=444.12 +/- 187.00
Episode length: 111.44 +/- 46.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 44       |
|    time_elapsed    | 1369     |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=301.20 +/- 70.43
Episode length: 75.62 +/- 17.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.6        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 90500       |
| train/                  |             |
|    approx_kl            | 0.008586338 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.1e+03     |
|    n_updates            | 440         |
|    policy_gradient_loss | 0.00367     |
|    value_loss           | 2.11e+03    |
-----------------------------------------
Eval num_timesteps=91000, episode_reward=334.50 +/- 69.84
Episode length: 84.00 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=319.06 +/- 79.99
Episode length: 80.08 +/- 20.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=324.28 +/- 67.47
Episode length: 81.44 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.4     |
|    mean_reward     | 324      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.9     |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 45       |
|    time_elapsed    | 1403     |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=375.44 +/- 89.45
Episode length: 94.22 +/- 22.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.2        |
|    mean_reward          | 375         |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.008754984 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.558      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0001      |
|    loss                 | 468         |
|    n_updates            | 450         |
|    policy_gradient_loss | 0.0026      |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=349.12 +/- 82.64
Episode length: 87.68 +/- 20.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.7     |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=345.10 +/- 95.52
Episode length: 86.62 +/- 23.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.6     |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=366.26 +/- 95.98
Episode length: 91.90 +/- 23.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.4     |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 46       |
|    time_elapsed    | 1442     |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=399.78 +/- 229.22
Episode length: 100.28 +/- 57.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.011654542 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0001      |
|    loss                 | 424         |
|    n_updates            | 460         |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=418.54 +/- 176.22
Episode length: 105.04 +/- 44.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=442.82 +/- 195.25
Episode length: 111.10 +/- 48.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=415.12 +/- 171.38
Episode length: 104.06 +/- 42.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.5     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 47       |
|    time_elapsed    | 1487     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=437.72 +/- 167.08
Episode length: 109.78 +/- 41.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 438         |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.018828614 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.0001      |
|    loss                 | 372         |
|    n_updates            | 470         |
|    policy_gradient_loss | 0.00953     |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=433.00 +/- 202.97
Episode length: 108.62 +/- 50.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=429.86 +/- 172.04
Episode length: 107.92 +/- 43.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=442.08 +/- 177.01
Episode length: 110.88 +/- 44.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 48       |
|    time_elapsed    | 1533     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=413.40 +/- 224.43
Episode length: 103.72 +/- 56.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 413         |
| time/                   |             |
|    total_timesteps      | 98500       |
| train/                  |             |
|    approx_kl            | 0.006365286 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0001      |
|    loss                 | 613         |
|    n_updates            | 480         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 1.63e+03    |
-----------------------------------------
Eval num_timesteps=99000, episode_reward=431.62 +/- 169.79
Episode length: 108.30 +/- 42.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=405.58 +/- 246.01
Episode length: 101.80 +/- 61.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=443.96 +/- 173.01
Episode length: 111.36 +/- 43.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88       |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 49       |
|    time_elapsed    | 1576     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=346.82 +/- 117.18
Episode length: 87.04 +/- 29.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87          |
|    mean_reward          | 347         |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.016748536 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.6        |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0001      |
|    loss                 | 703         |
|    n_updates            | 490         |
|    policy_gradient_loss | 0.0076      |
|    value_loss           | 2.23e+03    |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=381.28 +/- 123.11
Episode length: 95.70 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.7     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=353.40 +/- 104.44
Episode length: 88.72 +/- 26.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.7     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=381.04 +/- 96.13
Episode length: 95.60 +/- 24.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92       |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 50       |
|    time_elapsed    | 1616     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=397.58 +/- 168.81
Episode length: 99.84 +/- 42.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | 398         |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.011769605 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.583      |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0001      |
|    loss                 | 303         |
|    n_updates            | 500         |
|    policy_gradient_loss | 0.00895     |
|    value_loss           | 728         |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=402.86 +/- 164.27
Episode length: 101.04 +/- 41.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=438.74 +/- 130.30
Episode length: 110.00 +/- 32.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=438.06 +/- 123.84
Episode length: 109.84 +/- 30.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.8     |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 51       |
|    time_elapsed    | 1658     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=428.22 +/- 197.60
Episode length: 107.36 +/- 49.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 428         |
| time/                   |             |
|    total_timesteps      | 104500      |
| train/                  |             |
|    approx_kl            | 0.008149088 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0001      |
|    loss                 | 391         |
|    n_updates            | 510         |
|    policy_gradient_loss | 0.00753     |
|    value_loss           | 1.32e+03    |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=413.84 +/- 163.73
Episode length: 103.82 +/- 40.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=405.56 +/- 185.31
Episode length: 101.78 +/- 46.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=469.04 +/- 155.70
Episode length: 117.64 +/- 38.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 52       |
|    time_elapsed    | 1701     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=356.86 +/- 79.37
Episode length: 89.60 +/- 19.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.6         |
|    mean_reward          | 357          |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0070163524 |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.585       |
|    explained_variance   | 0.485        |
|    learning_rate        | 0.0001       |
|    loss                 | 851          |
|    n_updates            | 520          |
|    policy_gradient_loss | 0.0061       |
|    value_loss           | 1.88e+03     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=347.38 +/- 103.98
Episode length: 87.26 +/- 26.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.3     |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=370.82 +/- 81.50
Episode length: 93.16 +/- 20.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=346.88 +/- 78.21
Episode length: 87.08 +/- 19.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.1     |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=381.44 +/- 85.41
Episode length: 95.78 +/- 21.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.7     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 53       |
|    time_elapsed    | 1747     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=440.78 +/- 149.23
Episode length: 110.60 +/- 37.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 441         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.025873877 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0001      |
|    loss                 | 206         |
|    n_updates            | 530         |
|    policy_gradient_loss | 0.00774     |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=389.82 +/- 135.40
Episode length: 97.82 +/- 33.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=358.78 +/- 138.85
Episode length: 90.12 +/- 34.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=405.82 +/- 123.49
Episode length: 101.82 +/- 30.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.2     |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 54       |
|    time_elapsed    | 1789     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=425.16 +/- 129.65
Episode length: 106.70 +/- 32.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 425         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.018883126 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.23e+03    |
|    n_updates            | 540         |
|    policy_gradient_loss | 0.005       |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=404.12 +/- 158.58
Episode length: 101.44 +/- 39.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=416.10 +/- 125.81
Episode length: 104.40 +/- 31.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=413.52 +/- 145.93
Episode length: 103.86 +/- 36.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90       |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 55       |
|    time_elapsed    | 1831     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=341.62 +/- 93.12
Episode length: 85.82 +/- 23.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85.8       |
|    mean_reward          | 342        |
| time/                   |            |
|    total_timesteps      | 113000     |
| train/                  |            |
|    approx_kl            | 0.01678523 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0001     |
|    loss                 | 794        |
|    n_updates            | 550        |
|    policy_gradient_loss | 0.00602    |
|    value_loss           | 1.21e+03   |
----------------------------------------
Eval num_timesteps=113500, episode_reward=369.90 +/- 85.58
Episode length: 92.80 +/- 21.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=370.52 +/- 108.54
Episode length: 92.98 +/- 27.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=387.74 +/- 90.05
Episode length: 97.28 +/- 22.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.9     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 56       |
|    time_elapsed    | 1869     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=396.30 +/- 137.12
Episode length: 99.48 +/- 34.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.5        |
|    mean_reward          | 396         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.021043211 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0001      |
|    loss                 | 494         |
|    n_updates            | 560         |
|    policy_gradient_loss | 0.00531     |
|    value_loss           | 1.12e+03    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=378.18 +/- 136.60
Episode length: 94.94 +/- 34.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=436.48 +/- 144.55
Episode length: 109.44 +/- 36.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=400.46 +/- 109.03
Episode length: 100.46 +/- 27.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 400      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.1     |
|    ep_rew_mean     | 339      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 57       |
|    time_elapsed    | 1910     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=378.72 +/- 91.95
Episode length: 95.04 +/- 22.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95          |
|    mean_reward          | 379         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.015420917 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0001      |
|    loss                 | 966         |
|    n_updates            | 570         |
|    policy_gradient_loss | 0.014       |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=372.12 +/- 105.15
Episode length: 93.40 +/- 26.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=420.74 +/- 84.64
Episode length: 105.48 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=387.78 +/- 88.14
Episode length: 97.26 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.4     |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 58       |
|    time_elapsed    | 1948     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=415.58 +/- 152.70
Episode length: 104.24 +/- 38.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 416         |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.006979145 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0001      |
|    loss                 | 618         |
|    n_updates            | 580         |
|    policy_gradient_loss | 0.00875     |
|    value_loss           | 1.27e+03    |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=463.30 +/- 137.16
Episode length: 116.20 +/- 34.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=425.86 +/- 137.98
Episode length: 106.76 +/- 34.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=416.96 +/- 142.28
Episode length: 104.60 +/- 35.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.4     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 59       |
|    time_elapsed    | 1992     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=430.42 +/- 147.19
Episode length: 108.00 +/- 36.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 430         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.018030655 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0001      |
|    loss                 | 660         |
|    n_updates            | 590         |
|    policy_gradient_loss | 0.012       |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=402.72 +/- 180.89
Episode length: 101.04 +/- 45.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=367.68 +/- 157.96
Episode length: 92.34 +/- 39.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.3     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=419.60 +/- 161.81
Episode length: 105.24 +/- 40.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 60       |
|    time_elapsed    | 2035     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=374.48 +/- 194.37
Episode length: 93.96 +/- 48.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94          |
|    mean_reward          | 374         |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.021996804 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0001      |
|    loss                 | 337         |
|    n_updates            | 600         |
|    policy_gradient_loss | 0.0192      |
|    value_loss           | 1.32e+03    |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=419.76 +/- 205.93
Episode length: 105.30 +/- 51.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=339.84 +/- 180.29
Episode length: 85.28 +/- 45.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=436.60 +/- 226.32
Episode length: 109.56 +/- 56.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.2     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 61       |
|    time_elapsed    | 2077     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=368.12 +/- 210.39
Episode length: 92.40 +/- 52.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.4       |
|    mean_reward          | 368        |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.04783372 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.53      |
|    explained_variance   | 0.277      |
|    learning_rate        | 0.0001     |
|    loss                 | 968        |
|    n_updates            | 610        |
|    policy_gradient_loss | 0.0171     |
|    value_loss           | 2.05e+03   |
----------------------------------------
Eval num_timesteps=125500, episode_reward=370.08 +/- 233.33
Episode length: 92.84 +/- 58.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=317.66 +/- 187.58
Episode length: 79.76 +/- 46.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=329.92 +/- 202.44
Episode length: 82.86 +/- 50.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.9     |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 62       |
|    time_elapsed    | 2115     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=323.48 +/- 229.73
Episode length: 81.24 +/- 57.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.2        |
|    mean_reward          | 323         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.016362222 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.236       |
|    learning_rate        | 0.0001      |
|    loss                 | 533         |
|    n_updates            | 620         |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=443.76 +/- 265.32
Episode length: 111.28 +/- 66.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=379.82 +/- 243.37
Episode length: 95.30 +/- 60.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=479.40 +/- 328.78
Episode length: 120.20 +/- 82.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
New best mean reward!
Eval num_timesteps=129000, episode_reward=373.12 +/- 222.30
Episode length: 93.72 +/- 55.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.7     |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.5     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 63       |
|    time_elapsed    | 2167     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=383.94 +/- 227.46
Episode length: 96.40 +/- 56.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.4       |
|    mean_reward          | 384        |
| time/                   |            |
|    total_timesteps      | 129500     |
| train/                  |            |
|    approx_kl            | 0.04011781 |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.589     |
|    explained_variance   | 0.215      |
|    learning_rate        | 0.0001     |
|    loss                 | 636        |
|    n_updates            | 630        |
|    policy_gradient_loss | 0.0198     |
|    value_loss           | 2.19e+03   |
----------------------------------------
Eval num_timesteps=130000, episode_reward=377.16 +/- 251.41
Episode length: 94.64 +/- 62.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=446.20 +/- 243.47
Episode length: 111.90 +/- 60.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=388.64 +/- 241.50
Episode length: 97.52 +/- 60.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 64       |
|    time_elapsed    | 2211     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=370.06 +/- 221.52
Episode length: 92.92 +/- 55.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.9        |
|    mean_reward          | 370         |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.039389286 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.07e+03    |
|    n_updates            | 640         |
|    policy_gradient_loss | 0.0119      |
|    value_loss           | 1.91e+03    |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=359.60 +/- 232.21
Episode length: 90.20 +/- 58.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=412.10 +/- 202.36
Episode length: 103.44 +/- 50.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=373.56 +/- 251.86
Episode length: 93.78 +/- 62.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 65       |
|    time_elapsed    | 2251     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=326.00 +/- 212.65
Episode length: 81.84 +/- 53.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.8        |
|    mean_reward          | 326         |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.012834939 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.621      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.06e+03    |
|    n_updates            | 650         |
|    policy_gradient_loss | 0.0035      |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=355.86 +/- 175.49
Episode length: 89.28 +/- 43.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.3     |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=360.88 +/- 224.42
Episode length: 90.58 +/- 56.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=431.90 +/- 275.72
Episode length: 108.34 +/- 68.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.6     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 66       |
|    time_elapsed    | 2290     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=492.78 +/- 221.83
Episode length: 123.56 +/- 55.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 493         |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.019821974 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.01e+03    |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.00785     |
|    value_loss           | 2.02e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=136000, episode_reward=353.48 +/- 209.17
Episode length: 88.68 +/- 52.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.7     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=441.80 +/- 244.60
Episode length: 110.86 +/- 61.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=416.58 +/- 236.74
Episode length: 104.48 +/- 59.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.7     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 67       |
|    time_elapsed    | 2335     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=381.72 +/- 148.68
Episode length: 95.76 +/- 37.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.8        |
|    mean_reward          | 382         |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.013470676 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.601      |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0001      |
|    loss                 | 543         |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.00576     |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=408.70 +/- 134.24
Episode length: 102.54 +/- 33.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=401.24 +/- 122.84
Episode length: 100.58 +/- 30.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=386.18 +/- 125.13
Episode length: 97.00 +/- 31.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.8     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 68       |
|    time_elapsed    | 2377     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=425.12 +/- 223.48
Episode length: 106.64 +/- 55.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 107        |
|    mean_reward          | 425        |
| time/                   |            |
|    total_timesteps      | 139500     |
| train/                  |            |
|    approx_kl            | 0.04905577 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.576     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0001     |
|    loss                 | 579        |
|    n_updates            | 680        |
|    policy_gradient_loss | 0.00952    |
|    value_loss           | 1.44e+03   |
----------------------------------------
Eval num_timesteps=140000, episode_reward=423.34 +/- 252.76
Episode length: 106.20 +/- 63.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=382.92 +/- 186.37
Episode length: 96.14 +/- 46.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=395.38 +/- 195.46
Episode length: 99.16 +/- 48.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.2     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 69       |
|    time_elapsed    | 2418     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=429.52 +/- 264.95
Episode length: 107.84 +/- 66.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 430         |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.011024505 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0001      |
|    loss                 | 1.09e+03    |
|    n_updates            | 690         |
|    policy_gradient_loss | 0.00933     |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=457.46 +/- 204.27
Episode length: 114.82 +/- 51.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=416.44 +/- 259.65
Episode length: 104.52 +/- 64.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=460.16 +/- 218.08
Episode length: 115.42 +/- 54.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.8     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 70       |
|    time_elapsed    | 2464     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=254.52 +/- 54.03
Episode length: 64.00 +/- 13.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 64         |
|    mean_reward          | 255        |
| time/                   |            |
|    total_timesteps      | 143500     |
| train/                  |            |
|    approx_kl            | 0.11255802 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.552     |
|    explained_variance   | 0.319      |
|    learning_rate        | 0.0001     |
|    loss                 | 937        |
|    n_updates            | 700        |
|    policy_gradient_loss | 0.0133     |
|    value_loss           | 1.79e+03   |
----------------------------------------
Eval num_timesteps=144000, episode_reward=244.94 +/- 39.84
Episode length: 61.56 +/- 9.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.6     |
|    mean_reward     | 245      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=251.76 +/- 50.13
Episode length: 63.38 +/- 12.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.4     |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=259.62 +/- 49.93
Episode length: 65.26 +/- 12.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.3     |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.1     |
|    ep_rew_mean     | 339      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 71       |
|    time_elapsed    | 2494     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=369.10 +/- 101.88
Episode length: 92.62 +/- 25.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.6        |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.014619919 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0001      |
|    loss                 | 257         |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 569         |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=350.08 +/- 102.74
Episode length: 87.90 +/- 25.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=326.98 +/- 109.36
Episode length: 82.14 +/- 27.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=334.54 +/- 98.84
Episode length: 84.04 +/- 24.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.1     |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 72       |
|    time_elapsed    | 2532     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=333.00 +/- 82.42
Episode length: 83.60 +/- 20.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 83.6         |
|    mean_reward          | 333          |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0069949054 |
|    clip_fraction        | 0.2          |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.507       |
|    explained_variance   | 0.755        |
|    learning_rate        | 0.0001       |
|    loss                 | 724          |
|    n_updates            | 720          |
|    policy_gradient_loss | 0.00321      |
|    value_loss           | 939          |
------------------------------------------
Eval num_timesteps=148000, episode_reward=321.58 +/- 92.84
Episode length: 80.76 +/- 23.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.8     |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=319.88 +/- 79.91
Episode length: 80.36 +/- 19.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=316.18 +/- 82.54
Episode length: 79.58 +/- 20.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=340.58 +/- 84.11
Episode length: 85.50 +/- 21.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.5     |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.7     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 73       |
|    time_elapsed    | 2575     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=267.40 +/- 45.09
Episode length: 67.18 +/- 11.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.2        |
|    mean_reward          | 267         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.016443884 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0001      |
|    loss                 | 387         |
|    n_updates            | 730         |
|    policy_gradient_loss | 0.0181      |
|    value_loss           | 752         |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=258.80 +/- 47.41
Episode length: 65.10 +/- 11.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.1     |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=269.24 +/- 67.84
Episode length: 67.74 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.7     |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=268.52 +/- 60.00
Episode length: 67.54 +/- 14.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.5     |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 322      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 74       |
|    time_elapsed    | 2606     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=267.58 +/- 57.69
Episode length: 67.32 +/- 14.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 67.3       |
|    mean_reward          | 268        |
| time/                   |            |
|    total_timesteps      | 152000     |
| train/                  |            |
|    approx_kl            | 0.01947296 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.867      |
|    learning_rate        | 0.0001     |
|    loss                 | 203        |
|    n_updates            | 740        |
|    policy_gradient_loss | 0.000785   |
|    value_loss           | 520        |
----------------------------------------
Eval num_timesteps=152500, episode_reward=282.14 +/- 57.16
Episode length: 70.98 +/- 14.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71       |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=295.64 +/- 68.95
Episode length: 74.32 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=302.12 +/- 72.06
Episode length: 75.92 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.8     |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 75       |
|    time_elapsed    | 2636     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=307.26 +/- 74.22
Episode length: 77.22 +/- 18.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.2        |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.018066898 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0001      |
|    loss                 | 139         |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 515         |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=294.04 +/- 45.38
Episode length: 73.90 +/- 11.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=309.24 +/- 78.09
Episode length: 77.66 +/- 19.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=308.04 +/- 78.13
Episode length: 77.34 +/- 19.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.8     |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 76       |
|    time_elapsed    | 2668     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=428.80 +/- 120.95
Episode length: 107.56 +/- 30.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 429         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.010659532 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0001      |
|    loss                 | 187         |
|    n_updates            | 760         |
|    policy_gradient_loss | 0.00569     |
|    value_loss           | 584         |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=398.66 +/- 138.73
Episode length: 99.98 +/- 34.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=372.14 +/- 119.17
Episode length: 93.38 +/- 29.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=437.90 +/- 90.43
Episode length: 109.88 +/- 22.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 77       |
|    time_elapsed    | 2709     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=377.02 +/- 94.54
Episode length: 94.62 +/- 23.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.6        |
|    mean_reward          | 377         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.017657602 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.486      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0001      |
|    loss                 | 851         |
|    n_updates            | 770         |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 1.31e+03    |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=363.34 +/- 67.27
Episode length: 91.20 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=358.72 +/- 78.09
Episode length: 90.06 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=351.66 +/- 83.54
Episode length: 88.24 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.2     |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.7     |
|    ep_rew_mean     | 337      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 78       |
|    time_elapsed    | 2747     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=422.86 +/- 151.02
Episode length: 106.08 +/- 37.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 106          |
|    mean_reward          | 423          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0145844845 |
|    clip_fraction        | 0.158        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.85         |
|    learning_rate        | 0.0001       |
|    loss                 | 85.4         |
|    n_updates            | 780          |
|    policy_gradient_loss | 0.00591      |
|    value_loss           | 462          |
------------------------------------------
Eval num_timesteps=160500, episode_reward=436.82 +/- 172.06
Episode length: 109.58 +/- 43.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=431.22 +/- 127.62
Episode length: 108.20 +/- 31.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=407.22 +/- 162.92
Episode length: 102.16 +/- 40.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.2     |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 79       |
|    time_elapsed    | 2790     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=411.72 +/- 116.93
Episode length: 103.32 +/- 29.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 412         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.012858285 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.05e+03    |
|    n_updates            | 790         |
|    policy_gradient_loss | 0.00637     |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=379.70 +/- 112.21
Episode length: 95.28 +/- 28.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=366.20 +/- 109.72
Episode length: 91.90 +/- 27.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=400.42 +/- 105.96
Episode length: 100.50 +/- 26.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 400      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.6     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 80       |
|    time_elapsed    | 2829     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=329.30 +/- 62.26
Episode length: 82.66 +/- 15.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.7        |
|    mean_reward          | 329         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.049700484 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0001      |
|    loss                 | 318         |
|    n_updates            | 800         |
|    policy_gradient_loss | 0.0181      |
|    value_loss           | 889         |
-----------------------------------------
Eval num_timesteps=164500, episode_reward=311.78 +/- 69.55
Episode length: 78.36 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=331.88 +/- 69.13
Episode length: 83.38 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.4     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=331.72 +/- 77.55
Episode length: 83.34 +/- 19.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.4     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 81       |
|    time_elapsed    | 2866     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=374.74 +/- 102.61
Episode length: 94.04 +/- 25.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94          |
|    mean_reward          | 375         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.067350775 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0001      |
|    loss                 | 475         |
|    n_updates            | 810         |
|    policy_gradient_loss | 0.00944     |
|    value_loss           | 912         |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=352.54 +/- 115.86
Episode length: 88.48 +/- 28.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=415.24 +/- 117.23
Episode length: 104.18 +/- 29.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=370.80 +/- 96.92
Episode length: 93.06 +/- 24.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.1     |
|    ep_rew_mean     | 347      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 82       |
|    time_elapsed    | 2905     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=391.26 +/- 138.09
Episode length: 98.08 +/- 34.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.1        |
|    mean_reward          | 391         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.027726714 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0001      |
|    loss                 | 248         |
|    n_updates            | 820         |
|    policy_gradient_loss | 0.0168      |
|    value_loss           | 712         |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=395.68 +/- 150.23
Episode length: 99.36 +/- 37.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=394.38 +/- 138.45
Episode length: 98.94 +/- 34.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=394.76 +/- 141.60
Episode length: 98.98 +/- 35.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.3     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 83       |
|    time_elapsed    | 2948     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=353.82 +/- 136.69
Episode length: 88.80 +/- 34.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.8        |
|    mean_reward          | 354         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.012679553 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0001      |
|    loss                 | 234         |
|    n_updates            | 830         |
|    policy_gradient_loss | 0.0168      |
|    value_loss           | 878         |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=372.34 +/- 119.41
Episode length: 93.42 +/- 29.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=406.98 +/- 118.27
Episode length: 102.10 +/- 29.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=387.62 +/- 127.29
Episode length: 97.24 +/- 31.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=359.08 +/- 113.09
Episode length: 90.08 +/- 28.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 84       |
|    time_elapsed    | 2996     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=267.78 +/- 61.45
Episode length: 67.32 +/- 15.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.3      |
|    mean_reward          | 268       |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0195663 |
|    clip_fraction        | 0.231     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.721    |
|    explained_variance   | 0.815     |
|    learning_rate        | 0.0001    |
|    loss                 | 474       |
|    n_updates            | 840       |
|    policy_gradient_loss | 0.00961   |
|    value_loss           | 744       |
---------------------------------------
Eval num_timesteps=173000, episode_reward=279.64 +/- 66.31
Episode length: 70.24 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=270.90 +/- 60.16
Episode length: 68.14 +/- 15.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.1     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=258.78 +/- 51.03
Episode length: 65.06 +/- 12.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.1     |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.1     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 85       |
|    time_elapsed    | 3029     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=278.52 +/- 59.13
Episode length: 69.98 +/- 14.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70          |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.020383079 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0001      |
|    loss                 | 688         |
|    n_updates            | 850         |
|    policy_gradient_loss | 0.00793     |
|    value_loss           | 926         |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=288.28 +/- 52.50
Episode length: 72.48 +/- 13.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.5     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=269.82 +/- 54.23
Episode length: 67.84 +/- 13.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.8     |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=266.96 +/- 52.27
Episode length: 67.06 +/- 13.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.1     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.4     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 86       |
|    time_elapsed    | 3060     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=314.36 +/- 62.84
Episode length: 78.98 +/- 15.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79          |
|    mean_reward          | 314         |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.013750745 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.727      |
|    explained_variance   | 0.819       |
|    learning_rate        | 0.0001      |
|    loss                 | 409         |
|    n_updates            | 860         |
|    policy_gradient_loss | 0.00373     |
|    value_loss           | 500         |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=331.82 +/- 73.42
Episode length: 83.34 +/- 18.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=320.64 +/- 66.10
Episode length: 80.62 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=308.36 +/- 60.87
Episode length: 77.50 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.5     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 87       |
|    time_elapsed    | 3095     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=384.94 +/- 130.20
Episode length: 96.54 +/- 32.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.5        |
|    mean_reward          | 385         |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.028274482 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0001      |
|    loss                 | 435         |
|    n_updates            | 870         |
|    policy_gradient_loss | 0.00885     |
|    value_loss           | 458         |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=384.02 +/- 104.83
Episode length: 96.38 +/- 26.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=402.84 +/- 128.35
Episode length: 101.08 +/- 32.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=387.46 +/- 128.02
Episode length: 97.16 +/- 31.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.7     |
|    ep_rew_mean     | 309      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 88       |
|    time_elapsed    | 3134     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=381.56 +/- 94.62
Episode length: 95.78 +/- 23.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.8        |
|    mean_reward          | 382         |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.013006588 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.866      |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0001      |
|    loss                 | 409         |
|    n_updates            | 880         |
|    policy_gradient_loss | 0.00499     |
|    value_loss           | 1.43e+03    |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=361.68 +/- 94.85
Episode length: 90.82 +/- 23.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.8     |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=383.64 +/- 76.31
Episode length: 96.26 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.3     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=366.06 +/- 114.61
Episode length: 91.90 +/- 28.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 89       |
|    time_elapsed    | 3174     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=365.30 +/- 115.81
Episode length: 91.68 +/- 28.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.7        |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.011342879 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0001      |
|    loss                 | 219         |
|    n_updates            | 890         |
|    policy_gradient_loss | 0.00331     |
|    value_loss           | 779         |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=383.16 +/- 88.21
Episode length: 96.22 +/- 22.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=387.20 +/- 100.36
Episode length: 97.18 +/- 25.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=376.96 +/- 101.24
Episode length: 94.58 +/- 25.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 90       |
|    time_elapsed    | 3214     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=358.00 +/- 84.92
Episode length: 89.90 +/- 21.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.9        |
|    mean_reward          | 358         |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.020838283 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0001      |
|    loss                 | 495         |
|    n_updates            | 900         |
|    policy_gradient_loss | 0.0221      |
|    value_loss           | 891         |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=369.04 +/- 112.78
Episode length: 92.70 +/- 28.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=366.64 +/- 89.73
Episode length: 92.02 +/- 22.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 367      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=383.18 +/- 94.64
Episode length: 96.22 +/- 23.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.1     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 91       |
|    time_elapsed    | 3252     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=419.62 +/- 129.91
Episode length: 105.28 +/- 32.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 420         |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.033986192 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0001      |
|    loss                 | 319         |
|    n_updates            | 910         |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 691         |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=382.56 +/- 93.67
Episode length: 95.98 +/- 23.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=402.86 +/- 109.62
Episode length: 101.14 +/- 27.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=345.64 +/- 126.10
Episode length: 86.78 +/- 31.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.8     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.4     |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 92       |
|    time_elapsed    | 3293     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=402.14 +/- 102.78
Episode length: 100.88 +/- 25.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 101        |
|    mean_reward          | 402        |
| time/                   |            |
|    total_timesteps      | 188500     |
| train/                  |            |
|    approx_kl            | 0.01831207 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.629     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0001     |
|    loss                 | 359        |
|    n_updates            | 920        |
|    policy_gradient_loss | 0.00838    |
|    value_loss           | 711        |
----------------------------------------
Eval num_timesteps=189000, episode_reward=400.60 +/- 81.12
Episode length: 100.46 +/- 20.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=436.06 +/- 109.87
Episode length: 109.36 +/- 27.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=380.40 +/- 126.89
Episode length: 95.48 +/- 31.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 93       |
|    time_elapsed    | 3336     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=359.26 +/- 100.43
Episode length: 90.24 +/- 25.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.2        |
|    mean_reward          | 359         |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.026849546 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0001      |
|    loss                 | 719         |
|    n_updates            | 930         |
|    policy_gradient_loss | 0.0123      |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=381.52 +/- 85.59
Episode length: 95.78 +/- 21.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 382      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=357.58 +/- 99.67
Episode length: 89.76 +/- 24.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=399.38 +/- 104.72
Episode length: 100.22 +/- 26.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=387.70 +/- 109.99
Episode length: 97.28 +/- 27.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.6     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 94       |
|    time_elapsed    | 3384     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=423.54 +/- 121.73
Episode length: 106.26 +/- 30.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 424        |
| time/                   |            |
|    total_timesteps      | 193000     |
| train/                  |            |
|    approx_kl            | 0.01916652 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0001     |
|    loss                 | 176        |
|    n_updates            | 940        |
|    policy_gradient_loss | 0.012      |
|    value_loss           | 877        |
----------------------------------------
Eval num_timesteps=193500, episode_reward=413.64 +/- 136.34
Episode length: 103.76 +/- 34.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=357.10 +/- 127.23
Episode length: 89.60 +/- 31.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.6     |
|    mean_reward     | 357      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=407.46 +/- 111.46
Episode length: 102.24 +/- 27.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.3     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 95       |
|    time_elapsed    | 3427     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=388.56 +/- 149.86
Episode length: 97.50 +/- 37.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.5       |
|    mean_reward          | 389        |
| time/                   |            |
|    total_timesteps      | 195000     |
| train/                  |            |
|    approx_kl            | 0.03184884 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.615     |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0001     |
|    loss                 | 469        |
|    n_updates            | 950        |
|    policy_gradient_loss | 0.0181     |
|    value_loss           | 927        |
----------------------------------------
Eval num_timesteps=195500, episode_reward=424.78 +/- 125.31
Episode length: 106.58 +/- 31.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=459.10 +/- 137.24
Episode length: 115.16 +/- 34.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=420.88 +/- 117.12
Episode length: 105.56 +/- 29.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.8     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 96       |
|    time_elapsed    | 3468     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=396.98 +/- 142.22
Episode length: 99.64 +/- 35.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.6        |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.018979214 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.526      |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0001      |
|    loss                 | 333         |
|    n_updates            | 960         |
|    policy_gradient_loss | 0.00981     |
|    value_loss           | 948         |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=436.62 +/- 144.25
Episode length: 109.58 +/- 36.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=413.80 +/- 143.07
Episode length: 103.78 +/- 35.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=411.66 +/- 144.89
Episode length: 103.34 +/- 36.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 97       |
|    time_elapsed    | 3509     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=389.60 +/- 206.85
Episode length: 97.78 +/- 51.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.8       |
|    mean_reward          | 390        |
| time/                   |            |
|    total_timesteps      | 199000     |
| train/                  |            |
|    approx_kl            | 0.04511051 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.512     |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0001     |
|    loss                 | 174        |
|    n_updates            | 970        |
|    policy_gradient_loss | 0.0318     |
|    value_loss           | 997        |
----------------------------------------
Eval num_timesteps=199500, episode_reward=472.74 +/- 211.00
Episode length: 118.58 +/- 52.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=397.80 +/- 198.84
Episode length: 99.86 +/- 49.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=433.46 +/- 205.31
Episode length: 108.70 +/- 51.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 98       |
|    time_elapsed    | 3550     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=433.14 +/- 142.73
Episode length: 108.66 +/- 35.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 433         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.028056573 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.0001      |
|    loss                 | 697         |
|    n_updates            | 980         |
|    policy_gradient_loss | 0.00845     |
|    value_loss           | 1.93e+03    |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=401.42 +/- 160.20
Episode length: 100.72 +/- 40.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=404.46 +/- 151.54
Episode length: 101.46 +/- 37.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=410.12 +/- 160.96
Episode length: 102.98 +/- 40.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.4     |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 99       |
|    time_elapsed    | 3594     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=378.08 +/- 129.42
Episode length: 94.90 +/- 32.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 94.9      |
|    mean_reward          | 378       |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0872944 |
|    clip_fraction        | 0.251     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.563    |
|    explained_variance   | 0.703     |
|    learning_rate        | 0.0001    |
|    loss                 | 279       |
|    n_updates            | 990       |
|    policy_gradient_loss | 0.0131    |
|    value_loss           | 1.14e+03  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=408.52 +/- 120.29
Episode length: 102.48 +/- 30.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=391.98 +/- 105.82
Episode length: 98.28 +/- 26.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=382.34 +/- 98.21
Episode length: 95.94 +/- 24.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 382      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 100      |
|    time_elapsed    | 3636     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=408.74 +/- 138.38
Episode length: 102.58 +/- 34.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 409         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.020303342 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0001      |
|    loss                 | 211         |
|    n_updates            | 1000        |
|    policy_gradient_loss | 0.00869     |
|    value_loss           | 916         |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=390.14 +/- 117.76
Episode length: 97.92 +/- 29.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=413.94 +/- 136.65
Episode length: 103.86 +/- 34.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=429.04 +/- 144.09
Episode length: 107.62 +/- 36.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.6     |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 101      |
|    time_elapsed    | 3680     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=460.96 +/- 155.21
Episode length: 115.66 +/- 38.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 461         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.026391353 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0001      |
|    loss                 | 463         |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.00865     |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=207500, episode_reward=438.40 +/- 157.43
Episode length: 109.98 +/- 39.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=388.00 +/- 170.82
Episode length: 97.30 +/- 42.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=477.52 +/- 178.07
Episode length: 119.74 +/- 44.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.5     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 102      |
|    time_elapsed    | 3725     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=447.96 +/- 179.26
Episode length: 112.30 +/- 44.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 112        |
|    mean_reward          | 448        |
| time/                   |            |
|    total_timesteps      | 209000     |
| train/                  |            |
|    approx_kl            | 0.02108497 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.479     |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0001     |
|    loss                 | 390        |
|    n_updates            | 1020       |
|    policy_gradient_loss | 0.0136     |
|    value_loss           | 913        |
----------------------------------------
Eval num_timesteps=209500, episode_reward=449.62 +/- 203.27
Episode length: 112.82 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=458.14 +/- 167.26
Episode length: 114.92 +/- 41.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=431.12 +/- 200.73
Episode length: 108.14 +/- 50.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.5     |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 103      |
|    time_elapsed    | 3772     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=424.86 +/- 167.51
Episode length: 106.56 +/- 41.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 425         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.056622487 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0001      |
|    loss                 | 949         |
|    n_updates            | 1030        |
|    policy_gradient_loss | 0.0223      |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=392.70 +/- 140.95
Episode length: 98.54 +/- 35.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 393      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=405.88 +/- 166.55
Episode length: 101.84 +/- 41.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=385.04 +/- 152.70
Episode length: 96.60 +/- 38.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.8     |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 104      |
|    time_elapsed    | 3816     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=446.10 +/- 164.30
Episode length: 111.94 +/- 41.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 446         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.031707212 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.495      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0001      |
|    loss                 | 819         |
|    n_updates            | 1040        |
|    policy_gradient_loss | 0.0137      |
|    value_loss           | 1.12e+03    |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=467.28 +/- 211.94
Episode length: 117.06 +/- 52.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=403.56 +/- 175.08
Episode length: 101.22 +/- 43.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=416.34 +/- 173.99
Episode length: 104.36 +/- 43.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=409.44 +/- 185.23
Episode length: 102.68 +/- 46.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 105      |
|    time_elapsed    | 3870     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=458.30 +/- 162.11
Episode length: 114.90 +/- 40.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 458         |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.024554314 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0001      |
|    loss                 | 787         |
|    n_updates            | 1050        |
|    policy_gradient_loss | 0.0194      |
|    value_loss           | 1.81e+03    |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=459.02 +/- 207.05
Episode length: 115.10 +/- 51.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=418.58 +/- 182.51
Episode length: 105.02 +/- 45.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=447.18 +/- 165.52
Episode length: 112.16 +/- 41.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.8     |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 106      |
|    time_elapsed    | 3917     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=477.14 +/- 215.17
Episode length: 119.66 +/- 53.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 477         |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.029840214 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0001      |
|    loss                 | 706         |
|    n_updates            | 1060        |
|    policy_gradient_loss | 0.0143      |
|    value_loss           | 1.18e+03    |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=449.50 +/- 192.40
Episode length: 112.76 +/- 48.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=439.56 +/- 171.51
Episode length: 110.32 +/- 42.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=378.58 +/- 179.33
Episode length: 94.94 +/- 44.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 107      |
|    time_elapsed    | 3962     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=389.30 +/- 130.36
Episode length: 97.66 +/- 32.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.7        |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.018074462 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.501      |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0001      |
|    loss                 | 784         |
|    n_updates            | 1070        |
|    policy_gradient_loss | 0.0203      |
|    value_loss           | 1.47e+03    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=373.52 +/- 144.35
Episode length: 93.76 +/- 36.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=415.06 +/- 157.64
Episode length: 104.16 +/- 39.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=368.76 +/- 158.56
Episode length: 92.58 +/- 39.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 108      |
|    time_elapsed    | 4003     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=365.62 +/- 111.05
Episode length: 91.84 +/- 27.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.8        |
|    mean_reward          | 366         |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.013595127 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0001      |
|    loss                 | 433         |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 944         |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=367.24 +/- 121.26
Episode length: 92.18 +/- 30.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.2     |
|    mean_reward     | 367      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=375.98 +/- 84.40
Episode length: 94.44 +/- 21.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=368.26 +/- 99.92
Episode length: 92.52 +/- 25.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99       |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 109      |
|    time_elapsed    | 4045     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=401.66 +/- 118.90
Episode length: 100.78 +/- 29.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 402         |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.029404394 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0001      |
|    loss                 | 317         |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.0131      |
|    value_loss           | 978         |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=362.76 +/- 125.74
Episode length: 91.10 +/- 31.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.1     |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=380.36 +/- 109.52
Episode length: 95.46 +/- 27.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=389.66 +/- 136.04
Episode length: 97.80 +/- 34.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 110      |
|    time_elapsed    | 4087     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=350.42 +/- 78.68
Episode length: 87.92 +/- 19.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.9        |
|    mean_reward          | 350         |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.029777499 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0001      |
|    loss                 | 772         |
|    n_updates            | 1100        |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=340.54 +/- 65.26
Episode length: 85.54 +/- 16.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.5     |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=335.04 +/- 80.22
Episode length: 84.12 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.1     |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=352.54 +/- 70.58
Episode length: 88.52 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.7     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 111      |
|    time_elapsed    | 4125     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=384.70 +/- 153.37
Episode length: 96.52 +/- 38.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.5       |
|    mean_reward          | 385        |
| time/                   |            |
|    total_timesteps      | 227500     |
| train/                  |            |
|    approx_kl            | 0.02895965 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.468     |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0001     |
|    loss                 | 221        |
|    n_updates            | 1110       |
|    policy_gradient_loss | 0.0137     |
|    value_loss           | 644        |
----------------------------------------
Eval num_timesteps=228000, episode_reward=418.96 +/- 190.08
Episode length: 105.10 +/- 47.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=447.44 +/- 160.63
Episode length: 112.16 +/- 40.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=427.54 +/- 192.85
Episode length: 107.16 +/- 48.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 112      |
|    time_elapsed    | 4166     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=433.66 +/- 154.97
Episode length: 108.80 +/- 38.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 434         |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.024737932 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0001      |
|    loss                 | 598         |
|    n_updates            | 1120        |
|    policy_gradient_loss | 0.0227      |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=446.62 +/- 186.11
Episode length: 112.06 +/- 46.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=425.62 +/- 173.76
Episode length: 106.72 +/- 43.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=418.14 +/- 163.48
Episode length: 104.90 +/- 40.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.4     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 113      |
|    time_elapsed    | 4211     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=406.90 +/- 148.36
Episode length: 102.12 +/- 37.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 407        |
| time/                   |            |
|    total_timesteps      | 231500     |
| train/                  |            |
|    approx_kl            | 0.06871723 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.506     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.0001     |
|    loss                 | 516        |
|    n_updates            | 1130       |
|    policy_gradient_loss | 0.0114     |
|    value_loss           | 1.39e+03   |
----------------------------------------
Eval num_timesteps=232000, episode_reward=416.62 +/- 161.72
Episode length: 104.48 +/- 40.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=477.16 +/- 167.65
Episode length: 119.70 +/- 41.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=452.36 +/- 171.46
Episode length: 113.46 +/- 42.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.7     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 114      |
|    time_elapsed    | 4256     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=415.38 +/- 113.84
Episode length: 104.22 +/- 28.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 415         |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.025537182 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0001      |
|    loss                 | 445         |
|    n_updates            | 1140        |
|    policy_gradient_loss | 0.0136      |
|    value_loss           | 860         |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=389.18 +/- 114.90
Episode length: 97.74 +/- 28.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=361.22 +/- 122.06
Episode length: 90.68 +/- 30.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=399.48 +/- 137.26
Episode length: 100.26 +/- 34.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=382.60 +/- 125.02
Episode length: 96.02 +/- 31.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.3     |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 115      |
|    time_elapsed    | 4305     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=410.12 +/- 167.95
Episode length: 102.92 +/- 42.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 410         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.031215105 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.497      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0001      |
|    loss                 | 221         |
|    n_updates            | 1150        |
|    policy_gradient_loss | 0.0207      |
|    value_loss           | 766         |
-----------------------------------------
Eval num_timesteps=236500, episode_reward=412.50 +/- 140.88
Episode length: 103.48 +/- 35.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=387.76 +/- 152.34
Episode length: 97.34 +/- 38.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=456.02 +/- 144.27
Episode length: 114.32 +/- 36.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.8     |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 116      |
|    time_elapsed    | 4350     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=416.12 +/- 163.37
Episode length: 104.32 +/- 40.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 416         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.024508372 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.488      |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0001      |
|    loss                 | 577         |
|    n_updates            | 1160        |
|    policy_gradient_loss | 0.017       |
|    value_loss           | 961         |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=462.88 +/- 170.95
Episode length: 116.04 +/- 42.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=453.48 +/- 167.00
Episode length: 113.70 +/- 41.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=463.98 +/- 158.36
Episode length: 116.32 +/- 39.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 409      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 117      |
|    time_elapsed    | 4395     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=392.42 +/- 162.94
Episode length: 98.46 +/- 40.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.5        |
|    mean_reward          | 392         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.029887121 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0001      |
|    loss                 | 591         |
|    n_updates            | 1170        |
|    policy_gradient_loss | 0.021       |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=441.70 +/- 151.39
Episode length: 110.78 +/- 37.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=432.84 +/- 163.41
Episode length: 108.66 +/- 40.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=447.46 +/- 161.85
Episode length: 112.24 +/- 40.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 118      |
|    time_elapsed    | 4440     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=419.66 +/- 215.37
Episode length: 105.28 +/- 53.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 105        |
|    mean_reward          | 420        |
| time/                   |            |
|    total_timesteps      | 242000     |
| train/                  |            |
|    approx_kl            | 0.03643622 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.5       |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0001     |
|    loss                 | 800        |
|    n_updates            | 1180       |
|    policy_gradient_loss | 0.0187     |
|    value_loss           | 1.51e+03   |
----------------------------------------
Eval num_timesteps=242500, episode_reward=452.40 +/- 209.11
Episode length: 113.50 +/- 52.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=425.74 +/- 217.16
Episode length: 106.86 +/- 54.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=461.40 +/- 197.50
Episode length: 115.70 +/- 49.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 119      |
|    time_elapsed    | 4485     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=432.36 +/- 242.08
Episode length: 108.44 +/- 60.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 432         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.018765144 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.28e+03    |
|    n_updates            | 1190        |
|    policy_gradient_loss | 0.0132      |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=244500, episode_reward=424.48 +/- 210.15
Episode length: 106.46 +/- 52.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=431.28 +/- 213.80
Episode length: 108.20 +/- 53.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=446.64 +/- 244.22
Episode length: 112.06 +/- 61.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 120      |
|    time_elapsed    | 4526     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=475.06 +/- 237.19
Episode length: 119.12 +/- 59.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 119         |
|    mean_reward          | 475         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.023285132 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.1e+03     |
|    n_updates            | 1200        |
|    policy_gradient_loss | 0.0182      |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=246500, episode_reward=473.74 +/- 215.94
Episode length: 118.82 +/- 53.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=385.70 +/- 247.25
Episode length: 96.78 +/- 61.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=456.24 +/- 216.81
Episode length: 114.48 +/- 54.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 421      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 121      |
|    time_elapsed    | 4567     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=414.98 +/- 238.61
Episode length: 104.10 +/- 59.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | 415        |
| time/                   |            |
|    total_timesteps      | 248000     |
| train/                  |            |
|    approx_kl            | 0.04097573 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.401     |
|    explained_variance   | 0.514      |
|    learning_rate        | 0.0001     |
|    loss                 | 950        |
|    n_updates            | 1210       |
|    policy_gradient_loss | 0.0247     |
|    value_loss           | 1.7e+03    |
----------------------------------------
Eval num_timesteps=248500, episode_reward=481.94 +/- 244.34
Episode length: 120.84 +/- 61.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=451.44 +/- 227.79
Episode length: 113.26 +/- 56.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=387.08 +/- 202.45
Episode length: 97.20 +/- 50.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 122      |
|    time_elapsed    | 4603     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=447.02 +/- 221.33
Episode length: 112.14 +/- 55.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 112       |
|    mean_reward          | 447       |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0709374 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.344    |
|    explained_variance   | 0.36      |
|    learning_rate        | 0.0001    |
|    loss                 | 713       |
|    n_updates            | 1220      |
|    policy_gradient_loss | 0.0272    |
|    value_loss           | 2.02e+03  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=450.58 +/- 263.04
Episode length: 113.02 +/- 65.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=500.52 +/- 258.24
Episode length: 125.44 +/- 64.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
New best mean reward!
Eval num_timesteps=251500, episode_reward=423.58 +/- 245.19
Episode length: 106.16 +/- 61.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 123      |
|    time_elapsed    | 4644     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=464.10 +/- 215.75
Episode length: 116.46 +/- 53.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 116        |
|    mean_reward          | 464        |
| time/                   |            |
|    total_timesteps      | 252000     |
| train/                  |            |
|    approx_kl            | 0.15404312 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.508      |
|    learning_rate        | 0.0001     |
|    loss                 | 1e+03      |
|    n_updates            | 1230       |
|    policy_gradient_loss | 0.0316     |
|    value_loss           | 1.55e+03   |
----------------------------------------
Eval num_timesteps=252500, episode_reward=481.96 +/- 241.72
Episode length: 120.86 +/- 60.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=446.96 +/- 239.43
Episode length: 112.08 +/- 59.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=490.04 +/- 225.62
Episode length: 122.92 +/- 56.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 490      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 433      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 124      |
|    time_elapsed    | 4685     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=492.64 +/- 227.52
Episode length: 123.48 +/- 56.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 493         |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.121264964 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.36       |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0001      |
|    loss                 | 408         |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.06        |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=394.86 +/- 194.55
Episode length: 99.04 +/- 48.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=454.18 +/- 184.28
Episode length: 113.78 +/- 46.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=434.04 +/- 225.11
Episode length: 108.98 +/- 56.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=498.76 +/- 222.80
Episode length: 125.06 +/- 55.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 125      |
|    time_elapsed    | 4731     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=451.28 +/- 224.10
Episode length: 113.24 +/- 56.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 113       |
|    mean_reward          | 451       |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0512752 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.506    |
|    explained_variance   | 0.536     |
|    learning_rate        | 0.0001    |
|    loss                 | 522       |
|    n_updates            | 1250      |
|    policy_gradient_loss | 0.0268    |
|    value_loss           | 1.5e+03   |
---------------------------------------
Eval num_timesteps=257000, episode_reward=394.60 +/- 217.63
Episode length: 99.04 +/- 54.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=531.10 +/- 272.40
Episode length: 133.10 +/- 68.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 531      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
New best mean reward!
Eval num_timesteps=258000, episode_reward=364.88 +/- 202.10
Episode length: 91.60 +/- 50.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 432      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 126      |
|    time_elapsed    | 4770     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=399.80 +/- 138.78
Episode length: 100.30 +/- 34.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.058838535 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.0001      |
|    loss                 | 709         |
|    n_updates            | 1260        |
|    policy_gradient_loss | 0.0236      |
|    value_loss           | 2.13e+03    |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=396.00 +/- 176.43
Episode length: 99.40 +/- 44.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=421.38 +/- 167.04
Episode length: 105.70 +/- 41.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=442.00 +/- 175.49
Episode length: 110.86 +/- 43.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 422      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 127      |
|    time_elapsed    | 4811     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=430.98 +/- 156.80
Episode length: 108.12 +/- 39.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 431         |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.042586327 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0001      |
|    loss                 | 460         |
|    n_updates            | 1270        |
|    policy_gradient_loss | 0.0168      |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=416.28 +/- 170.34
Episode length: 104.40 +/- 42.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=397.72 +/- 162.74
Episode length: 99.80 +/- 40.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=395.74 +/- 136.76
Episode length: 99.28 +/- 34.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 128      |
|    time_elapsed    | 4853     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=462.56 +/- 241.05
Episode length: 115.98 +/- 60.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 116        |
|    mean_reward          | 463        |
| time/                   |            |
|    total_timesteps      | 262500     |
| train/                  |            |
|    approx_kl            | 0.11521222 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0001     |
|    loss                 | 368        |
|    n_updates            | 1280       |
|    policy_gradient_loss | 0.0203     |
|    value_loss           | 1.12e+03   |
----------------------------------------
Eval num_timesteps=263000, episode_reward=457.02 +/- 208.42
Episode length: 114.60 +/- 52.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=463.16 +/- 218.12
Episode length: 116.16 +/- 54.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=492.60 +/- 253.01
Episode length: 123.50 +/- 63.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 426      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 129      |
|    time_elapsed    | 4897     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=401.34 +/- 218.54
Episode length: 100.70 +/- 54.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 401         |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.055256404 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0001      |
|    loss                 | 400         |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.0192      |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=432.10 +/- 269.16
Episode length: 108.36 +/- 67.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=501.84 +/- 265.35
Episode length: 125.78 +/- 66.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=452.54 +/- 239.61
Episode length: 113.48 +/- 59.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 130      |
|    time_elapsed    | 4940     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=362.04 +/- 225.28
Episode length: 90.94 +/- 56.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.9        |
|    mean_reward          | 362         |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.033815768 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.07e+03    |
|    n_updates            | 1300        |
|    policy_gradient_loss | 0.022       |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=479.82 +/- 242.20
Episode length: 120.30 +/- 60.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 480      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=421.88 +/- 271.54
Episode length: 105.78 +/- 67.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=419.84 +/- 222.50
Episode length: 105.30 +/- 55.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 131      |
|    time_elapsed    | 4983     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=439.40 +/- 223.71
Episode length: 110.28 +/- 55.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 439         |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.058918595 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 822         |
|    n_updates            | 1310        |
|    policy_gradient_loss | 0.0674      |
|    value_loss           | 2.29e+03    |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=429.70 +/- 254.92
Episode length: 107.82 +/- 63.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=506.82 +/- 254.05
Episode length: 127.00 +/- 63.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=447.48 +/- 244.66
Episode length: 112.22 +/- 61.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 132      |
|    time_elapsed    | 5029     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=477.40 +/- 245.81
Episode length: 119.66 +/- 61.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 477         |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.040361248 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0001      |
|    loss                 | 924         |
|    n_updates            | 1320        |
|    policy_gradient_loss | 0.026       |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=388.00 +/- 240.20
Episode length: 97.34 +/- 60.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=489.00 +/- 240.23
Episode length: 122.60 +/- 60.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=407.30 +/- 226.23
Episode length: 102.24 +/- 56.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 453      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 133      |
|    time_elapsed    | 5074     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=363.30 +/- 218.05
Episode length: 91.24 +/- 54.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.2        |
|    mean_reward          | 363         |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.044388674 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0001      |
|    loss                 | 870         |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.0232      |
|    value_loss           | 1.59e+03    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=414.44 +/- 249.95
Episode length: 103.98 +/- 62.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=429.80 +/- 248.30
Episode length: 107.80 +/- 62.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=425.24 +/- 250.25
Episode length: 106.64 +/- 62.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 134      |
|    time_elapsed    | 5117     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=465.96 +/- 218.23
Episode length: 116.90 +/- 54.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 117       |
|    mean_reward          | 466       |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0661242 |
|    clip_fraction        | 0.341     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.323    |
|    explained_variance   | 0.288     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.12e+03  |
|    n_updates            | 1340      |
|    policy_gradient_loss | 0.0531    |
|    value_loss           | 2.66e+03  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=485.42 +/- 222.35
Episode length: 121.74 +/- 55.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=461.54 +/- 210.51
Episode length: 115.72 +/- 52.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=394.58 +/- 186.46
Episode length: 99.02 +/- 46.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 407      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 135      |
|    time_elapsed    | 5160     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=429.02 +/- 222.06
Episode length: 107.52 +/- 55.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 429        |
| time/                   |            |
|    total_timesteps      | 276500     |
| train/                  |            |
|    approx_kl            | 0.06263251 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.0001     |
|    loss                 | 326        |
|    n_updates            | 1350       |
|    policy_gradient_loss | 0.0328     |
|    value_loss           | 1.35e+03   |
----------------------------------------
Eval num_timesteps=277000, episode_reward=443.02 +/- 250.27
Episode length: 111.14 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=461.18 +/- 215.39
Episode length: 115.66 +/- 53.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=440.58 +/- 204.30
Episode length: 110.50 +/- 51.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=407.64 +/- 208.15
Episode length: 102.28 +/- 52.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 428      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 136      |
|    time_elapsed    | 5212     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=466.14 +/- 235.74
Episode length: 116.86 +/- 58.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 117         |
|    mean_reward          | 466         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.028134612 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0001      |
|    loss                 | 433         |
|    n_updates            | 1360        |
|    policy_gradient_loss | 0.0241      |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=459.66 +/- 235.90
Episode length: 115.32 +/- 58.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=414.40 +/- 203.85
Episode length: 103.94 +/- 50.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=403.90 +/- 228.26
Episode length: 101.36 +/- 57.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 137      |
|    time_elapsed    | 5254     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=409.80 +/- 293.24
Episode length: 102.74 +/- 73.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 103       |
|    mean_reward          | 410       |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0676343 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.412    |
|    explained_variance   | 0.419     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.08e+03  |
|    n_updates            | 1370      |
|    policy_gradient_loss | 0.0354    |
|    value_loss           | 1.85e+03  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=471.42 +/- 229.70
Episode length: 118.22 +/- 57.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=471.30 +/- 259.55
Episode length: 118.18 +/- 64.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=416.58 +/- 260.97
Episode length: 104.54 +/- 65.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 405      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 138      |
|    time_elapsed    | 5296     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=462.92 +/- 285.03
Episode length: 116.04 +/- 71.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 116        |
|    mean_reward          | 463        |
| time/                   |            |
|    total_timesteps      | 283000     |
| train/                  |            |
|    approx_kl            | 0.05803299 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.312      |
|    learning_rate        | 0.0001     |
|    loss                 | 921        |
|    n_updates            | 1380       |
|    policy_gradient_loss | 0.0252     |
|    value_loss           | 2.09e+03   |
----------------------------------------
Eval num_timesteps=283500, episode_reward=419.30 +/- 233.82
Episode length: 105.22 +/- 58.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=406.78 +/- 264.33
Episode length: 102.00 +/- 66.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=459.46 +/- 278.91
Episode length: 115.24 +/- 69.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 139      |
|    time_elapsed    | 5335     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=436.78 +/- 239.02
Episode length: 109.58 +/- 59.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 437         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.053813927 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.0001      |
|    loss                 | 824         |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.0331      |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=400.76 +/- 252.29
Episode length: 100.58 +/- 63.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=468.34 +/- 262.97
Episode length: 117.48 +/- 65.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=481.60 +/- 263.13
Episode length: 120.78 +/- 65.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 433      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 140      |
|    time_elapsed    | 5381     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=384.46 +/- 219.42
Episode length: 96.48 +/- 54.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.5       |
|    mean_reward          | 384        |
| time/                   |            |
|    total_timesteps      | 287000     |
| train/                  |            |
|    approx_kl            | 0.05970738 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.331     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0001     |
|    loss                 | 359        |
|    n_updates            | 1400       |
|    policy_gradient_loss | 0.033      |
|    value_loss           | 1.3e+03    |
----------------------------------------
Eval num_timesteps=287500, episode_reward=426.88 +/- 218.97
Episode length: 107.12 +/- 54.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=367.88 +/- 209.18
Episode length: 92.36 +/- 52.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=409.86 +/- 216.99
Episode length: 102.90 +/- 54.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 437      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 141      |
|    time_elapsed    | 5420     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=456.66 +/- 272.01
Episode length: 114.64 +/- 67.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 457         |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.055372734 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.0001      |
|    loss                 | 845         |
|    n_updates            | 1410        |
|    policy_gradient_loss | 0.0436      |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=456.72 +/- 302.23
Episode length: 114.52 +/- 75.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=426.92 +/- 241.99
Episode length: 107.06 +/- 60.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=399.44 +/- 224.83
Episode length: 100.22 +/- 56.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 452      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 142      |
|    time_elapsed    | 5464     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=482.62 +/- 249.99
Episode length: 120.92 +/- 62.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 121        |
|    mean_reward          | 483        |
| time/                   |            |
|    total_timesteps      | 291000     |
| train/                  |            |
|    approx_kl            | 0.06415957 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.499     |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0001     |
|    loss                 | 608        |
|    n_updates            | 1420       |
|    policy_gradient_loss | 0.026      |
|    value_loss           | 1.63e+03   |
----------------------------------------
Eval num_timesteps=291500, episode_reward=442.04 +/- 250.81
Episode length: 110.96 +/- 62.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=397.32 +/- 228.49
Episode length: 99.76 +/- 57.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=463.20 +/- 221.42
Episode length: 116.22 +/- 55.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 465      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 143      |
|    time_elapsed    | 5506     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=435.60 +/- 234.95
Episode length: 109.34 +/- 58.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 436         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.038734395 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.478      |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.0001      |
|    loss                 | 687         |
|    n_updates            | 1430        |
|    policy_gradient_loss | 0.0258      |
|    value_loss           | 1.59e+03    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=407.90 +/- 245.53
Episode length: 102.36 +/- 61.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=463.34 +/- 249.41
Episode length: 116.26 +/- 62.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=443.94 +/- 220.29
Episode length: 111.34 +/- 55.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 461      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 144      |
|    time_elapsed    | 5546     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=498.34 +/- 241.25
Episode length: 124.98 +/- 60.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 125        |
|    mean_reward          | 498        |
| time/                   |            |
|    total_timesteps      | 295000     |
| train/                  |            |
|    approx_kl            | 0.11578202 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0001     |
|    loss                 | 540        |
|    n_updates            | 1440       |
|    policy_gradient_loss | 0.0321     |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=295500, episode_reward=481.02 +/- 234.02
Episode length: 120.56 +/- 58.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=472.86 +/- 239.14
Episode length: 118.60 +/- 59.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=438.90 +/- 225.77
Episode length: 110.10 +/- 56.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 474      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 145      |
|    time_elapsed    | 5590     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=451.56 +/- 187.76
Episode length: 113.28 +/- 46.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 452         |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.039532214 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0001      |
|    loss                 | 682         |
|    n_updates            | 1450        |
|    policy_gradient_loss | 0.0255      |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=415.98 +/- 198.96
Episode length: 104.42 +/- 49.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=461.28 +/- 200.90
Episode length: 115.74 +/- 50.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=443.08 +/- 227.48
Episode length: 111.18 +/- 56.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=396.06 +/- 180.49
Episode length: 99.46 +/- 45.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 146      |
|    time_elapsed    | 5641     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=460.88 +/- 208.64
Episode length: 115.62 +/- 52.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 461         |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.079246335 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.0001      |
|    loss                 | 825         |
|    n_updates            | 1460        |
|    policy_gradient_loss | 0.0258      |
|    value_loss           | 2.23e+03    |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=354.64 +/- 187.74
Episode length: 89.06 +/- 46.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.1     |
|    mean_reward     | 355      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=443.00 +/- 190.19
Episode length: 111.06 +/- 47.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=440.96 +/- 179.51
Episode length: 110.64 +/- 44.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 147      |
|    time_elapsed    | 5684     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=470.02 +/- 170.67
Episode length: 117.86 +/- 42.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 118        |
|    mean_reward          | 470        |
| time/                   |            |
|    total_timesteps      | 301500     |
| train/                  |            |
|    approx_kl            | 0.09890374 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.23e+03   |
|    n_updates            | 1470       |
|    policy_gradient_loss | 0.0239     |
|    value_loss           | 1.91e+03   |
----------------------------------------
Eval num_timesteps=302000, episode_reward=415.82 +/- 193.13
Episode length: 104.30 +/- 48.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=424.52 +/- 189.96
Episode length: 106.48 +/- 47.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=459.88 +/- 196.73
Episode length: 115.38 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 437      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 148      |
|    time_elapsed    | 5727     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=491.82 +/- 247.82
Episode length: 123.28 +/- 61.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 492         |
| time/                   |             |
|    total_timesteps      | 303500      |
| train/                  |             |
|    approx_kl            | 0.050383992 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.328      |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.0001      |
|    loss                 | 474         |
|    n_updates            | 1480        |
|    policy_gradient_loss | 0.017       |
|    value_loss           | 1.47e+03    |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=407.08 +/- 196.55
Episode length: 102.24 +/- 49.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=452.02 +/- 273.70
Episode length: 113.36 +/- 68.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=404.14 +/- 251.67
Episode length: 101.46 +/- 62.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 149      |
|    time_elapsed    | 5769     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=428.90 +/- 270.53
Episode length: 107.60 +/- 67.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 429         |
| time/                   |             |
|    total_timesteps      | 305500      |
| train/                  |             |
|    approx_kl            | 0.096067734 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0001      |
|    loss                 | 450         |
|    n_updates            | 1490        |
|    policy_gradient_loss | 0.0336      |
|    value_loss           | 1.46e+03    |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=350.14 +/- 205.89
Episode length: 87.86 +/- 51.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=398.12 +/- 237.05
Episode length: 99.90 +/- 59.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=433.84 +/- 259.40
Episode length: 108.82 +/- 64.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97       |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 150      |
|    time_elapsed    | 5805     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=416.78 +/- 224.39
Episode length: 104.54 +/- 56.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 105        |
|    mean_reward          | 417        |
| time/                   |            |
|    total_timesteps      | 307500     |
| train/                  |            |
|    approx_kl            | 0.05789357 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.364      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.37e+03   |
|    n_updates            | 1500       |
|    policy_gradient_loss | 0.049      |
|    value_loss           | 2.51e+03   |
----------------------------------------
Eval num_timesteps=308000, episode_reward=362.64 +/- 238.39
Episode length: 91.10 +/- 59.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.1     |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=409.36 +/- 230.94
Episode length: 102.72 +/- 57.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=421.82 +/- 266.85
Episode length: 105.82 +/- 66.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.1     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 151      |
|    time_elapsed    | 5846     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=362.36 +/- 207.18
Episode length: 90.92 +/- 51.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.9        |
|    mean_reward          | 362         |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.061687037 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.377      |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.0001      |
|    loss                 | 922         |
|    n_updates            | 1510        |
|    policy_gradient_loss | 0.0419      |
|    value_loss           | 2.11e+03    |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=388.06 +/- 239.19
Episode length: 97.42 +/- 59.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=360.22 +/- 225.86
Episode length: 90.46 +/- 56.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=405.66 +/- 284.15
Episode length: 101.76 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 152      |
|    time_elapsed    | 5883     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=404.04 +/- 254.95
Episode length: 101.32 +/- 63.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 404         |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.051413715 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.0001      |
|    loss                 | 649         |
|    n_updates            | 1520        |
|    policy_gradient_loss | 0.0378      |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=386.30 +/- 233.58
Episode length: 96.90 +/- 58.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=456.82 +/- 329.09
Episode length: 114.56 +/- 82.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=344.32 +/- 215.50
Episode length: 86.42 +/- 53.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.4     |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.4     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 153      |
|    time_elapsed    | 5924     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=366.48 +/- 180.18
Episode length: 92.00 +/- 45.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92         |
|    mean_reward          | 366        |
| time/                   |            |
|    total_timesteps      | 313500     |
| train/                  |            |
|    approx_kl            | 0.03809271 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0001     |
|    loss                 | 807        |
|    n_updates            | 1530       |
|    policy_gradient_loss | 0.0365     |
|    value_loss           | 1.67e+03   |
----------------------------------------
Eval num_timesteps=314000, episode_reward=387.28 +/- 252.00
Episode length: 97.28 +/- 62.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=342.02 +/- 188.45
Episode length: 85.90 +/- 47.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.9     |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=426.06 +/- 245.66
Episode length: 106.90 +/- 61.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 154      |
|    time_elapsed    | 5964     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=451.20 +/- 306.09
Episode length: 113.14 +/- 76.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 113        |
|    mean_reward          | 451        |
| time/                   |            |
|    total_timesteps      | 315500     |
| train/                  |            |
|    approx_kl            | 0.09026861 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.416     |
|    explained_variance   | 0.279      |
|    learning_rate        | 0.0001     |
|    loss                 | 813        |
|    n_updates            | 1540       |
|    policy_gradient_loss | 0.0388     |
|    value_loss           | 1.9e+03    |
----------------------------------------
Eval num_timesteps=316000, episode_reward=356.96 +/- 231.23
Episode length: 89.64 +/- 57.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.6     |
|    mean_reward     | 357      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=424.14 +/- 300.55
Episode length: 106.38 +/- 75.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=424.92 +/- 298.46
Episode length: 106.60 +/- 74.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99       |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 155      |
|    time_elapsed    | 6005     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=316.90 +/- 204.50
Episode length: 79.56 +/- 51.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.6       |
|    mean_reward          | 317        |
| time/                   |            |
|    total_timesteps      | 317500     |
| train/                  |            |
|    approx_kl            | 0.12895094 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.222      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.14e+03   |
|    n_updates            | 1550       |
|    policy_gradient_loss | 0.0383     |
|    value_loss           | 1.88e+03   |
----------------------------------------
Eval num_timesteps=318000, episode_reward=396.70 +/- 226.54
Episode length: 99.56 +/- 56.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=376.36 +/- 284.96
Episode length: 94.46 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=320.32 +/- 199.76
Episode length: 80.46 +/- 49.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 156      |
|    time_elapsed    | 6038     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=359.70 +/- 214.46
Episode length: 90.32 +/- 53.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.3        |
|    mean_reward          | 360         |
| time/                   |             |
|    total_timesteps      | 319500      |
| train/                  |             |
|    approx_kl            | 0.057987697 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.0001      |
|    loss                 | 808         |
|    n_updates            | 1560        |
|    policy_gradient_loss | 0.0366      |
|    value_loss           | 2.23e+03    |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=394.20 +/- 221.58
Episode length: 98.90 +/- 55.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=393.76 +/- 261.67
Episode length: 98.80 +/- 65.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=375.96 +/- 263.40
Episode length: 94.30 +/- 65.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.3     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=395.22 +/- 249.38
Episode length: 99.10 +/- 62.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.6     |
|    ep_rew_mean     | 381      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 157      |
|    time_elapsed    | 6082     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=420.74 +/- 261.80
Episode length: 105.54 +/- 65.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 421        |
| time/                   |            |
|    total_timesteps      | 322000     |
| train/                  |            |
|    approx_kl            | 0.12590857 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.4       |
|    explained_variance   | 0.186      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.08e+03   |
|    n_updates            | 1570       |
|    policy_gradient_loss | 0.0367     |
|    value_loss           | 2.11e+03   |
----------------------------------------
Eval num_timesteps=322500, episode_reward=425.18 +/- 272.43
Episode length: 106.68 +/- 68.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=476.74 +/- 255.99
Episode length: 119.50 +/- 63.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=425.56 +/- 238.45
Episode length: 106.78 +/- 59.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.2     |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 158      |
|    time_elapsed    | 6122     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=445.76 +/- 235.80
Episode length: 111.76 +/- 58.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 446         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.077879965 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0001      |
|    loss                 | 860         |
|    n_updates            | 1580        |
|    policy_gradient_loss | 0.0518      |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=425.04 +/- 204.55
Episode length: 106.60 +/- 51.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=465.94 +/- 260.34
Episode length: 116.90 +/- 65.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=509.28 +/- 222.27
Episode length: 127.68 +/- 55.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.4     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 159      |
|    time_elapsed    | 6163     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=460.84 +/- 227.99
Episode length: 115.58 +/- 57.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 116        |
|    mean_reward          | 461        |
| time/                   |            |
|    total_timesteps      | 326000     |
| train/                  |            |
|    approx_kl            | 0.05877261 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.467      |
|    learning_rate        | 0.0001     |
|    loss                 | 410        |
|    n_updates            | 1590       |
|    policy_gradient_loss | 0.0285     |
|    value_loss           | 1.18e+03   |
----------------------------------------
Eval num_timesteps=326500, episode_reward=382.58 +/- 178.07
Episode length: 96.04 +/- 44.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=410.64 +/- 188.71
Episode length: 103.10 +/- 47.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=419.32 +/- 227.68
Episode length: 105.14 +/- 56.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 405      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 160      |
|    time_elapsed    | 6205     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=381.42 +/- 217.08
Episode length: 95.76 +/- 54.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.8        |
|    mean_reward          | 381         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.048226412 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0001      |
|    loss                 | 660         |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.0181      |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=449.46 +/- 198.55
Episode length: 112.76 +/- 49.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=475.74 +/- 221.55
Episode length: 119.30 +/- 55.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=468.96 +/- 245.55
Episode length: 117.56 +/- 61.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 429      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 161      |
|    time_elapsed    | 6247     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=396.18 +/- 204.05
Episode length: 99.36 +/- 51.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.4        |
|    mean_reward          | 396         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.021755282 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.33       |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.09e+03    |
|    n_updates            | 1610        |
|    policy_gradient_loss | 0.0295      |
|    value_loss           | 1.47e+03    |
-----------------------------------------
Eval num_timesteps=330500, episode_reward=414.98 +/- 198.79
Episode length: 104.14 +/- 49.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=478.22 +/- 255.53
Episode length: 119.96 +/- 63.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=442.34 +/- 256.64
Episode length: 110.90 +/- 64.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 455      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 162      |
|    time_elapsed    | 6289     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=397.16 +/- 251.60
Episode length: 99.62 +/- 62.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.6        |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.028115826 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.327      |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.3e+03     |
|    n_updates            | 1620        |
|    policy_gradient_loss | 0.0215      |
|    value_loss           | 1.65e+03    |
-----------------------------------------
Eval num_timesteps=332500, episode_reward=413.74 +/- 227.32
Episode length: 103.84 +/- 56.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=403.66 +/- 245.84
Episode length: 101.30 +/- 61.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=415.26 +/- 277.13
Episode length: 104.24 +/- 69.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 458      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 163      |
|    time_elapsed    | 6329     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=362.60 +/- 249.92
Episode length: 91.06 +/- 62.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 91.1       |
|    mean_reward          | 363        |
| time/                   |            |
|    total_timesteps      | 334000     |
| train/                  |            |
|    approx_kl            | 0.06569736 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.0001     |
|    loss                 | 883        |
|    n_updates            | 1630       |
|    policy_gradient_loss | 0.0368     |
|    value_loss           | 2.01e+03   |
----------------------------------------
Eval num_timesteps=334500, episode_reward=406.02 +/- 204.86
Episode length: 101.92 +/- 51.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=379.58 +/- 252.47
Episode length: 95.32 +/- 63.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=416.08 +/- 260.83
Episode length: 104.42 +/- 65.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 457      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 164      |
|    time_elapsed    | 6364     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=350.34 +/- 216.78
Episode length: 87.98 +/- 54.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88          |
|    mean_reward          | 350         |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.027758278 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | 0.391       |
|    learning_rate        | 0.0001      |
|    loss                 | 667         |
|    n_updates            | 1640        |
|    policy_gradient_loss | 0.0363      |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=336500, episode_reward=371.90 +/- 244.98
Episode length: 93.32 +/- 61.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=395.92 +/- 237.64
Episode length: 99.32 +/- 59.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=360.42 +/- 213.48
Episode length: 90.48 +/- 53.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 165      |
|    time_elapsed    | 6401     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=315.44 +/- 182.54
Episode length: 79.22 +/- 45.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.2       |
|    mean_reward          | 315        |
| time/                   |            |
|    total_timesteps      | 338000     |
| train/                  |            |
|    approx_kl            | 0.07395394 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.22       |
|    learning_rate        | 0.0001     |
|    loss                 | 980        |
|    n_updates            | 1650       |
|    policy_gradient_loss | 0.0359     |
|    value_loss           | 2.01e+03   |
----------------------------------------
Eval num_timesteps=338500, episode_reward=350.96 +/- 178.68
Episode length: 88.08 +/- 44.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=337.18 +/- 240.84
Episode length: 84.72 +/- 60.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.7     |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=313.94 +/- 212.04
Episode length: 78.86 +/- 53.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 166      |
|    time_elapsed    | 6434     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=354.36 +/- 229.93
Episode length: 88.98 +/- 57.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 89         |
|    mean_reward          | 354        |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.19745804 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.249      |
|    learning_rate        | 0.0001     |
|    loss                 | 973        |
|    n_updates            | 1660       |
|    policy_gradient_loss | 0.0388     |
|    value_loss           | 2.51e+03   |
----------------------------------------
Eval num_timesteps=340500, episode_reward=378.38 +/- 263.47
Episode length: 95.02 +/- 65.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=340.84 +/- 211.44
Episode length: 85.60 +/- 52.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.6     |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=319.12 +/- 149.77
Episode length: 80.24 +/- 37.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=358.52 +/- 207.12
Episode length: 89.98 +/- 51.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.6     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 167      |
|    time_elapsed    | 6478     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=391.06 +/- 264.02
Episode length: 98.16 +/- 66.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.2       |
|    mean_reward          | 391        |
| time/                   |            |
|    total_timesteps      | 342500     |
| train/                  |            |
|    approx_kl            | 0.06287171 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.551     |
|    explained_variance   | 0.296      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.07e+03   |
|    n_updates            | 1670       |
|    policy_gradient_loss | 0.0463     |
|    value_loss           | 2.26e+03   |
----------------------------------------
Eval num_timesteps=343000, episode_reward=395.12 +/- 234.10
Episode length: 99.20 +/- 58.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=371.66 +/- 222.40
Episode length: 93.26 +/- 55.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=338.62 +/- 203.77
Episode length: 84.98 +/- 50.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85       |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.3     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 168      |
|    time_elapsed    | 6514     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=439.72 +/- 272.53
Episode length: 110.36 +/- 68.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 110        |
|    mean_reward          | 440        |
| time/                   |            |
|    total_timesteps      | 344500     |
| train/                  |            |
|    approx_kl            | 0.07621613 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.535     |
|    explained_variance   | 0.271      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.09e+03   |
|    n_updates            | 1680       |
|    policy_gradient_loss | 0.0472     |
|    value_loss           | 1.92e+03   |
----------------------------------------
Eval num_timesteps=345000, episode_reward=472.00 +/- 327.01
Episode length: 118.34 +/- 81.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=409.78 +/- 253.38
Episode length: 102.86 +/- 63.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=404.10 +/- 243.79
Episode length: 101.36 +/- 60.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.6     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 169      |
|    time_elapsed    | 6556     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=328.10 +/- 198.60
Episode length: 82.40 +/- 49.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 82.4       |
|    mean_reward          | 328        |
| time/                   |            |
|    total_timesteps      | 346500     |
| train/                  |            |
|    approx_kl            | 0.10865046 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.578     |
|    explained_variance   | 0.0744     |
|    learning_rate        | 0.0001     |
|    loss                 | 833        |
|    n_updates            | 1690       |
|    policy_gradient_loss | 0.0701     |
|    value_loss           | 1.57e+03   |
----------------------------------------
Eval num_timesteps=347000, episode_reward=358.38 +/- 196.96
Episode length: 90.06 +/- 49.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=348.56 +/- 199.19
Episode length: 87.52 +/- 49.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.5     |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=314.10 +/- 225.49
Episode length: 78.92 +/- 56.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 170      |
|    time_elapsed    | 6589     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=370.80 +/- 242.42
Episode length: 93.16 +/- 60.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.2       |
|    mean_reward          | 371        |
| time/                   |            |
|    total_timesteps      | 348500     |
| train/                  |            |
|    approx_kl            | 0.08911635 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.495     |
|    explained_variance   | 0.0726     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.07e+03   |
|    n_updates            | 1700       |
|    policy_gradient_loss | 0.0431     |
|    value_loss           | 2.19e+03   |
----------------------------------------
Eval num_timesteps=349000, episode_reward=310.70 +/- 186.34
Episode length: 78.08 +/- 46.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=371.22 +/- 193.80
Episode length: 93.14 +/- 48.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=400.58 +/- 238.03
Episode length: 100.50 +/- 59.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.9     |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 171      |
|    time_elapsed    | 6625     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=332.08 +/- 154.32
Episode length: 83.44 +/- 38.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.4        |
|    mean_reward          | 332         |
| time/                   |             |
|    total_timesteps      | 350500      |
| train/                  |             |
|    approx_kl            | 0.035943903 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.462      |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.05e+03    |
|    n_updates            | 1710        |
|    policy_gradient_loss | 0.0273      |
|    value_loss           | 1.99e+03    |
-----------------------------------------
Eval num_timesteps=351000, episode_reward=415.66 +/- 287.60
Episode length: 104.34 +/- 71.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=361.62 +/- 205.85
Episode length: 90.74 +/- 51.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=370.98 +/- 210.61
Episode length: 93.10 +/- 52.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.8     |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 172      |
|    time_elapsed    | 6663     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=328.22 +/- 210.29
Episode length: 82.36 +/- 52.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.4        |
|    mean_reward          | 328         |
| time/                   |             |
|    total_timesteps      | 352500      |
| train/                  |             |
|    approx_kl            | 0.048591286 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0001      |
|    loss                 | 653         |
|    n_updates            | 1720        |
|    policy_gradient_loss | 0.0244      |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=353000, episode_reward=405.32 +/- 276.95
Episode length: 101.68 +/- 69.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=337.52 +/- 210.16
Episode length: 84.68 +/- 52.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.7     |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=327.24 +/- 230.83
Episode length: 82.22 +/- 57.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.2     |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.3     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 173      |
|    time_elapsed    | 6698     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=455.94 +/- 321.26
Episode length: 114.26 +/- 80.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 456         |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.035554394 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 930         |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.0341      |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=343.92 +/- 175.52
Episode length: 86.34 +/- 43.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=276.02 +/- 148.91
Episode length: 69.44 +/- 37.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.4     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=332.28 +/- 198.15
Episode length: 83.36 +/- 49.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.4     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.2     |
|    ep_rew_mean     | 339      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 174      |
|    time_elapsed    | 6733     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=331.86 +/- 207.91
Episode length: 83.36 +/- 51.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 83.4       |
|    mean_reward          | 332        |
| time/                   |            |
|    total_timesteps      | 356500     |
| train/                  |            |
|    approx_kl            | 0.10436475 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.303      |
|    learning_rate        | 0.0001     |
|    loss                 | 955        |
|    n_updates            | 1740       |
|    policy_gradient_loss | 0.0361     |
|    value_loss           | 1.9e+03    |
----------------------------------------
Eval num_timesteps=357000, episode_reward=360.94 +/- 234.55
Episode length: 90.60 +/- 58.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=478.42 +/- 295.42
Episode length: 120.00 +/- 73.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=404.96 +/- 264.26
Episode length: 101.56 +/- 66.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 175      |
|    time_elapsed    | 6771     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=385.02 +/- 272.68
Episode length: 96.66 +/- 68.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.7       |
|    mean_reward          | 385        |
| time/                   |            |
|    total_timesteps      | 358500     |
| train/                  |            |
|    approx_kl            | 0.06426081 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.508     |
|    explained_variance   | 0.26       |
|    learning_rate        | 0.0001     |
|    loss                 | 1.24e+03   |
|    n_updates            | 1750       |
|    policy_gradient_loss | 0.0352     |
|    value_loss           | 1.77e+03   |
----------------------------------------
Eval num_timesteps=359000, episode_reward=421.54 +/- 309.60
Episode length: 105.78 +/- 77.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=376.20 +/- 227.48
Episode length: 94.38 +/- 56.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=389.94 +/- 230.95
Episode length: 97.88 +/- 57.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.3     |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 176      |
|    time_elapsed    | 6808     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=387.34 +/- 312.77
Episode length: 97.20 +/- 78.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.2        |
|    mean_reward          | 387         |
| time/                   |             |
|    total_timesteps      | 360500      |
| train/                  |             |
|    approx_kl            | 0.044145405 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.0001      |
|    loss                 | 1.08e+03    |
|    n_updates            | 1760        |
|    policy_gradient_loss | 0.0408      |
|    value_loss           | 1.96e+03    |
-----------------------------------------
Eval num_timesteps=361000, episode_reward=363.26 +/- 252.72
Episode length: 91.16 +/- 63.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=408.48 +/- 248.35
Episode length: 102.50 +/- 62.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=448.22 +/- 286.61
Episode length: 112.46 +/- 71.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.8     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 177      |
|    time_elapsed    | 6846     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=400.04 +/- 226.68
Episode length: 100.38 +/- 56.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 362500      |
| train/                  |             |
|    approx_kl            | 0.044846058 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.0001      |
|    loss                 | 800         |
|    n_updates            | 1770        |
|    policy_gradient_loss | 0.0443      |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=363000, episode_reward=427.50 +/- 311.06
Episode length: 107.24 +/- 77.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=425.54 +/- 297.48
Episode length: 106.80 +/- 74.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=334.76 +/- 186.43
Episode length: 83.98 +/- 46.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84       |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=330.42 +/- 189.05
Episode length: 82.98 +/- 47.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83       |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.3     |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 178      |
|    time_elapsed    | 6891     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=317.16 +/- 186.05
Episode length: 79.72 +/- 46.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.7       |
|    mean_reward          | 317        |
| time/                   |            |
|    total_timesteps      | 365000     |
| train/                  |            |
|    approx_kl            | 0.15086977 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0001     |
|    loss                 | 986        |
|    n_updates            | 1780       |
|    policy_gradient_loss | 0.0363     |
|    value_loss           | 2.14e+03   |
----------------------------------------
Eval num_timesteps=365500, episode_reward=282.56 +/- 154.65
Episode length: 71.06 +/- 38.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.1     |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=312.24 +/- 219.42
Episode length: 78.42 +/- 54.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=318.90 +/- 156.26
Episode length: 80.10 +/- 39.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 179      |
|    time_elapsed    | 6923     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=299.34 +/- 207.57
Episode length: 75.18 +/- 51.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.2        |
|    mean_reward          | 299         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.070602015 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0001      |
|    loss                 | 864         |
|    n_updates            | 1790        |
|    policy_gradient_loss | 0.0333      |
|    value_loss           | 1.73e+03    |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=312.58 +/- 208.46
Episode length: 78.46 +/- 52.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=371.82 +/- 217.54
Episode length: 93.34 +/- 54.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=344.50 +/- 242.64
Episode length: 86.52 +/- 60.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.5     |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.1     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 180      |
|    time_elapsed    | 6954     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=291.62 +/- 180.64
Episode length: 73.24 +/- 45.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 73.2       |
|    mean_reward          | 292        |
| time/                   |            |
|    total_timesteps      | 369000     |
| train/                  |            |
|    approx_kl            | 0.03823404 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.31      |
|    explained_variance   | 0.253      |
|    learning_rate        | 0.0001     |
|    loss                 | 709        |
|    n_updates            | 1800       |
|    policy_gradient_loss | 0.0374     |
|    value_loss           | 1.78e+03   |
----------------------------------------
Eval num_timesteps=369500, episode_reward=287.20 +/- 150.28
Episode length: 72.14 +/- 37.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.1     |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=334.02 +/- 202.16
Episode length: 83.84 +/- 50.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.8     |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=289.44 +/- 239.32
Episode length: 72.74 +/- 59.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | 277      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 181      |
|    time_elapsed    | 6985     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=286.40 +/- 168.95
Episode length: 72.02 +/- 42.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.063030384 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | 0.234       |
|    learning_rate        | 0.0001      |
|    loss                 | 672         |
|    n_updates            | 1810        |
|    policy_gradient_loss | 0.0333      |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=275.26 +/- 203.78
Episode length: 69.22 +/- 50.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=250.98 +/- 136.73
Episode length: 63.14 +/- 34.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.1     |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=265.76 +/- 117.69
Episode length: 66.80 +/- 29.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.1     |
|    ep_rew_mean     | 267      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 182      |
|    time_elapsed    | 7017     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=299.68 +/- 239.78
Episode length: 75.26 +/- 59.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.3        |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.024813842 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.238      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0001      |
|    loss                 | 868         |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.0307      |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=373500, episode_reward=267.68 +/- 101.00
Episode length: 67.34 +/- 25.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.3     |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=292.38 +/- 161.35
Episode length: 73.54 +/- 40.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=319.58 +/- 198.69
Episode length: 80.28 +/- 49.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.3     |
|    ep_rew_mean     | 264      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 183      |
|    time_elapsed    | 7049     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=267.30 +/- 138.15
Episode length: 67.14 +/- 34.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.1        |
|    mean_reward          | 267         |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.045480818 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0001      |
|    loss                 | 513         |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.03        |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=289.56 +/- 193.44
Episode length: 72.74 +/- 48.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=272.34 +/- 176.47
Episode length: 68.42 +/- 44.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=331.46 +/- 250.27
Episode length: 83.22 +/- 62.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.2     |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 184      |
|    time_elapsed    | 7080     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=277.98 +/- 194.10
Episode length: 69.88 +/- 48.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.9      |
|    mean_reward          | 278       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0834862 |
|    clip_fraction        | 0.19      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.247    |
|    explained_variance   | 0.234     |
|    learning_rate        | 0.0001    |
|    loss                 | 800       |
|    n_updates            | 1840      |
|    policy_gradient_loss | 0.0348    |
|    value_loss           | 1.4e+03   |
---------------------------------------
Eval num_timesteps=377500, episode_reward=240.42 +/- 149.52
Episode length: 60.46 +/- 37.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=299.78 +/- 185.22
Episode length: 75.38 +/- 46.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=240.96 +/- 106.34
Episode length: 60.54 +/- 26.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.4     |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 185      |
|    time_elapsed    | 7110     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=289.12 +/- 165.04
Episode length: 72.62 +/- 41.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.6        |
|    mean_reward          | 289         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.040235147 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.0001      |
|    loss                 | 782         |
|    n_updates            | 1850        |
|    policy_gradient_loss | 0.027       |
|    value_loss           | 1.52e+03    |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=290.84 +/- 185.14
Episode length: 73.00 +/- 46.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=261.02 +/- 123.99
Episode length: 65.60 +/- 31.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.6     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=271.20 +/- 182.00
Episode length: 68.18 +/- 45.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.8     |
|    ep_rew_mean     | 266      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 186      |
|    time_elapsed    | 7140     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=274.96 +/- 167.76
Episode length: 69.02 +/- 41.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69          |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.057665646 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0001      |
|    loss                 | 582         |
|    n_updates            | 1860        |
|    policy_gradient_loss | 0.0245      |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=381500, episode_reward=250.90 +/- 126.95
Episode length: 63.00 +/- 31.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63       |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=263.70 +/- 134.98
Episode length: 66.26 +/- 33.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=323.80 +/- 305.74
Episode length: 81.34 +/- 76.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 324      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.6     |
|    ep_rew_mean     | 241      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 187      |
|    time_elapsed    | 7170     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=246.52 +/- 123.99
Episode length: 61.96 +/- 30.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62          |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.039007325 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.0001      |
|    loss                 | 698         |
|    n_updates            | 1870        |
|    policy_gradient_loss | 0.0279      |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=251.42 +/- 123.91
Episode length: 63.24 +/- 31.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.2     |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=260.54 +/- 151.78
Episode length: 65.42 +/- 37.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=231.86 +/- 135.84
Episode length: 58.36 +/- 33.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=276.80 +/- 158.00
Episode length: 69.52 +/- 39.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 62.7     |
|    ep_rew_mean     | 249      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 188      |
|    time_elapsed    | 7202     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=293.74 +/- 191.86
Episode length: 73.76 +/- 48.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.8        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 385500      |
| train/                  |             |
|    approx_kl            | 0.040366035 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 584         |
|    n_updates            | 1880        |
|    policy_gradient_loss | 0.0116      |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=283.14 +/- 134.84
Episode length: 71.08 +/- 33.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.1     |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=303.70 +/- 188.94
Episode length: 76.22 +/- 47.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=293.26 +/- 151.61
Episode length: 73.72 +/- 37.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 62.2     |
|    ep_rew_mean     | 248      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 189      |
|    time_elapsed    | 7231     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=269.56 +/- 93.73
Episode length: 67.70 +/- 23.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.7        |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 387500      |
| train/                  |             |
|    approx_kl            | 0.042042863 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0001      |
|    loss                 | 586         |
|    n_updates            | 1890        |
|    policy_gradient_loss | 0.0252      |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=278.00 +/- 100.01
Episode length: 69.82 +/- 25.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=273.74 +/- 129.65
Episode length: 68.82 +/- 32.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=265.88 +/- 120.41
Episode length: 66.88 +/- 30.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.9     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.9     |
|    ep_rew_mean     | 262      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 190      |
|    time_elapsed    | 7262     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=324.70 +/- 222.46
Episode length: 81.52 +/- 55.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.5        |
|    mean_reward          | 325         |
| time/                   |             |
|    total_timesteps      | 389500      |
| train/                  |             |
|    approx_kl            | 0.034110926 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.163       |
|    learning_rate        | 0.0001      |
|    loss                 | 623         |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.0356      |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=254.82 +/- 162.35
Episode length: 64.06 +/- 40.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.1     |
|    mean_reward     | 255      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=318.84 +/- 234.81
Episode length: 80.12 +/- 58.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=310.76 +/- 201.67
Episode length: 78.04 +/- 50.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78       |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.8     |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 191      |
|    time_elapsed    | 7295     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=303.14 +/- 202.15
Episode length: 76.08 +/- 50.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.1        |
|    mean_reward          | 303         |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.062191702 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 696         |
|    n_updates            | 1910        |
|    policy_gradient_loss | 0.0254      |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=314.74 +/- 219.49
Episode length: 79.02 +/- 54.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=319.32 +/- 155.87
Episode length: 80.20 +/- 38.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=282.86 +/- 185.54
Episode length: 71.02 +/- 46.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71       |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 307      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 192      |
|    time_elapsed    | 7328     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=322.12 +/- 190.28
Episode length: 80.82 +/- 47.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 80.8       |
|    mean_reward          | 322        |
| time/                   |            |
|    total_timesteps      | 393500     |
| train/                  |            |
|    approx_kl            | 0.08052884 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | -0.0228    |
|    learning_rate        | 0.0001     |
|    loss                 | 747        |
|    n_updates            | 1920       |
|    policy_gradient_loss | 0.026      |
|    value_loss           | 1.29e+03   |
----------------------------------------
Eval num_timesteps=394000, episode_reward=377.22 +/- 247.64
Episode length: 94.68 +/- 61.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=329.18 +/- 207.98
Episode length: 82.66 +/- 51.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.7     |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=307.08 +/- 197.73
Episode length: 77.14 +/- 49.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.4     |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 193      |
|    time_elapsed    | 7364     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=292.86 +/- 199.70
Episode length: 73.62 +/- 49.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 73.6       |
|    mean_reward          | 293        |
| time/                   |            |
|    total_timesteps      | 395500     |
| train/                  |            |
|    approx_kl            | 0.03322114 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.247      |
|    learning_rate        | 0.0001     |
|    loss                 | 839        |
|    n_updates            | 1930       |
|    policy_gradient_loss | 0.0269     |
|    value_loss           | 1.59e+03   |
----------------------------------------
Eval num_timesteps=396000, episode_reward=326.82 +/- 230.17
Episode length: 82.06 +/- 57.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=318.94 +/- 194.34
Episode length: 80.10 +/- 48.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=304.96 +/- 163.21
Episode length: 76.62 +/- 40.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 305      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79       |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 194      |
|    time_elapsed    | 7396     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=444.68 +/- 343.99
Episode length: 111.56 +/- 86.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 445         |
| time/                   |             |
|    total_timesteps      | 397500      |
| train/                  |             |
|    approx_kl            | 0.045386136 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.277      |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0001      |
|    loss                 | 727         |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.033       |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=398000, episode_reward=365.86 +/- 249.69
Episode length: 91.82 +/- 62.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=387.22 +/- 219.74
Episode length: 97.16 +/- 54.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=370.20 +/- 208.70
Episode length: 92.96 +/- 52.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.1     |
|    ep_rew_mean     | 303      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 195      |
|    time_elapsed    | 7435     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=440.24 +/- 229.38
Episode length: 110.44 +/- 57.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 440         |
| time/                   |             |
|    total_timesteps      | 399500      |
| train/                  |             |
|    approx_kl            | 0.057881985 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.257      |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.0001      |
|    loss                 | 556         |
|    n_updates            | 1950        |
|    policy_gradient_loss | 0.033       |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=400.54 +/- 210.61
Episode length: 100.52 +/- 52.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=388.80 +/- 237.18
Episode length: 97.54 +/- 59.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=432.56 +/- 232.68
Episode length: 108.54 +/- 58.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.6     |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 196      |
|    time_elapsed    | 7478     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=353.46 +/- 209.74
Episode length: 88.72 +/- 52.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 88.7      |
|    mean_reward          | 353       |
| time/                   |           |
|    total_timesteps      | 401500    |
| train/                  |           |
|    approx_kl            | 0.2268771 |
|    clip_fraction        | 0.316     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.339    |
|    explained_variance   | 0.205     |
|    learning_rate        | 0.0001    |
|    loss                 | 433       |
|    n_updates            | 1960      |
|    policy_gradient_loss | 0.0762    |
|    value_loss           | 1.22e+03  |
---------------------------------------
Eval num_timesteps=402000, episode_reward=383.62 +/- 246.97
Episode length: 96.30 +/- 61.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.3     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=449.90 +/- 288.92
Episode length: 112.80 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=396.16 +/- 292.87
Episode length: 99.42 +/- 73.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 197      |
|    time_elapsed    | 7517     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=420.60 +/- 209.45
Episode length: 105.44 +/- 52.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 421         |
| time/                   |             |
|    total_timesteps      | 403500      |
| train/                  |             |
|    approx_kl            | 0.025739599 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.372      |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.0001      |
|    loss                 | 539         |
|    n_updates            | 1970        |
|    policy_gradient_loss | 0.0233      |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=397.48 +/- 197.73
Episode length: 99.70 +/- 49.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=430.54 +/- 206.98
Episode length: 107.98 +/- 51.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=377.18 +/- 192.92
Episode length: 94.62 +/- 48.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=343.20 +/- 194.14
Episode length: 86.16 +/- 48.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.2     |
|    mean_reward     | 343      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 198      |
|    time_elapsed    | 7565     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=388.82 +/- 178.78
Episode length: 97.58 +/- 44.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.027772743 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.412      |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0001      |
|    loss                 | 776         |
|    n_updates            | 1980        |
|    policy_gradient_loss | 0.0184      |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=406500, episode_reward=403.08 +/- 208.17
Episode length: 101.10 +/- 52.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=429.02 +/- 198.91
Episode length: 107.60 +/- 49.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=414.12 +/- 227.48
Episode length: 103.88 +/- 56.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.5     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 199      |
|    time_elapsed    | 7603     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=386.96 +/- 190.32
Episode length: 97.12 +/- 47.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.1        |
|    mean_reward          | 387         |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.073729545 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.377      |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0001      |
|    loss                 | 796         |
|    n_updates            | 1990        |
|    policy_gradient_loss | 0.0293      |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=410.28 +/- 182.26
Episode length: 102.90 +/- 45.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=391.20 +/- 188.37
Episode length: 98.22 +/- 47.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 391      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=386.86 +/- 189.38
Episode length: 97.04 +/- 47.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 200      |
|    time_elapsed    | 7641     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=429.34 +/- 219.03
Episode length: 107.72 +/- 54.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 429         |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.050213937 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.326      |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0001      |
|    loss                 | 688         |
|    n_updates            | 2000        |
|    policy_gradient_loss | 0.0216      |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=419.38 +/- 204.80
Episode length: 105.26 +/- 51.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=443.48 +/- 229.11
Episode length: 111.22 +/- 57.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=377.88 +/- 194.71
Episode length: 94.94 +/- 48.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.9     |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 201      |
|    time_elapsed    | 7685     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=392.44 +/- 204.22
Episode length: 98.50 +/- 51.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.5       |
|    mean_reward          | 392        |
| time/                   |            |
|    total_timesteps      | 412000     |
| train/                  |            |
|    approx_kl            | 0.07557122 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.17e+03   |
|    n_updates            | 2010       |
|    policy_gradient_loss | 0.0256     |
|    value_loss           | 1.65e+03   |
----------------------------------------
Eval num_timesteps=412500, episode_reward=377.34 +/- 190.90
Episode length: 94.72 +/- 47.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=363.46 +/- 219.19
Episode length: 91.28 +/- 54.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.3     |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=388.70 +/- 209.73
Episode length: 97.52 +/- 52.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.6     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 202      |
|    time_elapsed    | 7722     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=361.54 +/- 217.26
Episode length: 90.80 +/- 54.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 90.8       |
|    mean_reward          | 362        |
| time/                   |            |
|    total_timesteps      | 414000     |
| train/                  |            |
|    approx_kl            | 0.04763463 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.412      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.06e+03   |
|    n_updates            | 2020       |
|    policy_gradient_loss | 0.0237     |
|    value_loss           | 1.82e+03   |
----------------------------------------
Eval num_timesteps=414500, episode_reward=377.62 +/- 225.53
Episode length: 94.76 +/- 56.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=353.76 +/- 202.19
Episode length: 88.82 +/- 50.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.8     |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=382.56 +/- 247.27
Episode length: 96.00 +/- 61.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.1     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 203      |
|    time_elapsed    | 7760     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=418.18 +/- 235.70
Episode length: 104.92 +/- 58.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 105        |
|    mean_reward          | 418        |
| time/                   |            |
|    total_timesteps      | 416000     |
| train/                  |            |
|    approx_kl            | 0.03906072 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.297     |
|    explained_variance   | 0.457      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.02e+03   |
|    n_updates            | 2030       |
|    policy_gradient_loss | 0.0179     |
|    value_loss           | 1.49e+03   |
----------------------------------------
Eval num_timesteps=416500, episode_reward=339.02 +/- 202.22
Episode length: 85.14 +/- 50.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.1     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=346.76 +/- 209.09
Episode length: 87.04 +/- 52.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87       |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=375.28 +/- 219.69
Episode length: 94.16 +/- 54.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.2     |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 204      |
|    time_elapsed    | 7798     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=344.82 +/- 215.03
Episode length: 86.56 +/- 53.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 86.6       |
|    mean_reward          | 345        |
| time/                   |            |
|    total_timesteps      | 418000     |
| train/                  |            |
|    approx_kl            | 0.07291839 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.287     |
|    explained_variance   | 0.377      |
|    learning_rate        | 0.0001     |
|    loss                 | 540        |
|    n_updates            | 2040       |
|    policy_gradient_loss | 0.0439     |
|    value_loss           | 1.75e+03   |
----------------------------------------
Eval num_timesteps=418500, episode_reward=391.52 +/- 212.02
Episode length: 98.24 +/- 53.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=359.74 +/- 212.87
Episode length: 90.24 +/- 53.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=335.74 +/- 210.06
Episode length: 84.28 +/- 52.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.3     |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.2     |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 205      |
|    time_elapsed    | 7834     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=314.56 +/- 269.81
Episode length: 78.98 +/- 67.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79         |
|    mean_reward          | 315        |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.07691831 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.26e+03   |
|    n_updates            | 2050       |
|    policy_gradient_loss | 0.0497     |
|    value_loss           | 1.79e+03   |
----------------------------------------
Eval num_timesteps=420500, episode_reward=305.86 +/- 230.68
Episode length: 76.86 +/- 57.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=288.76 +/- 154.57
Episode length: 72.64 +/- 38.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.6     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=296.50 +/- 159.56
Episode length: 74.50 +/- 39.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.6     |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 206      |
|    time_elapsed    | 7865     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=300.14 +/- 170.91
Episode length: 75.36 +/- 42.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75.4       |
|    mean_reward          | 300        |
| time/                   |            |
|    total_timesteps      | 422000     |
| train/                  |            |
|    approx_kl            | 0.05905164 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.223     |
|    explained_variance   | 0.289      |
|    learning_rate        | 0.0001     |
|    loss                 | 887        |
|    n_updates            | 2060       |
|    policy_gradient_loss | 0.0317     |
|    value_loss           | 1.94e+03   |
----------------------------------------
Eval num_timesteps=422500, episode_reward=267.74 +/- 173.78
Episode length: 67.24 +/- 43.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.2     |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=303.54 +/- 190.06
Episode length: 76.30 +/- 47.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=317.70 +/- 191.19
Episode length: 79.80 +/- 47.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.2     |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 207      |
|    time_elapsed    | 7895     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=266.64 +/- 148.89
Episode length: 66.96 +/- 37.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 67         |
|    mean_reward          | 267        |
| time/                   |            |
|    total_timesteps      | 424000     |
| train/                  |            |
|    approx_kl            | 0.10521786 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.176     |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.01e+03   |
|    n_updates            | 2070       |
|    policy_gradient_loss | 0.0299     |
|    value_loss           | 1.81e+03   |
----------------------------------------
Eval num_timesteps=424500, episode_reward=326.90 +/- 197.11
Episode length: 82.10 +/- 49.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.1     |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=297.32 +/- 161.36
Episode length: 74.68 +/- 40.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=314.06 +/- 201.53
Episode length: 78.84 +/- 50.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.8     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.8     |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 208      |
|    time_elapsed    | 7924     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=277.16 +/- 147.68
Episode length: 69.62 +/- 36.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 69.6       |
|    mean_reward          | 277        |
| time/                   |            |
|    total_timesteps      | 426000     |
| train/                  |            |
|    approx_kl            | 0.09814589 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.242     |
|    explained_variance   | 0.227      |
|    learning_rate        | 0.0001     |
|    loss                 | 751        |
|    n_updates            | 2080       |
|    policy_gradient_loss | 0.0288     |
|    value_loss           | 1.68e+03   |
----------------------------------------
Eval num_timesteps=426500, episode_reward=294.90 +/- 214.40
Episode length: 74.06 +/- 53.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.1     |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=266.66 +/- 156.14
Episode length: 67.12 +/- 39.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.1     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=260.00 +/- 133.79
Episode length: 65.34 +/- 33.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.3     |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=312.48 +/- 200.15
Episode length: 78.50 +/- 50.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.5     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67       |
|    ep_rew_mean     | 267      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 209      |
|    time_elapsed    | 7962     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=371.20 +/- 213.20
Episode length: 93.20 +/- 53.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.2        |
|    mean_reward          | 371         |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.049795244 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.0001      |
|    loss                 | 734         |
|    n_updates            | 2090        |
|    policy_gradient_loss | 0.0214      |
|    value_loss           | 1.51e+03    |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=348.62 +/- 204.31
Episode length: 87.56 +/- 51.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.6     |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=302.24 +/- 180.54
Episode length: 75.90 +/- 45.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=328.92 +/- 194.07
Episode length: 82.58 +/- 48.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.6     |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 210      |
|    time_elapsed    | 7997     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=391.80 +/- 226.29
Episode length: 98.28 +/- 56.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.3       |
|    mean_reward          | 392        |
| time/                   |            |
|    total_timesteps      | 430500     |
| train/                  |            |
|    approx_kl            | 0.05064101 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.32      |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.0001     |
|    loss                 | 581        |
|    n_updates            | 2100       |
|    policy_gradient_loss | 0.031      |
|    value_loss           | 1.05e+03   |
----------------------------------------
Eval num_timesteps=431000, episode_reward=415.64 +/- 206.77
Episode length: 104.26 +/- 51.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=375.98 +/- 217.97
Episode length: 94.38 +/- 54.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=436.14 +/- 251.76
Episode length: 109.46 +/- 62.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.4     |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 211      |
|    time_elapsed    | 8036     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=406.90 +/- 232.78
Episode length: 102.14 +/- 58.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 407        |
| time/                   |            |
|    total_timesteps      | 432500     |
| train/                  |            |
|    approx_kl            | 0.08694744 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.384      |
|    learning_rate        | 0.0001     |
|    loss                 | 403        |
|    n_updates            | 2110       |
|    policy_gradient_loss | 0.0305     |
|    value_loss           | 1.21e+03   |
----------------------------------------
Eval num_timesteps=433000, episode_reward=408.58 +/- 242.74
Episode length: 102.54 +/- 60.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=371.14 +/- 205.94
Episode length: 93.12 +/- 51.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=447.46 +/- 203.49
Episode length: 112.34 +/- 50.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.6     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 212      |
|    time_elapsed    | 8074     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=430.76 +/- 221.87
Episode length: 108.12 +/- 55.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 431        |
| time/                   |            |
|    total_timesteps      | 434500     |
| train/                  |            |
|    approx_kl            | 0.09839627 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.291     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.0001     |
|    loss                 | 539        |
|    n_updates            | 2120       |
|    policy_gradient_loss | 0.0272     |
|    value_loss           | 1.66e+03   |
----------------------------------------
Eval num_timesteps=435000, episode_reward=464.44 +/- 236.87
Episode length: 116.46 +/- 59.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=424.50 +/- 257.01
Episode length: 106.46 +/- 64.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=477.84 +/- 239.98
Episode length: 119.82 +/- 60.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.8     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 213      |
|    time_elapsed    | 8116     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=435.86 +/- 215.12
Episode length: 109.40 +/- 53.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 436         |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.060652204 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.315      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0001      |
|    loss                 | 615         |
|    n_updates            | 2130        |
|    policy_gradient_loss | 0.0261      |
|    value_loss           | 1.47e+03    |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=452.48 +/- 230.06
Episode length: 113.48 +/- 57.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=421.64 +/- 207.96
Episode length: 105.78 +/- 51.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=434.94 +/- 193.86
Episode length: 109.12 +/- 48.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.5     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 214      |
|    time_elapsed    | 8159     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=439.40 +/- 178.71
Episode length: 110.24 +/- 44.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 439         |
| time/                   |             |
|    total_timesteps      | 438500      |
| train/                  |             |
|    approx_kl            | 0.076615155 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.0001      |
|    loss                 | 517         |
|    n_updates            | 2140        |
|    policy_gradient_loss | 0.0405      |
|    value_loss           | 1.62e+03    |
-----------------------------------------
Eval num_timesteps=439000, episode_reward=414.92 +/- 161.04
Episode length: 104.10 +/- 40.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=395.36 +/- 175.39
Episode length: 99.24 +/- 43.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=463.38 +/- 192.59
Episode length: 116.18 +/- 48.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.2     |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 215      |
|    time_elapsed    | 8200     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=419.76 +/- 150.79
Episode length: 105.32 +/- 37.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 420         |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.025156464 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.331      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0001      |
|    loss                 | 883         |
|    n_updates            | 2150        |
|    policy_gradient_loss | 0.0322      |
|    value_loss           | 1.43e+03    |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=444.00 +/- 148.45
Episode length: 111.36 +/- 37.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=403.48 +/- 123.00
Episode length: 101.24 +/- 30.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=444.60 +/- 156.23
Episode length: 111.52 +/- 39.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.2     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 216      |
|    time_elapsed    | 8241     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=430.32 +/- 164.05
Episode length: 108.02 +/- 40.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 430        |
| time/                   |            |
|    total_timesteps      | 442500     |
| train/                  |            |
|    approx_kl            | 0.03508777 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0001     |
|    loss                 | 162        |
|    n_updates            | 2160       |
|    policy_gradient_loss | 0.0301     |
|    value_loss           | 908        |
----------------------------------------
Eval num_timesteps=443000, episode_reward=404.42 +/- 158.18
Episode length: 101.46 +/- 39.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=417.18 +/- 161.17
Episode length: 104.68 +/- 40.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=407.54 +/- 149.40
Episode length: 102.34 +/- 37.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.2     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 217      |
|    time_elapsed    | 8282     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=429.20 +/- 133.75
Episode length: 107.66 +/- 33.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 429        |
| time/                   |            |
|    total_timesteps      | 444500     |
| train/                  |            |
|    approx_kl            | 0.08545503 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.322     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0001     |
|    loss                 | 625        |
|    n_updates            | 2170       |
|    policy_gradient_loss | 0.0208     |
|    value_loss           | 1.08e+03   |
----------------------------------------
Eval num_timesteps=445000, episode_reward=406.82 +/- 175.56
Episode length: 102.08 +/- 43.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=431.60 +/- 134.45
Episode length: 108.28 +/- 33.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=408.50 +/- 151.95
Episode length: 102.48 +/- 38.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.8     |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 218      |
|    time_elapsed    | 8324     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=419.08 +/- 153.20
Episode length: 105.20 +/- 38.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 419         |
| time/                   |             |
|    total_timesteps      | 446500      |
| train/                  |             |
|    approx_kl            | 0.025169084 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.296      |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0001      |
|    loss                 | 248         |
|    n_updates            | 2180        |
|    policy_gradient_loss | 0.0146      |
|    value_loss           | 705         |
-----------------------------------------
Eval num_timesteps=447000, episode_reward=340.38 +/- 174.22
Episode length: 85.50 +/- 43.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.5     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=381.02 +/- 193.92
Episode length: 95.60 +/- 48.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=394.58 +/- 172.19
Episode length: 99.00 +/- 43.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=396.84 +/- 156.98
Episode length: 99.56 +/- 39.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.1     |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 219      |
|    time_elapsed    | 8372     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=369.16 +/- 212.00
Episode length: 92.68 +/- 53.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.7        |
|    mean_reward          | 369         |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.043634504 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.279      |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.0001      |
|    loss                 | 488         |
|    n_updates            | 2190        |
|    policy_gradient_loss | 0.0204      |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=350.74 +/- 217.11
Episode length: 88.06 +/- 54.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=368.62 +/- 193.77
Episode length: 92.48 +/- 48.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=414.38 +/- 209.71
Episode length: 103.94 +/- 52.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.8     |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 220      |
|    time_elapsed    | 8410     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=448.58 +/- 175.29
Episode length: 112.48 +/- 43.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 449         |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.046388008 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.295      |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.0001      |
|    loss                 | 836         |
|    n_updates            | 2200        |
|    policy_gradient_loss | 0.0389      |
|    value_loss           | 1.85e+03    |
-----------------------------------------
Eval num_timesteps=451500, episode_reward=420.10 +/- 221.64
Episode length: 105.40 +/- 55.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=403.08 +/- 176.61
Episode length: 101.16 +/- 44.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=426.10 +/- 213.14
Episode length: 106.94 +/- 53.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.9     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 221      |
|    time_elapsed    | 8453     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=401.32 +/- 235.56
Episode length: 100.68 +/- 58.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 101        |
|    mean_reward          | 401        |
| time/                   |            |
|    total_timesteps      | 453000     |
| train/                  |            |
|    approx_kl            | 0.05139279 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.0001     |
|    loss                 | 522        |
|    n_updates            | 2210       |
|    policy_gradient_loss | 0.039      |
|    value_loss           | 1.85e+03   |
----------------------------------------
Eval num_timesteps=453500, episode_reward=356.40 +/- 245.94
Episode length: 89.52 +/- 61.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=367.94 +/- 222.05
Episode length: 92.30 +/- 55.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.3     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=334.90 +/- 210.72
Episode length: 84.10 +/- 52.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.1     |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86       |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 222      |
|    time_elapsed    | 8490     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=450.20 +/- 229.46
Episode length: 112.90 +/- 57.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 113       |
|    mean_reward          | 450       |
| time/                   |           |
|    total_timesteps      | 455000    |
| train/                  |           |
|    approx_kl            | 0.0560764 |
|    clip_fraction        | 0.186     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.251    |
|    explained_variance   | 0.196     |
|    learning_rate        | 0.0001    |
|    loss                 | 937       |
|    n_updates            | 2220      |
|    policy_gradient_loss | 0.0365    |
|    value_loss           | 2.02e+03  |
---------------------------------------
Eval num_timesteps=455500, episode_reward=391.00 +/- 225.05
Episode length: 98.10 +/- 56.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.1     |
|    mean_reward     | 391      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=414.68 +/- 230.93
Episode length: 104.08 +/- 57.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=352.82 +/- 217.96
Episode length: 88.54 +/- 54.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 223      |
|    time_elapsed    | 8528     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=365.70 +/- 221.49
Episode length: 91.80 +/- 55.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.8        |
|    mean_reward          | 366         |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.039538667 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.0001      |
|    loss                 | 583         |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.0372      |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=315.50 +/- 233.02
Episode length: 79.24 +/- 58.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=317.28 +/- 192.37
Episode length: 79.66 +/- 48.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=271.26 +/- 177.43
Episode length: 68.14 +/- 44.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.1     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.3     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 224      |
|    time_elapsed    | 8562     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=359.16 +/- 229.36
Episode length: 90.16 +/- 57.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.2        |
|    mean_reward          | 359         |
| time/                   |             |
|    total_timesteps      | 459000      |
| train/                  |             |
|    approx_kl            | 0.030689191 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0001      |
|    loss                 | 722         |
|    n_updates            | 2240        |
|    policy_gradient_loss | 0.0306      |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=459500, episode_reward=340.50 +/- 224.80
Episode length: 85.46 +/- 56.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.5     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=301.36 +/- 192.25
Episode length: 75.64 +/- 48.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=355.04 +/- 220.46
Episode length: 89.12 +/- 55.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.1     |
|    mean_reward     | 355      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 225      |
|    time_elapsed    | 8598     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=298.68 +/- 168.00
Episode length: 75.02 +/- 41.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | 299         |
| time/                   |             |
|    total_timesteps      | 461000      |
| train/                  |             |
|    approx_kl            | 0.051510766 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0001      |
|    loss                 | 627         |
|    n_updates            | 2250        |
|    policy_gradient_loss | 0.024       |
|    value_loss           | 1.73e+03    |
-----------------------------------------
Eval num_timesteps=461500, episode_reward=263.82 +/- 141.08
Episode length: 66.32 +/- 35.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=335.86 +/- 200.27
Episode length: 84.38 +/- 50.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.4     |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=261.34 +/- 155.39
Episode length: 65.66 +/- 38.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.7     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79       |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 226      |
|    time_elapsed    | 8628     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=223.50 +/- 119.75
Episode length: 56.20 +/- 29.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 56.2       |
|    mean_reward          | 224        |
| time/                   |            |
|    total_timesteps      | 463000     |
| train/                  |            |
|    approx_kl            | 0.05266889 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.219     |
|    explained_variance   | 0.155      |
|    learning_rate        | 0.0001     |
|    loss                 | 685        |
|    n_updates            | 2260       |
|    policy_gradient_loss | 0.0238     |
|    value_loss           | 2e+03      |
----------------------------------------
Eval num_timesteps=463500, episode_reward=222.46 +/- 114.82
Episode length: 55.94 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=240.82 +/- 212.31
Episode length: 60.56 +/- 53.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.6     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=221.76 +/- 132.01
Episode length: 55.78 +/- 32.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 63.8     |
|    ep_rew_mean     | 254      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 227      |
|    time_elapsed    | 8653     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=287.70 +/- 196.34
Episode length: 72.30 +/- 49.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 72.3       |
|    mean_reward          | 288        |
| time/                   |            |
|    total_timesteps      | 465000     |
| train/                  |            |
|    approx_kl            | 0.02324971 |
|    clip_fraction        | 0.0818     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.187     |
|    explained_variance   | 0.342      |
|    learning_rate        | 0.0001     |
|    loss                 | 592        |
|    n_updates            | 2270       |
|    policy_gradient_loss | 0.00604    |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=465500, episode_reward=344.20 +/- 193.00
Episode length: 86.50 +/- 48.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.5     |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=303.40 +/- 159.50
Episode length: 76.28 +/- 39.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=262.24 +/- 165.65
Episode length: 65.94 +/- 41.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 63.2     |
|    ep_rew_mean     | 251      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 228      |
|    time_elapsed    | 8685     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=362.28 +/- 251.65
Episode length: 90.92 +/- 62.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.9        |
|    mean_reward          | 362         |
| time/                   |             |
|    total_timesteps      | 467000      |
| train/                  |             |
|    approx_kl            | 0.058768883 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0001      |
|    loss                 | 328         |
|    n_updates            | 2280        |
|    policy_gradient_loss | 0.0278      |
|    value_loss           | 1e+03       |
-----------------------------------------
Eval num_timesteps=467500, episode_reward=379.98 +/- 239.97
Episode length: 95.34 +/- 60.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=329.82 +/- 195.74
Episode length: 82.78 +/- 48.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.8     |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=362.00 +/- 210.22
Episode length: 90.88 +/- 52.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.1     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 229      |
|    time_elapsed    | 8721     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=371.02 +/- 216.34
Episode length: 93.12 +/- 54.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.1        |
|    mean_reward          | 371         |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.038706794 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.0001      |
|    loss                 | 556         |
|    n_updates            | 2290        |
|    policy_gradient_loss | 0.0203      |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=352.62 +/- 217.98
Episode length: 88.52 +/- 54.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=265.90 +/- 163.43
Episode length: 66.84 +/- 40.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=331.20 +/- 225.28
Episode length: 83.16 +/- 56.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.2     |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=320.12 +/- 174.65
Episode length: 80.40 +/- 43.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.2     |
|    ep_rew_mean     | 303      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 230      |
|    time_elapsed    | 8760     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=329.00 +/- 214.74
Episode length: 82.64 +/- 53.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.6        |
|    mean_reward          | 329         |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.050267205 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.269      |
|    explained_variance   | 0.231       |
|    learning_rate        | 0.0001      |
|    loss                 | 690         |
|    n_updates            | 2300        |
|    policy_gradient_loss | 0.026       |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=312.98 +/- 231.54
Episode length: 78.58 +/- 57.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=375.76 +/- 252.94
Episode length: 94.40 +/- 63.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=339.30 +/- 183.68
Episode length: 85.18 +/- 46.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.9     |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 231      |
|    time_elapsed    | 8794     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=361.56 +/- 201.83
Episode length: 90.76 +/- 50.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 90.8       |
|    mean_reward          | 362        |
| time/                   |            |
|    total_timesteps      | 473500     |
| train/                  |            |
|    approx_kl            | 0.07081145 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.303     |
|    explained_variance   | 0.255      |
|    learning_rate        | 0.0001     |
|    loss                 | 774        |
|    n_updates            | 2310       |
|    policy_gradient_loss | 0.0192     |
|    value_loss           | 1.67e+03   |
----------------------------------------
Eval num_timesteps=474000, episode_reward=371.68 +/- 191.89
Episode length: 93.36 +/- 47.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=364.68 +/- 191.72
Episode length: 91.54 +/- 47.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=337.02 +/- 183.85
Episode length: 84.64 +/- 45.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.6     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 232      |
|    time_elapsed    | 8830     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=356.34 +/- 244.79
Episode length: 89.46 +/- 61.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.5        |
|    mean_reward          | 356         |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.038470984 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0001      |
|    loss                 | 559         |
|    n_updates            | 2320        |
|    policy_gradient_loss | 0.0223      |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=359.44 +/- 209.86
Episode length: 90.24 +/- 52.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=325.20 +/- 214.73
Episode length: 81.60 +/- 53.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.6     |
|    mean_reward     | 325      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=351.06 +/- 187.58
Episode length: 88.10 +/- 46.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.8     |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 233      |
|    time_elapsed    | 8865     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=308.52 +/- 208.22
Episode length: 77.50 +/- 52.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.5        |
|    mean_reward          | 309         |
| time/                   |             |
|    total_timesteps      | 477500      |
| train/                  |             |
|    approx_kl            | 0.068884596 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.3        |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.0001      |
|    loss                 | 743         |
|    n_updates            | 2330        |
|    policy_gradient_loss | 0.0156      |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Eval num_timesteps=478000, episode_reward=277.64 +/- 189.43
Episode length: 69.72 +/- 47.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=349.52 +/- 222.59
Episode length: 87.76 +/- 55.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.8     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=354.04 +/- 220.20
Episode length: 88.88 +/- 54.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.9     |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 234      |
|    time_elapsed    | 8897     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=359.74 +/- 197.33
Episode length: 90.34 +/- 49.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 90.3       |
|    mean_reward          | 360        |
| time/                   |            |
|    total_timesteps      | 479500     |
| train/                  |            |
|    approx_kl            | 0.02297266 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.289     |
|    explained_variance   | 0.231      |
|    learning_rate        | 0.0001     |
|    loss                 | 890        |
|    n_updates            | 2340       |
|    policy_gradient_loss | 0.0139     |
|    value_loss           | 2.04e+03   |
----------------------------------------
Eval num_timesteps=480000, episode_reward=406.94 +/- 219.54
Episode length: 102.04 +/- 54.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=325.52 +/- 179.77
Episode length: 81.78 +/- 44.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.8     |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=386.20 +/- 222.46
Episode length: 96.94 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87       |
|    ep_rew_mean     | 347      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 235      |
|    time_elapsed    | 8933     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=393.92 +/- 179.90
Episode length: 98.86 +/- 44.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.9       |
|    mean_reward          | 394        |
| time/                   |            |
|    total_timesteps      | 481500     |
| train/                  |            |
|    approx_kl            | 0.07057619 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.382     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0001     |
|    loss                 | 609        |
|    n_updates            | 2350       |
|    policy_gradient_loss | 0.0238     |
|    value_loss           | 858        |
----------------------------------------
Eval num_timesteps=482000, episode_reward=418.02 +/- 220.89
Episode length: 104.84 +/- 55.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=397.14 +/- 207.09
Episode length: 99.68 +/- 51.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=401.08 +/- 211.51
Episode length: 100.60 +/- 52.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.6     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 236      |
|    time_elapsed    | 8971     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=410.28 +/- 157.82
Episode length: 102.92 +/- 39.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 410         |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.028213345 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0001      |
|    loss                 | 703         |
|    n_updates            | 2360        |
|    policy_gradient_loss | 0.0177      |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=364.56 +/- 161.01
Episode length: 91.52 +/- 40.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=360.42 +/- 171.70
Episode length: 90.46 +/- 42.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=407.42 +/- 159.75
Episode length: 102.22 +/- 39.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.6     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 237      |
|    time_elapsed    | 9008     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=399.98 +/- 145.33
Episode length: 100.34 +/- 36.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.044089414 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.443      |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0001      |
|    loss                 | 692         |
|    n_updates            | 2370        |
|    policy_gradient_loss | 0.0194      |
|    value_loss           | 1.56e+03    |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=388.46 +/- 126.23
Episode length: 97.52 +/- 31.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=402.70 +/- 145.82
Episode length: 101.02 +/- 36.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=363.38 +/- 152.69
Episode length: 91.18 +/- 38.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.2     |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 238      |
|    time_elapsed    | 9047     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=435.88 +/- 163.19
Episode length: 109.36 +/- 40.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 436         |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.027431078 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.409      |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0001      |
|    loss                 | 556         |
|    n_updates            | 2380        |
|    policy_gradient_loss | 0.0185      |
|    value_loss           | 951         |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=356.44 +/- 159.48
Episode length: 89.54 +/- 39.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=391.96 +/- 190.51
Episode length: 98.38 +/- 47.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=409.78 +/- 157.41
Episode length: 102.78 +/- 39.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 239      |
|    time_elapsed    | 9085     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=350.44 +/- 169.52
Episode length: 87.92 +/- 42.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.9        |
|    mean_reward          | 350         |
| time/                   |             |
|    total_timesteps      | 489500      |
| train/                  |             |
|    approx_kl            | 0.043514594 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.391      |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0001      |
|    loss                 | 337         |
|    n_updates            | 2390        |
|    policy_gradient_loss | 0.0287      |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=447.40 +/- 174.80
Episode length: 112.16 +/- 43.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=374.58 +/- 188.70
Episode length: 93.94 +/- 47.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.9     |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=331.98 +/- 178.08
Episode length: 83.34 +/- 44.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=336.48 +/- 176.34
Episode length: 84.46 +/- 44.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 240      |
|    time_elapsed    | 9128     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=386.42 +/- 212.94
Episode length: 96.94 +/- 53.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96.9      |
|    mean_reward          | 386       |
| time/                   |           |
|    total_timesteps      | 492000    |
| train/                  |           |
|    approx_kl            | 0.0414232 |
|    clip_fraction        | 0.201     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.342    |
|    explained_variance   | 0.658     |
|    learning_rate        | 0.0001    |
|    loss                 | 539       |
|    n_updates            | 2400      |
|    policy_gradient_loss | 0.0137    |
|    value_loss           | 1.23e+03  |
---------------------------------------
Eval num_timesteps=492500, episode_reward=364.50 +/- 202.43
Episode length: 91.56 +/- 50.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 364      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=391.86 +/- 189.14
Episode length: 98.42 +/- 47.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=388.26 +/- 216.09
Episode length: 97.46 +/- 54.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 241      |
|    time_elapsed    | 9164     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=370.30 +/- 190.79
Episode length: 92.90 +/- 47.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.9       |
|    mean_reward          | 370        |
| time/                   |            |
|    total_timesteps      | 494000     |
| train/                  |            |
|    approx_kl            | 0.05207073 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.316     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0001     |
|    loss                 | 590        |
|    n_updates            | 2410       |
|    policy_gradient_loss | 0.0203     |
|    value_loss           | 1.44e+03   |
----------------------------------------
Eval num_timesteps=494500, episode_reward=386.00 +/- 172.12
Episode length: 96.84 +/- 43.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=388.96 +/- 150.09
Episode length: 97.60 +/- 37.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=438.50 +/- 206.22
Episode length: 109.98 +/- 51.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 242      |
|    time_elapsed    | 9205     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=385.56 +/- 112.56
Episode length: 96.76 +/- 28.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.8       |
|    mean_reward          | 386        |
| time/                   |            |
|    total_timesteps      | 496000     |
| train/                  |            |
|    approx_kl            | 0.02539983 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.0001     |
|    loss                 | 295        |
|    n_updates            | 2420       |
|    policy_gradient_loss | 0.0195     |
|    value_loss           | 1.27e+03   |
----------------------------------------
Eval num_timesteps=496500, episode_reward=365.44 +/- 112.63
Episode length: 91.74 +/- 28.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.7     |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=394.46 +/- 89.73
Episode length: 98.98 +/- 22.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=403.58 +/- 88.95
Episode length: 101.28 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 410      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 243      |
|    time_elapsed    | 9237     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=379.24 +/- 117.24
Episode length: 95.20 +/- 29.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 95.2       |
|    mean_reward          | 379        |
| time/                   |            |
|    total_timesteps      | 498000     |
| train/                  |            |
|    approx_kl            | 0.08147293 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0001     |
|    loss                 | 393        |
|    n_updates            | 2430       |
|    policy_gradient_loss | 0.029      |
|    value_loss           | 791        |
----------------------------------------
Eval num_timesteps=498500, episode_reward=433.48 +/- 127.52
Episode length: 108.82 +/- 31.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=371.96 +/- 109.88
Episode length: 93.36 +/- 27.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=379.88 +/- 115.23
Episode length: 95.34 +/- 28.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 244      |
|    time_elapsed    | 9278     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=385.54 +/- 117.51
Episode length: 96.74 +/- 29.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.7        |
|    mean_reward          | 386         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.014980221 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0001      |
|    loss                 | 461         |
|    n_updates            | 2440        |
|    policy_gradient_loss | 0.0141      |
|    value_loss           | 869         |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=379.88 +/- 105.66
Episode length: 95.34 +/- 26.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=397.32 +/- 125.41
Episode length: 99.68 +/- 31.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=346.70 +/- 121.28
Episode length: 87.06 +/- 30.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.1     |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.8     |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 245      |
|    time_elapsed    | 9317     |
|    total_timesteps | 501760   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/take-cover/ppo-6/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
