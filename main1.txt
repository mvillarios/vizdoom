/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-394.54 +/- 76.00
Episode length: 50.62 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-404.52 +/- 84.41
Episode length: 50.90 +/- 20.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-411.54 +/- 74.68
Episode length: 52.90 +/- 19.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-425.26 +/- 73.11
Episode length: 50.20 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-429.12 +/- 72.77
Episode length: 52.18 +/- 18.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 2500      |
| train/                  |           |
|    approx_kl            | 0.0106881 |
|    clip_fraction        | 0.0663    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.07     |
|    explained_variance   | 6.62e-05  |
|    learning_rate        | 0.001     |
|    loss                 | 1.24e+03  |
|    n_updates            | 8         |
|    policy_gradient_loss | -0.00797  |
|    value_loss           | 2.52e+03  |
---------------------------------------
Eval num_timesteps=3000, episode_reward=-434.32 +/- 73.95
Episode length: 53.18 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-430.92 +/- 74.34
Episode length: 45.52 +/- 12.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-431.48 +/- 59.05
Episode length: 49.64 +/- 16.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | -292     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=4500, episode_reward=-417.80 +/- 64.68
Episode length: 51.70 +/- 15.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.7         |
|    mean_reward          | -418         |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0097234715 |
|    clip_fraction        | 0.168        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.00115      |
|    learning_rate        | 0.001        |
|    loss                 | 1.78e+03     |
|    n_updates            | 9            |
|    policy_gradient_loss | 0.00897      |
|    value_loss           | 2.8e+03      |
------------------------------------------
Eval num_timesteps=5000, episode_reward=-410.57 +/- 71.58
Episode length: 49.48 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-416.60 +/- 66.07
Episode length: 52.28 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-417.80 +/- 78.28
Episode length: 51.04 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.4     |
|    ep_rew_mean     | -321     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=6500, episode_reward=-401.00 +/- 75.54
Episode length: 48.32 +/- 16.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.3        |
|    mean_reward          | -401        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.016704742 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.00139     |
|    learning_rate        | 0.001       |
|    loss                 | 4.19e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0171      |
|    value_loss           | 6.7e+03     |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=-417.80 +/- 63.28
Episode length: 49.68 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-424.98 +/- 75.24
Episode length: 53.72 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-425.60 +/- 71.25
Episode length: 47.74 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 62       |
|    ep_rew_mean     | -328     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 4        |
|    time_elapsed    | 35       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=8500, episode_reward=-414.20 +/- 60.33
Episode length: 48.88 +/- 14.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.9        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.012650313 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.00301     |
|    learning_rate        | 0.001       |
|    loss                 | 2e+03       |
|    n_updates            | 11          |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 4.29e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-418.40 +/- 81.86
Episode length: 52.82 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-408.19 +/- 71.57
Episode length: 50.10 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-413.00 +/- 76.58
Episode length: 49.12 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.4     |
|    ep_rew_mean     | -321     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 5        |
|    time_elapsed    | 43       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=10500, episode_reward=-433.39 +/- 58.92
Episode length: 50.74 +/- 16.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -433        |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.037196156 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.00334     |
|    learning_rate        | 0.001       |
|    loss                 | 1.9e+03     |
|    n_updates            | 12          |
|    policy_gradient_loss | 0.0269      |
|    value_loss           | 3.5e+03     |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-424.39 +/- 59.57
Episode length: 50.96 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-416.59 +/- 71.81
Episode length: 49.62 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-390.79 +/- 86.69
Episode length: 46.86 +/- 14.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -338     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 6        |
|    time_elapsed    | 52       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.18
Eval num_timesteps=12500, episode_reward=-417.20 +/- 64.13
Episode length: 53.08 +/- 19.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.1       |
|    mean_reward          | -417       |
| time/                   |            |
|    total_timesteps      | 12500      |
| train/                  |            |
|    approx_kl            | 0.09144954 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.00115    |
|    learning_rate        | 0.001      |
|    loss                 | 2.93e+03   |
|    n_updates            | 13         |
|    policy_gradient_loss | 0.0569     |
|    value_loss           | 6.09e+03   |
----------------------------------------
Eval num_timesteps=13000, episode_reward=-409.99 +/- 72.01
Episode length: 50.80 +/- 14.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-389.59 +/- 73.73
Episode length: 47.58 +/- 21.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
New best mean reward!
Eval num_timesteps=14000, episode_reward=-424.99 +/- 45.81
Episode length: 47.14 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -364     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 7        |
|    time_elapsed    | 61       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=14500, episode_reward=-395.59 +/- 85.28
Episode length: 50.32 +/- 16.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.3        |
|    mean_reward          | -396        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.081830546 |
|    clip_fraction        | 0.488       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.641      |
|    explained_variance   | -0.00126    |
|    learning_rate        | 0.001       |
|    loss                 | 2.79e+03    |
|    n_updates            | 14          |
|    policy_gradient_loss | 0.0351      |
|    value_loss           | 5.93e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-393.19 +/- 82.28
Episode length: 44.54 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-422.00 +/- 58.92
Episode length: 51.60 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-412.99 +/- 63.18
Episode length: 49.32 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 8        |
|    time_elapsed    | 69       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=16500, episode_reward=-422.59 +/- 56.37
Episode length: 47.98 +/- 14.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48         |
|    mean_reward          | -423       |
| time/                   |            |
|    total_timesteps      | 16500      |
| train/                  |            |
|    approx_kl            | 0.04860704 |
|    clip_fraction        | 0.0312     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.225     |
|    explained_variance   | -0.00143   |
|    learning_rate        | 0.001      |
|    loss                 | 2.88e+03   |
|    n_updates            | 15         |
|    policy_gradient_loss | 0.0167     |
|    value_loss           | 4.67e+03   |
----------------------------------------
Eval num_timesteps=17000, episode_reward=-417.19 +/- 69.52
Episode length: 49.84 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-399.19 +/- 73.20
Episode length: 45.44 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-403.99 +/- 78.65
Episode length: 46.38 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 9        |
|    time_elapsed    | 77       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=18500, episode_reward=-397.39 +/- 67.88
Episode length: 50.24 +/- 19.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.2       |
|    mean_reward          | -397       |
| time/                   |            |
|    total_timesteps      | 18500      |
| train/                  |            |
|    approx_kl            | 0.01768626 |
|    clip_fraction        | 0.00781    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.038     |
|    explained_variance   | -0.000504  |
|    learning_rate        | 0.001      |
|    loss                 | 2.75e+03   |
|    n_updates            | 16         |
|    policy_gradient_loss | 0.00224    |
|    value_loss           | 4.37e+03   |
----------------------------------------
Eval num_timesteps=19000, episode_reward=-421.99 +/- 61.90
Episode length: 51.28 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-397.39 +/- 72.25
Episode length: 43.16 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.2     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-408.19 +/- 76.67
Episode length: 45.38 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.6     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 10       |
|    time_elapsed    | 85       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=-412.99 +/- 63.75
Episode length: 48.64 +/- 17.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.009538025 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00226    |
|    explained_variance   | -0.00107    |
|    learning_rate        | 0.001       |
|    loss                 | 3.04e+03    |
|    n_updates            | 17          |
|    policy_gradient_loss | 0.00227     |
|    value_loss           | 5.76e+03    |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=-410.59 +/- 81.44
Episode length: 45.84 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-427.39 +/- 59.10
Episode length: 50.08 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-425.59 +/- 60.30
Episode length: 52.00 +/- 19.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-419.00 +/- 68.43
Episode length: 53.96 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.8     |
|    ep_rew_mean     | -430     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 11       |
|    time_elapsed    | 95       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-409.99 +/- 67.63
Episode length: 51.38 +/- 22.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -410      |
| time/                   |           |
|    total_timesteps      | 23000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-06 |
|    explained_variance   | -0.00342  |
|    learning_rate        | 0.001     |
|    loss                 | 4.78e+03  |
|    n_updates            | 27        |
|    policy_gradient_loss | 4.75e-07  |
|    value_loss           | 8.76e+03  |
---------------------------------------
Eval num_timesteps=23500, episode_reward=-429.19 +/- 57.81
Episode length: 47.36 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-405.19 +/- 81.75
Episode length: 51.52 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-404.00 +/- 80.24
Episode length: 51.68 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 12       |
|    time_elapsed    | 104      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-409.99 +/- 71.00
Episode length: 48.90 +/- 14.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.9      |
|    mean_reward          | -410      |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-15 |
|    explained_variance   | -0.0126   |
|    learning_rate        | 0.001     |
|    loss                 | 1.24e+05  |
|    n_updates            | 37        |
|    policy_gradient_loss | 1.01e-08  |
|    value_loss           | 2.38e+05  |
---------------------------------------
Eval num_timesteps=25500, episode_reward=-418.39 +/- 68.98
Episode length: 53.02 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-424.37 +/- 75.16
Episode length: 48.86 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-418.39 +/- 71.79
Episode length: 48.84 +/- 15.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 13       |
|    time_elapsed    | 113      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.24
Eval num_timesteps=27000, episode_reward=-405.80 +/- 75.90
Episode length: 44.84 +/- 13.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.8        |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.016113441 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.024      |
|    explained_variance   | -0.00384    |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+04    |
|    n_updates            | 39          |
|    policy_gradient_loss | 0.00396     |
|    value_loss           | 2.5e+04     |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-416.58 +/- 73.29
Episode length: 48.04 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-413.60 +/- 78.46
Episode length: 50.20 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-417.20 +/- 80.78
Episode length: 50.74 +/- 19.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 14       |
|    time_elapsed    | 122      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.65
Eval num_timesteps=29000, episode_reward=-421.40 +/- 60.18
Episode length: 49.88 +/- 13.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.9      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 0.8258869 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.367    |
|    explained_variance   | -0.00373  |
|    learning_rate        | 0.001     |
|    loss                 | 2.05e+03  |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.142     |
|    value_loss           | 4.29e+03  |
---------------------------------------
Eval num_timesteps=29500, episode_reward=-431.00 +/- 59.54
Episode length: 51.56 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-419.58 +/- 68.16
Episode length: 51.60 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-421.40 +/- 55.52
Episode length: 49.80 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 15       |
|    time_elapsed    | 130      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=31000, episode_reward=-414.80 +/- 76.41
Episode length: 48.42 +/- 16.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.4        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.039338637 |
|    clip_fraction        | 0.0026      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0124     |
|    explained_variance   | -0.00359    |
|    learning_rate        | 0.001       |
|    loss                 | 2.77e+03    |
|    n_updates            | 41          |
|    policy_gradient_loss | -0.000726   |
|    value_loss           | 5.54e+03    |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=-400.40 +/- 82.04
Episode length: 47.52 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-402.20 +/- 85.59
Episode length: 46.16 +/- 12.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-416.59 +/- 68.46
Episode length: 48.82 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 16       |
|    time_elapsed    | 138      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=-397.39 +/- 97.14
Episode length: 49.48 +/- 18.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -397      |
| time/                   |           |
|    total_timesteps      | 33000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.29e-10 |
|    explained_variance   | -0.00487  |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+03  |
|    n_updates            | 51        |
|    policy_gradient_loss | 5.18e-10  |
|    value_loss           | 5.59e+03  |
---------------------------------------
Eval num_timesteps=33500, episode_reward=-422.60 +/- 66.61
Episode length: 51.06 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-422.00 +/- 63.34
Episode length: 46.98 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-415.40 +/- 81.88
Episode length: 48.16 +/- 13.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 17       |
|    time_elapsed    | 147      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-412.40 +/- 79.55
Episode length: 50.20 +/- 19.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-07 |
|    explained_variance   | -0.00435  |
|    learning_rate        | 0.001     |
|    loss                 | 6.95e+03  |
|    n_updates            | 61        |
|    policy_gradient_loss | 4.94e-09  |
|    value_loss           | 1.63e+04  |
---------------------------------------
Eval num_timesteps=35500, episode_reward=-414.80 +/- 79.63
Episode length: 48.08 +/- 20.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-417.20 +/- 72.31
Episode length: 50.34 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-397.40 +/- 70.74
Episode length: 50.98 +/- 20.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 18       |
|    time_elapsed    | 156      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-391.39 +/- 77.21
Episode length: 48.38 +/- 18.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -391      |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-09 |
|    explained_variance   | -0.00101  |
|    learning_rate        | 0.001     |
|    loss                 | 2.9e+03   |
|    n_updates            | 71        |
|    policy_gradient_loss | 1.84e-09  |
|    value_loss           | 4.94e+03  |
---------------------------------------
Eval num_timesteps=37500, episode_reward=-417.20 +/- 68.48
Episode length: 51.54 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-411.20 +/- 79.47
Episode length: 49.54 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-426.18 +/- 69.89
Episode length: 50.06 +/- 21.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 19       |
|    time_elapsed    | 165      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-410.00 +/- 68.42
Episode length: 46.72 +/- 14.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -410      |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-06 |
|    explained_variance   | -0.00617  |
|    learning_rate        | 0.001     |
|    loss                 | 1.1e+04   |
|    n_updates            | 81        |
|    policy_gradient_loss | 4.19e-09  |
|    value_loss           | 2.18e+04  |
---------------------------------------
Eval num_timesteps=39500, episode_reward=-413.60 +/- 66.54
Episode length: 48.34 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-421.99 +/- 61.90
Episode length: 49.92 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-401.00 +/- 77.65
Episode length: 48.64 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 20       |
|    time_elapsed    | 174      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-422.00 +/- 53.48
Episode length: 52.48 +/- 20.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.5      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-08 |
|    explained_variance   | 0.000703  |
|    learning_rate        | 0.001     |
|    loss                 | 2.37e+03  |
|    n_updates            | 91        |
|    policy_gradient_loss | 8.32e-10  |
|    value_loss           | 4.91e+03  |
---------------------------------------
Eval num_timesteps=41500, episode_reward=-416.00 +/- 64.41
Episode length: 46.40 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-414.20 +/- 62.68
Episode length: 48.10 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-416.57 +/- 73.08
Episode length: 49.54 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-423.19 +/- 67.64
Episode length: 53.18 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 21       |
|    time_elapsed    | 185      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-421.40 +/- 58.67
Episode length: 53.56 +/- 19.89
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 53.6           |
|    mean_reward          | -421           |
| time/                   |                |
|    total_timesteps      | 43500          |
| train/                  |                |
|    approx_kl            | -1.2223609e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -4.2e-06       |
|    explained_variance   | -0.00751       |
|    learning_rate        | 0.001          |
|    loss                 | 1.2e+04        |
|    n_updates            | 101            |
|    policy_gradient_loss | -3.9e-09       |
|    value_loss           | 2.55e+04       |
--------------------------------------------
Eval num_timesteps=44000, episode_reward=-410.60 +/- 63.85
Episode length: 52.22 +/- 20.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-399.18 +/- 67.60
Episode length: 48.86 +/- 14.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-396.80 +/- 58.90
Episode length: 47.42 +/- 16.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 22       |
|    time_elapsed    | 194      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-412.39 +/- 71.43
Episode length: 52.08 +/- 17.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.1      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 45500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.54e-08 |
|    explained_variance   | 0.00132   |
|    learning_rate        | 0.001     |
|    loss                 | 1.68e+03  |
|    n_updates            | 111       |
|    policy_gradient_loss | -2.07e-08 |
|    value_loss           | 5.3e+03   |
---------------------------------------
Eval num_timesteps=46000, episode_reward=-407.60 +/- 72.00
Episode length: 50.62 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-420.79 +/- 70.17
Episode length: 49.18 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-424.99 +/- 69.52
Episode length: 54.74 +/- 14.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 23       |
|    time_elapsed    | 203      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 6 due to reaching max kl: 3.01
Eval num_timesteps=47500, episode_reward=-411.80 +/- 50.77
Episode length: 51.66 +/- 17.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.7       |
|    mean_reward          | -412       |
| time/                   |            |
|    total_timesteps      | 47500      |
| train/                  |            |
|    approx_kl            | 0.33497193 |
|    clip_fraction        | 0.00952    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.004     |
|    explained_variance   | -0.00634   |
|    learning_rate        | 0.001      |
|    loss                 | 1.22e+04   |
|    n_updates            | 118        |
|    policy_gradient_loss | 0.00264    |
|    value_loss           | 2.72e+04   |
----------------------------------------
Eval num_timesteps=48000, episode_reward=-411.20 +/- 64.47
Episode length: 51.00 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-397.39 +/- 72.25
Episode length: 50.14 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-426.79 +/- 43.42
Episode length: 53.00 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 24       |
|    time_elapsed    | 212      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=49500, episode_reward=-428.00 +/- 51.49
Episode length: 50.82 +/- 13.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.049366914 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.047      |
|    explained_variance   | 0.00941     |
|    learning_rate        | 0.001       |
|    loss                 | 3.58e+03    |
|    n_updates            | 119         |
|    policy_gradient_loss | 0.00509     |
|    value_loss           | 5.9e+03     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-391.39 +/- 76.51
Episode length: 47.74 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-404.59 +/- 76.43
Episode length: 49.28 +/- 15.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-417.79 +/- 66.87
Episode length: 47.58 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 25       |
|    time_elapsed    | 221      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=-429.20 +/- 58.74
Episode length: 51.68 +/- 14.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 51500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.71e-07 |
|    explained_variance   | 0.0096    |
|    learning_rate        | 0.001     |
|    loss                 | 2.31e+03  |
|    n_updates            | 129       |
|    policy_gradient_loss | -5.59e-07 |
|    value_loss           | 4.51e+03  |
---------------------------------------
Eval num_timesteps=52000, episode_reward=-404.60 +/- 84.48
Episode length: 50.60 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-421.99 +/- 64.74
Episode length: 48.00 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-417.18 +/- 87.46
Episode length: 52.50 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 26       |
|    time_elapsed    | 230      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=-408.79 +/- 73.38
Episode length: 43.12 +/- 14.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 43.1     |
|    mean_reward          | -409     |
| time/                   |          |
|    total_timesteps      | 53500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0231   |
|    learning_rate        | 0.001    |
|    loss                 | 1.36e+05 |
|    n_updates            | 139      |
|    policy_gradient_loss | 6.69e-09 |
|    value_loss           | 2.96e+05 |
--------------------------------------
Eval num_timesteps=54000, episode_reward=-432.19 +/- 54.82
Episode length: 52.48 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-413.59 +/- 72.25
Episode length: 49.38 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-420.19 +/- 60.57
Episode length: 49.86 +/- 15.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 27       |
|    time_elapsed    | 238      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-397.99 +/- 84.99
Episode length: 49.16 +/- 17.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.2      |
|    mean_reward          | -398      |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.56e-15 |
|    explained_variance   | 0.0267    |
|    learning_rate        | 0.001     |
|    loss                 | 7.64e+03  |
|    n_updates            | 149       |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 1.37e+04  |
---------------------------------------
Eval num_timesteps=56000, episode_reward=-408.20 +/- 75.96
Episode length: 53.92 +/- 20.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-423.19 +/- 65.19
Episode length: 48.86 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-430.40 +/- 68.14
Episode length: 55.04 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 28       |
|    time_elapsed    | 247      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-409.99 +/- 70.24
Episode length: 50.96 +/- 16.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -410      |
| time/                   |           |
|    total_timesteps      | 57500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.99e-17 |
|    explained_variance   | 0.00721   |
|    learning_rate        | 0.001     |
|    loss                 | 2.6e+03   |
|    n_updates            | 159       |
|    policy_gradient_loss | 3.75e-10  |
|    value_loss           | 5e+03     |
---------------------------------------
Eval num_timesteps=58000, episode_reward=-407.59 +/- 73.49
Episode length: 48.00 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-413.60 +/- 71.25
Episode length: 51.74 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-436.40 +/- 69.55
Episode length: 52.20 +/- 19.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 29       |
|    time_elapsed    | 256      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-428.60 +/- 67.68
Episode length: 53.28 +/- 15.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.3      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.1e-14  |
|    explained_variance   | 0.0298    |
|    learning_rate        | 0.001     |
|    loss                 | 9.16e+03  |
|    n_updates            | 169       |
|    policy_gradient_loss | -9.63e-10 |
|    value_loss           | 1.48e+04  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=-405.19 +/- 84.14
Episode length: 51.14 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-399.77 +/- 94.84
Episode length: 46.98 +/- 13.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-405.79 +/- 73.24
Episode length: 50.10 +/- 19.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 30       |
|    time_elapsed    | 265      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-425.59 +/- 60.30
Episode length: 52.28 +/- 14.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.3      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.87e-17 |
|    explained_variance   | 0.00662   |
|    learning_rate        | 0.001     |
|    loss                 | 2.14e+03  |
|    n_updates            | 179       |
|    policy_gradient_loss | -1.69e-10 |
|    value_loss           | 4.67e+03  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=-418.39 +/- 74.98
Episode length: 48.42 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-416.60 +/- 74.28
Episode length: 49.54 +/- 13.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-409.99 +/- 79.83
Episode length: 46.56 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 31       |
|    time_elapsed    | 273      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-430.40 +/- 53.65
Episode length: 52.34 +/- 15.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.3      |
|    mean_reward          | -430      |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.58e-15 |
|    explained_variance   | 0.0272    |
|    learning_rate        | 0.001     |
|    loss                 | 7.82e+03  |
|    n_updates            | 189       |
|    policy_gradient_loss | -4.92e-09 |
|    value_loss           | 1.6e+04   |
---------------------------------------
Eval num_timesteps=64000, episode_reward=-414.79 +/- 71.54
Episode length: 47.32 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-426.20 +/- 68.33
Episode length: 53.72 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-411.19 +/- 76.47
Episode length: 47.74 +/- 15.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-405.80 +/- 75.42
Episode length: 51.96 +/- 20.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 32       |
|    time_elapsed    | 284      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-425.59 +/- 73.49
Episode length: 45.92 +/- 13.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.9      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-17 |
|    explained_variance   | 0.00723   |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+03  |
|    n_updates            | 199       |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 4.38e+03  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=-406.39 +/- 64.72
Episode length: 51.46 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-404.59 +/- 90.85
Episode length: 49.00 +/- 14.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-400.39 +/- 84.63
Episode length: 48.46 +/- 17.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 33       |
|    time_elapsed    | 293      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-417.19 +/- 68.21
Episode length: 52.72 +/- 22.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.85e-15 |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.001     |
|    loss                 | 5.69e+03  |
|    n_updates            | 209       |
|    policy_gradient_loss | 8.41e-09  |
|    value_loss           | 1.51e+04  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=-430.99 +/- 56.44
Episode length: 49.14 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-401.59 +/- 79.83
Episode length: 52.54 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-414.19 +/- 70.76
Episode length: 48.64 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 34       |
|    time_elapsed    | 301      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-414.19 +/- 78.02
Episode length: 48.82 +/- 18.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-17 |
|    explained_variance   | 0.00643   |
|    learning_rate        | 0.001     |
|    loss                 | 2.45e+03  |
|    n_updates            | 219       |
|    policy_gradient_loss | 5.06e-10  |
|    value_loss           | 4.87e+03  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=-414.19 +/- 84.87
Episode length: 48.52 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-391.99 +/- 66.12
Episode length: 54.52 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-421.39 +/- 67.51
Episode length: 50.84 +/- 19.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 35       |
|    time_elapsed    | 310      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-425.59 +/- 53.33
Episode length: 50.06 +/- 16.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-16 |
|    explained_variance   | 0.0177    |
|    learning_rate        | 0.001     |
|    loss                 | 7.28e+03  |
|    n_updates            | 229       |
|    policy_gradient_loss | -3.36e-09 |
|    value_loss           | 1.42e+04  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=-423.19 +/- 76.61
Episode length: 51.22 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-399.80 +/- 96.45
Episode length: 50.64 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-410.00 +/- 74.95
Episode length: 48.16 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 36       |
|    time_elapsed    | 319      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-414.17 +/- 85.64
Episode length: 49.68 +/- 15.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-18 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.001     |
|    loss                 | 1.71e+03  |
|    n_updates            | 239       |
|    policy_gradient_loss | -1.51e-10 |
|    value_loss           | 4.88e+03  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=-411.79 +/- 72.65
Episode length: 52.82 +/- 18.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-416.00 +/- 66.34
Episode length: 50.80 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-417.19 +/- 75.95
Episode length: 47.44 +/- 17.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -434     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 37       |
|    time_elapsed    | 328      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-406.39 +/- 70.32
Episode length: 44.04 +/- 13.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44        |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.34e-17 |
|    explained_variance   | 0.0237    |
|    learning_rate        | 0.001     |
|    loss                 | 8.08e+03  |
|    n_updates            | 249       |
|    policy_gradient_loss | 1.65e-09  |
|    value_loss           | 1.36e+04  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=-410.59 +/- 77.13
Episode length: 51.14 +/- 18.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-423.80 +/- 65.12
Episode length: 50.14 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-417.79 +/- 74.02
Episode length: 51.80 +/- 18.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -434     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 38       |
|    time_elapsed    | 336      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-429.19 +/- 56.87
Episode length: 49.56 +/- 18.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.48e-19 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 1.8e+03   |
|    n_updates            | 259       |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 5.21e+03  |
---------------------------------------
Eval num_timesteps=78500, episode_reward=-418.39 +/- 66.32
Episode length: 50.70 +/- 15.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-397.40 +/- 80.73
Episode length: 53.84 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-416.59 +/- 80.78
Episode length: 52.08 +/- 15.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -429     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 39       |
|    time_elapsed    | 346      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-418.99 +/- 66.57
Episode length: 47.46 +/- 17.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.5      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-17 |
|    explained_variance   | 0.0263    |
|    learning_rate        | 0.001     |
|    loss                 | 6.99e+03  |
|    n_updates            | 269       |
|    policy_gradient_loss | -2.01e-09 |
|    value_loss           | 1.38e+04  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=-408.80 +/- 65.88
Episode length: 55.08 +/- 19.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-432.79 +/- 57.54
Episode length: 53.56 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-424.39 +/- 79.27
Episode length: 45.14 +/- 13.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 40       |
|    time_elapsed    | 354      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-407.00 +/- 84.79
Episode length: 46.66 +/- 16.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 46.7     |
|    mean_reward          | -407     |
| time/                   |          |
|    total_timesteps      | 82000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.7e-19 |
|    explained_variance   | 0.00837  |
|    learning_rate        | 0.001    |
|    loss                 | 2.49e+03 |
|    n_updates            | 279      |
|    policy_gradient_loss | 1.98e-10 |
|    value_loss           | 4.92e+03 |
--------------------------------------
Eval num_timesteps=82500, episode_reward=-436.99 +/- 51.16
Episode length: 49.82 +/- 12.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-428.59 +/- 57.63
Episode length: 52.62 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-441.20 +/- 51.77
Episode length: 53.92 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 41       |
|    time_elapsed    | 363      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-426.79 +/- 64.44
Episode length: 52.28 +/- 15.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.3      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6e-18    |
|    explained_variance   | 0.0219    |
|    learning_rate        | 0.001     |
|    loss                 | 7.55e+03  |
|    n_updates            | 289       |
|    policy_gradient_loss | -2.34e-09 |
|    value_loss           | 1.39e+04  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=-402.19 +/- 71.87
Episode length: 48.64 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-411.80 +/- 74.61
Episode length: 49.82 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-403.99 +/- 67.04
Episode length: 49.14 +/- 12.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-414.79 +/- 74.01
Episode length: 52.10 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 42       |
|    time_elapsed    | 374      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-403.99 +/- 70.19
Episode length: 51.04 +/- 18.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -404      |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.57e-20 |
|    explained_variance   | 0.0104    |
|    learning_rate        | 0.001     |
|    loss                 | 2.39e+03  |
|    n_updates            | 299       |
|    policy_gradient_loss | 2.11e-09  |
|    value_loss           | 5.44e+03  |
---------------------------------------
Eval num_timesteps=87000, episode_reward=-428.00 +/- 56.81
Episode length: 50.76 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-417.19 +/- 58.86
Episode length: 49.24 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-415.40 +/- 75.23
Episode length: 50.20 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 43       |
|    time_elapsed    | 383      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-407.59 +/- 63.78
Episode length: 49.92 +/- 16.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.9      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.87e-18 |
|    explained_variance   | 0.02      |
|    learning_rate        | 0.001     |
|    loss                 | 4.65e+03  |
|    n_updates            | 309       |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 1.2e+04   |
---------------------------------------
Eval num_timesteps=89000, episode_reward=-400.39 +/- 102.52
Episode length: 48.88 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-432.20 +/- 63.92
Episode length: 53.88 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-423.79 +/- 68.62
Episode length: 55.38 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 44       |
|    time_elapsed    | 392      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-405.79 +/- 71.75
Episode length: 46.52 +/- 15.26
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 46.5     |
|    mean_reward          | -406     |
| time/                   |          |
|    total_timesteps      | 90500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.1e-20 |
|    explained_variance   | 0.0102   |
|    learning_rate        | 0.001    |
|    loss                 | 3.11e+03 |
|    n_updates            | 319      |
|    policy_gradient_loss | 1.47e-09 |
|    value_loss           | 4.32e+03 |
--------------------------------------
Eval num_timesteps=91000, episode_reward=-409.40 +/- 68.10
Episode length: 49.42 +/- 15.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-411.80 +/- 69.35
Episode length: 55.72 +/- 21.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-414.19 +/- 75.44
Episode length: 52.92 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 45       |
|    time_elapsed    | 401      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-408.19 +/- 63.30
Episode length: 49.66 +/- 19.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.55e-18 |
|    explained_variance   | 0.0166    |
|    learning_rate        | 0.001     |
|    loss                 | 5.38e+03  |
|    n_updates            | 329       |
|    policy_gradient_loss | 3.38e-09  |
|    value_loss           | 1.08e+04  |
---------------------------------------
Eval num_timesteps=93000, episode_reward=-414.19 +/- 78.71
Episode length: 51.48 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-430.99 +/- 63.07
Episode length: 47.40 +/- 14.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-405.20 +/- 74.85
Episode length: 50.52 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -429     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 46       |
|    time_elapsed    | 410      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-424.39 +/- 72.88
Episode length: 52.04 +/- 18.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -424      |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.55e-20 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.001     |
|    loss                 | 2.04e+03  |
|    n_updates            | 339       |
|    policy_gradient_loss | -1.03e-09 |
|    value_loss           | 4.37e+03  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=-411.19 +/- 67.47
Episode length: 50.44 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-426.79 +/- 66.91
Episode length: 51.22 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-410.59 +/- 73.55
Episode length: 50.18 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 47       |
|    time_elapsed    | 419      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-429.19 +/- 54.61
Episode length: 46.54 +/- 14.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.5      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.14e-18 |
|    explained_variance   | 0.0197    |
|    learning_rate        | 0.001     |
|    loss                 | 5.21e+03  |
|    n_updates            | 349       |
|    policy_gradient_loss | 7.17e-09  |
|    value_loss           | 9.82e+03  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=-422.60 +/- 66.07
Episode length: 52.62 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-426.19 +/- 64.81
Episode length: 50.10 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-423.79 +/- 56.54
Episode length: 48.22 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 48       |
|    time_elapsed    | 427      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-417.80 +/- 62.70
Episode length: 49.04 +/- 15.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49        |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.96e-20 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 2.56e+03  |
|    n_updates            | 359       |
|    policy_gradient_loss | 2.3e-10   |
|    value_loss           | 4.74e+03  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=-411.19 +/- 71.61
Episode length: 51.40 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-413.59 +/- 68.15
Episode length: 52.24 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-413.59 +/- 70.49
Episode length: 53.38 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 49       |
|    time_elapsed    | 436      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-434.59 +/- 55.72
Episode length: 51.20 +/- 20.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.2      |
|    mean_reward          | -435      |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7e-18    |
|    explained_variance   | 0.0339    |
|    learning_rate        | 0.001     |
|    loss                 | 5.07e+03  |
|    n_updates            | 369       |
|    policy_gradient_loss | -6.37e-10 |
|    value_loss           | 9.41e+03  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=-418.99 +/- 69.74
Episode length: 50.84 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-427.99 +/- 73.89
Episode length: 53.92 +/- 20.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-410.60 +/- 76.90
Episode length: 49.72 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 50       |
|    time_elapsed    | 445      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-411.19 +/- 71.10
Episode length: 44.72 +/- 14.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44.7      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.56e-20 |
|    explained_variance   | 0.00955   |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+03  |
|    n_updates            | 379       |
|    policy_gradient_loss | -1.25e-09 |
|    value_loss           | 4.36e+03  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=-411.19 +/- 66.12
Episode length: 48.80 +/- 17.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-423.20 +/- 61.20
Episode length: 50.70 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-414.19 +/- 65.48
Episode length: 48.16 +/- 14.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 51       |
|    time_elapsed    | 454      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-420.19 +/- 53.29
Episode length: 49.26 +/- 16.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.3      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.99e-18 |
|    explained_variance   | 0.0268    |
|    learning_rate        | 0.001     |
|    loss                 | 5.11e+03  |
|    n_updates            | 389       |
|    policy_gradient_loss | 9.26e-10  |
|    value_loss           | 8.54e+03  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=-427.39 +/- 71.25
Episode length: 54.00 +/- 21.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-410.60 +/- 65.25
Episode length: 50.52 +/- 14.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-412.99 +/- 76.81
Episode length: 50.78 +/- 13.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 52       |
|    time_elapsed    | 463      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-406.99 +/- 73.41
Episode length: 47.56 +/- 14.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.6      |
|    mean_reward          | -407      |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.84e-20 |
|    explained_variance   | 0.0084    |
|    learning_rate        | 0.001     |
|    loss                 | 1.69e+03  |
|    n_updates            | 399       |
|    policy_gradient_loss | -2.77e-09 |
|    value_loss           | 4.65e+03  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=-413.60 +/- 68.93
Episode length: 48.38 +/- 14.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-415.40 +/- 75.71
Episode length: 50.50 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-415.39 +/- 63.56
Episode length: 49.52 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-430.99 +/- 51.78
Episode length: 49.06 +/- 14.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 53       |
|    time_elapsed    | 473      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-400.99 +/- 79.71
Episode length: 50.72 +/- 22.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -401      |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.74e-18 |
|    explained_variance   | 0.0286    |
|    learning_rate        | 0.001     |
|    loss                 | 4.49e+03  |
|    n_updates            | 409       |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 8.51e+03  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=-412.99 +/- 72.97
Episode length: 50.10 +/- 14.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-419.59 +/- 65.18
Episode length: 47.74 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-414.19 +/- 71.02
Episode length: 52.26 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 54       |
|    time_elapsed    | 482      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-416.59 +/- 75.00
Episode length: 48.10 +/- 15.16
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 48.1     |
|    mean_reward          | -417     |
| time/                   |          |
|    total_timesteps      | 111000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4.4e-20 |
|    explained_variance   | 0.0099   |
|    learning_rate        | 0.001    |
|    loss                 | 2.14e+03 |
|    n_updates            | 419      |
|    policy_gradient_loss | 1.3e-09  |
|    value_loss           | 4.08e+03 |
--------------------------------------
Eval num_timesteps=111500, episode_reward=-433.40 +/- 60.12
Episode length: 48.50 +/- 13.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-409.39 +/- 67.30
Episode length: 49.76 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-415.39 +/- 75.23
Episode length: 50.70 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 55       |
|    time_elapsed    | 490      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-411.20 +/- 69.57
Episode length: 52.90 +/- 16.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.9      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.68e-18 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.001     |
|    loss                 | 3.87e+03  |
|    n_updates            | 429       |
|    policy_gradient_loss | 4e-09     |
|    value_loss           | 7.05e+03  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=-426.20 +/- 63.13
Episode length: 54.56 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-414.79 +/- 61.82
Episode length: 50.18 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-424.39 +/- 75.55
Episode length: 49.70 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 56       |
|    time_elapsed    | 499      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-420.19 +/- 74.67
Episode length: 52.18 +/- 18.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.17e-20 |
|    explained_variance   | 0.00745   |
|    learning_rate        | 0.001     |
|    loss                 | 1.61e+03  |
|    n_updates            | 439       |
|    policy_gradient_loss | -2.72e-10 |
|    value_loss           | 4.36e+03  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=-403.39 +/- 69.82
Episode length: 48.62 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-424.99 +/- 61.27
Episode length: 50.96 +/- 21.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-415.39 +/- 63.84
Episode length: 45.30 +/- 17.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 57       |
|    time_elapsed    | 508      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-399.79 +/- 73.59
Episode length: 48.42 +/- 14.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -400      |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.25e-18 |
|    explained_variance   | 0.00924   |
|    learning_rate        | 0.001     |
|    loss                 | 3.9e+03   |
|    n_updates            | 449       |
|    policy_gradient_loss | 1.78e-09  |
|    value_loss           | 7.56e+03  |
---------------------------------------
Eval num_timesteps=117500, episode_reward=-406.39 +/- 79.68
Episode length: 50.42 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-406.40 +/- 74.30
Episode length: 53.74 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-412.99 +/- 64.03
Episode length: 45.76 +/- 13.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 58       |
|    time_elapsed    | 517      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-418.39 +/- 79.86
Episode length: 48.50 +/- 19.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.53e-20 |
|    explained_variance   | 0.00677   |
|    learning_rate        | 0.001     |
|    loss                 | 2.11e+03  |
|    n_updates            | 459       |
|    policy_gradient_loss | -1.93e-09 |
|    value_loss           | 4.55e+03  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=-413.59 +/- 73.49
Episode length: 47.42 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-401.59 +/- 71.75
Episode length: 48.68 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-415.39 +/- 69.51
Episode length: 48.16 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 59       |
|    time_elapsed    | 525      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-415.39 +/- 69.25
Episode length: 52.84 +/- 15.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 121000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.37e-18 |
|    explained_variance   | 0.0199    |
|    learning_rate        | 0.001     |
|    loss                 | 2.81e+03  |
|    n_updates            | 469       |
|    policy_gradient_loss | 2.07e-09  |
|    value_loss           | 6.58e+03  |
---------------------------------------
Eval num_timesteps=121500, episode_reward=-412.40 +/- 63.71
Episode length: 49.28 +/- 13.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-421.39 +/- 73.14
Episode length: 52.80 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-406.39 +/- 74.06
Episode length: 48.26 +/- 13.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -434     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 60       |
|    time_elapsed    | 534      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-429.20 +/- 52.94
Episode length: 47.22 +/- 17.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.2      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.78e-20 |
|    explained_variance   | 0.0091    |
|    learning_rate        | 0.001     |
|    loss                 | 3.2e+03   |
|    n_updates            | 479       |
|    policy_gradient_loss | -6.34e-10 |
|    value_loss           | 4.99e+03  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=-432.19 +/- 57.08
Episode length: 52.06 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-432.19 +/- 67.48
Episode length: 50.28 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-425.59 +/- 54.99
Episode length: 44.30 +/- 13.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -433     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 61       |
|    time_elapsed    | 543      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-412.99 +/- 75.63
Episode length: 49.08 +/- 17.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -413      |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.15e-17 |
|    explained_variance   | 0.0293    |
|    learning_rate        | 0.001     |
|    loss                 | 3.44e+03  |
|    n_updates            | 489       |
|    policy_gradient_loss | 2.9e-09   |
|    value_loss           | 6.87e+03  |
---------------------------------------
Eval num_timesteps=125500, episode_reward=-415.39 +/- 70.54
Episode length: 49.72 +/- 13.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-411.20 +/- 70.60
Episode length: 47.82 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-415.39 +/- 75.71
Episode length: 51.46 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.7     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 62       |
|    time_elapsed    | 551      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-396.19 +/- 94.81
Episode length: 47.00 +/- 15.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47        |
|    mean_reward          | -396      |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-19 |
|    explained_variance   | 0.00943   |
|    learning_rate        | 0.001     |
|    loss                 | 2.26e+03  |
|    n_updates            | 499       |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 4.98e+03  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=-431.59 +/- 57.86
Episode length: 51.64 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-412.99 +/- 70.20
Episode length: 51.54 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-421.39 +/- 73.39
Episode length: 47.96 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-417.79 +/- 63.28
Episode length: 53.28 +/- 21.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 63       |
|    time_elapsed    | 562      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-394.39 +/- 83.04
Episode length: 44.02 +/- 14.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44        |
|    mean_reward          | -394      |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-17 |
|    explained_variance   | 0.0197    |
|    learning_rate        | 0.001     |
|    loss                 | 3.13e+03  |
|    n_updates            | 509       |
|    policy_gradient_loss | 1.83e-09  |
|    value_loss           | 6.49e+03  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=-406.99 +/- 78.16
Episode length: 54.80 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-432.19 +/- 64.48
Episode length: 53.34 +/- 18.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-406.39 +/- 79.23
Episode length: 46.62 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 64       |
|    time_elapsed    | 571      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-422.60 +/- 74.28
Episode length: 54.02 +/- 17.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54        |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 131500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-19 |
|    explained_variance   | 0.00832   |
|    learning_rate        | 0.001     |
|    loss                 | 2.42e+03  |
|    n_updates            | 519       |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 3.98e+03  |
---------------------------------------
Eval num_timesteps=132000, episode_reward=-407.59 +/- 70.23
Episode length: 47.84 +/- 15.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-411.79 +/- 81.30
Episode length: 50.86 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-419.59 +/- 63.50
Episode length: 49.26 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 65       |
|    time_elapsed    | 579      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-412.40 +/- 73.18
Episode length: 49.76 +/- 13.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.67e-17 |
|    explained_variance   | 0.0159    |
|    learning_rate        | 0.001     |
|    loss                 | 2.78e+03  |
|    n_updates            | 529       |
|    policy_gradient_loss | -2.37e-09 |
|    value_loss           | 6.78e+03  |
---------------------------------------
Eval num_timesteps=134000, episode_reward=-430.99 +/- 71.37
Episode length: 51.94 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-432.80 +/- 62.92
Episode length: 52.04 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-423.19 +/- 68.95
Episode length: 51.94 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.4     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 66       |
|    time_elapsed    | 588      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-432.79 +/- 58.16
Episode length: 52.10 +/- 15.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.1      |
|    mean_reward          | -433      |
| time/                   |           |
|    total_timesteps      | 135500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-19 |
|    explained_variance   | 0.00841   |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+03  |
|    n_updates            | 539       |
|    policy_gradient_loss | 2.79e-10  |
|    value_loss           | 4.22e+03  |
---------------------------------------
Eval num_timesteps=136000, episode_reward=-408.19 +/- 61.27
Episode length: 49.52 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-423.19 +/- 60.31
Episode length: 50.76 +/- 20.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-429.20 +/- 56.55
Episode length: 50.10 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 67       |
|    time_elapsed    | 597      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-409.40 +/- 80.01
Episode length: 50.68 +/- 15.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -409      |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.96e-17 |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.001     |
|    loss                 | 3.03e+03  |
|    n_updates            | 549       |
|    policy_gradient_loss | 5.08e-09  |
|    value_loss           | 6.79e+03  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=-408.20 +/- 68.22
Episode length: 46.82 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-402.79 +/- 71.49
Episode length: 47.96 +/- 15.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-418.99 +/- 71.02
Episode length: 47.42 +/- 15.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 68       |
|    time_elapsed    | 606      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-422.59 +/- 63.00
Episode length: 53.98 +/- 19.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54        |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.15e-19 |
|    explained_variance   | 0.00718   |
|    learning_rate        | 0.001     |
|    loss                 | 1.77e+03  |
|    n_updates            | 559       |
|    policy_gradient_loss | 6.58e-10  |
|    value_loss           | 4.11e+03  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=-423.19 +/- 73.00
Episode length: 45.88 +/- 15.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-411.79 +/- 73.64
Episode length: 52.50 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-422.59 +/- 69.52
Episode length: 48.46 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 69       |
|    time_elapsed    | 614      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-423.19 +/- 76.61
Episode length: 50.52 +/- 20.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 141500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.09e-17 |
|    explained_variance   | 0.0257    |
|    learning_rate        | 0.001     |
|    loss                 | 3e+03     |
|    n_updates            | 569       |
|    policy_gradient_loss | -2.33e-11 |
|    value_loss           | 6.8e+03   |
---------------------------------------
Eval num_timesteps=142000, episode_reward=-416.59 +/- 65.25
Episode length: 48.40 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-415.99 +/- 71.81
Episode length: 47.20 +/- 15.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-430.39 +/- 59.08
Episode length: 53.78 +/- 21.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 70       |
|    time_elapsed    | 623      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-403.39 +/- 82.14
Episode length: 52.04 +/- 16.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -403      |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.01e-19 |
|    explained_variance   | 0.00988   |
|    learning_rate        | 0.001     |
|    loss                 | 2.67e+03  |
|    n_updates            | 579       |
|    policy_gradient_loss | 3.09e-10  |
|    value_loss           | 4.39e+03  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=-423.80 +/- 69.92
Episode length: 51.66 +/- 13.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-418.99 +/- 65.75
Episode length: 49.18 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-403.39 +/- 74.32
Episode length: 48.24 +/- 13.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 71       |
|    time_elapsed    | 632      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-418.99 +/- 71.02
Episode length: 53.34 +/- 18.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.3      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.68e-17 |
|    explained_variance   | 0.0255    |
|    learning_rate        | 0.001     |
|    loss                 | 3.5e+03   |
|    n_updates            | 589       |
|    policy_gradient_loss | -1.59e-09 |
|    value_loss           | 6.34e+03  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=-407.59 +/- 83.36
Episode length: 51.92 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-405.79 +/- 71.50
Episode length: 49.36 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-420.19 +/- 68.91
Episode length: 47.32 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 72       |
|    time_elapsed    | 641      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-418.99 +/- 65.75
Episode length: 50.10 +/- 14.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.96e-19 |
|    explained_variance   | 0.0149    |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+03  |
|    n_updates            | 599       |
|    policy_gradient_loss | -1.79e-09 |
|    value_loss           | 4.23e+03  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=-420.79 +/- 73.18
Episode length: 49.46 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-409.36 +/- 91.88
Episode length: 49.88 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-426.79 +/- 68.51
Episode length: 50.20 +/- 20.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-432.20 +/- 68.80
Episode length: 52.26 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 73       |
|    time_elapsed    | 652      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-422.00 +/- 60.43
Episode length: 51.58 +/- 13.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.6      |
|    mean_reward          | -422      |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-16 |
|    explained_variance   | 0.0259    |
|    learning_rate        | 0.001     |
|    loss                 | 3.2e+03   |
|    n_updates            | 609       |
|    policy_gradient_loss | 2.97e-10  |
|    value_loss           | 6.31e+03  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=-410.59 +/- 70.29
Episode length: 50.66 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-414.79 +/- 65.77
Episode length: 48.46 +/- 19.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-400.99 +/- 65.59
Episode length: 48.58 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 74       |
|    time_elapsed    | 660      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-423.19 +/- 71.26
Episode length: 47.36 +/- 14.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.4      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.29e-19 |
|    explained_variance   | 0.00822   |
|    learning_rate        | 0.001     |
|    loss                 | 1.98e+03  |
|    n_updates            | 619       |
|    policy_gradient_loss | -3.9e-10  |
|    value_loss           | 4.68e+03  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=-418.99 +/- 70.51
Episode length: 49.88 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-425.59 +/- 68.94
Episode length: 53.58 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-418.99 +/- 63.24
Episode length: 51.86 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 75       |
|    time_elapsed    | 669      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-408.79 +/- 78.13
Episode length: 45.60 +/- 14.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.6      |
|    mean_reward          | -409      |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.03e-16 |
|    explained_variance   | 0.0209    |
|    learning_rate        | 0.001     |
|    loss                 | 3.75e+03  |
|    n_updates            | 629       |
|    policy_gradient_loss | -1.51e-09 |
|    value_loss           | 7.15e+03  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=-429.19 +/- 54.94
Episode length: 48.30 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-419.00 +/- 63.81
Episode length: 50.72 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-402.79 +/- 79.59
Episode length: 49.68 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 76       |
|    time_elapsed    | 678      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-432.79 +/- 46.85
Episode length: 48.78 +/- 17.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -433      |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-18 |
|    explained_variance   | 0.00959   |
|    learning_rate        | 0.001     |
|    loss                 | 2.26e+03  |
|    n_updates            | 639       |
|    policy_gradient_loss | -4.86e-10 |
|    value_loss           | 4.51e+03  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=-408.80 +/- 70.12
Episode length: 54.12 +/- 21.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-421.99 +/- 69.57
Episode length: 48.66 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-423.19 +/- 80.73
Episode length: 51.98 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 77       |
|    time_elapsed    | 687      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-419.59 +/- 73.97
Episode length: 47.62 +/- 15.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.6      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.01e-16 |
|    explained_variance   | 0.0175    |
|    learning_rate        | 0.001     |
|    loss                 | 2.41e+03  |
|    n_updates            | 649       |
|    policy_gradient_loss | 1.27e-09  |
|    value_loss           | 5.73e+03  |
---------------------------------------
Eval num_timesteps=158500, episode_reward=-429.79 +/- 57.05
Episode length: 49.82 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-424.39 +/- 68.30
Episode length: 47.72 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-416.59 +/- 62.71
Episode length: 48.68 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 78       |
|    time_elapsed    | 695      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-411.19 +/- 72.62
Episode length: 49.98 +/- 19.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.89e-18 |
|    explained_variance   | 0.00991   |
|    learning_rate        | 0.001     |
|    loss                 | 1.43e+03  |
|    n_updates            | 659       |
|    policy_gradient_loss | -1.08e-10 |
|    value_loss           | 4.31e+03  |
---------------------------------------
Eval num_timesteps=160500, episode_reward=-413.59 +/- 69.20
Episode length: 51.56 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-420.19 +/- 62.03
Episode length: 49.32 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-415.39 +/- 62.70
Episode length: 51.26 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 79       |
|    time_elapsed    | 704      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-406.39 +/- 74.79
Episode length: 49.64 +/- 15.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.85e-16 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.001     |
|    loss                 | 2.68e+03  |
|    n_updates            | 669       |
|    policy_gradient_loss | 2.91e-10  |
|    value_loss           | 5.94e+03  |
---------------------------------------
Eval num_timesteps=162500, episode_reward=-408.79 +/- 76.26
Episode length: 50.22 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-398.59 +/- 77.36
Episode length: 49.38 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-408.77 +/- 94.48
Episode length: 50.58 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 80       |
|    time_elapsed    | 713      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-418.99 +/- 63.52
Episode length: 46.02 +/- 17.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46        |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.33e-18 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.001     |
|    loss                 | 3.39e+03  |
|    n_updates            | 679       |
|    policy_gradient_loss | 4.85e-09  |
|    value_loss           | 5.64e+03  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=-414.78 +/- 87.65
Episode length: 48.32 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-415.99 +/- 63.28
Episode length: 51.14 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-417.19 +/- 72.81
Episode length: 50.26 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 81       |
|    time_elapsed    | 722      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-428.60 +/- 67.42
Episode length: 52.76 +/- 19.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.03e-16 |
|    explained_variance   | 0.0127    |
|    learning_rate        | 0.001     |
|    loss                 | 3.07e+03  |
|    n_updates            | 689       |
|    policy_gradient_loss | -2.58e-09 |
|    value_loss           | 5.06e+03  |
---------------------------------------
Eval num_timesteps=166500, episode_reward=-418.40 +/- 72.29
Episode length: 53.48 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-407.60 +/- 80.50
Episode length: 49.26 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-408.19 +/- 86.59
Episode length: 48.50 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 82       |
|    time_elapsed    | 731      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-423.19 +/- 63.23
Episode length: 53.38 +/- 20.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.4      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6e-18    |
|    explained_variance   | 0.00628   |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+03  |
|    n_updates            | 699       |
|    policy_gradient_loss | -1.03e-09 |
|    value_loss           | 4.14e+03  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=-407.59 +/- 78.46
Episode length: 49.02 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-400.40 +/- 67.61
Episode length: 50.12 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-400.99 +/- 84.96
Episode length: 50.62 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 83       |
|    time_elapsed    | 739      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-423.20 +/- 67.09
Episode length: 50.84 +/- 19.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-15 |
|    explained_variance   | 0.0157    |
|    learning_rate        | 0.001     |
|    loss                 | 3.07e+03  |
|    n_updates            | 709       |
|    policy_gradient_loss | -5.06e-10 |
|    value_loss           | 5.89e+03  |
---------------------------------------
Eval num_timesteps=170500, episode_reward=-417.79 +/- 69.51
Episode length: 53.82 +/- 18.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-415.39 +/- 69.25
Episode length: 43.34 +/- 12.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-399.19 +/- 82.48
Episode length: 51.20 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-419.59 +/- 74.70
Episode length: 52.58 +/- 22.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 84       |
|    time_elapsed    | 750      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-424.39 +/- 68.56
Episode length: 53.60 +/- 19.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.6      |
|    mean_reward          | -424      |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.49e-17 |
|    explained_variance   | 0.00976   |
|    learning_rate        | 0.001     |
|    loss                 | 2.78e+03  |
|    n_updates            | 719       |
|    policy_gradient_loss | -1.73e-09 |
|    value_loss           | 4.92e+03  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=-436.39 +/- 60.70
Episode length: 50.90 +/- 13.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-424.99 +/- 55.74
Episode length: 52.24 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-399.19 +/- 76.57
Episode length: 48.08 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 85       |
|    time_elapsed    | 759      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-427.39 +/- 59.40
Episode length: 52.72 +/- 20.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.08e-15 |
|    explained_variance   | 0.0213    |
|    learning_rate        | 0.001     |
|    loss                 | 3.18e+03  |
|    n_updates            | 729       |
|    policy_gradient_loss | -1.28e-09 |
|    value_loss           | 5.85e+03  |
---------------------------------------
Eval num_timesteps=175000, episode_reward=-437.59 +/- 51.26
Episode length: 50.36 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-423.19 +/- 52.67
Episode length: 49.72 +/- 16.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-417.79 +/- 79.42
Episode length: 49.32 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 86       |
|    time_elapsed    | 768      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-414.79 +/- 81.42
Episode length: 50.60 +/- 17.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-17 |
|    explained_variance   | 0.00978   |
|    learning_rate        | 0.001     |
|    loss                 | 2.3e+03   |
|    n_updates            | 739       |
|    policy_gradient_loss | -2.95e-09 |
|    value_loss           | 4.43e+03  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=-405.79 +/- 87.78
Episode length: 54.82 +/- 19.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-426.19 +/- 62.55
Episode length: 52.78 +/- 20.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-423.79 +/- 76.32
Episode length: 47.96 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.7     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 87       |
|    time_elapsed    | 777      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-418.99 +/- 69.48
Episode length: 50.56 +/- 16.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.03e-15 |
|    explained_variance   | 0.0171    |
|    learning_rate        | 0.001     |
|    loss                 | 3.02e+03  |
|    n_updates            | 749       |
|    policy_gradient_loss | -4.95e-10 |
|    value_loss           | 6.62e+03  |
---------------------------------------
Eval num_timesteps=179000, episode_reward=-426.80 +/- 59.20
Episode length: 49.84 +/- 15.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-414.79 +/- 73.52
Episode length: 47.86 +/- 16.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-418.39 +/- 71.03
Episode length: 53.72 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.6     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 88       |
|    time_elapsed    | 786      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-410.59 +/- 67.68
Episode length: 50.86 +/- 15.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.52e-17 |
|    explained_variance   | 0.00926   |
|    learning_rate        | 0.001     |
|    loss                 | 1.79e+03  |
|    n_updates            | 759       |
|    policy_gradient_loss | -7.54e-10 |
|    value_loss           | 5.1e+03   |
---------------------------------------
Eval num_timesteps=181000, episode_reward=-434.59 +/- 61.56
Episode length: 54.50 +/- 20.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-409.40 +/- 71.70
Episode length: 52.48 +/- 19.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-399.19 +/- 84.19
Episode length: 47.76 +/- 14.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 89       |
|    time_elapsed    | 795      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-391.99 +/- 84.95
Episode length: 46.46 +/- 14.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.5      |
|    mean_reward          | -392      |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.47e-15 |
|    explained_variance   | 0.0192    |
|    learning_rate        | 0.001     |
|    loss                 | 2.24e+03  |
|    n_updates            | 769       |
|    policy_gradient_loss | -2.2e-09  |
|    value_loss           | 4.82e+03  |
---------------------------------------
Eval num_timesteps=183000, episode_reward=-411.79 +/- 80.19
Episode length: 47.34 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-425.60 +/- 61.48
Episode length: 48.96 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-435.79 +/- 59.10
Episode length: 45.40 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 90       |
|    time_elapsed    | 804      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-429.19 +/- 64.30
Episode length: 50.44 +/- 14.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.23e-17 |
|    explained_variance   | 0.00647   |
|    learning_rate        | 0.001     |
|    loss                 | 3.42e+03  |
|    n_updates            | 779       |
|    policy_gradient_loss | -4.95e-10 |
|    value_loss           | 4.48e+03  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=-430.39 +/- 67.61
Episode length: 54.28 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-429.79 +/- 63.05
Episode length: 45.94 +/- 14.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-421.99 +/- 66.39
Episode length: 55.30 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 91       |
|    time_elapsed    | 813      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-426.79 +/- 64.72
Episode length: 52.66 +/- 18.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-14 |
|    explained_variance   | 0.0158    |
|    learning_rate        | 0.001     |
|    loss                 | 2.32e+03  |
|    n_updates            | 789       |
|    policy_gradient_loss | -7.48e-10 |
|    value_loss           | 5.73e+03  |
---------------------------------------
Eval num_timesteps=187000, episode_reward=-415.39 +/- 73.05
Episode length: 47.72 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-417.19 +/- 67.95
Episode length: 50.48 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-411.79 +/- 57.43
Episode length: 49.10 +/- 20.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 92       |
|    time_elapsed    | 821      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-415.99 +/- 74.52
Episode length: 44.74 +/- 13.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44.7      |
|    mean_reward          | -416      |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.91e-16 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.001     |
|    loss                 | 1.28e+03  |
|    n_updates            | 799       |
|    policy_gradient_loss | -1.53e-09 |
|    value_loss           | 4.52e+03  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=-419.60 +/- 66.81
Episode length: 52.34 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-414.79 +/- 79.89
Episode length: 49.84 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-423.19 +/- 71.26
Episode length: 51.42 +/- 18.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 93       |
|    time_elapsed    | 830      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-433.99 +/- 51.91
Episode length: 49.44 +/- 17.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.4      |
|    mean_reward          | -434      |
| time/                   |           |
|    total_timesteps      | 190500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.18e-14 |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.001     |
|    loss                 | 2.12e+03  |
|    n_updates            | 809       |
|    policy_gradient_loss | 1.27e-09  |
|    value_loss           | 5.32e+03  |
---------------------------------------
Eval num_timesteps=191000, episode_reward=-415.99 +/- 61.26
Episode length: 52.32 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-424.38 +/- 85.89
Episode length: 49.36 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-409.99 +/- 72.26
Episode length: 48.98 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-418.99 +/- 72.52
Episode length: 45.46 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 94       |
|    time_elapsed    | 840      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-424.99 +/- 63.30
Episode length: 47.82 +/- 18.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.8      |
|    mean_reward          | -425      |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.72e-16 |
|    explained_variance   | 0.00823   |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+03  |
|    n_updates            | 819       |
|    policy_gradient_loss | 1.43e-09  |
|    value_loss           | 4.47e+03  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=-413.60 +/- 79.83
Episode length: 54.06 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-387.80 +/- 91.82
Episode length: 48.72 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
New best mean reward!
Eval num_timesteps=194500, episode_reward=-409.40 +/- 70.69
Episode length: 49.08 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 95       |
|    time_elapsed    | 849      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-399.19 +/- 72.71
Episode length: 49.72 +/- 14.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -399      |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.66e-14 |
|    explained_variance   | 0.0203    |
|    learning_rate        | 0.001     |
|    loss                 | 2.07e+03  |
|    n_updates            | 829       |
|    policy_gradient_loss | -2.33e-11 |
|    value_loss           | 5.05e+03  |
---------------------------------------
Eval num_timesteps=195500, episode_reward=-398.59 +/- 79.43
Episode length: 51.12 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-438.20 +/- 48.85
Episode length: 53.30 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-424.99 +/- 67.16
Episode length: 48.70 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 96       |
|    time_elapsed    | 858      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-409.39 +/- 71.20
Episode length: 47.02 +/- 19.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47        |
|    mean_reward          | -409      |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.73e-16 |
|    explained_variance   | 0.0153    |
|    learning_rate        | 0.001     |
|    loss                 | 2.13e+03  |
|    n_updates            | 839       |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 4.36e+03  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=-423.79 +/- 71.45
Episode length: 51.96 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-419.00 +/- 62.95
Episode length: 52.36 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-415.39 +/- 73.54
Episode length: 52.62 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 97       |
|    time_elapsed    | 867      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-417.20 +/- 66.88
Episode length: 54.14 +/- 16.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 54.1     |
|    mean_reward          | -417     |
| time/                   |          |
|    total_timesteps      | 199000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.2e-13 |
|    explained_variance   | 0.0255   |
|    learning_rate        | 0.001    |
|    loss                 | 2.76e+03 |
|    n_updates            | 849      |
|    policy_gradient_loss | 3.64e-10 |
|    value_loss           | 4.99e+03 |
--------------------------------------
Eval num_timesteps=199500, episode_reward=-407.59 +/- 68.67
Episode length: 50.84 +/- 17.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-429.79 +/- 65.01
Episode length: 48.76 +/- 16.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-416.59 +/- 67.15
Episode length: 49.20 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 98       |
|    time_elapsed    | 876      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-418.99 +/- 63.24
Episode length: 49.68 +/- 15.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.57e-16 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.001     |
|    loss                 | 1.79e+03  |
|    n_updates            | 859       |
|    policy_gradient_loss | 8.79e-10  |
|    value_loss           | 4.16e+03  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=-422.00 +/- 73.10
Episode length: 50.62 +/- 21.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-417.19 +/- 70.29
Episode length: 49.92 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-424.99 +/- 72.57
Episode length: 53.12 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 99       |
|    time_elapsed    | 885      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-431.59 +/- 67.08
Episode length: 53.30 +/- 18.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.3      |
|    mean_reward          | -432      |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.63e-31 |
|    explained_variance   | 0.0211    |
|    learning_rate        | 0.001     |
|    loss                 | 2.57e+03  |
|    n_updates            | 869       |
|    policy_gradient_loss | -9.63e-10 |
|    value_loss           | 5.44e+03  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=-409.99 +/- 77.08
Episode length: 47.46 +/- 14.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-400.99 +/- 72.62
Episode length: 45.62 +/- 19.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-421.99 +/- 83.02
Episode length: 47.38 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 100      |
|    time_elapsed    | 894      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-421.39 +/- 76.28
Episode length: 51.64 +/- 19.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.6      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.84e-29 |
|    explained_variance   | 0.0201    |
|    learning_rate        | 0.001     |
|    loss                 | 9.49e+03  |
|    n_updates            | 879       |
|    policy_gradient_loss | -8.16e-09 |
|    value_loss           | 1.72e+04  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=-425.59 +/- 68.97
Episode length: 53.18 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-422.59 +/- 63.85
Episode length: 51.22 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-410.59 +/- 89.85
Episode length: 50.00 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 101      |
|    time_elapsed    | 903      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-426.19 +/- 62.27
Episode length: 49.06 +/- 14.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -426      |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.25e-13 |
|    explained_variance   | 0.0169    |
|    learning_rate        | 0.001     |
|    loss                 | 3.13e+03  |
|    n_updates            | 889       |
|    policy_gradient_loss | 3.84e-10  |
|    value_loss           | 5.08e+03  |
---------------------------------------
Eval num_timesteps=207500, episode_reward=-411.79 +/- 78.37
Episode length: 51.18 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-420.19 +/- 71.22
Episode length: 49.66 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-409.99 +/- 66.01
Episode length: 47.36 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 102      |
|    time_elapsed    | 912      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-411.79 +/- 69.36
Episode length: 53.18 +/- 16.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.35e-15 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 2.89e+03  |
|    n_updates            | 899       |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 4.94e+03  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=-430.99 +/- 65.86
Episode length: 51.88 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-406.99 +/- 78.39
Episode length: 51.92 +/- 21.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-437.00 +/- 63.69
Episode length: 55.34 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 103      |
|    time_elapsed    | 921      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-410.59 +/- 67.68
Episode length: 54.36 +/- 19.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.4      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.38e-13 |
|    explained_variance   | 0.0175    |
|    learning_rate        | 0.001     |
|    loss                 | 2.82e+03  |
|    n_updates            | 909       |
|    policy_gradient_loss | -7.22e-10 |
|    value_loss           | 4.55e+03  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=-420.19 +/- 68.91
Episode length: 54.56 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-398.59 +/- 70.80
Episode length: 52.26 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-426.19 +/- 64.81
Episode length: 50.92 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 104      |
|    time_elapsed    | 930      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-402.19 +/- 84.10
Episode length: 43.76 +/- 13.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 43.8      |
|    mean_reward          | -402      |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.03e-15 |
|    explained_variance   | 0.00836   |
|    learning_rate        | 0.001     |
|    loss                 | 2.75e+03  |
|    n_updates            | 919       |
|    policy_gradient_loss | 1.53e-09  |
|    value_loss           | 4.59e+03  |
---------------------------------------
Eval num_timesteps=213500, episode_reward=-426.20 +/- 66.19
Episode length: 48.56 +/- 14.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-433.39 +/- 65.84
Episode length: 52.88 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-436.99 +/- 55.87
Episode length: 51.64 +/- 18.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-404.60 +/- 77.36
Episode length: 48.46 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 105      |
|    time_elapsed    | 940      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-423.19 +/- 63.23
Episode length: 50.60 +/- 17.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.96e-13 |
|    explained_variance   | 0.0141    |
|    learning_rate        | 0.001     |
|    loss                 | 2.79e+03  |
|    n_updates            | 929       |
|    policy_gradient_loss | -2.04e-10 |
|    value_loss           | 5.33e+03  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=-429.19 +/- 58.12
Episode length: 50.42 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-417.79 +/- 81.23
Episode length: 45.94 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-435.19 +/- 56.49
Episode length: 49.74 +/- 12.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 106      |
|    time_elapsed    | 949      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-417.19 +/- 59.47
Episode length: 50.14 +/- 15.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.73e-15 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 2.87e+03  |
|    n_updates            | 939       |
|    policy_gradient_loss | 1.98e-09  |
|    value_loss           | 5.22e+03  |
---------------------------------------
Eval num_timesteps=218000, episode_reward=-416.59 +/- 81.88
Episode length: 52.06 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-419.59 +/- 76.84
Episode length: 49.30 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-411.18 +/- 72.66
Episode length: 47.50 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 107      |
|    time_elapsed    | 958      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-429.19 +/- 65.96
Episode length: 51.18 +/- 21.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.2      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.62e-13 |
|    explained_variance   | 0.0225    |
|    learning_rate        | 0.001     |
|    loss                 | 2.6e+03   |
|    n_updates            | 949       |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 5.31e+03  |
---------------------------------------
Eval num_timesteps=220000, episode_reward=-408.79 +/- 77.67
Episode length: 47.90 +/- 14.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-409.99 +/- 76.61
Episode length: 49.74 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-420.20 +/- 76.58
Episode length: 52.02 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 108      |
|    time_elapsed    | 967      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-399.20 +/- 67.31
Episode length: 48.68 +/- 15.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -399      |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.53e-15 |
|    explained_variance   | 0.00409   |
|    learning_rate        | 0.001     |
|    loss                 | 1.82e+03  |
|    n_updates            | 959       |
|    policy_gradient_loss | 1.22e-10  |
|    value_loss           | 4.41e+03  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=-415.99 +/- 80.10
Episode length: 45.64 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-407.60 +/- 75.89
Episode length: 46.92 +/- 18.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-409.40 +/- 75.85
Episode length: 52.70 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 109      |
|    time_elapsed    | 975      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-424.40 +/- 71.39
Episode length: 52.08 +/- 16.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.1      |
|    mean_reward          | -424      |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.26e-13 |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.001     |
|    loss                 | 2.13e+03  |
|    n_updates            | 969       |
|    policy_gradient_loss | 5.59e-10  |
|    value_loss           | 4.87e+03  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=-405.79 +/- 69.97
Episode length: 48.80 +/- 20.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-418.39 +/- 65.22
Episode length: 54.46 +/- 20.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-430.39 +/- 67.61
Episode length: 49.72 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 110      |
|    time_elapsed    | 984      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-418.99 +/- 66.57
Episode length: 48.54 +/- 15.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.18e-15 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 1.89e+03  |
|    n_updates            | 979       |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 4.16e+03  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=-420.20 +/- 69.69
Episode length: 55.76 +/- 18.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-416.56 +/- 84.85
Episode length: 48.70 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-409.39 +/- 63.73
Episode length: 48.94 +/- 19.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 111      |
|    time_elapsed    | 993      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-421.39 +/- 70.13
Episode length: 50.88 +/- 19.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-12 |
|    explained_variance   | 0.0241    |
|    learning_rate        | 0.001     |
|    loss                 | 2.71e+03  |
|    n_updates            | 989       |
|    policy_gradient_loss | -1.06e-09 |
|    value_loss           | 5.23e+03  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=-417.79 +/- 51.34
Episode length: 52.00 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-425.00 +/- 74.77
Episode length: 50.56 +/- 13.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-416.60 +/- 60.37
Episode length: 49.30 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 112      |
|    time_elapsed    | 1002     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-405.79 +/- 78.69
Episode length: 47.18 +/- 14.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.2      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-14 |
|    explained_variance   | 0.00773   |
|    learning_rate        | 0.001     |
|    loss                 | 2.55e+03  |
|    n_updates            | 999       |
|    policy_gradient_loss | -1.8e-09  |
|    value_loss           | 4.69e+03  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=-399.19 +/- 79.11
Episode length: 48.96 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-404.00 +/- 86.50
Episode length: 50.58 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-415.39 +/- 61.83
Episode length: 50.96 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 113      |
|    time_elapsed    | 1010     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-433.39 +/- 67.19
Episode length: 50.14 +/- 17.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -433      |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.13e-13 |
|    explained_variance   | 0.0213    |
|    learning_rate        | 0.001     |
|    loss                 | 1.67e+03  |
|    n_updates            | 1009      |
|    policy_gradient_loss | -6.81e-10 |
|    value_loss           | 4.2e+03   |
---------------------------------------
Eval num_timesteps=232000, episode_reward=-412.39 +/- 72.69
Episode length: 47.88 +/- 13.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-412.99 +/- 67.05
Episode length: 46.58 +/- 13.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-407.00 +/- 73.90
Episode length: 54.12 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 114      |
|    time_elapsed    | 1019     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-438.19 +/- 47.73
Episode length: 54.44 +/- 19.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.4      |
|    mean_reward          | -438      |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.86e-15 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.001     |
|    loss                 | 2.03e+03  |
|    n_updates            | 1019      |
|    policy_gradient_loss | 9.92e-10  |
|    value_loss           | 4.48e+03  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=-419.59 +/- 82.49
Episode length: 53.06 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-413.60 +/- 72.00
Episode length: 51.30 +/- 20.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-429.79 +/- 63.33
Episode length: 50.34 +/- 15.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-418.39 +/- 62.97
Episode length: 53.22 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 115      |
|    time_elapsed    | 1030     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-429.20 +/- 60.55
Episode length: 55.58 +/- 15.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 55.6      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.71e-13 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.001     |
|    loss                 | 2.63e+03  |
|    n_updates            | 1029      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 5.22e+03  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=-420.19 +/- 59.67
Episode length: 46.30 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-420.79 +/- 66.48
Episode length: 48.02 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-415.99 +/- 59.77
Episode length: 50.68 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53       |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 116      |
|    time_elapsed    | 1039     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-414.79 +/- 80.53
Episode length: 49.88 +/- 18.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.9      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.9e-15  |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.001     |
|    loss                 | 2.31e+03  |
|    n_updates            | 1039      |
|    policy_gradient_loss | -4.92e-10 |
|    value_loss           | 4.62e+03  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=-423.19 +/- 72.51
Episode length: 46.68 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-418.39 +/- 56.66
Episode length: 50.24 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-415.39 +/- 67.94
Episode length: 50.92 +/- 20.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 117      |
|    time_elapsed    | 1048     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-416.59 +/- 67.42
Episode length: 50.46 +/- 17.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.02e-12 |
|    explained_variance   | 0.0225    |
|    learning_rate        | 0.001     |
|    loss                 | 2.19e+03  |
|    n_updates            | 1049      |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 4.89e+03  |
---------------------------------------
Eval num_timesteps=240500, episode_reward=-412.99 +/- 68.12
Episode length: 49.54 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-422.59 +/- 63.29
Episode length: 49.48 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-409.99 +/- 73.00
Episode length: 46.74 +/- 13.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.2     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 118      |
|    time_elapsed    | 1056     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-414.79 +/- 81.86
Episode length: 52.20 +/- 18.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.73e-14 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.001     |
|    loss                 | 2.23e+03  |
|    n_updates            | 1059      |
|    policy_gradient_loss | 1.94e-09  |
|    value_loss           | 4.98e+03  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=-396.19 +/- 69.38
Episode length: 47.98 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-417.19 +/- 82.10
Episode length: 49.96 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-414.79 +/- 75.70
Episode length: 48.20 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.5     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 119      |
|    time_elapsed    | 1065     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-424.99 +/- 57.33
Episode length: 51.76 +/- 18.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.8      |
|    mean_reward          | -425      |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-12 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.001     |
|    loss                 | 1.9e+03   |
|    n_updates            | 1069      |
|    policy_gradient_loss | -7.97e-10 |
|    value_loss           | 4.42e+03  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=-445.99 +/- 47.72
Episode length: 56.78 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | -446     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-402.20 +/- 76.01
Episode length: 50.30 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-404.59 +/- 77.60
Episode length: 50.34 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 120      |
|    time_elapsed    | 1074     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-415.39 +/- 70.28
Episode length: 48.22 +/- 14.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.2      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.87e-14 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.001     |
|    loss                 | 2.45e+03  |
|    n_updates            | 1079      |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 4.05e+03  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=-409.99 +/- 81.40
Episode length: 49.94 +/- 13.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-416.59 +/- 63.57
Episode length: 47.46 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-408.19 +/- 70.56
Episode length: 52.86 +/- 22.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 121      |
|    time_elapsed    | 1083     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-416.59 +/- 72.06
Episode length: 51.34 +/- 16.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.3      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.65e-12 |
|    explained_variance   | 0.021     |
|    learning_rate        | 0.001     |
|    loss                 | 2.31e+03  |
|    n_updates            | 1089      |
|    policy_gradient_loss | -2.21e-10 |
|    value_loss           | 4.49e+03  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=-405.19 +/- 76.05
Episode length: 49.60 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-411.80 +/- 74.36
Episode length: 52.18 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-426.79 +/- 51.74
Episode length: 52.58 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.5     |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 122      |
|    time_elapsed    | 1092     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-412.99 +/- 74.19
Episode length: 50.74 +/- 17.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -413      |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.92e-14 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.001     |
|    loss                 | 2.58e+03  |
|    n_updates            | 1099      |
|    policy_gradient_loss | -3e-10    |
|    value_loss           | 4.81e+03  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=-413.60 +/- 68.15
Episode length: 51.66 +/- 14.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-406.39 +/- 92.83
Episode length: 48.48 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-417.19 +/- 67.41
Episode length: 47.76 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 123      |
|    time_elapsed    | 1100     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-402.80 +/- 76.83
Episode length: 51.08 +/- 16.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -403      |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-11 |
|    explained_variance   | 0.0134    |
|    learning_rate        | 0.001     |
|    loss                 | 1.83e+03  |
|    n_updates            | 1109      |
|    policy_gradient_loss | 7.71e-10  |
|    value_loss           | 4.1e+03   |
---------------------------------------
Eval num_timesteps=252500, episode_reward=-415.99 +/- 71.06
Episode length: 46.50 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-427.99 +/- 69.62
Episode length: 50.44 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-413.00 +/- 60.27
Episode length: 50.64 +/- 15.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 124      |
|    time_elapsed    | 1109     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-417.19 +/- 81.46
Episode length: 53.44 +/- 18.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.4      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-13 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+03  |
|    n_updates            | 1119      |
|    policy_gradient_loss | 2.08e-09  |
|    value_loss           | 4.28e+03  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=-420.19 +/- 71.47
Episode length: 51.40 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-429.19 +/- 62.31
Episode length: 51.60 +/- 19.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-414.20 +/- 79.85
Episode length: 50.36 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-408.77 +/- 87.58
Episode length: 49.42 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 125      |
|    time_elapsed    | 1120     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-406.40 +/- 86.61
Episode length: 52.76 +/- 20.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-11 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.001     |
|    loss                 | 2.11e+03  |
|    n_updates            | 1129      |
|    policy_gradient_loss | -7.55e-10 |
|    value_loss           | 4.28e+03  |
---------------------------------------
Eval num_timesteps=257000, episode_reward=-417.79 +/- 71.05
Episode length: 48.28 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-394.99 +/- 81.67
Episode length: 48.34 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-402.19 +/- 85.17
Episode length: 50.70 +/- 20.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 126      |
|    time_elapsed    | 1128     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-435.79 +/- 54.67
Episode length: 51.50 +/- 15.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -436      |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.75e-13 |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+03  |
|    n_updates            | 1139      |
|    policy_gradient_loss | 1.08e-09  |
|    value_loss           | 4.08e+03  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=-423.19 +/- 58.19
Episode length: 54.36 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-419.60 +/- 77.77
Episode length: 51.62 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-424.99 +/- 66.89
Episode length: 50.54 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 127      |
|    time_elapsed    | 1137     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-414.19 +/- 51.64
Episode length: 50.38 +/- 15.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.47e-24 |
|    explained_variance   | 0.0258    |
|    learning_rate        | 0.001     |
|    loss                 | 2.23e+03  |
|    n_updates            | 1149      |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 4.49e+03  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=-410.59 +/- 72.56
Episode length: 51.98 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-414.79 +/- 59.14
Episode length: 51.52 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-418.39 +/- 68.72
Episode length: 53.38 +/- 17.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 128      |
|    time_elapsed    | 1146     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-409.39 +/- 68.62
Episode length: 49.72 +/- 16.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -409      |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.76e-21 |
|    explained_variance   | 0.0199    |
|    learning_rate        | 0.001     |
|    loss                 | 5.23e+03  |
|    n_updates            | 1159      |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 1.01e+04  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=-405.79 +/- 82.92
Episode length: 47.88 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-406.40 +/- 78.77
Episode length: 48.26 +/- 14.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-431.59 +/- 62.93
Episode length: 50.00 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 129      |
|    time_elapsed    | 1155     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-422.59 +/- 71.56
Episode length: 52.90 +/- 20.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.9      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.61e-08 |
|    explained_variance   | 0.022     |
|    learning_rate        | 0.001     |
|    loss                 | 2.14e+03  |
|    n_updates            | 1169      |
|    policy_gradient_loss | 1.66e-10  |
|    value_loss           | 4.73e+03  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=-416.59 +/- 73.05
Episode length: 50.92 +/- 14.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-426.19 +/- 67.53
Episode length: 44.82 +/- 13.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-414.79 +/- 74.50
Episode length: 50.06 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 130      |
|    time_elapsed    | 1164     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-409.99 +/- 78.93
Episode length: 51.04 +/- 19.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -410      |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.35e-10 |
|    explained_variance   | 0.0126    |
|    learning_rate        | 0.001     |
|    loss                 | 2.93e+03  |
|    n_updates            | 1179      |
|    policy_gradient_loss | 2.62e-09  |
|    value_loss           | 4.92e+03  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=-425.59 +/- 58.48
Episode length: 52.18 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-415.39 +/- 65.79
Episode length: 45.72 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-412.99 +/- 76.58
Episode length: 46.76 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 131      |
|    time_elapsed    | 1172     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-417.19 +/- 65.24
Episode length: 50.66 +/- 17.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.2e-22  |
|    explained_variance   | 0.0213    |
|    learning_rate        | 0.001     |
|    loss                 | 1.87e+03  |
|    n_updates            | 1189      |
|    policy_gradient_loss | -7.83e-10 |
|    value_loss           | 4.84e+03  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=-426.20 +/- 62.26
Episode length: 50.24 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-412.39 +/- 70.93
Episode length: 47.74 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-415.39 +/- 85.32
Episode length: 50.28 +/- 16.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 132      |
|    time_elapsed    | 1181     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-429.19 +/- 64.02
Episode length: 49.70 +/- 16.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.37e-19 |
|    explained_variance   | 0.0385    |
|    learning_rate        | 0.001     |
|    loss                 | 4.26e+03  |
|    n_updates            | 1199      |
|    policy_gradient_loss | -8.91e-10 |
|    value_loss           | 9.59e+03  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=-418.40 +/- 67.92
Episode length: 48.02 +/- 15.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-423.19 +/- 67.63
Episode length: 49.14 +/- 15.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-416.60 +/- 62.14
Episode length: 50.18 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 133      |
|    time_elapsed    | 1190     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-417.20 +/- 73.54
Episode length: 51.38 +/- 14.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.3e-21  |
|    explained_variance   | 0.0342    |
|    learning_rate        | 0.001     |
|    loss                 | 3.08e+03  |
|    n_updates            | 1209      |
|    policy_gradient_loss | -1.26e-09 |
|    value_loss           | 3.94e+03  |
---------------------------------------
Eval num_timesteps=273000, episode_reward=-430.99 +/- 50.02
Episode length: 51.86 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-411.19 +/- 79.93
Episode length: 48.92 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-419.59 +/- 67.62
Episode length: 53.00 +/- 18.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 134      |
|    time_elapsed    | 1199     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-430.99 +/- 66.95
Episode length: 50.66 +/- 14.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -431      |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.72e-19 |
|    explained_variance   | 0.0245    |
|    learning_rate        | 0.001     |
|    loss                 | 3.43e+03  |
|    n_updates            | 1219      |
|    policy_gradient_loss | -8.56e-10 |
|    value_loss           | 8.36e+03  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=-418.99 +/- 71.02
Episode length: 46.98 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-416.60 +/- 60.37
Episode length: 51.12 +/- 15.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-424.39 +/- 64.22
Episode length: 51.16 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 135      |
|    time_elapsed    | 1208     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-417.79 +/- 71.55
Episode length: 51.58 +/- 20.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.6      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.64e-21 |
|    explained_variance   | 0.0252    |
|    learning_rate        | 0.001     |
|    loss                 | 2.78e+03  |
|    n_updates            | 1229      |
|    policy_gradient_loss | -7.63e-10 |
|    value_loss           | 4.87e+03  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=-414.18 +/- 87.84
Episode length: 49.22 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-420.79 +/- 74.88
Episode length: 49.92 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-405.79 +/- 68.68
Episode length: 45.98 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-404.59 +/- 81.66
Episode length: 47.94 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 136      |
|    time_elapsed    | 1218     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-430.99 +/- 53.16
Episode length: 50.40 +/- 16.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -431      |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-16 |
|    explained_variance   | 0.0384    |
|    learning_rate        | 0.001     |
|    loss                 | 3.61e+03  |
|    n_updates            | 1239      |
|    policy_gradient_loss | 4.77e-09  |
|    value_loss           | 7.12e+03  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=-432.79 +/- 67.87
Episode length: 50.00 +/- 14.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-421.39 +/- 63.39
Episode length: 47.56 +/- 14.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-419.59 +/- 62.07
Episode length: 51.32 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 137      |
|    time_elapsed    | 1227     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-420.20 +/- 77.05
Episode length: 48.50 +/- 18.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-18 |
|    explained_variance   | 0.0278    |
|    learning_rate        | 0.001     |
|    loss                 | 1.83e+03  |
|    n_updates            | 1249      |
|    policy_gradient_loss | 2.04e-11  |
|    value_loss           | 4.12e+03  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=-415.99 +/- 74.76
Episode length: 52.42 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-421.39 +/- 61.66
Episode length: 51.30 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-430.39 +/- 82.70
Episode length: 50.46 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 138      |
|    time_elapsed    | 1236     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-433.39 +/- 62.18
Episode length: 52.68 +/- 19.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -433      |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-17 |
|    explained_variance   | 0.04      |
|    learning_rate        | 0.001     |
|    loss                 | 3.06e+03  |
|    n_updates            | 1259      |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 7.46e+03  |
---------------------------------------
Eval num_timesteps=283500, episode_reward=-413.59 +/- 78.69
Episode length: 51.38 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-423.19 +/- 66.56
Episode length: 53.10 +/- 18.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-434.59 +/- 56.05
Episode length: 46.60 +/- 20.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 139      |
|    time_elapsed    | 1244     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-399.79 +/- 61.01
Episode length: 47.84 +/- 15.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.8      |
|    mean_reward          | -400      |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.67e-08 |
|    explained_variance   | 0.0224    |
|    learning_rate        | 0.001     |
|    loss                 | 1.74e+03  |
|    n_updates            | 1269      |
|    policy_gradient_loss | 3.03e-10  |
|    value_loss           | 4.57e+03  |
---------------------------------------
Eval num_timesteps=285500, episode_reward=-409.99 +/- 72.51
Episode length: 50.94 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-409.39 +/- 79.33
Episode length: 51.78 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-398.60 +/- 90.85
Episode length: 52.50 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 140      |
|    time_elapsed    | 1253     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-419.59 +/- 67.62
Episode length: 52.24 +/- 18.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.58e-10 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.001     |
|    loss                 | 2.15e+03  |
|    n_updates            | 1279      |
|    policy_gradient_loss | -1.46e-09 |
|    value_loss           | 4.6e+03   |
---------------------------------------
Eval num_timesteps=287500, episode_reward=-412.96 +/- 86.45
Episode length: 51.60 +/- 16.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-432.19 +/- 54.83
Episode length: 53.78 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-430.99 +/- 59.85
Episode length: 55.22 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 141      |
|    time_elapsed    | 1263     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-432.79 +/- 66.80
Episode length: 52.44 +/- 19.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.4      |
|    mean_reward          | -433      |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.27e-20 |
|    explained_variance   | 0.0303    |
|    learning_rate        | 0.001     |
|    loss                 | 1.99e+03  |
|    n_updates            | 1289      |
|    policy_gradient_loss | 2.62e-11  |
|    value_loss           | 3.82e+03  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=-413.59 +/- 84.24
Episode length: 48.22 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-428.59 +/- 57.00
Episode length: 51.56 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-416.60 +/- 81.22
Episode length: 52.72 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 142      |
|    time_elapsed    | 1272     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-411.19 +/- 61.90
Episode length: 50.28 +/- 18.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.44e-16 |
|    explained_variance   | 0.0472    |
|    learning_rate        | 0.001     |
|    loss                 | 4.68e+03  |
|    n_updates            | 1299      |
|    policy_gradient_loss | -7.71e-10 |
|    value_loss           | 9.11e+03  |
---------------------------------------
Eval num_timesteps=291500, episode_reward=-412.39 +/- 79.09
Episode length: 48.36 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-416.59 +/- 74.03
Episode length: 50.82 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-415.99 +/- 77.59
Episode length: 56.16 +/- 20.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 143      |
|    time_elapsed    | 1281     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-407.59 +/- 78.00
Episode length: 51.06 +/- 15.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.04e-18 |
|    explained_variance   | 0.0301    |
|    learning_rate        | 0.001     |
|    loss                 | 2.12e+03  |
|    n_updates            | 1309      |
|    policy_gradient_loss | 2.63e-09  |
|    value_loss           | 3.84e+03  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=-421.99 +/- 66.12
Episode length: 53.22 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-403.99 +/- 71.96
Episode length: 51.44 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-413.59 +/- 85.28
Episode length: 46.88 +/- 15.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 144      |
|    time_elapsed    | 1290     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-409.39 +/- 72.70
Episode length: 52.48 +/- 20.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 52.5     |
|    mean_reward          | -409     |
| time/                   |          |
|    total_timesteps      | 295000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.1e-16 |
|    explained_variance   | 0.0402   |
|    learning_rate        | 0.001    |
|    loss                 | 3.72e+03 |
|    n_updates            | 1319     |
|    policy_gradient_loss | 5.24e-10 |
|    value_loss           | 7.5e+03  |
--------------------------------------
Eval num_timesteps=295500, episode_reward=-409.99 +/- 82.50
Episode length: 46.60 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-414.80 +/- 69.50
Episode length: 50.74 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-429.19 +/- 70.44
Episode length: 52.70 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 145      |
|    time_elapsed    | 1298     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-428.60 +/- 54.41
Episode length: 56.02 +/- 18.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 56        |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.33e-18 |
|    explained_variance   | 0.031     |
|    learning_rate        | 0.001     |
|    loss                 | 2.01e+03  |
|    n_updates            | 1329      |
|    policy_gradient_loss | 6.9e-10   |
|    value_loss           | 3.72e+03  |
---------------------------------------
Eval num_timesteps=297500, episode_reward=-426.79 +/- 62.46
Episode length: 49.12 +/- 15.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-403.39 +/- 79.47
Episode length: 45.92 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-432.20 +/- 57.70
Episode length: 49.92 +/- 14.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-422.59 +/- 69.78
Episode length: 49.38 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 146      |
|    time_elapsed    | 1309     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-428.59 +/- 57.32
Episode length: 51.40 +/- 17.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.83e-16 |
|    explained_variance   | 0.0403    |
|    learning_rate        | 0.001     |
|    loss                 | 2.88e+03  |
|    n_updates            | 1339      |
|    policy_gradient_loss | 1.54e-09  |
|    value_loss           | 6.95e+03  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=-418.99 +/- 73.51
Episode length: 52.60 +/- 19.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-406.99 +/- 75.11
Episode length: 50.84 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-404.60 +/- 60.08
Episode length: 51.72 +/- 16.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 147      |
|    time_elapsed    | 1318     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=-408.19 +/- 72.57
Episode length: 45.86 +/- 17.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.9      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.21e-18 |
|    explained_variance   | 0.0314    |
|    learning_rate        | 0.001     |
|    loss                 | 2.54e+03  |
|    n_updates            | 1349      |
|    policy_gradient_loss | 1.78e-10  |
|    value_loss           | 4.31e+03  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=-420.79 +/- 75.12
Episode length: 46.74 +/- 14.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-418.99 +/- 64.09
Episode length: 49.80 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-400.36 +/- 91.12
Episode length: 48.30 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 148      |
|    time_elapsed    | 1326     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=-427.39 +/- 62.65
Episode length: 51.02 +/- 19.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 303500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-14 |
|    explained_variance   | 0.0481    |
|    learning_rate        | 0.001     |
|    loss                 | 3.83e+03  |
|    n_updates            | 1359      |
|    policy_gradient_loss | 1.11e-09  |
|    value_loss           | 6.94e+03  |
---------------------------------------
Eval num_timesteps=304000, episode_reward=-422.59 +/- 69.78
Episode length: 52.98 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-416.59 +/- 66.34
Episode length: 48.52 +/- 13.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-409.39 +/- 76.56
Episode length: 48.18 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 149      |
|    time_elapsed    | 1335     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=-414.19 +/- 64.37
Episode length: 45.82 +/- 13.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.8      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.69e-17 |
|    explained_variance   | 0.0444    |
|    learning_rate        | 0.001     |
|    loss                 | 2.36e+03  |
|    n_updates            | 1369      |
|    policy_gradient_loss | -2.44e-09 |
|    value_loss           | 4.26e+03  |
---------------------------------------
Eval num_timesteps=306000, episode_reward=-415.40 +/- 59.16
Episode length: 50.72 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-411.19 +/- 66.12
Episode length: 51.18 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-400.99 +/- 76.48
Episode length: 51.30 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 150      |
|    time_elapsed    | 1344     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=-423.79 +/- 63.73
Episode length: 52.42 +/- 17.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.4      |
|    mean_reward          | -424      |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.74e-15 |
|    explained_variance   | 0.046     |
|    learning_rate        | 0.001     |
|    loss                 | 2.94e+03  |
|    n_updates            | 1379      |
|    policy_gradient_loss | -1.58e-09 |
|    value_loss           | 5.33e+03  |
---------------------------------------
Eval num_timesteps=308000, episode_reward=-411.79 +/- 69.35
Episode length: 49.54 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-414.19 +/- 78.71
Episode length: 52.62 +/- 20.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-423.79 +/- 65.68
Episode length: 50.16 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.2     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 151      |
|    time_elapsed    | 1353     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=-411.19 +/- 85.58
Episode length: 48.08 +/- 16.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.1      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.37e-17 |
|    explained_variance   | 0.0229    |
|    learning_rate        | 0.001     |
|    loss                 | 1.73e+03  |
|    n_updates            | 1389      |
|    policy_gradient_loss | 2.02e-09  |
|    value_loss           | 4.04e+03  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=-423.19 +/- 67.09
Episode length: 50.02 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-422.60 +/- 67.68
Episode length: 50.74 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-417.20 +/- 71.05
Episode length: 51.10 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 152      |
|    time_elapsed    | 1362     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=-420.80 +/- 67.29
Episode length: 53.92 +/- 15.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.9      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 311500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.35e-14 |
|    explained_variance   | 0.0551    |
|    learning_rate        | 0.001     |
|    loss                 | 3.04e+03  |
|    n_updates            | 1399      |
|    policy_gradient_loss | -4.34e-10 |
|    value_loss           | 5.5e+03   |
---------------------------------------
Eval num_timesteps=312000, episode_reward=-400.99 +/- 63.92
Episode length: 45.76 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-411.19 +/- 70.60
Episode length: 52.12 +/- 17.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-424.99 +/- 67.43
Episode length: 50.16 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 153      |
|    time_elapsed    | 1371     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=-400.99 +/- 91.49
Episode length: 51.82 +/- 18.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.8      |
|    mean_reward          | -401      |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.29e-16 |
|    explained_variance   | 0.0421    |
|    learning_rate        | 0.001     |
|    loss                 | 1.44e+03  |
|    n_updates            | 1409      |
|    policy_gradient_loss | 7.74e-10  |
|    value_loss           | 3.34e+03  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=-419.59 +/- 73.24
Episode length: 46.60 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-416.59 +/- 64.97
Episode length: 49.96 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-421.39 +/- 74.61
Episode length: 49.92 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 154      |
|    time_elapsed    | 1380     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=-414.19 +/- 84.23
Episode length: 51.66 +/- 18.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.72e-14 |
|    explained_variance   | 0.0545    |
|    learning_rate        | 0.001     |
|    loss                 | 3.13e+03  |
|    n_updates            | 1419      |
|    policy_gradient_loss | -1.66e-10 |
|    value_loss           | 5.5e+03   |
---------------------------------------
Eval num_timesteps=316000, episode_reward=-410.59 +/- 71.56
Episode length: 48.78 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-404.59 +/- 73.06
Episode length: 53.00 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-435.80 +/- 69.72
Episode length: 54.46 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -432     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 155      |
|    time_elapsed    | 1389     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=-420.20 +/- 58.75
Episode length: 52.78 +/- 19.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.63e-16 |
|    explained_variance   | 0.04      |
|    learning_rate        | 0.001     |
|    loss                 | 1.99e+03  |
|    n_updates            | 1429      |
|    policy_gradient_loss | 2.92e-09  |
|    value_loss           | 4.44e+03  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=-403.39 +/- 79.69
Episode length: 46.20 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-433.39 +/- 51.76
Episode length: 48.20 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-421.39 +/- 67.51
Episode length: 47.26 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 156      |
|    time_elapsed    | 1397     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=-419.59 +/- 61.19
Episode length: 49.76 +/- 13.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.54e-13 |
|    explained_variance   | 0.0678    |
|    learning_rate        | 0.001     |
|    loss                 | 2.69e+03  |
|    n_updates            | 1439      |
|    policy_gradient_loss | 2.21e-09  |
|    value_loss           | 5.03e+03  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=-413.58 +/- 85.98
Episode length: 49.92 +/- 15.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=-416.00 +/- 52.39
Episode length: 49.84 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-434.00 +/- 65.13
Episode length: 51.52 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=-429.79 +/- 53.80
Episode length: 53.24 +/- 19.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 157      |
|    time_elapsed    | 1408     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=-406.39 +/- 73.08
Episode length: 47.74 +/- 19.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.7      |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.44e-15 |
|    explained_variance   | 0.0513    |
|    learning_rate        | 0.001     |
|    loss                 | 2.26e+03  |
|    n_updates            | 1449      |
|    policy_gradient_loss | -1.19e-10 |
|    value_loss           | 4.01e+03  |
---------------------------------------
Eval num_timesteps=322500, episode_reward=-408.79 +/- 64.50
Episode length: 49.66 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=-430.40 +/- 55.95
Episode length: 52.06 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=-430.39 +/- 63.49
Episode length: 51.46 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 158      |
|    time_elapsed    | 1417     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=-411.79 +/- 73.88
Episode length: 50.04 +/- 17.20
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50       |
|    mean_reward          | -412     |
| time/                   |          |
|    total_timesteps      | 324000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5e-13   |
|    explained_variance   | 0.0724   |
|    learning_rate        | 0.001    |
|    loss                 | 2.35e+03 |
|    n_updates            | 1459     |
|    policy_gradient_loss | 1.46e-10 |
|    value_loss           | 4.88e+03 |
--------------------------------------
Eval num_timesteps=324500, episode_reward=-424.39 +/- 68.82
Episode length: 48.96 +/- 14.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-404.59 +/- 55.40
Episode length: 47.54 +/- 13.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=-409.99 +/- 84.01
Episode length: 48.68 +/- 20.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 159      |
|    time_elapsed    | 1425     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=-426.79 +/- 60.70
Episode length: 51.68 +/- 17.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.27e-15 |
|    explained_variance   | 0.0486    |
|    learning_rate        | 0.001     |
|    loss                 | 1.72e+03  |
|    n_updates            | 1469      |
|    policy_gradient_loss | 6.4e-11   |
|    value_loss           | 3.52e+03  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=-402.80 +/- 77.76
Episode length: 53.34 +/- 20.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=-391.96 +/- 93.55
Episode length: 51.64 +/- 22.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=-410.59 +/- 86.38
Episode length: 46.90 +/- 12.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 160      |
|    time_elapsed    | 1434     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=-421.39 +/- 66.98
Episode length: 51.90 +/- 19.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.9      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.45e-12 |
|    explained_variance   | 0.0697    |
|    learning_rate        | 0.001     |
|    loss                 | 2.55e+03  |
|    n_updates            | 1479      |
|    policy_gradient_loss | -1.02e-09 |
|    value_loss           | 4.7e+03   |
---------------------------------------
Eval num_timesteps=328500, episode_reward=-404.00 +/- 78.88
Episode length: 51.76 +/- 18.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-414.80 +/- 75.93
Episode length: 53.40 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=-417.19 +/- 66.07
Episode length: 49.22 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 161      |
|    time_elapsed    | 1443     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-428.00 +/- 46.72
Episode length: 58.24 +/- 18.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 58.2      |
|    mean_reward          | -428      |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-14 |
|    explained_variance   | 0.0511    |
|    learning_rate        | 0.001     |
|    loss                 | 991       |
|    n_updates            | 1489      |
|    policy_gradient_loss | -9.02e-10 |
|    value_loss           | 3.77e+03  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=-388.36 +/- 100.01
Episode length: 46.06 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=-418.99 +/- 60.62
Episode length: 45.74 +/- 12.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=-409.39 +/- 91.16
Episode length: 44.60 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 162      |
|    time_elapsed    | 1452     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=-426.80 +/- 61.29
Episode length: 51.68 +/- 16.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.67e-12 |
|    explained_variance   | 0.0755    |
|    learning_rate        | 0.001     |
|    loss                 | 2.86e+03  |
|    n_updates            | 1499      |
|    policy_gradient_loss | -5.18e-10 |
|    value_loss           | 5.01e+03  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=-412.99 +/- 74.19
Episode length: 48.46 +/- 14.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-424.39 +/- 72.39
Episode length: 50.20 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=-409.39 +/- 65.40
Episode length: 52.22 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | -427     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 163      |
|    time_elapsed    | 1461     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-391.39 +/- 73.64
Episode length: 47.66 +/- 15.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47.7     |
|    mean_reward          | -391     |
| time/                   |          |
|    total_timesteps      | 334000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5e-14   |
|    explained_variance   | 0.0613   |
|    learning_rate        | 0.001    |
|    loss                 | 2.44e+03 |
|    n_updates            | 1509     |
|    policy_gradient_loss | -1.5e-09 |
|    value_loss           | 3.92e+03 |
--------------------------------------
Eval num_timesteps=334500, episode_reward=-421.99 +/- 75.76
Episode length: 48.90 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=-419.59 +/- 59.40
Episode length: 52.84 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=-421.39 +/- 68.04
Episode length: 50.78 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 164      |
|    time_elapsed    | 1469     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=-417.79 +/- 66.06
Episode length: 50.42 +/- 16.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.24e-12 |
|    explained_variance   | 0.0849    |
|    learning_rate        | 0.001     |
|    loss                 | 2.26e+03  |
|    n_updates            | 1519      |
|    policy_gradient_loss | 2.48e-09  |
|    value_loss           | 4.71e+03  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=-422.60 +/- 56.68
Episode length: 51.72 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-406.40 +/- 77.39
Episode length: 51.22 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=-408.19 +/- 76.44
Episode length: 50.06 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 165      |
|    time_elapsed    | 1478     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=-441.80 +/- 55.19
Episode length: 52.80 +/- 14.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -442      |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.63e-14 |
|    explained_variance   | 0.0736    |
|    learning_rate        | 0.001     |
|    loss                 | 2.12e+03  |
|    n_updates            | 1529      |
|    policy_gradient_loss | -2.57e-09 |
|    value_loss           | 4.41e+03  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=-415.40 +/- 62.13
Episode length: 48.98 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=-421.39 +/- 65.62
Episode length: 50.28 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=-425.59 +/- 62.93
Episode length: 50.68 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 166      |
|    time_elapsed    | 1487     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-439.39 +/- 58.42
Episode length: 48.46 +/- 15.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -439      |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-11 |
|    explained_variance   | 0.106     |
|    learning_rate        | 0.001     |
|    loss                 | 2.56e+03  |
|    n_updates            | 1539      |
|    policy_gradient_loss | -1.15e-09 |
|    value_loss           | 4.78e+03  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=-420.19 +/- 69.94
Episode length: 50.40 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-397.99 +/- 78.84
Episode length: 49.66 +/- 17.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=-427.40 +/- 64.63
Episode length: 50.32 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-406.39 +/- 76.45
Episode length: 51.42 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 167      |
|    time_elapsed    | 1498     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=-428.59 +/- 55.40
Episode length: 48.22 +/- 15.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.2      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 342500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-13 |
|    explained_variance   | 0.0735    |
|    learning_rate        | 0.001     |
|    loss                 | 2.18e+03  |
|    n_updates            | 1549      |
|    policy_gradient_loss | -1.47e-09 |
|    value_loss           | 3.97e+03  |
---------------------------------------
Eval num_timesteps=343000, episode_reward=-413.59 +/- 73.73
Episode length: 45.54 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=-414.20 +/- 83.59
Episode length: 49.98 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-425.59 +/- 68.15
Episode length: 47.06 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 168      |
|    time_elapsed    | 1506     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=-409.39 +/- 64.29
Episode length: 49.74 +/- 15.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -409      |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.93e-11 |
|    explained_variance   | 0.104     |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+03  |
|    n_updates            | 1559      |
|    policy_gradient_loss | -2.53e-10 |
|    value_loss           | 4e+03     |
---------------------------------------
Eval num_timesteps=345000, episode_reward=-399.80 +/- 76.70
Episode length: 52.54 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=-405.79 +/- 85.07
Episode length: 48.86 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=-405.19 +/- 66.99
Episode length: 46.40 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 169      |
|    time_elapsed    | 1515     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=-420.19 +/- 63.75
Episode length: 48.44 +/- 19.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -420      |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.06e-13 |
|    explained_variance   | 0.0784    |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+03  |
|    n_updates            | 1569      |
|    policy_gradient_loss | -3.78e-10 |
|    value_loss           | 3.64e+03  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=-423.79 +/- 55.58
Episode length: 49.86 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-421.39 +/- 62.24
Episode length: 50.14 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-414.19 +/- 66.30
Episode length: 46.48 +/- 12.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 170      |
|    time_elapsed    | 1524     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=-430.39 +/- 55.95
Episode length: 53.46 +/- 20.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.5      |
|    mean_reward          | -430      |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.51e-10 |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.001     |
|    loss                 | 2.33e+03  |
|    n_updates            | 1579      |
|    policy_gradient_loss | 1.74e-09  |
|    value_loss           | 4.49e+03  |
---------------------------------------
Eval num_timesteps=349000, episode_reward=-408.79 +/- 73.62
Episode length: 49.02 +/- 19.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=-406.99 +/- 66.19
Episode length: 48.88 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-415.39 +/- 73.54
Episode length: 50.14 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 171      |
|    time_elapsed    | 1533     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=-418.40 +/- 62.69
Episode length: 54.20 +/- 18.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.2      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.47e-12 |
|    explained_variance   | 0.082     |
|    learning_rate        | 0.001     |
|    loss                 | 1.87e+03  |
|    n_updates            | 1589      |
|    policy_gradient_loss | -4.07e-10 |
|    value_loss           | 3.36e+03  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=-405.19 +/- 73.64
Episode length: 48.58 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-414.80 +/- 68.19
Episode length: 50.42 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-415.39 +/- 81.66
Episode length: 48.34 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 172      |
|    time_elapsed    | 1542     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=-423.19 +/- 71.00
Episode length: 50.54 +/- 21.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -423      |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.99e-10 |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.001     |
|    loss                 | 2.77e+03  |
|    n_updates            | 1599      |
|    policy_gradient_loss | 4.8e-10   |
|    value_loss           | 5.32e+03  |
---------------------------------------
Eval num_timesteps=353000, episode_reward=-411.79 +/- 70.64
Episode length: 48.94 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=-407.59 +/- 78.92
Episode length: 48.84 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-403.99 +/- 78.88
Episode length: 46.82 +/- 19.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 173      |
|    time_elapsed    | 1551     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=-421.39 +/- 59.58
Episode length: 50.84 +/- 15.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -421      |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.07e-12 |
|    explained_variance   | 0.111     |
|    learning_rate        | 0.001     |
|    loss                 | 1.89e+03  |
|    n_updates            | 1609      |
|    policy_gradient_loss | 4.19e-10  |
|    value_loss           | 3.33e+03  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=-414.19 +/- 67.11
Episode length: 47.82 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=-418.39 +/- 72.54
Episode length: 51.78 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=-400.99 +/- 64.48
Episode length: 47.10 +/- 14.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 174      |
|    time_elapsed    | 1560     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=-411.20 +/- 64.74
Episode length: 50.76 +/- 16.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-09 |
|    explained_variance   | 0.162     |
|    learning_rate        | 0.001     |
|    loss                 | 1.95e+03  |
|    n_updates            | 1619      |
|    policy_gradient_loss | 6.66e-10  |
|    value_loss           | 4.32e+03  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=-421.39 +/- 65.89
Episode length: 50.92 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-411.19 +/- 57.06
Episode length: 52.64 +/- 20.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=-417.19 +/- 66.34
Episode length: 52.16 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 175      |
|    time_elapsed    | 1569     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=-430.40 +/- 70.73
Episode length: 53.86 +/- 16.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.9      |
|    mean_reward          | -430      |
| time/                   |           |
|    total_timesteps      | 358500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.48e-11 |
|    explained_variance   | 0.129     |
|    learning_rate        | 0.001     |
|    loss                 | 1.57e+03  |
|    n_updates            | 1629      |
|    policy_gradient_loss | -7.63e-10 |
|    value_loss           | 4.4e+03   |
---------------------------------------
Eval num_timesteps=359000, episode_reward=-391.99 +/- 76.47
Episode length: 45.70 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=-421.99 +/- 75.28
Episode length: 51.24 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-424.39 +/- 68.30
Episode length: 53.88 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 176      |
|    time_elapsed    | 1578     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=-414.80 +/- 61.53
Episode length: 57.54 +/- 15.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 57.5      |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.48e-08 |
|    explained_variance   | 0.204     |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+03  |
|    n_updates            | 1639      |
|    policy_gradient_loss | 9.92e-10  |
|    value_loss           | 4.27e+03  |
---------------------------------------
Eval num_timesteps=361000, episode_reward=-420.19 +/- 68.64
Episode length: 51.16 +/- 12.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-427.39 +/- 68.15
Episode length: 53.56 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-424.39 +/- 63.37
Episode length: 53.04 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 177      |
|    time_elapsed    | 1587     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=-412.99 +/- 63.75
Episode length: 46.74 +/- 14.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -413      |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.1e-10  |
|    explained_variance   | 0.15      |
|    learning_rate        | 0.001     |
|    loss                 | 1.6e+03   |
|    n_updates            | 1649      |
|    policy_gradient_loss | -1.51e-09 |
|    value_loss           | 3.5e+03   |
---------------------------------------
Eval num_timesteps=363000, episode_reward=-411.19 +/- 71.10
Episode length: 49.20 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-418.99 +/- 59.43
Episode length: 47.64 +/- 12.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-400.99 +/- 75.30
Episode length: 44.08 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-414.19 +/- 73.02
Episode length: 49.12 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 178      |
|    time_elapsed    | 1597     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=-403.39 +/- 71.60
Episode length: 46.58 +/- 16.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.6      |
|    mean_reward          | -403      |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-07 |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.001     |
|    loss                 | 1.37e+03  |
|    n_updates            | 1659      |
|    policy_gradient_loss | 1.43e-09  |
|    value_loss           | 3.51e+03  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=-407.58 +/- 92.64
Episode length: 49.00 +/- 19.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-418.39 +/- 68.19
Episode length: 50.86 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-407.59 +/- 72.50
Episode length: 47.22 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.6     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 179      |
|    time_elapsed    | 1606     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=-414.79 +/- 67.92
Episode length: 49.02 +/- 17.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49        |
|    mean_reward          | -415      |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.45e-09 |
|    explained_variance   | 0.164     |
|    learning_rate        | 0.001     |
|    loss                 | 1.72e+03  |
|    n_updates            | 1669      |
|    policy_gradient_loss | 8.32e-10  |
|    value_loss           | 3.75e+03  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=-417.19 +/- 65.24
Episode length: 45.32 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-412.39 +/- 75.84
Episode length: 49.14 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-431.00 +/- 65.04
Episode length: 51.76 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54       |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 180      |
|    time_elapsed    | 1614     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=-417.79 +/- 74.99
Episode length: 49.58 +/- 17.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -418      |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.17e-16 |
|    explained_variance   | 0.276     |
|    learning_rate        | 0.001     |
|    loss                 | 2.41e+03  |
|    n_updates            | 1679      |
|    policy_gradient_loss | 9.6e-10   |
|    value_loss           | 3.77e+03  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=-412.99 +/- 72.72
Episode length: 51.70 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-420.79 +/- 61.70
Episode length: 50.68 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-429.80 +/- 65.01
Episode length: 55.46 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 181      |
|    time_elapsed    | 1624     |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 1.12
Eval num_timesteps=371000, episode_reward=-422.44 +/- 76.90
Episode length: 51.74 +/- 14.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.7       |
|    mean_reward          | -422       |
| time/                   |            |
|    total_timesteps      | 371000     |
| train/                  |            |
|    approx_kl            | 0.15997335 |
|    clip_fraction        | 0.00693    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.00425   |
|    explained_variance   | 0.443      |
|    learning_rate        | 0.001      |
|    loss                 | 4.07e+03   |
|    n_updates            | 1689       |
|    policy_gradient_loss | 0.0017     |
|    value_loss           | 6.58e+03   |
----------------------------------------
Eval num_timesteps=371500, episode_reward=-401.65 +/- 85.98
Episode length: 51.00 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-429.73 +/- 66.35
Episode length: 56.84 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-444.11 +/- 69.34
Episode length: 54.30 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -444     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -380     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 182      |
|    time_elapsed    | 1634     |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.38
Eval num_timesteps=373000, episode_reward=-435.55 +/- 68.33
Episode length: 52.32 +/- 18.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.3       |
|    mean_reward          | -436       |
| time/                   |            |
|    total_timesteps      | 373000     |
| train/                  |            |
|    approx_kl            | 0.19064412 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.242     |
|    explained_variance   | 0.0367     |
|    learning_rate        | 0.001      |
|    loss                 | 1.42e+03   |
|    n_updates            | 1690       |
|    policy_gradient_loss | 0.0201     |
|    value_loss           | 3.02e+03   |
----------------------------------------
Eval num_timesteps=373500, episode_reward=-431.33 +/- 54.80
Episode length: 50.38 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-421.28 +/- 72.70
Episode length: 51.94 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-424.37 +/- 69.17
Episode length: 48.16 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.2     |
|    ep_rew_mean     | -379     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 183      |
|    time_elapsed    | 1642     |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=375000, episode_reward=-446.91 +/- 61.84
Episode length: 50.00 +/- 15.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -447        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.015188419 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0612     |
|    explained_variance   | 0.104       |
|    learning_rate        | 0.001       |
|    loss                 | 2.57e+03    |
|    n_updates            | 1691        |
|    policy_gradient_loss | -0.000174   |
|    value_loss           | 4.91e+03    |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=-423.23 +/- 72.27
Episode length: 47.50 +/- 13.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-424.79 +/- 78.11
Episode length: 49.60 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-416.13 +/- 67.93
Episode length: 45.06 +/- 12.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.2     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 184      |
|    time_elapsed    | 1651     |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=377000, episode_reward=-428.61 +/- 60.33
Episode length: 49.98 +/- 18.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.008724841 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.001       |
|    loss                 | 1.52e+03    |
|    n_updates            | 1692        |
|    policy_gradient_loss | 0.0065      |
|    value_loss           | 4.38e+03    |
-----------------------------------------
Eval num_timesteps=377500, episode_reward=-441.01 +/- 63.85
Episode length: 52.40 +/- 16.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-422.03 +/- 67.66
Episode length: 47.80 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-409.79 +/- 91.61
Episode length: 51.02 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -423     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 185      |
|    time_elapsed    | 1659     |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 2.69
Eval num_timesteps=379000, episode_reward=-411.20 +/- 74.08
Episode length: 46.06 +/- 17.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.1      |
|    mean_reward          | -411      |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 1.3430378 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.319    |
|    explained_variance   | 0.203     |
|    learning_rate        | 0.001     |
|    loss                 | 2.04e+03  |
|    n_updates            | 1693      |
|    policy_gradient_loss | 0.243     |
|    value_loss           | 4.06e+03  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=-422.60 +/- 65.52
Episode length: 53.52 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-437.00 +/- 51.17
Episode length: 57.76 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.8     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-391.99 +/- 77.18
Episode length: 46.28 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 186      |
|    time_elapsed    | 1668     |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.39
Eval num_timesteps=381000, episode_reward=-408.79 +/- 81.52
Episode length: 53.06 +/- 17.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.1       |
|    mean_reward          | -409       |
| time/                   |            |
|    total_timesteps      | 381000     |
| train/                  |            |
|    approx_kl            | 0.19539303 |
|    clip_fraction        | 0.0586     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.28       |
|    learning_rate        | 0.001      |
|    loss                 | 2e+03      |
|    n_updates            | 1694       |
|    policy_gradient_loss | 0.0282     |
|    value_loss           | 3.63e+03   |
----------------------------------------
Eval num_timesteps=381500, episode_reward=-409.40 +/- 69.41
Episode length: 45.08 +/- 13.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-413.59 +/- 65.44
Episode length: 49.50 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-437.00 +/- 51.86
Episode length: 54.16 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 187      |
|    time_elapsed    | 1677     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=-411.80 +/- 69.35
Episode length: 52.60 +/- 15.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52.6          |
|    mean_reward          | -412          |
| time/                   |               |
|    total_timesteps      | 383000        |
| train/                  |               |
|    approx_kl            | 1.4924444e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -7.16e-05     |
|    explained_variance   | 0.295         |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+03       |
|    n_updates            | 1704          |
|    policy_gradient_loss | -0.000222     |
|    value_loss           | 4e+03         |
-------------------------------------------
Eval num_timesteps=383500, episode_reward=-406.40 +/- 75.98
Episode length: 45.58 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-423.20 +/- 70.49
Episode length: 49.56 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-412.40 +/- 65.66
Episode length: 52.90 +/- 19.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-415.40 +/- 62.13
Episode length: 48.32 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 188      |
|    time_elapsed    | 1688     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=-412.99 +/- 67.86
Episode length: 48.00 +/- 19.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48        |
|    mean_reward          | -413      |
| time/                   |           |
|    total_timesteps      | 385500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.73e-12 |
|    explained_variance   | 0.191     |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+03  |
|    n_updates            | 1714      |
|    policy_gradient_loss | -7.63e-10 |
|    value_loss           | 3.06e+03  |
---------------------------------------
Eval num_timesteps=386000, episode_reward=-430.40 +/- 64.61
Episode length: 52.78 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-422.00 +/- 55.13
Episode length: 52.56 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-420.80 +/- 64.56
Episode length: 52.28 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 189      |
|    time_elapsed    | 1697     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=-411.76 +/- 95.71
Episode length: 50.68 +/- 17.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -412      |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-09 |
|    explained_variance   | 0.315     |
|    learning_rate        | 0.001     |
|    loss                 | 2.05e+03  |
|    n_updates            | 1724      |
|    policy_gradient_loss | -1.72e-09 |
|    value_loss           | 3.8e+03   |
---------------------------------------
Eval num_timesteps=388000, episode_reward=-417.20 +/- 71.56
Episode length: 49.06 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-424.40 +/- 72.14
Episode length: 52.26 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-444.79 +/- 54.72
Episode length: 49.04 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -445     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 190      |
|    time_elapsed    | 1706     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=-404.00 +/- 80.01
Episode length: 48.94 +/- 15.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.9      |
|    mean_reward          | -404      |
| time/                   |           |
|    total_timesteps      | 389500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.49e-11 |
|    explained_variance   | 0.226     |
|    learning_rate        | 0.001     |
|    loss                 | 1.6e+03   |
|    n_updates            | 1734      |
|    policy_gradient_loss | 1.41e-09  |
|    value_loss           | 3.7e+03   |
---------------------------------------
Eval num_timesteps=390000, episode_reward=-422.60 +/- 70.80
Episode length: 48.06 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-416.60 +/- 79.43
Episode length: 49.50 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-416.00 +/- 76.66
Episode length: 53.52 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 191      |
|    time_elapsed    | 1715     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=-416.60 +/- 61.85
Episode length: 50.78 +/- 17.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -417      |
| time/                   |           |
|    total_timesteps      | 391500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-08 |
|    explained_variance   | 0.33      |
|    learning_rate        | 0.001     |
|    loss                 | 1.73e+03  |
|    n_updates            | 1744      |
|    policy_gradient_loss | -3.43e-10 |
|    value_loss           | 3.53e+03  |
---------------------------------------
Eval num_timesteps=392000, episode_reward=-421.39 +/- 75.81
Episode length: 53.28 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-419.59 +/- 64.62
Episode length: 53.76 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-386.60 +/- 80.11
Episode length: 46.32 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 192      |
|    time_elapsed    | 1724     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=-427.40 +/- 54.99
Episode length: 54.48 +/- 19.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.5      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.32e-11 |
|    explained_variance   | 0.25      |
|    learning_rate        | 0.001     |
|    loss                 | 1.4e+03   |
|    n_updates            | 1754      |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 3.16e+03  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=-419.59 +/- 63.78
Episode length: 48.48 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-417.80 +/- 68.73
Episode length: 50.46 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-413.00 +/- 64.87
Episode length: 50.14 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 193      |
|    time_elapsed    | 1733     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=-428.59 +/- 68.23
Episode length: 49.44 +/- 16.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.4      |
|    mean_reward          | -429      |
| time/                   |           |
|    total_timesteps      | 395500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.99e-08 |
|    explained_variance   | 0.392     |
|    learning_rate        | 0.001     |
|    loss                 | 1.63e+03  |
|    n_updates            | 1764      |
|    policy_gradient_loss | -8.27e-10 |
|    value_loss           | 3.74e+03  |
---------------------------------------
Eval num_timesteps=396000, episode_reward=-419.60 +/- 74.46
Episode length: 46.24 +/- 15.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-436.39 +/- 65.84
Episode length: 50.70 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-384.19 +/- 85.51
Episode length: 45.96 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 194      |
|    time_elapsed    | 1742     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=-423.80 +/- 67.57
Episode length: 47.28 +/- 12.92
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47.3     |
|    mean_reward          | -424     |
| time/                   |          |
|    total_timesteps      | 397500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.6e-10 |
|    explained_variance   | 0.298    |
|    learning_rate        | 0.001    |
|    loss                 | 1.4e+03  |
|    n_updates            | 1774     |
|    policy_gradient_loss | 9.34e-10 |
|    value_loss           | 3.39e+03 |
--------------------------------------
Eval num_timesteps=398000, episode_reward=-423.80 +/- 75.13
Episode length: 52.96 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-414.79 +/- 78.27
Episode length: 51.76 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-417.19 +/- 64.41
Episode length: 51.10 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 195      |
|    time_elapsed    | 1751     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=-413.00 +/- 64.03
Episode length: 48.34 +/- 17.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -413      |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.57e-07 |
|    explained_variance   | 0.401     |
|    learning_rate        | 0.001     |
|    loss                 | 1.78e+03  |
|    n_updates            | 1784      |
|    policy_gradient_loss | -1.8e-10  |
|    value_loss           | 3.82e+03  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=-396.80 +/- 79.00
Episode length: 48.94 +/- 14.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-416.59 +/- 75.24
Episode length: 51.70 +/- 12.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-419.00 +/- 67.11
Episode length: 52.96 +/- 19.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 196      |
|    time_elapsed    | 1760     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=-441.20 +/- 57.69
Episode length: 51.34 +/- 17.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.3      |
|    mean_reward          | -441      |
| time/                   |           |
|    total_timesteps      | 401500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.03e-09 |
|    explained_variance   | 0.302     |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+03  |
|    n_updates            | 1794      |
|    policy_gradient_loss | 5.15e-10  |
|    value_loss           | 3.34e+03  |
---------------------------------------
Eval num_timesteps=402000, episode_reward=-435.20 +/- 54.54
Episode length: 50.20 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-423.80 +/- 71.45
Episode length: 49.64 +/- 14.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-398.00 +/- 74.37
Episode length: 51.92 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 197      |
|    time_elapsed    | 1769     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=-432.80 +/- 72.24
Episode length: 51.96 +/- 17.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 52            |
|    mean_reward          | -433          |
| time/                   |               |
|    total_timesteps      | 403500        |
| train/                  |               |
|    approx_kl            | -2.415618e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -1.48e-06     |
|    explained_variance   | 0.43          |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+03      |
|    n_updates            | 1804          |
|    policy_gradient_loss | 5.32e-08      |
|    value_loss           | 3.54e+03      |
-------------------------------------------
Eval num_timesteps=404000, episode_reward=-411.20 +/- 85.58
Episode length: 45.02 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-401.00 +/- 82.38
Episode length: 47.68 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-431.00 +/- 58.01
Episode length: 49.02 +/- 14.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-410.58 +/- 74.81
Episode length: 50.64 +/- 21.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 198      |
|    time_elapsed    | 1779     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=-402.18 +/- 79.30
Episode length: 48.80 +/- 17.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -402      |
| time/                   |           |
|    total_timesteps      | 406000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.49e-08 |
|    explained_variance   | 0.335     |
|    learning_rate        | 0.001     |
|    loss                 | 1.53e+03  |
|    n_updates            | 1814      |
|    policy_gradient_loss | -9.89e-08 |
|    value_loss           | 3.86e+03  |
---------------------------------------
Eval num_timesteps=406500, episode_reward=-412.39 +/- 69.39
Episode length: 50.96 +/- 23.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-408.79 +/- 78.36
Episode length: 52.74 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-426.20 +/- 58.08
Episode length: 50.32 +/- 13.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 199      |
|    time_elapsed    | 1788     |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.11
Eval num_timesteps=408000, episode_reward=-397.47 +/- 58.47
Episode length: 49.54 +/- 16.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -397        |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.019277116 |
|    clip_fraction        | 0.00606     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0055     |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.001       |
|    loss                 | 1.35e+03    |
|    n_updates            | 1823        |
|    policy_gradient_loss | 0.000595    |
|    value_loss           | 3.08e+03    |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=-397.88 +/- 75.47
Episode length: 45.12 +/- 13.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-400.16 +/- 72.16
Episode length: 46.54 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-388.36 +/- 74.54
Episode length: 43.38 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.4     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | -391     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 200      |
|    time_elapsed    | 1797     |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 2.03
Eval num_timesteps=410000, episode_reward=-287.84 +/- 121.34
Episode length: 35.22 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | -288      |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 1.0129005 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.294     |
|    learning_rate        | 0.001     |
|    loss                 | 1.84e+03  |
|    n_updates            | 1824      |
|    policy_gradient_loss | 0.183     |
|    value_loss           | 4.65e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=410500, episode_reward=-248.49 +/- 119.13
Episode length: 36.36 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
New best mean reward!
Eval num_timesteps=411000, episode_reward=-272.77 +/- 128.78
Episode length: 35.58 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=-290.57 +/- 108.66
Episode length: 35.36 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | -330     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 201      |
|    time_elapsed    | 1803     |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=412000, episode_reward=-253.05 +/- 138.87
Episode length: 37.10 +/- 5.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 37.1       |
|    mean_reward          | -253       |
| time/                   |            |
|    total_timesteps      | 412000     |
| train/                  |            |
|    approx_kl            | 0.04291172 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0724    |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.001      |
|    loss                 | 2.02e+03   |
|    n_updates            | 1825       |
|    policy_gradient_loss | -0.00264   |
|    value_loss           | 4.27e+03   |
----------------------------------------
Eval num_timesteps=412500, episode_reward=-302.57 +/- 92.96
Episode length: 35.14 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-297.14 +/- 96.82
Episode length: 35.54 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=-291.80 +/- 101.76
Episode length: 34.48 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 202      |
|    time_elapsed    | 1810     |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.20
Eval num_timesteps=414000, episode_reward=-267.94 +/- 118.08
Episode length: 36.76 +/- 6.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.8        |
|    mean_reward          | -268        |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.025443323 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00115    |
|    explained_variance   | 0.291       |
|    learning_rate        | 0.001       |
|    loss                 | 2.3e+03     |
|    n_updates            | 1826        |
|    policy_gradient_loss | -7.33e-05   |
|    value_loss           | 4.29e+03    |
-----------------------------------------
Eval num_timesteps=414500, episode_reward=-261.07 +/- 152.20
Episode length: 36.62 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=-257.95 +/- 127.27
Episode length: 34.96 +/- 7.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=-280.70 +/- 117.54
Episode length: 34.42 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -270     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 203      |
|    time_elapsed    | 1817     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=-269.38 +/- 135.50
Episode length: 34.76 +/- 6.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | -269          |
| time/                   |               |
|    total_timesteps      | 416000        |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -3.71e-08     |
|    explained_variance   | 0.448         |
|    learning_rate        | 0.001         |
|    loss                 | 2.92e+03      |
|    n_updates            | 1836          |
|    policy_gradient_loss | -1.74e-09     |
|    value_loss           | 5.52e+03      |
-------------------------------------------
Eval num_timesteps=416500, episode_reward=-261.86 +/- 104.36
Episode length: 36.58 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=-292.46 +/- 120.41
Episode length: 35.70 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=-259.45 +/- 125.71
Episode length: 35.78 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | -285     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 204      |
|    time_elapsed    | 1824     |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=418000, episode_reward=-286.70 +/- 129.67
Episode length: 35.04 +/- 7.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35         |
|    mean_reward          | -287       |
| time/                   |            |
|    total_timesteps      | 418000     |
| train/                  |            |
|    approx_kl            | 0.00799581 |
|    clip_fraction        | 0.0394     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0829    |
|    explained_variance   | 0.441      |
|    learning_rate        | 0.001      |
|    loss                 | 2.03e+03   |
|    n_updates            | 1838       |
|    policy_gradient_loss | 0.00612    |
|    value_loss           | 4.56e+03   |
----------------------------------------
Eval num_timesteps=418500, episode_reward=-279.53 +/- 128.88
Episode length: 34.86 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=-239.28 +/- 154.69
Episode length: 37.30 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
New best mean reward!
Eval num_timesteps=419500, episode_reward=-300.69 +/- 101.41
Episode length: 34.02 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.2     |
|    ep_rew_mean     | -318     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 205      |
|    time_elapsed    | 1830     |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=420000, episode_reward=-292.37 +/- 102.52
Episode length: 34.32 +/- 7.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | -292        |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.039883714 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.126       |
|    learning_rate        | 0.001       |
|    loss                 | 3.23e+03    |
|    n_updates            | 1839        |
|    policy_gradient_loss | 0.00369     |
|    value_loss           | 6.79e+03    |
-----------------------------------------
Eval num_timesteps=420500, episode_reward=-267.78 +/- 116.93
Episode length: 36.76 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=-281.73 +/- 116.43
Episode length: 36.44 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=-283.72 +/- 105.53
Episode length: 34.98 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | -312     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 206      |
|    time_elapsed    | 1837     |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=422000, episode_reward=-281.42 +/- 121.87
Episode length: 35.22 +/- 6.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.2        |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.009705272 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0612     |
|    explained_variance   | 0.175       |
|    learning_rate        | 0.001       |
|    loss                 | 2.6e+03     |
|    n_updates            | 1840        |
|    policy_gradient_loss | 0.00156     |
|    value_loss           | 4.69e+03    |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=-297.45 +/- 97.59
Episode length: 35.60 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=-288.85 +/- 118.04
Episode length: 34.50 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=-271.99 +/- 118.18
Episode length: 35.50 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | -284     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 207      |
|    time_elapsed    | 1844     |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=424000, episode_reward=-276.93 +/- 132.98
Episode length: 34.58 +/- 6.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | -277        |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.013726144 |
|    clip_fraction        | 0.0026      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00516    |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.001       |
|    loss                 | 2.15e+03    |
|    n_updates            | 1841        |
|    policy_gradient_loss | 0.000989    |
|    value_loss           | 5.03e+03    |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=-282.89 +/- 120.06
Episode length: 35.46 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-314.11 +/- 83.83
Episode length: 33.54 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | -314     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=-231.94 +/- 138.39
Episode length: 37.44 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | -270     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 208      |
|    time_elapsed    | 1850     |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=426000, episode_reward=-363.60 +/- 67.15
Episode length: 33.98 +/- 14.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34          |
|    mean_reward          | -364        |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.005284235 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0202     |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.001       |
|    loss                 | 1.78e+03    |
|    n_updates            | 1842        |
|    policy_gradient_loss | 0.00347     |
|    value_loss           | 4.11e+03    |
-----------------------------------------
Eval num_timesteps=426500, episode_reward=-374.62 +/- 49.67
Episode length: 30.24 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.2     |
|    mean_reward     | -375     |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=-371.04 +/- 58.04
Episode length: 30.38 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.4     |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=-368.94 +/- 53.36
Episode length: 35.20 +/- 21.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=-377.76 +/- 40.05
Episode length: 32.36 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | -328     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 209      |
|    time_elapsed    | 1858     |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.68
Eval num_timesteps=428500, episode_reward=-409.27 +/- 68.06
Episode length: 48.28 +/- 14.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.3       |
|    mean_reward          | -409       |
| time/                   |            |
|    total_timesteps      | 428500     |
| train/                  |            |
|    approx_kl            | 0.33836842 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.152     |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.001      |
|    loss                 | 1.33e+03   |
|    n_updates            | 1843       |
|    policy_gradient_loss | 0.06       |
|    value_loss           | 3.4e+03    |
----------------------------------------
Eval num_timesteps=429000, episode_reward=-415.95 +/- 65.84
Episode length: 45.36 +/- 13.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=-405.23 +/- 74.19
Episode length: 50.94 +/- 15.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-399.55 +/- 76.14
Episode length: 51.14 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | -383     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 210      |
|    time_elapsed    | 1866     |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.21
Eval num_timesteps=430500, episode_reward=-392.16 +/- 71.41
Episode length: 46.64 +/- 16.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.6        |
|    mean_reward          | -392        |
| time/                   |             |
|    total_timesteps      | 430500      |
| train/                  |             |
|    approx_kl            | 0.104040295 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.001       |
|    loss                 | 1.9e+03     |
|    n_updates            | 1844        |
|    policy_gradient_loss | 0.00471     |
|    value_loss           | 3.53e+03    |
-----------------------------------------
Eval num_timesteps=431000, episode_reward=-391.95 +/- 76.59
Episode length: 45.12 +/- 12.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=-390.94 +/- 73.99
Episode length: 49.30 +/- 13.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=-393.71 +/- 78.80
Episode length: 47.66 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.4     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 211      |
|    time_elapsed    | 1874     |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=432500, episode_reward=-407.04 +/- 59.21
Episode length: 48.68 +/- 14.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.009673825 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0924     |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.001       |
|    loss                 | 2.1e+03     |
|    n_updates            | 1845        |
|    policy_gradient_loss | 0.00435     |
|    value_loss           | 3.85e+03    |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=-395.54 +/- 75.24
Episode length: 45.60 +/- 12.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=-394.53 +/- 70.72
Episode length: 46.08 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=-386.83 +/- 80.30
Episode length: 45.78 +/- 14.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.7     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 212      |
|    time_elapsed    | 1882     |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=434500, episode_reward=-399.46 +/- 69.16
Episode length: 46.58 +/- 13.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.6        |
|    mean_reward          | -399        |
| time/                   |             |
|    total_timesteps      | 434500      |
| train/                  |             |
|    approx_kl            | 0.055878606 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0803     |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.001       |
|    loss                 | 1.52e+03    |
|    n_updates            | 1846        |
|    policy_gradient_loss | 0.00508     |
|    value_loss           | 2.92e+03    |
-----------------------------------------
Eval num_timesteps=435000, episode_reward=-382.06 +/- 72.25
Episode length: 49.72 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=-384.14 +/- 80.40
Episode length: 44.18 +/- 12.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=-410.08 +/- 64.08
Episode length: 49.30 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 213      |
|    time_elapsed    | 1890     |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=436500, episode_reward=-391.84 +/- 77.32
Episode length: 45.74 +/- 13.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.7        |
|    mean_reward          | -392        |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.011442074 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0533     |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+03    |
|    n_updates            | 1847        |
|    policy_gradient_loss | 0.00361     |
|    value_loss           | 2.24e+03    |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=-403.98 +/- 74.12
Episode length: 43.92 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=-403.90 +/- 60.00
Episode length: 45.82 +/- 13.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=-400.66 +/- 78.45
Episode length: 45.42 +/- 14.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.2     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 214      |
|    time_elapsed    | 1898     |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=438500, episode_reward=-390.31 +/- 73.20
Episode length: 49.56 +/- 16.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.6       |
|    mean_reward          | -390       |
| time/                   |            |
|    total_timesteps      | 438500     |
| train/                  |            |
|    approx_kl            | 0.04472565 |
|    clip_fraction        | 0.0664     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0786    |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.001      |
|    loss                 | 1.02e+03   |
|    n_updates            | 1848       |
|    policy_gradient_loss | 0.00888    |
|    value_loss           | 2.2e+03    |
----------------------------------------
Eval num_timesteps=439000, episode_reward=-411.26 +/- 59.86
Episode length: 47.52 +/- 11.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=-403.15 +/- 85.05
Episode length: 45.68 +/- 14.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-412.65 +/- 73.76
Episode length: 45.02 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -391     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 215      |
|    time_elapsed    | 1906     |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=440500, episode_reward=-402.04 +/- 75.47
Episode length: 46.90 +/- 13.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.9        |
|    mean_reward          | -402        |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.009345853 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0688     |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+03    |
|    n_updates            | 1849        |
|    policy_gradient_loss | 0.00564     |
|    value_loss           | 2e+03       |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=-384.40 +/- 90.22
Episode length: 47.26 +/- 12.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=-393.64 +/- 83.87
Episode length: 47.52 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=-393.26 +/- 72.27
Episode length: 44.52 +/- 15.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -391     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 216      |
|    time_elapsed    | 1914     |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=442500, episode_reward=-407.54 +/- 74.03
Episode length: 44.58 +/- 16.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44.6       |
|    mean_reward          | -408       |
| time/                   |            |
|    total_timesteps      | 442500     |
| train/                  |            |
|    approx_kl            | 0.05142384 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0637    |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.001      |
|    loss                 | 1.18e+03   |
|    n_updates            | 1850       |
|    policy_gradient_loss | 0.00603    |
|    value_loss           | 2.09e+03   |
----------------------------------------
Eval num_timesteps=443000, episode_reward=-382.38 +/- 82.30
Episode length: 44.74 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=-409.91 +/- 81.63
Episode length: 48.66 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=-410.08 +/- 67.44
Episode length: 52.28 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.3     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 217      |
|    time_elapsed    | 1923     |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=444500, episode_reward=-407.89 +/- 72.73
Episode length: 51.46 +/- 16.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.5       |
|    mean_reward          | -408       |
| time/                   |            |
|    total_timesteps      | 444500     |
| train/                  |            |
|    approx_kl            | 0.07655808 |
|    clip_fraction        | 0.0469     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0547    |
|    explained_variance   | 0.514      |
|    learning_rate        | 0.001      |
|    loss                 | 1.64e+03   |
|    n_updates            | 1851       |
|    policy_gradient_loss | -0.000465  |
|    value_loss           | 2.77e+03   |
----------------------------------------
Eval num_timesteps=445000, episode_reward=-404.99 +/- 70.17
Episode length: 48.94 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=-404.42 +/- 65.01
Episode length: 48.90 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=-390.78 +/- 82.06
Episode length: 45.56 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.6     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 218      |
|    time_elapsed    | 1931     |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=446500, episode_reward=-430.25 +/- 54.79
Episode length: 48.90 +/- 17.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.9       |
|    mean_reward          | -430       |
| time/                   |            |
|    total_timesteps      | 446500     |
| train/                  |            |
|    approx_kl            | 0.05268337 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0499    |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.001      |
|    loss                 | 881        |
|    n_updates            | 1852       |
|    policy_gradient_loss | 0.0109     |
|    value_loss           | 1.97e+03   |
----------------------------------------
Eval num_timesteps=447000, episode_reward=-411.64 +/- 64.82
Episode length: 49.70 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=-406.03 +/- 66.44
Episode length: 54.54 +/- 18.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=-407.95 +/- 64.56
Episode length: 48.82 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=-418.22 +/- 58.19
Episode length: 47.00 +/- 11.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.1     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 219      |
|    time_elapsed    | 1941     |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=449000, episode_reward=-402.95 +/- 80.37
Episode length: 47.34 +/- 17.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.3        |
|    mean_reward          | -403        |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.033230953 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0184     |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.001       |
|    loss                 | 1.6e+03     |
|    n_updates            | 1853        |
|    policy_gradient_loss | 0.00757     |
|    value_loss           | 2.54e+03    |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=-393.45 +/- 72.13
Episode length: 48.88 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-413.12 +/- 84.63
Episode length: 46.36 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=-409.68 +/- 73.01
Episode length: 46.36 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 220      |
|    time_elapsed    | 1949     |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=451000, episode_reward=-414.54 +/- 75.65
Episode length: 52.06 +/- 17.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.1       |
|    mean_reward          | -415       |
| time/                   |            |
|    total_timesteps      | 451000     |
| train/                  |            |
|    approx_kl            | 0.02122822 |
|    clip_fraction        | 0.0234     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.026     |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.001      |
|    loss                 | 856        |
|    n_updates            | 1854       |
|    policy_gradient_loss | 0.00402    |
|    value_loss           | 2.11e+03   |
----------------------------------------
Eval num_timesteps=451500, episode_reward=-426.36 +/- 49.87
Episode length: 46.86 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=-419.16 +/- 71.17
Episode length: 52.06 +/- 14.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=-403.56 +/- 64.11
Episode length: 48.96 +/- 14.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 221      |
|    time_elapsed    | 1958     |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=453000, episode_reward=-384.43 +/- 72.55
Episode length: 46.86 +/- 14.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.9        |
|    mean_reward          | -384        |
| time/                   |             |
|    total_timesteps      | 453000      |
| train/                  |             |
|    approx_kl            | 0.008431192 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0708     |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.001       |
|    loss                 | 1.46e+03    |
|    n_updates            | 1855        |
|    policy_gradient_loss | 0.0056      |
|    value_loss           | 2.6e+03     |
-----------------------------------------
Eval num_timesteps=453500, episode_reward=-432.41 +/- 53.36
Episode length: 47.74 +/- 12.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=-404.26 +/- 79.05
Episode length: 46.04 +/- 15.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=-397.21 +/- 65.48
Episode length: 50.18 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 222      |
|    time_elapsed    | 1966     |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=455000, episode_reward=-423.15 +/- 63.37
Episode length: 51.10 +/- 18.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.1         |
|    mean_reward          | -423         |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0150480475 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.172       |
|    explained_variance   | 0.634        |
|    learning_rate        | 0.001        |
|    loss                 | 920          |
|    n_updates            | 1856         |
|    policy_gradient_loss | 0.0135       |
|    value_loss           | 1.57e+03     |
------------------------------------------
Eval num_timesteps=455500, episode_reward=-402.79 +/- 73.11
Episode length: 45.80 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=-397.18 +/- 72.17
Episode length: 46.82 +/- 15.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=-401.84 +/- 82.17
Episode length: 47.72 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 223      |
|    time_elapsed    | 1975     |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=457000, episode_reward=-391.87 +/- 77.45
Episode length: 44.00 +/- 15.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44          |
|    mean_reward          | -392        |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.046240933 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.001       |
|    loss                 | 935         |
|    n_updates            | 1857        |
|    policy_gradient_loss | 0.028       |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=-406.92 +/- 71.79
Episode length: 49.64 +/- 14.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=-399.98 +/- 71.79
Episode length: 46.60 +/- 13.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=-415.88 +/- 69.73
Episode length: 47.58 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 224      |
|    time_elapsed    | 1983     |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.18
Eval num_timesteps=459000, episode_reward=-391.81 +/- 50.73
Episode length: 48.72 +/- 14.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.7       |
|    mean_reward          | -392       |
| time/                   |            |
|    total_timesteps      | 459000     |
| train/                  |            |
|    approx_kl            | 0.09200839 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.001      |
|    loss                 | 738        |
|    n_updates            | 1858       |
|    policy_gradient_loss | 0.0364     |
|    value_loss           | 2.16e+03   |
----------------------------------------
Eval num_timesteps=459500, episode_reward=-371.24 +/- 78.57
Episode length: 41.20 +/- 12.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-363.90 +/- 68.54
Episode length: 44.52 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=-368.37 +/- 79.89
Episode length: 42.38 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.7     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 225      |
|    time_elapsed    | 1991     |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.26
Eval num_timesteps=461000, episode_reward=-380.60 +/- 67.24
Episode length: 46.06 +/- 13.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.1       |
|    mean_reward          | -381       |
| time/                   |            |
|    total_timesteps      | 461000     |
| train/                  |            |
|    approx_kl            | 0.13090387 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.408     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.001      |
|    loss                 | 926        |
|    n_updates            | 1859       |
|    policy_gradient_loss | 0.0444     |
|    value_loss           | 2.47e+03   |
----------------------------------------
Eval num_timesteps=461500, episode_reward=-372.53 +/- 80.20
Episode length: 46.70 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=-379.94 +/- 57.10
Episode length: 44.12 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=-361.92 +/- 84.59
Episode length: 41.16 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.9     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 226      |
|    time_elapsed    | 2000     |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=463000, episode_reward=-389.93 +/- 68.40
Episode length: 45.44 +/- 16.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.4        |
|    mean_reward          | -390        |
| time/                   |             |
|    total_timesteps      | 463000      |
| train/                  |             |
|    approx_kl            | 0.043526947 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.001       |
|    loss                 | 1.15e+03    |
|    n_updates            | 1860        |
|    policy_gradient_loss | 0.00612     |
|    value_loss           | 2.95e+03    |
-----------------------------------------
Eval num_timesteps=463500, episode_reward=-371.35 +/- 76.60
Episode length: 42.42 +/- 13.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=-387.31 +/- 57.49
Episode length: 44.68 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=-379.39 +/- 60.56
Episode length: 43.88 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | -379     |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 227      |
|    time_elapsed    | 2008     |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=465000, episode_reward=-383.30 +/- 60.55
Episode length: 44.02 +/- 15.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44          |
|    mean_reward          | -383        |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.031484403 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.001       |
|    loss                 | 1.47e+03    |
|    n_updates            | 1861        |
|    policy_gradient_loss | 0.01        |
|    value_loss           | 2.67e+03    |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=-389.08 +/- 59.28
Episode length: 49.28 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -389     |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=-407.37 +/- 56.74
Episode length: 45.78 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=-386.75 +/- 60.07
Episode length: 45.02 +/- 15.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43       |
|    ep_rew_mean     | -383     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 228      |
|    time_elapsed    | 2016     |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=467000, episode_reward=-395.31 +/- 57.42
Episode length: 45.90 +/- 12.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.9         |
|    mean_reward          | -395         |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0077378144 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.07        |
|    explained_variance   | 0.551        |
|    learning_rate        | 0.001        |
|    loss                 | 1.88e+03     |
|    n_updates            | 1862         |
|    policy_gradient_loss | 0.00184      |
|    value_loss           | 3.1e+03      |
------------------------------------------
Eval num_timesteps=467500, episode_reward=-389.81 +/- 66.15
Episode length: 49.38 +/- 17.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=-400.64 +/- 62.37
Episode length: 51.86 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=-397.57 +/- 57.44
Episode length: 48.56 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | -378     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 229      |
|    time_elapsed    | 2025     |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=469000, episode_reward=-406.34 +/- 70.22
Episode length: 50.24 +/- 16.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.2        |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.008882009 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0507     |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.001       |
|    loss                 | 1.14e+03    |
|    n_updates            | 1863        |
|    policy_gradient_loss | 0.00519     |
|    value_loss           | 2.21e+03    |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=-385.61 +/- 71.63
Episode length: 43.80 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.8     |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-394.94 +/- 65.60
Episode length: 46.92 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=-411.34 +/- 74.02
Episode length: 46.96 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=-381.05 +/- 73.98
Episode length: 44.12 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.6     |
|    ep_rew_mean     | -386     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 230      |
|    time_elapsed    | 2034     |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=471500, episode_reward=-402.23 +/- 61.93
Episode length: 48.00 +/- 16.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48          |
|    mean_reward          | -402        |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.005944352 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0466     |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.001       |
|    loss                 | 1.52e+03    |
|    n_updates            | 1864        |
|    policy_gradient_loss | 0.00726     |
|    value_loss           | 2.7e+03     |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=-409.51 +/- 67.26
Episode length: 46.08 +/- 15.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=-397.79 +/- 54.01
Episode length: 47.48 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=-399.23 +/- 75.13
Episode length: 47.02 +/- 14.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.7     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 231      |
|    time_elapsed    | 2043     |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=473500, episode_reward=-401.69 +/- 68.94
Episode length: 48.72 +/- 15.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.7       |
|    mean_reward          | -402       |
| time/                   |            |
|    total_timesteps      | 473500     |
| train/                  |            |
|    approx_kl            | 0.02880472 |
|    clip_fraction        | 0.0417     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0375    |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.001      |
|    loss                 | 1.46e+03   |
|    n_updates            | 1865       |
|    policy_gradient_loss | 0.0159     |
|    value_loss           | 2.5e+03    |
----------------------------------------
Eval num_timesteps=474000, episode_reward=-404.36 +/- 53.60
Episode length: 48.36 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=-414.73 +/- 57.92
Episode length: 47.46 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=-415.86 +/- 63.94
Episode length: 47.28 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 232      |
|    time_elapsed    | 2051     |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=475500, episode_reward=-405.37 +/- 72.52
Episode length: 47.98 +/- 16.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48           |
|    mean_reward          | -405         |
| time/                   |              |
|    total_timesteps      | 475500       |
| train/                  |              |
|    approx_kl            | 0.0115888985 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0253      |
|    explained_variance   | 0.636        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+03     |
|    n_updates            | 1866         |
|    policy_gradient_loss | 0.00282      |
|    value_loss           | 1.9e+03      |
------------------------------------------
Eval num_timesteps=476000, episode_reward=-406.35 +/- 69.72
Episode length: 48.34 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-403.45 +/- 74.79
Episode length: 50.32 +/- 19.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-398.15 +/- 60.70
Episode length: 46.20 +/- 13.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 233      |
|    time_elapsed    | 2059     |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=477500, episode_reward=-415.01 +/- 75.49
Episode length: 50.30 +/- 16.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.3        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 477500      |
| train/                  |             |
|    approx_kl            | 0.019297788 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0544     |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+03    |
|    n_updates            | 1867        |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 2.34e+03    |
-----------------------------------------
Eval num_timesteps=478000, episode_reward=-422.16 +/- 61.52
Episode length: 54.10 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-412.14 +/- 67.59
Episode length: 49.36 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=-431.96 +/- 45.48
Episode length: 49.42 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 234      |
|    time_elapsed    | 2068     |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 3.24
Eval num_timesteps=479500, episode_reward=-407.47 +/- 64.85
Episode length: 54.00 +/- 19.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54        |
|    mean_reward          | -407      |
| time/                   |           |
|    total_timesteps      | 479500    |
| train/                  |           |
|    approx_kl            | 0.8129797 |
|    clip_fraction        | 0.252     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.192    |
|    explained_variance   | 0.546     |
|    learning_rate        | 0.001     |
|    loss                 | 1.15e+03  |
|    n_updates            | 1868      |
|    policy_gradient_loss | 0.0579    |
|    value_loss           | 2.37e+03  |
---------------------------------------
Eval num_timesteps=480000, episode_reward=-424.16 +/- 61.76
Episode length: 53.52 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=-409.25 +/- 68.76
Episode length: 47.68 +/- 18.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-398.99 +/- 75.88
Episode length: 48.12 +/- 18.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 235      |
|    time_elapsed    | 2077     |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.19
Eval num_timesteps=481500, episode_reward=-413.24 +/- 62.84
Episode length: 49.60 +/- 17.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 481500      |
| train/                  |             |
|    approx_kl            | 0.092976995 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.001       |
|    loss                 | 761         |
|    n_updates            | 1869        |
|    policy_gradient_loss | 0.00722     |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=-426.72 +/- 59.45
Episode length: 50.06 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-403.98 +/- 79.33
Episode length: 46.40 +/- 13.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=-436.87 +/- 65.54
Episode length: 51.96 +/- 20.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 236      |
|    time_elapsed    | 2085     |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=483500, episode_reward=-415.35 +/- 73.01
Episode length: 51.30 +/- 16.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.3        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.010534982 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0286     |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.001       |
|    loss                 | 864         |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.000993   |
|    value_loss           | 2.26e+03    |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=-395.03 +/- 92.85
Episode length: 49.86 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-409.39 +/- 73.92
Episode length: 50.42 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-414.11 +/- 66.76
Episode length: 53.74 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 237      |
|    time_elapsed    | 2094     |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=485500, episode_reward=-415.40 +/- 73.78
Episode length: 52.06 +/- 17.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.1        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.009664857 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0181     |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.001       |
|    loss                 | 1.24e+03    |
|    n_updates            | 1871        |
|    policy_gradient_loss | 0.000285    |
|    value_loss           | 2.26e+03    |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=-418.99 +/- 58.82
Episode length: 50.46 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-409.66 +/- 77.70
Episode length: 51.00 +/- 20.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=-424.39 +/- 62.23
Episode length: 45.70 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 238      |
|    time_elapsed    | 2102     |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=487500, episode_reward=-433.94 +/- 51.15
Episode length: 52.14 +/- 14.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.1         |
|    mean_reward          | -434         |
| time/                   |              |
|    total_timesteps      | 487500       |
| train/                  |              |
|    approx_kl            | 0.0033028647 |
|    clip_fraction        | 0.00279      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.000417    |
|    explained_variance   | 0.604        |
|    learning_rate        | 0.001        |
|    loss                 | 1.09e+03     |
|    n_updates            | 1872         |
|    policy_gradient_loss | 0.000978     |
|    value_loss           | 2.67e+03     |
------------------------------------------
Eval num_timesteps=488000, episode_reward=-417.20 +/- 86.58
Episode length: 52.44 +/- 16.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-408.66 +/- 89.25
Episode length: 49.30 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-411.19 +/- 79.02
Episode length: 48.44 +/- 14.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 239      |
|    time_elapsed    | 2111     |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=489500, episode_reward=-426.13 +/- 51.15
Episode length: 51.36 +/- 16.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.4         |
|    mean_reward          | -426         |
| time/                   |              |
|    total_timesteps      | 489500       |
| train/                  |              |
|    approx_kl            | 0.0028740978 |
|    clip_fraction        | 0.00174      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0035      |
|    explained_variance   | 0.702        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+03     |
|    n_updates            | 1873         |
|    policy_gradient_loss | -0.000388    |
|    value_loss           | 3.36e+03     |
------------------------------------------
Eval num_timesteps=490000, episode_reward=-409.82 +/- 62.78
Episode length: 48.22 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-427.19 +/- 54.52
Episode length: 51.24 +/- 21.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=-419.58 +/- 61.46
Episode length: 53.20 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-397.15 +/- 92.68
Episode length: 46.40 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 240      |
|    time_elapsed    | 2121     |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=492000, episode_reward=-425.83 +/- 59.70
Episode length: 48.74 +/- 13.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -426        |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.017810844 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.014      |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.001       |
|    loss                 | 1.39e+03    |
|    n_updates            | 1874        |
|    policy_gradient_loss | -0.00138    |
|    value_loss           | 2.13e+03    |
-----------------------------------------
Eval num_timesteps=492500, episode_reward=-421.84 +/- 64.80
Episode length: 48.78 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-425.52 +/- 61.80
Episode length: 49.96 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=-403.00 +/- 77.69
Episode length: 51.34 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -420     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 241      |
|    time_elapsed    | 2130     |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=494000, episode_reward=-407.63 +/- 86.28
Episode length: 47.34 +/- 12.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.3        |
|    mean_reward          | -408        |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.009770936 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0336     |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 1875        |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 2.09e+03    |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=-422.45 +/- 55.50
Episode length: 50.04 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-409.17 +/- 64.66
Episode length: 49.26 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=-416.64 +/- 74.58
Episode length: 50.68 +/- 15.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 242      |
|    time_elapsed    | 2138     |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=496000, episode_reward=-417.94 +/- 65.65
Episode length: 50.88 +/- 15.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.9       |
|    mean_reward          | -418       |
| time/                   |            |
|    total_timesteps      | 496000     |
| train/                  |            |
|    approx_kl            | 0.00951548 |
|    clip_fraction        | 0.0104     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.025     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.001      |
|    loss                 | 1.12e+03   |
|    n_updates            | 1876       |
|    policy_gradient_loss | 0.000491   |
|    value_loss           | 1.97e+03   |
----------------------------------------
Eval num_timesteps=496500, episode_reward=-425.45 +/- 64.34
Episode length: 47.98 +/- 13.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-408.52 +/- 66.84
Episode length: 44.38 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=-410.67 +/- 70.44
Episode length: 45.34 +/- 15.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 243      |
|    time_elapsed    | 2146     |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=498000, episode_reward=-427.56 +/- 59.75
Episode length: 55.48 +/- 18.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.5        |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.012164703 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0312     |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+03    |
|    n_updates            | 1877        |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 2.54e+03    |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=-411.93 +/- 71.10
Episode length: 50.30 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=-406.22 +/- 75.68
Episode length: 49.20 +/- 16.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=-415.02 +/- 55.08
Episode length: 52.36 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 244      |
|    time_elapsed    | 2155     |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=500000, episode_reward=-410.61 +/- 67.22
Episode length: 46.16 +/- 15.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.2        |
|    mean_reward          | -411        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.008255911 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0519     |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.001       |
|    loss                 | 764         |
|    n_updates            | 1878        |
|    policy_gradient_loss | 0.000657    |
|    value_loss           | 1.56e+03    |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=-418.41 +/- 65.80
Episode length: 51.56 +/- 21.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-411.73 +/- 70.08
Episode length: 49.28 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=-428.95 +/- 50.34
Episode length: 50.94 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 245      |
|    time_elapsed    | 2163     |
|    total_timesteps | 501760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=502000, episode_reward=-421.54 +/- 67.23
Episode length: 48.96 +/- 16.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | -422        |
| time/                   |             |
|    total_timesteps      | 502000      |
| train/                  |             |
|    approx_kl            | 0.021953836 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0568     |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.001       |
|    loss                 | 1.12e+03    |
|    n_updates            | 1879        |
|    policy_gradient_loss | 0.00155     |
|    value_loss           | 2.58e+03    |
-----------------------------------------
Eval num_timesteps=502500, episode_reward=-405.95 +/- 77.00
Episode length: 50.94 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=-412.32 +/- 64.40
Episode length: 46.36 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=-420.87 +/- 75.33
Episode length: 47.22 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 246      |
|    time_elapsed    | 2172     |
|    total_timesteps | 503808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=504000, episode_reward=-434.86 +/- 54.83
Episode length: 53.62 +/- 16.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.6       |
|    mean_reward          | -435       |
| time/                   |            |
|    total_timesteps      | 504000     |
| train/                  |            |
|    approx_kl            | 0.06607202 |
|    clip_fraction        | 0.0234     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0422    |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.001      |
|    loss                 | 1.15e+03   |
|    n_updates            | 1880       |
|    policy_gradient_loss | 0.0292     |
|    value_loss           | 2.17e+03   |
----------------------------------------
Eval num_timesteps=504500, episode_reward=-415.60 +/- 66.49
Episode length: 49.48 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=-394.04 +/- 74.91
Episode length: 51.62 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=-423.24 +/- 64.59
Episode length: 48.70 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 247      |
|    time_elapsed    | 2180     |
|    total_timesteps | 505856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=506000, episode_reward=-423.85 +/- 65.07
Episode length: 49.60 +/- 15.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 506000      |
| train/                  |             |
|    approx_kl            | 0.017317709 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0298     |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.001       |
|    loss                 | 1.22e+03    |
|    n_updates            | 1881        |
|    policy_gradient_loss | -0.000183   |
|    value_loss           | 1.95e+03    |
-----------------------------------------
Eval num_timesteps=506500, episode_reward=-410.50 +/- 66.53
Episode length: 51.78 +/- 15.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=-418.23 +/- 57.77
Episode length: 48.20 +/- 14.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=-422.01 +/- 66.42
Episode length: 51.84 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 248      |
|    time_elapsed    | 2189     |
|    total_timesteps | 507904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=508000, episode_reward=-407.37 +/- 81.41
Episode length: 43.96 +/- 13.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44           |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 508000       |
| train/                  |              |
|    approx_kl            | 0.0070524192 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0587      |
|    explained_variance   | 0.666        |
|    learning_rate        | 0.001        |
|    loss                 | 1e+03        |
|    n_updates            | 1882         |
|    policy_gradient_loss | 0.00157      |
|    value_loss           | 1.97e+03     |
------------------------------------------
Eval num_timesteps=508500, episode_reward=-425.95 +/- 78.70
Episode length: 51.18 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=-405.66 +/- 71.17
Episode length: 51.74 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=-414.31 +/- 62.48
Episode length: 51.60 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 249      |
|    time_elapsed    | 2197     |
|    total_timesteps | 509952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=510000, episode_reward=-399.29 +/- 83.11
Episode length: 49.90 +/- 18.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -399        |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.006737661 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.001       |
|    loss                 | 1.07e+03    |
|    n_updates            | 1883        |
|    policy_gradient_loss | 0.000874    |
|    value_loss           | 1.81e+03    |
-----------------------------------------
Eval num_timesteps=510500, episode_reward=-403.63 +/- 68.91
Episode length: 51.46 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=-418.44 +/- 65.67
Episode length: 51.62 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=-405.47 +/- 68.23
Episode length: 48.06 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=-437.33 +/- 42.99
Episode length: 51.38 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 250      |
|    time_elapsed    | 2207     |
|    total_timesteps | 512000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=512500, episode_reward=-419.53 +/- 71.45
Episode length: 49.98 +/- 16.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -420         |
| time/                   |              |
|    total_timesteps      | 512500       |
| train/                  |              |
|    approx_kl            | 0.0075538703 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.143       |
|    explained_variance   | 0.599        |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+03      |
|    n_updates            | 1884         |
|    policy_gradient_loss | 0.000387     |
|    value_loss           | 1.86e+03     |
------------------------------------------
Eval num_timesteps=513000, episode_reward=-418.87 +/- 56.87
Episode length: 46.22 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=-423.96 +/- 54.04
Episode length: 49.52 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=-420.13 +/- 78.07
Episode length: 53.18 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 251      |
|    time_elapsed    | 2216     |
|    total_timesteps | 514048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=514500, episode_reward=-423.71 +/- 70.84
Episode length: 48.86 +/- 12.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.9         |
|    mean_reward          | -424         |
| time/                   |              |
|    total_timesteps      | 514500       |
| train/                  |              |
|    approx_kl            | 0.0074060005 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.601        |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+03     |
|    n_updates            | 1885         |
|    policy_gradient_loss | -0.000197    |
|    value_loss           | 2.34e+03     |
------------------------------------------
Eval num_timesteps=515000, episode_reward=-414.52 +/- 73.26
Episode length: 49.26 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=-410.81 +/- 61.73
Episode length: 51.00 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=-420.67 +/- 67.92
Episode length: 50.68 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 252      |
|    time_elapsed    | 2224     |
|    total_timesteps | 516096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=516500, episode_reward=-418.42 +/- 78.17
Episode length: 49.58 +/- 15.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 516500      |
| train/                  |             |
|    approx_kl            | 0.008484737 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.001       |
|    loss                 | 842         |
|    n_updates            | 1886        |
|    policy_gradient_loss | 0.0033      |
|    value_loss           | 1.57e+03    |
-----------------------------------------
Eval num_timesteps=517000, episode_reward=-412.82 +/- 66.01
Episode length: 46.46 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=-431.17 +/- 67.29
Episode length: 49.00 +/- 14.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=-424.58 +/- 67.41
Episode length: 52.28 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 253      |
|    time_elapsed    | 2233     |
|    total_timesteps | 518144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=518500, episode_reward=-412.73 +/- 69.40
Episode length: 49.78 +/- 18.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 518500      |
| train/                  |             |
|    approx_kl            | 0.007059054 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.001       |
|    loss                 | 631         |
|    n_updates            | 1887        |
|    policy_gradient_loss | 0.00402     |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=519000, episode_reward=-421.34 +/- 55.44
Episode length: 50.88 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=-418.14 +/- 53.70
Episode length: 47.64 +/- 14.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-413.25 +/- 81.58
Episode length: 44.40 +/- 13.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 254      |
|    time_elapsed    | 2241     |
|    total_timesteps | 520192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=520500, episode_reward=-402.52 +/- 83.61
Episode length: 52.70 +/- 18.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.7        |
|    mean_reward          | -403        |
| time/                   |             |
|    total_timesteps      | 520500      |
| train/                  |             |
|    approx_kl            | 0.012057283 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.001       |
|    loss                 | 871         |
|    n_updates            | 1888        |
|    policy_gradient_loss | 0.000822    |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=521000, episode_reward=-410.01 +/- 66.04
Episode length: 50.34 +/- 16.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=-422.62 +/- 52.85
Episode length: 52.68 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=-414.85 +/- 65.62
Episode length: 48.04 +/- 12.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 255      |
|    time_elapsed    | 2250     |
|    total_timesteps | 522240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=522500, episode_reward=-413.85 +/- 66.37
Episode length: 51.24 +/- 17.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.2        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 522500      |
| train/                  |             |
|    approx_kl            | 0.011517195 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.001       |
|    loss                 | 856         |
|    n_updates            | 1889        |
|    policy_gradient_loss | 0.00575     |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=523000, episode_reward=-409.95 +/- 75.18
Episode length: 52.00 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=-403.35 +/- 53.98
Episode length: 49.58 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=-409.89 +/- 74.11
Episode length: 48.30 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 256      |
|    time_elapsed    | 2259     |
|    total_timesteps | 524288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=524500, episode_reward=-387.22 +/- 75.45
Episode length: 51.64 +/- 16.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.6        |
|    mean_reward          | -387        |
| time/                   |             |
|    total_timesteps      | 524500      |
| train/                  |             |
|    approx_kl            | 0.007844219 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.001       |
|    loss                 | 1.11e+03    |
|    n_updates            | 1890        |
|    policy_gradient_loss | 0.00368     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=-397.85 +/- 78.42
Episode length: 48.48 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=-405.91 +/- 70.24
Episode length: 44.02 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44       |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=-405.30 +/- 82.06
Episode length: 50.18 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 257      |
|    time_elapsed    | 2267     |
|    total_timesteps | 526336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=526500, episode_reward=-403.20 +/- 74.58
Episode length: 49.64 +/- 18.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -403        |
| time/                   |             |
|    total_timesteps      | 526500      |
| train/                  |             |
|    approx_kl            | 0.004919463 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.001       |
|    loss                 | 995         |
|    n_updates            | 1891        |
|    policy_gradient_loss | 0.00141     |
|    value_loss           | 2.16e+03    |
-----------------------------------------
Eval num_timesteps=527000, episode_reward=-409.03 +/- 65.30
Episode length: 46.54 +/- 14.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=-410.51 +/- 57.56
Episode length: 55.04 +/- 21.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=-415.57 +/- 53.97
Episode length: 52.62 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 258      |
|    time_elapsed    | 2276     |
|    total_timesteps | 528384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=528500, episode_reward=-397.14 +/- 86.48
Episode length: 49.06 +/- 16.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.1         |
|    mean_reward          | -397         |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0076739807 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.722        |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+03     |
|    n_updates            | 1892         |
|    policy_gradient_loss | -8.57e-05    |
|    value_loss           | 1.78e+03     |
------------------------------------------
Eval num_timesteps=529000, episode_reward=-386.74 +/- 86.38
Episode length: 45.22 +/- 13.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=-425.87 +/- 73.30
Episode length: 49.44 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-405.99 +/- 68.51
Episode length: 49.90 +/- 11.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -427     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 259      |
|    time_elapsed    | 2284     |
|    total_timesteps | 530432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=530500, episode_reward=-414.75 +/- 53.92
Episode length: 49.40 +/- 16.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.4        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 530500      |
| train/                  |             |
|    approx_kl            | 0.006749533 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.001       |
|    loss                 | 982         |
|    n_updates            | 1893        |
|    policy_gradient_loss | 0.000706    |
|    value_loss           | 1.9e+03     |
-----------------------------------------
Eval num_timesteps=531000, episode_reward=-422.60 +/- 58.79
Episode length: 51.62 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=-410.53 +/- 76.16
Episode length: 49.70 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=-424.77 +/- 54.19
Episode length: 51.98 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 260      |
|    time_elapsed    | 2293     |
|    total_timesteps | 532480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=532500, episode_reward=-407.43 +/- 78.93
Episode length: 44.94 +/- 15.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44.9         |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0061852233 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.658        |
|    learning_rate        | 0.001        |
|    loss                 | 973          |
|    n_updates            | 1894         |
|    policy_gradient_loss | 0.00276      |
|    value_loss           | 2.02e+03     |
------------------------------------------
Eval num_timesteps=533000, episode_reward=-412.96 +/- 64.58
Episode length: 51.80 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=-403.60 +/- 75.02
Episode length: 48.84 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=-403.76 +/- 69.14
Episode length: 49.22 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=-409.61 +/- 68.46
Episode length: 50.84 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 261      |
|    time_elapsed    | 2303     |
|    total_timesteps | 534528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=535000, episode_reward=-418.85 +/- 73.38
Episode length: 44.92 +/- 14.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.9        |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 535000      |
| train/                  |             |
|    approx_kl            | 0.008621489 |
|    clip_fraction        | 0.0484      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.001       |
|    loss                 | 983         |
|    n_updates            | 1895        |
|    policy_gradient_loss | -3.93e-06   |
|    value_loss           | 1.92e+03    |
-----------------------------------------
Eval num_timesteps=535500, episode_reward=-398.18 +/- 66.78
Episode length: 48.96 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=-405.83 +/- 78.62
Episode length: 45.26 +/- 13.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=-416.37 +/- 61.81
Episode length: 52.28 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 262      |
|    time_elapsed    | 2311     |
|    total_timesteps | 536576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=537000, episode_reward=-410.27 +/- 59.89
Episode length: 50.28 +/- 14.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.3         |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0059602913 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.001        |
|    loss                 | 1.01e+03     |
|    n_updates            | 1896         |
|    policy_gradient_loss | 0.00584      |
|    value_loss           | 2.11e+03     |
------------------------------------------
Eval num_timesteps=537500, episode_reward=-411.41 +/- 72.33
Episode length: 48.24 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=-413.86 +/- 71.38
Episode length: 50.36 +/- 18.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=-418.21 +/- 58.38
Episode length: 48.72 +/- 14.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 263      |
|    time_elapsed    | 2319     |
|    total_timesteps | 538624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=539000, episode_reward=-398.10 +/- 73.23
Episode length: 49.14 +/- 17.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.1         |
|    mean_reward          | -398         |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 0.0072865286 |
|    clip_fraction        | 0.0558       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.143       |
|    explained_variance   | 0.574        |
|    learning_rate        | 0.001        |
|    loss                 | 876          |
|    n_updates            | 1897         |
|    policy_gradient_loss | 0.00482      |
|    value_loss           | 1.78e+03     |
------------------------------------------
Eval num_timesteps=539500, episode_reward=-412.36 +/- 56.35
Episode length: 52.46 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-420.23 +/- 56.46
Episode length: 47.84 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=-409.44 +/- 76.92
Episode length: 53.38 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 264      |
|    time_elapsed    | 2328     |
|    total_timesteps | 540672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=541000, episode_reward=-395.13 +/- 72.62
Episode length: 48.20 +/- 17.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.2        |
|    mean_reward          | -395        |
| time/                   |             |
|    total_timesteps      | 541000      |
| train/                  |             |
|    approx_kl            | 0.005842174 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.001       |
|    loss                 | 767         |
|    n_updates            | 1898        |
|    policy_gradient_loss | 0.000264    |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=541500, episode_reward=-421.02 +/- 63.55
Episode length: 50.30 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=-420.49 +/- 58.04
Episode length: 46.58 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=-426.99 +/- 55.65
Episode length: 49.78 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 265      |
|    time_elapsed    | 2336     |
|    total_timesteps | 542720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=543000, episode_reward=-416.77 +/- 63.90
Episode length: 49.24 +/- 16.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -417        |
| time/                   |             |
|    total_timesteps      | 543000      |
| train/                  |             |
|    approx_kl            | 0.009123144 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.001       |
|    loss                 | 920         |
|    n_updates            | 1899        |
|    policy_gradient_loss | 0.00267     |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=543500, episode_reward=-416.90 +/- 77.17
Episode length: 50.70 +/- 22.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=-415.73 +/- 68.17
Episode length: 52.26 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=-407.48 +/- 62.43
Episode length: 52.76 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 266      |
|    time_elapsed    | 2345     |
|    total_timesteps | 544768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=545000, episode_reward=-404.25 +/- 74.29
Episode length: 46.66 +/- 15.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.7       |
|    mean_reward          | -404       |
| time/                   |            |
|    total_timesteps      | 545000     |
| train/                  |            |
|    approx_kl            | 0.00778245 |
|    clip_fraction        | 0.0254     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.001      |
|    loss                 | 1.14e+03   |
|    n_updates            | 1900       |
|    policy_gradient_loss | 0.00015    |
|    value_loss           | 2.27e+03   |
----------------------------------------
Eval num_timesteps=545500, episode_reward=-394.55 +/- 76.69
Episode length: 47.86 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=-411.14 +/- 76.35
Episode length: 49.06 +/- 16.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=-396.18 +/- 84.58
Episode length: 47.82 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 267      |
|    time_elapsed    | 2353     |
|    total_timesteps | 546816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=547000, episode_reward=-416.23 +/- 56.12
Episode length: 51.22 +/- 16.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.2         |
|    mean_reward          | -416         |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 0.0062405444 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.137       |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.001        |
|    loss                 | 818          |
|    n_updates            | 1901         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 1.67e+03     |
------------------------------------------
Eval num_timesteps=547500, episode_reward=-407.19 +/- 70.90
Episode length: 49.24 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=-409.43 +/- 69.49
Episode length: 51.06 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=-408.90 +/- 68.71
Episode length: 48.98 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 268      |
|    time_elapsed    | 2361     |
|    total_timesteps | 548864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.59
Eval num_timesteps=549000, episode_reward=-417.54 +/- 62.20
Episode length: 50.42 +/- 17.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.4       |
|    mean_reward          | -418       |
| time/                   |            |
|    total_timesteps      | 549000     |
| train/                  |            |
|    approx_kl            | 0.06921186 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.269     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.001      |
|    loss                 | 993        |
|    n_updates            | 1902       |
|    policy_gradient_loss | 0.0234     |
|    value_loss           | 1.98e+03   |
----------------------------------------
Eval num_timesteps=549500, episode_reward=-417.65 +/- 58.09
Episode length: 49.26 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-412.78 +/- 81.86
Episode length: 51.82 +/- 19.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=-419.70 +/- 63.55
Episode length: 52.20 +/- 19.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 269      |
|    time_elapsed    | 2370     |
|    total_timesteps | 550912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.89
Eval num_timesteps=551000, episode_reward=-413.85 +/- 68.43
Episode length: 49.14 +/- 14.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.1       |
|    mean_reward          | -414       |
| time/                   |            |
|    total_timesteps      | 551000     |
| train/                  |            |
|    approx_kl            | 0.44261047 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.001      |
|    loss                 | 954        |
|    n_updates            | 1903       |
|    policy_gradient_loss | 0.0519     |
|    value_loss           | 1.91e+03   |
----------------------------------------
Eval num_timesteps=551500, episode_reward=-395.46 +/- 89.65
Episode length: 50.46 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=-419.04 +/- 67.18
Episode length: 50.20 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=-424.15 +/- 66.85
Episode length: 51.56 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 270      |
|    time_elapsed    | 2379     |
|    total_timesteps | 552960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=553000, episode_reward=-408.19 +/- 69.37
Episode length: 49.20 +/- 15.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -408        |
| time/                   |             |
|    total_timesteps      | 553000      |
| train/                  |             |
|    approx_kl            | 0.014213308 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.001       |
|    loss                 | 964         |
|    n_updates            | 1904        |
|    policy_gradient_loss | 0.00272     |
|    value_loss           | 2.28e+03    |
-----------------------------------------
Eval num_timesteps=553500, episode_reward=-420.11 +/- 61.65
Episode length: 56.84 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=-411.81 +/- 73.10
Episode length: 51.16 +/- 18.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=-422.30 +/- 67.84
Episode length: 53.70 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=-419.93 +/- 62.90
Episode length: 48.48 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 271      |
|    time_elapsed    | 2390     |
|    total_timesteps | 555008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=555500, episode_reward=-405.80 +/- 73.24
Episode length: 56.58 +/- 21.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56.6         |
|    mean_reward          | -406         |
| time/                   |              |
|    total_timesteps      | 555500       |
| train/                  |              |
|    approx_kl            | 0.0070656613 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.642        |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+03     |
|    n_updates            | 1905         |
|    policy_gradient_loss | 0.000614     |
|    value_loss           | 2.1e+03      |
------------------------------------------
Eval num_timesteps=556000, episode_reward=-412.20 +/- 83.55
Episode length: 51.24 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=-430.91 +/- 56.35
Episode length: 52.10 +/- 14.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=-410.47 +/- 73.65
Episode length: 53.70 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 272      |
|    time_elapsed    | 2399     |
|    total_timesteps | 557056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=557500, episode_reward=-409.85 +/- 86.43
Episode length: 51.72 +/- 16.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.7        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 557500      |
| train/                  |             |
|    approx_kl            | 0.016614899 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.001       |
|    loss                 | 1.16e+03    |
|    n_updates            | 1906        |
|    policy_gradient_loss | 0.0027      |
|    value_loss           | 2.27e+03    |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=-400.28 +/- 70.58
Episode length: 48.66 +/- 13.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=-421.52 +/- 77.47
Episode length: 53.56 +/- 22.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=-413.00 +/- 59.67
Episode length: 50.60 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 273      |
|    time_elapsed    | 2408     |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=559500, episode_reward=-403.77 +/- 69.65
Episode length: 49.86 +/- 14.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -404        |
| time/                   |             |
|    total_timesteps      | 559500      |
| train/                  |             |
|    approx_kl            | 0.014885828 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.001       |
|    loss                 | 875         |
|    n_updates            | 1907        |
|    policy_gradient_loss | 0.00322     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=-424.04 +/- 52.48
Episode length: 54.86 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=-419.76 +/- 58.78
Episode length: 55.96 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=-413.28 +/- 64.17
Episode length: 50.26 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 274      |
|    time_elapsed    | 2417     |
|    total_timesteps | 561152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=561500, episode_reward=-404.50 +/- 59.13
Episode length: 45.92 +/- 16.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.9         |
|    mean_reward          | -404         |
| time/                   |              |
|    total_timesteps      | 561500       |
| train/                  |              |
|    approx_kl            | 0.0061903503 |
|    clip_fraction        | 0.0518       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.334       |
|    explained_variance   | 0.654        |
|    learning_rate        | 0.001        |
|    loss                 | 830          |
|    n_updates            | 1908         |
|    policy_gradient_loss | 0.000447     |
|    value_loss           | 1.81e+03     |
------------------------------------------
Eval num_timesteps=562000, episode_reward=-424.75 +/- 68.57
Episode length: 53.34 +/- 16.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=-415.77 +/- 68.38
Episode length: 49.52 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=-425.69 +/- 59.85
Episode length: 54.98 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 275      |
|    time_elapsed    | 2425     |
|    total_timesteps | 563200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=563500, episode_reward=-417.74 +/- 59.09
Episode length: 47.96 +/- 17.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48          |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 563500      |
| train/                  |             |
|    approx_kl            | 0.005723401 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.001       |
|    loss                 | 1.65e+03    |
|    n_updates            | 1909        |
|    policy_gradient_loss | 0.00217     |
|    value_loss           | 2.71e+03    |
-----------------------------------------
Eval num_timesteps=564000, episode_reward=-427.88 +/- 77.89
Episode length: 54.26 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=-423.20 +/- 58.49
Episode length: 51.24 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=-418.89 +/- 60.84
Episode length: 53.36 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 276      |
|    time_elapsed    | 2434     |
|    total_timesteps | 565248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=565500, episode_reward=-436.77 +/- 50.28
Episode length: 55.64 +/- 15.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.6        |
|    mean_reward          | -437        |
| time/                   |             |
|    total_timesteps      | 565500      |
| train/                  |             |
|    approx_kl            | 0.007886181 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.001       |
|    loss                 | 983         |
|    n_updates            | 1910        |
|    policy_gradient_loss | 0.00315     |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=566000, episode_reward=-403.24 +/- 66.24
Episode length: 49.50 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=-418.89 +/- 70.65
Episode length: 54.00 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=-409.13 +/- 73.42
Episode length: 54.04 +/- 21.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 277      |
|    time_elapsed    | 2443     |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=567500, episode_reward=-396.73 +/- 73.46
Episode length: 47.74 +/- 16.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.7        |
|    mean_reward          | -397        |
| time/                   |             |
|    total_timesteps      | 567500      |
| train/                  |             |
|    approx_kl            | 0.010018538 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.227      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.001       |
|    loss                 | 501         |
|    n_updates            | 1911        |
|    policy_gradient_loss | -0.00033    |
|    value_loss           | 1.81e+03    |
-----------------------------------------
Eval num_timesteps=568000, episode_reward=-399.63 +/- 76.60
Episode length: 50.30 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=-401.95 +/- 86.66
Episode length: 52.56 +/- 19.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=-408.96 +/- 72.71
Episode length: 50.66 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 278      |
|    time_elapsed    | 2452     |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=569500, episode_reward=-413.85 +/- 72.09
Episode length: 48.62 +/- 15.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 569500      |
| train/                  |             |
|    approx_kl            | 0.021858457 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.286      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.001       |
|    loss                 | 1.06e+03    |
|    n_updates            | 1912        |
|    policy_gradient_loss | 0.00692     |
|    value_loss           | 2.38e+03    |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=-388.91 +/- 85.87
Episode length: 49.64 +/- 14.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -389     |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=-413.46 +/- 69.00
Episode length: 45.62 +/- 10.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=-426.33 +/- 53.23
Episode length: 54.00 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 279      |
|    time_elapsed    | 2461     |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.04
Eval num_timesteps=571500, episode_reward=-408.43 +/- 79.42
Episode length: 49.76 +/- 15.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -408      |
| time/                   |           |
|    total_timesteps      | 571500    |
| train/                  |           |
|    approx_kl            | 0.5201763 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.553    |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.001     |
|    loss                 | 1.2e+03   |
|    n_updates            | 1913      |
|    policy_gradient_loss | 0.146     |
|    value_loss           | 2.08e+03  |
---------------------------------------
Eval num_timesteps=572000, episode_reward=-422.37 +/- 66.32
Episode length: 49.60 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=-422.42 +/- 58.19
Episode length: 51.66 +/- 16.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=-403.92 +/- 71.33
Episode length: 47.18 +/- 15.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.5     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 280      |
|    time_elapsed    | 2469     |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.89
Eval num_timesteps=573500, episode_reward=-409.59 +/- 69.38
Episode length: 47.26 +/- 14.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.3      |
|    mean_reward          | -410      |
| time/                   |           |
|    total_timesteps      | 573500    |
| train/                  |           |
|    approx_kl            | 0.4452467 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.42     |
|    explained_variance   | 0.639     |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+03  |
|    n_updates            | 1914      |
|    policy_gradient_loss | 0.08      |
|    value_loss           | 2.16e+03  |
---------------------------------------
Eval num_timesteps=574000, episode_reward=-419.12 +/- 66.28
Episode length: 50.58 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=-413.36 +/- 74.85
Episode length: 46.70 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=-409.64 +/- 78.84
Episode length: 52.84 +/- 19.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.2     |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 281      |
|    time_elapsed    | 2477     |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=575500, episode_reward=-405.05 +/- 66.25
Episode length: 45.92 +/- 16.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.9        |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 575500      |
| train/                  |             |
|    approx_kl            | 0.033542477 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.001       |
|    loss                 | 534         |
|    n_updates            | 1915        |
|    policy_gradient_loss | 0.00842     |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=576000, episode_reward=-413.65 +/- 73.15
Episode length: 46.58 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=-410.55 +/- 82.34
Episode length: 49.24 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=-413.25 +/- 74.36
Episode length: 45.94 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=-418.78 +/- 66.70
Episode length: 51.02 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 282      |
|    time_elapsed    | 2487     |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=578000, episode_reward=-404.09 +/- 75.79
Episode length: 46.64 +/- 16.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.6         |
|    mean_reward          | -404         |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0046693194 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.69         |
|    learning_rate        | 0.001        |
|    loss                 | 876          |
|    n_updates            | 1916         |
|    policy_gradient_loss | 0.000393     |
|    value_loss           | 1.82e+03     |
------------------------------------------
Eval num_timesteps=578500, episode_reward=-419.63 +/- 65.95
Episode length: 48.66 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=-423.34 +/- 63.12
Episode length: 53.10 +/- 19.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=-425.67 +/- 56.67
Episode length: 49.02 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 283      |
|    time_elapsed    | 2495     |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=580000, episode_reward=-410.96 +/- 63.97
Episode length: 48.10 +/- 13.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -411        |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.012884723 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.001       |
|    loss                 | 756         |
|    n_updates            | 1917        |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=580500, episode_reward=-415.83 +/- 61.80
Episode length: 51.82 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=-388.27 +/- 67.18
Episode length: 48.78 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=-417.97 +/- 74.37
Episode length: 49.94 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 284      |
|    time_elapsed    | 2504     |
|    total_timesteps | 581632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=582000, episode_reward=-401.30 +/- 87.02
Episode length: 51.36 +/- 16.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.4         |
|    mean_reward          | -401         |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0062925685 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.284       |
|    explained_variance   | 0.625        |
|    learning_rate        | 0.001        |
|    loss                 | 857          |
|    n_updates            | 1918         |
|    policy_gradient_loss | 0.00268      |
|    value_loss           | 1.88e+03     |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-408.35 +/- 63.15
Episode length: 49.88 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=-419.90 +/- 58.45
Episode length: 51.34 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=-405.84 +/- 70.88
Episode length: 49.96 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 285      |
|    time_elapsed    | 2512     |
|    total_timesteps | 583680   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=584000, episode_reward=-405.78 +/- 69.25
Episode length: 48.38 +/- 18.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.4        |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 584000      |
| train/                  |             |
|    approx_kl            | 0.007987345 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.001       |
|    loss                 | 637         |
|    n_updates            | 1920        |
|    policy_gradient_loss | 0.0035      |
|    value_loss           | 2.26e+03    |
-----------------------------------------
Eval num_timesteps=584500, episode_reward=-409.63 +/- 74.13
Episode length: 49.36 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=-398.62 +/- 82.51
Episode length: 48.26 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=-403.27 +/- 70.38
Episode length: 44.40 +/- 15.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 286      |
|    time_elapsed    | 2520     |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=586000, episode_reward=-410.20 +/- 66.67
Episode length: 45.44 +/- 13.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.4         |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 0.0048891916 |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.53         |
|    learning_rate        | 0.001        |
|    loss                 | 1.19e+03     |
|    n_updates            | 1921         |
|    policy_gradient_loss | 0.000625     |
|    value_loss           | 2.76e+03     |
------------------------------------------
Eval num_timesteps=586500, episode_reward=-410.05 +/- 65.71
Episode length: 49.16 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=-411.08 +/- 77.49
Episode length: 49.88 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=-414.17 +/- 65.75
Episode length: 51.00 +/- 15.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 287      |
|    time_elapsed    | 2529     |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=588000, episode_reward=-412.98 +/- 69.17
Episode length: 50.36 +/- 14.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.4       |
|    mean_reward          | -413       |
| time/                   |            |
|    total_timesteps      | 588000     |
| train/                  |            |
|    approx_kl            | 0.07214701 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.704     |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.001      |
|    loss                 | 1.24e+03   |
|    n_updates            | 1922       |
|    policy_gradient_loss | 0.00987    |
|    value_loss           | 2.08e+03   |
----------------------------------------
Eval num_timesteps=588500, episode_reward=-410.99 +/- 74.93
Episode length: 47.16 +/- 13.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=-428.57 +/- 59.23
Episode length: 48.96 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=-416.71 +/- 61.13
Episode length: 46.92 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 288      |
|    time_elapsed    | 2537     |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.33
Eval num_timesteps=590000, episode_reward=-417.17 +/- 61.69
Episode length: 45.58 +/- 13.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.6       |
|    mean_reward          | -417       |
| time/                   |            |
|    total_timesteps      | 590000     |
| train/                  |            |
|    approx_kl            | 0.16403387 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.001      |
|    loss                 | 1.19e+03   |
|    n_updates            | 1923       |
|    policy_gradient_loss | 0.118      |
|    value_loss           | 2.52e+03   |
----------------------------------------
Eval num_timesteps=590500, episode_reward=-403.49 +/- 71.51
Episode length: 50.40 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=-403.45 +/- 69.98
Episode length: 47.28 +/- 14.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=-408.45 +/- 62.08
Episode length: 45.94 +/- 19.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 289      |
|    time_elapsed    | 2545     |
|    total_timesteps | 591872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=592000, episode_reward=-422.40 +/- 51.17
Episode length: 52.26 +/- 15.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.3       |
|    mean_reward          | -422       |
| time/                   |            |
|    total_timesteps      | 592000     |
| train/                  |            |
|    approx_kl            | 0.06296416 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.708     |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.001      |
|    loss                 | 878        |
|    n_updates            | 1924       |
|    policy_gradient_loss | 0.0467     |
|    value_loss           | 3.04e+03   |
----------------------------------------
Eval num_timesteps=592500, episode_reward=-396.49 +/- 70.89
Episode length: 45.80 +/- 13.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=-398.39 +/- 68.96
Episode length: 48.76 +/- 15.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=-414.46 +/- 58.33
Episode length: 46.60 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 290      |
|    time_elapsed    | 2554     |
|    total_timesteps | 593920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=594000, episode_reward=-385.60 +/- 67.38
Episode length: 43.98 +/- 15.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44         |
|    mean_reward          | -386       |
| time/                   |            |
|    total_timesteps      | 594000     |
| train/                  |            |
|    approx_kl            | 0.01785857 |
|    clip_fraction        | 0.0938     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.542     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.001      |
|    loss                 | 1.03e+03   |
|    n_updates            | 1925       |
|    policy_gradient_loss | 0.00386    |
|    value_loss           | 2e+03      |
----------------------------------------
Eval num_timesteps=594500, episode_reward=-408.08 +/- 57.68
Episode length: 43.90 +/- 12.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=-400.34 +/- 65.98
Episode length: 48.72 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=-411.81 +/- 63.54
Episode length: 48.14 +/- 13.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 291      |
|    time_elapsed    | 2562     |
|    total_timesteps | 595968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=596000, episode_reward=-420.27 +/- 58.92
Episode length: 49.26 +/- 15.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.3       |
|    mean_reward          | -420       |
| time/                   |            |
|    total_timesteps      | 596000     |
| train/                  |            |
|    approx_kl            | 0.01230461 |
|    clip_fraction        | 0.0352     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.001      |
|    loss                 | 1.04e+03   |
|    n_updates            | 1926       |
|    policy_gradient_loss | 0.00826    |
|    value_loss           | 2.43e+03   |
----------------------------------------
Eval num_timesteps=596500, episode_reward=-407.08 +/- 64.18
Episode length: 52.18 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=-408.65 +/- 60.47
Episode length: 46.16 +/- 13.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=-395.05 +/- 71.63
Episode length: 45.04 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-408.67 +/- 73.28
Episode length: 48.88 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 292      |
|    time_elapsed    | 2572     |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=598500, episode_reward=-399.16 +/- 62.79
Episode length: 48.66 +/- 16.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -399        |
| time/                   |             |
|    total_timesteps      | 598500      |
| train/                  |             |
|    approx_kl            | 0.017173849 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.344      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.001       |
|    loss                 | 1.18e+03    |
|    n_updates            | 1927        |
|    policy_gradient_loss | 0.00502     |
|    value_loss           | 1.98e+03    |
-----------------------------------------
Eval num_timesteps=599000, episode_reward=-417.02 +/- 55.43
Episode length: 49.02 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=-404.93 +/- 61.29
Episode length: 49.60 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-424.82 +/- 56.78
Episode length: 48.54 +/- 14.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 293      |
|    time_elapsed    | 2580     |
|    total_timesteps | 600064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=600500, episode_reward=-412.42 +/- 80.25
Episode length: 46.42 +/- 12.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.4        |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 600500      |
| train/                  |             |
|    approx_kl            | 0.015535099 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.001       |
|    loss                 | 955         |
|    n_updates            | 1928        |
|    policy_gradient_loss | 0.00224     |
|    value_loss           | 1.57e+03    |
-----------------------------------------
Eval num_timesteps=601000, episode_reward=-413.48 +/- 77.88
Episode length: 50.24 +/- 16.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 601000   |
---------------------------------
Eval num_timesteps=601500, episode_reward=-396.35 +/- 72.55
Episode length: 43.66 +/- 12.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.7     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 601500   |
---------------------------------
Eval num_timesteps=602000, episode_reward=-423.07 +/- 59.00
Episode length: 49.68 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 294      |
|    time_elapsed    | 2589     |
|    total_timesteps | 602112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=602500, episode_reward=-413.74 +/- 72.11
Episode length: 52.96 +/- 21.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53          |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 602500      |
| train/                  |             |
|    approx_kl            | 0.032178465 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.498      |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.001       |
|    loss                 | 700         |
|    n_updates            | 1929        |
|    policy_gradient_loss | 0.0143      |
|    value_loss           | 1.49e+03    |
-----------------------------------------
Eval num_timesteps=603000, episode_reward=-401.24 +/- 86.08
Episode length: 49.38 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 603000   |
---------------------------------
Eval num_timesteps=603500, episode_reward=-418.90 +/- 82.56
Episode length: 54.06 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 603500   |
---------------------------------
Eval num_timesteps=604000, episode_reward=-424.72 +/- 52.92
Episode length: 50.24 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 295      |
|    time_elapsed    | 2598     |
|    total_timesteps | 604160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.60
Eval num_timesteps=604500, episode_reward=-426.93 +/- 70.69
Episode length: 47.70 +/- 14.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.7      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 604500    |
| train/                  |           |
|    approx_kl            | 0.2994371 |
|    clip_fraction        | 0.418     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.642    |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.001     |
|    loss                 | 869       |
|    n_updates            | 1930      |
|    policy_gradient_loss | 0.175     |
|    value_loss           | 1.57e+03  |
---------------------------------------
Eval num_timesteps=605000, episode_reward=-411.63 +/- 74.85
Episode length: 48.44 +/- 19.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
Eval num_timesteps=605500, episode_reward=-419.84 +/- 54.04
Episode length: 50.70 +/- 13.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 605500   |
---------------------------------
Eval num_timesteps=606000, episode_reward=-422.93 +/- 64.95
Episode length: 50.66 +/- 14.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 296      |
|    time_elapsed    | 2606     |
|    total_timesteps | 606208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.40
Eval num_timesteps=606500, episode_reward=-433.40 +/- 66.87
Episode length: 53.86 +/- 22.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.9       |
|    mean_reward          | -433       |
| time/                   |            |
|    total_timesteps      | 606500     |
| train/                  |            |
|    approx_kl            | 0.19874632 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.415     |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.001      |
|    loss                 | 978        |
|    n_updates            | 1931       |
|    policy_gradient_loss | 0.0399     |
|    value_loss           | 1.72e+03   |
----------------------------------------
Eval num_timesteps=607000, episode_reward=-418.18 +/- 67.78
Episode length: 50.78 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 607000   |
---------------------------------
Eval num_timesteps=607500, episode_reward=-414.18 +/- 62.07
Episode length: 49.46 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 607500   |
---------------------------------
Eval num_timesteps=608000, episode_reward=-420.23 +/- 57.16
Episode length: 51.50 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46       |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 297      |
|    time_elapsed    | 2615     |
|    total_timesteps | 608256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=608500, episode_reward=-430.36 +/- 64.30
Episode length: 51.84 +/- 14.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.8        |
|    mean_reward          | -430        |
| time/                   |             |
|    total_timesteps      | 608500      |
| train/                  |             |
|    approx_kl            | 0.014751052 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.001       |
|    loss                 | 1.07e+03    |
|    n_updates            | 1932        |
|    policy_gradient_loss | 0.00159     |
|    value_loss           | 2.27e+03    |
-----------------------------------------
Eval num_timesteps=609000, episode_reward=-412.33 +/- 78.11
Episode length: 50.02 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 609000   |
---------------------------------
Eval num_timesteps=609500, episode_reward=-408.11 +/- 75.20
Episode length: 48.96 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 609500   |
---------------------------------
Eval num_timesteps=610000, episode_reward=-415.83 +/- 84.14
Episode length: 50.56 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 298      |
|    time_elapsed    | 2623     |
|    total_timesteps | 610304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=610500, episode_reward=-421.30 +/- 49.64
Episode length: 53.98 +/- 16.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54           |
|    mean_reward          | -421         |
| time/                   |              |
|    total_timesteps      | 610500       |
| train/                  |              |
|    approx_kl            | 0.0058329175 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.233       |
|    explained_variance   | 0.719        |
|    learning_rate        | 0.001        |
|    loss                 | 898          |
|    n_updates            | 1933         |
|    policy_gradient_loss | 0.00102      |
|    value_loss           | 2.08e+03     |
------------------------------------------
Eval num_timesteps=611000, episode_reward=-426.64 +/- 67.03
Episode length: 51.86 +/- 19.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 611000   |
---------------------------------
Eval num_timesteps=611500, episode_reward=-426.71 +/- 62.95
Episode length: 45.50 +/- 18.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 611500   |
---------------------------------
Eval num_timesteps=612000, episode_reward=-408.74 +/- 71.61
Episode length: 49.42 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 299      |
|    time_elapsed    | 2632     |
|    total_timesteps | 612352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=612500, episode_reward=-400.83 +/- 68.41
Episode length: 49.76 +/- 13.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.8         |
|    mean_reward          | -401         |
| time/                   |              |
|    total_timesteps      | 612500       |
| train/                  |              |
|    approx_kl            | 0.0077178543 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.238       |
|    explained_variance   | 0.761        |
|    learning_rate        | 0.001        |
|    loss                 | 639          |
|    n_updates            | 1934         |
|    policy_gradient_loss | -1.63e-05    |
|    value_loss           | 1.55e+03     |
------------------------------------------
Eval num_timesteps=613000, episode_reward=-417.65 +/- 81.77
Episode length: 48.42 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 613000   |
---------------------------------
Eval num_timesteps=613500, episode_reward=-429.14 +/- 65.10
Episode length: 50.28 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 613500   |
---------------------------------
Eval num_timesteps=614000, episode_reward=-423.60 +/- 68.71
Episode length: 49.96 +/- 15.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -428     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 300      |
|    time_elapsed    | 2640     |
|    total_timesteps | 614400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=614500, episode_reward=-414.05 +/- 62.78
Episode length: 50.70 +/- 17.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.7       |
|    mean_reward          | -414       |
| time/                   |            |
|    total_timesteps      | 614500     |
| train/                  |            |
|    approx_kl            | 0.00493242 |
|    clip_fraction        | 0.0398     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.324     |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.001      |
|    loss                 | 940        |
|    n_updates            | 1935       |
|    policy_gradient_loss | 0.000606   |
|    value_loss           | 1.9e+03    |
----------------------------------------
Eval num_timesteps=615000, episode_reward=-408.76 +/- 93.23
Episode length: 49.40 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 615000   |
---------------------------------
Eval num_timesteps=615500, episode_reward=-405.36 +/- 70.28
Episode length: 45.76 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 615500   |
---------------------------------
Eval num_timesteps=616000, episode_reward=-418.34 +/- 76.12
Episode length: 50.82 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -436     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 301      |
|    time_elapsed    | 2649     |
|    total_timesteps | 616448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=616500, episode_reward=-423.74 +/- 70.15
Episode length: 47.54 +/- 16.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 616500      |
| train/                  |             |
|    approx_kl            | 0.005534699 |
|    clip_fraction        | 0.0576      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.001       |
|    loss                 | 1.38e+03    |
|    n_updates            | 1936        |
|    policy_gradient_loss | 0.00585     |
|    value_loss           | 1.76e+03    |
-----------------------------------------
Eval num_timesteps=617000, episode_reward=-396.65 +/- 81.75
Episode length: 48.66 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 617000   |
---------------------------------
Eval num_timesteps=617500, episode_reward=-424.24 +/- 62.11
Episode length: 49.18 +/- 14.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 617500   |
---------------------------------
Eval num_timesteps=618000, episode_reward=-425.50 +/- 72.17
Episode length: 52.16 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 302      |
|    time_elapsed    | 2657     |
|    total_timesteps | 618496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=618500, episode_reward=-417.71 +/- 75.42
Episode length: 47.02 +/- 16.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47          |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 618500      |
| train/                  |             |
|    approx_kl            | 0.032750554 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.001       |
|    loss                 | 884         |
|    n_updates            | 1937        |
|    policy_gradient_loss | 0.0679      |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=619000, episode_reward=-393.17 +/- 68.41
Episode length: 47.02 +/- 14.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 619000   |
---------------------------------
Eval num_timesteps=619500, episode_reward=-396.19 +/- 91.72
Episode length: 46.58 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 619500   |
---------------------------------
Eval num_timesteps=620000, episode_reward=-432.14 +/- 61.30
Episode length: 51.32 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=620500, episode_reward=-414.78 +/- 65.21
Episode length: 51.22 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 620500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 303      |
|    time_elapsed    | 2667     |
|    total_timesteps | 620544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.33
Eval num_timesteps=621000, episode_reward=-422.73 +/- 58.29
Episode length: 49.22 +/- 18.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.2       |
|    mean_reward          | -423       |
| time/                   |            |
|    total_timesteps      | 621000     |
| train/                  |            |
|    approx_kl            | 0.16527948 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.771     |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.001      |
|    loss                 | 916        |
|    n_updates            | 1938       |
|    policy_gradient_loss | 0.0876     |
|    value_loss           | 1.65e+03   |
----------------------------------------
Eval num_timesteps=621500, episode_reward=-410.40 +/- 67.19
Episode length: 47.66 +/- 15.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 621500   |
---------------------------------
Eval num_timesteps=622000, episode_reward=-427.28 +/- 54.86
Episode length: 49.34 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 622000   |
---------------------------------
Eval num_timesteps=622500, episode_reward=-434.23 +/- 55.47
Episode length: 50.80 +/- 20.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 622500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.1     |
|    ep_rew_mean     | -426     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 304      |
|    time_elapsed    | 2675     |
|    total_timesteps | 622592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.19
Eval num_timesteps=623000, episode_reward=-410.00 +/- 77.08
Episode length: 47.18 +/- 15.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 623000      |
| train/                  |             |
|    approx_kl            | 0.095493846 |
|    clip_fraction        | 0.398       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.001       |
|    loss                 | 833         |
|    n_updates            | 1939        |
|    policy_gradient_loss | 0.0291      |
|    value_loss           | 2.12e+03    |
-----------------------------------------
Eval num_timesteps=623500, episode_reward=-415.53 +/- 64.68
Episode length: 47.04 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 623500   |
---------------------------------
Eval num_timesteps=624000, episode_reward=-403.19 +/- 70.60
Episode length: 47.70 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
Eval num_timesteps=624500, episode_reward=-417.54 +/- 63.00
Episode length: 50.20 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 624500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 305      |
|    time_elapsed    | 2683     |
|    total_timesteps | 624640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=625000, episode_reward=-409.62 +/- 75.06
Episode length: 47.98 +/- 15.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48          |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.019870387 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.001       |
|    loss                 | 658         |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.00981     |
|    value_loss           | 1.28e+03    |
-----------------------------------------
Eval num_timesteps=625500, episode_reward=-421.39 +/- 63.67
Episode length: 51.04 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 625500   |
---------------------------------
Eval num_timesteps=626000, episode_reward=-406.20 +/- 64.53
Episode length: 49.52 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 626000   |
---------------------------------
Eval num_timesteps=626500, episode_reward=-403.70 +/- 82.94
Episode length: 49.42 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 626500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 306      |
|    time_elapsed    | 2692     |
|    total_timesteps | 626688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.85
Eval num_timesteps=627000, episode_reward=-406.73 +/- 88.88
Episode length: 50.42 +/- 17.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -407      |
| time/                   |           |
|    total_timesteps      | 627000    |
| train/                  |           |
|    approx_kl            | 0.3750228 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.497    |
|    explained_variance   | 0.671     |
|    learning_rate        | 0.001     |
|    loss                 | 1.04e+03  |
|    n_updates            | 1941      |
|    policy_gradient_loss | 0.061     |
|    value_loss           | 1.83e+03  |
---------------------------------------
Eval num_timesteps=627500, episode_reward=-421.16 +/- 76.78
Episode length: 51.48 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 627500   |
---------------------------------
Eval num_timesteps=628000, episode_reward=-408.12 +/- 68.17
Episode length: 48.18 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 628000   |
---------------------------------
Eval num_timesteps=628500, episode_reward=-413.38 +/- 78.25
Episode length: 49.80 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 628500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 307      |
|    time_elapsed    | 2701     |
|    total_timesteps | 628736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.02
Eval num_timesteps=629000, episode_reward=-426.68 +/- 57.23
Episode length: 51.28 +/- 18.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.3      |
|    mean_reward          | -427      |
| time/                   |           |
|    total_timesteps      | 629000    |
| train/                  |           |
|    approx_kl            | 0.5102372 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.571    |
|    explained_variance   | 0.668     |
|    learning_rate        | 0.001     |
|    loss                 | 947       |
|    n_updates            | 1942      |
|    policy_gradient_loss | 0.178     |
|    value_loss           | 1.87e+03  |
---------------------------------------
Eval num_timesteps=629500, episode_reward=-425.58 +/- 75.58
Episode length: 56.52 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.5     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 629500   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-426.19 +/- 54.89
Episode length: 49.74 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
Eval num_timesteps=630500, episode_reward=-427.34 +/- 53.67
Episode length: 51.30 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 630500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 308      |
|    time_elapsed    | 2709     |
|    total_timesteps | 630784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=631000, episode_reward=-409.17 +/- 80.02
Episode length: 44.60 +/- 13.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44.6       |
|    mean_reward          | -409       |
| time/                   |            |
|    total_timesteps      | 631000     |
| train/                  |            |
|    approx_kl            | 0.03621512 |
|    clip_fraction        | 0.0508     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.291     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.001      |
|    loss                 | 1.13e+03   |
|    n_updates            | 1943       |
|    policy_gradient_loss | -0.000935  |
|    value_loss           | 2.13e+03   |
----------------------------------------
Eval num_timesteps=631500, episode_reward=-428.44 +/- 65.38
Episode length: 54.34 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 631500   |
---------------------------------
Eval num_timesteps=632000, episode_reward=-418.19 +/- 71.84
Episode length: 51.18 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 632000   |
---------------------------------
Eval num_timesteps=632500, episode_reward=-421.87 +/- 72.99
Episode length: 56.12 +/- 19.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 632500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 309      |
|    time_elapsed    | 2718     |
|    total_timesteps | 632832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=633000, episode_reward=-406.92 +/- 66.10
Episode length: 46.78 +/- 15.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.8         |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 633000       |
| train/                  |              |
|    approx_kl            | 0.0053124637 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.243       |
|    explained_variance   | 0.72         |
|    learning_rate        | 0.001        |
|    loss                 | 826          |
|    n_updates            | 1944         |
|    policy_gradient_loss | 0.00175      |
|    value_loss           | 1.98e+03     |
------------------------------------------
Eval num_timesteps=633500, episode_reward=-409.31 +/- 76.47
Episode length: 50.20 +/- 17.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 633500   |
---------------------------------
Eval num_timesteps=634000, episode_reward=-414.74 +/- 71.91
Episode length: 47.28 +/- 12.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 634000   |
---------------------------------
Eval num_timesteps=634500, episode_reward=-408.04 +/- 76.99
Episode length: 50.38 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 310      |
|    time_elapsed    | 2726     |
|    total_timesteps | 634880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=635000, episode_reward=-427.38 +/- 52.23
Episode length: 47.00 +/- 15.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47           |
|    mean_reward          | -427         |
| time/                   |              |
|    total_timesteps      | 635000       |
| train/                  |              |
|    approx_kl            | 0.0054600234 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0.748        |
|    learning_rate        | 0.001        |
|    loss                 | 897          |
|    n_updates            | 1945         |
|    policy_gradient_loss | 0.000501     |
|    value_loss           | 1.58e+03     |
------------------------------------------
Eval num_timesteps=635500, episode_reward=-409.25 +/- 67.75
Episode length: 49.50 +/- 16.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 635500   |
---------------------------------
Eval num_timesteps=636000, episode_reward=-404.13 +/- 81.08
Episode length: 46.82 +/- 20.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
Eval num_timesteps=636500, episode_reward=-405.56 +/- 79.40
Episode length: 51.68 +/- 19.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 636500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 311      |
|    time_elapsed    | 2734     |
|    total_timesteps | 636928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=637000, episode_reward=-424.55 +/- 56.54
Episode length: 49.46 +/- 14.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 637000      |
| train/                  |             |
|    approx_kl            | 0.007003227 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.001       |
|    loss                 | 551         |
|    n_updates            | 1946        |
|    policy_gradient_loss | 0.00348     |
|    value_loss           | 1.43e+03    |
-----------------------------------------
Eval num_timesteps=637500, episode_reward=-413.09 +/- 67.17
Episode length: 47.70 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 637500   |
---------------------------------
Eval num_timesteps=638000, episode_reward=-407.32 +/- 69.41
Episode length: 49.82 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 638000   |
---------------------------------
Eval num_timesteps=638500, episode_reward=-415.56 +/- 79.24
Episode length: 52.22 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 638500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 312      |
|    time_elapsed    | 2743     |
|    total_timesteps | 638976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.25
Eval num_timesteps=639000, episode_reward=-416.17 +/- 80.89
Episode length: 56.14 +/- 20.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 56.1       |
|    mean_reward          | -416       |
| time/                   |            |
|    total_timesteps      | 639000     |
| train/                  |            |
|    approx_kl            | 0.12305423 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.74      |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.001      |
|    loss                 | 816        |
|    n_updates            | 1947       |
|    policy_gradient_loss | 0.0764     |
|    value_loss           | 1.68e+03   |
----------------------------------------
Eval num_timesteps=639500, episode_reward=-431.10 +/- 64.25
Episode length: 50.50 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 639500   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-428.63 +/- 55.83
Episode length: 47.00 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=640500, episode_reward=-424.48 +/- 62.01
Episode length: 53.10 +/- 14.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 640500   |
---------------------------------
Eval num_timesteps=641000, episode_reward=-410.50 +/- 66.62
Episode length: 47.88 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 313      |
|    time_elapsed    | 2753     |
|    total_timesteps | 641024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.19
Eval num_timesteps=641500, episode_reward=-422.24 +/- 62.35
Episode length: 53.84 +/- 19.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.8       |
|    mean_reward          | -422       |
| time/                   |            |
|    total_timesteps      | 641500     |
| train/                  |            |
|    approx_kl            | 0.09549592 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.001      |
|    loss                 | 1.04e+03   |
|    n_updates            | 1948       |
|    policy_gradient_loss | 0.0542     |
|    value_loss           | 1.82e+03   |
----------------------------------------
Eval num_timesteps=642000, episode_reward=-421.61 +/- 61.91
Episode length: 47.32 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
Eval num_timesteps=642500, episode_reward=-413.07 +/- 67.46
Episode length: 53.26 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 642500   |
---------------------------------
Eval num_timesteps=643000, episode_reward=-404.55 +/- 90.83
Episode length: 49.32 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 314      |
|    time_elapsed    | 2762     |
|    total_timesteps | 643072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=643500, episode_reward=-415.29 +/- 64.29
Episode length: 51.14 +/- 16.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 643500      |
| train/                  |             |
|    approx_kl            | 0.030811446 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.001       |
|    loss                 | 759         |
|    n_updates            | 1949        |
|    policy_gradient_loss | 0.0223      |
|    value_loss           | 1.48e+03    |
-----------------------------------------
Eval num_timesteps=644000, episode_reward=-402.46 +/- 80.39
Episode length: 45.56 +/- 12.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 644000   |
---------------------------------
Eval num_timesteps=644500, episode_reward=-412.40 +/- 74.55
Episode length: 48.36 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 644500   |
---------------------------------
Eval num_timesteps=645000, episode_reward=-403.77 +/- 65.70
Episode length: 46.22 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 315      |
|    time_elapsed    | 2770     |
|    total_timesteps | 645120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=645500, episode_reward=-419.17 +/- 63.65
Episode length: 49.86 +/- 17.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 645500      |
| train/                  |             |
|    approx_kl            | 0.016668076 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.001       |
|    loss                 | 1.17e+03    |
|    n_updates            | 1950        |
|    policy_gradient_loss | 0.0128      |
|    value_loss           | 1.92e+03    |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=-429.05 +/- 52.46
Episode length: 49.18 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 646000   |
---------------------------------
Eval num_timesteps=646500, episode_reward=-407.83 +/- 64.60
Episode length: 51.70 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 646500   |
---------------------------------
Eval num_timesteps=647000, episode_reward=-417.97 +/- 58.32
Episode length: 52.50 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 316      |
|    time_elapsed    | 2779     |
|    total_timesteps | 647168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=647500, episode_reward=-431.46 +/- 53.15
Episode length: 51.02 +/- 18.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -431        |
| time/                   |             |
|    total_timesteps      | 647500      |
| train/                  |             |
|    approx_kl            | 0.039452106 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.73       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.001       |
|    loss                 | 1.07e+03    |
|    n_updates            | 1951        |
|    policy_gradient_loss | 0.0318      |
|    value_loss           | 2.38e+03    |
-----------------------------------------
Eval num_timesteps=648000, episode_reward=-411.45 +/- 66.15
Episode length: 52.46 +/- 14.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
Eval num_timesteps=648500, episode_reward=-408.26 +/- 77.75
Episode length: 52.68 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 648500   |
---------------------------------
Eval num_timesteps=649000, episode_reward=-402.03 +/- 84.51
Episode length: 48.36 +/- 14.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 317      |
|    time_elapsed    | 2787     |
|    total_timesteps | 649216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.27
Eval num_timesteps=649500, episode_reward=-413.28 +/- 77.29
Episode length: 50.12 +/- 16.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.1       |
|    mean_reward          | -413       |
| time/                   |            |
|    total_timesteps      | 649500     |
| train/                  |            |
|    approx_kl            | 0.13642386 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.001      |
|    loss                 | 908        |
|    n_updates            | 1952       |
|    policy_gradient_loss | 0.0773     |
|    value_loss           | 2e+03      |
----------------------------------------
Eval num_timesteps=650000, episode_reward=-406.09 +/- 75.12
Episode length: 49.28 +/- 17.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
Eval num_timesteps=650500, episode_reward=-424.37 +/- 63.13
Episode length: 53.98 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 650500   |
---------------------------------
Eval num_timesteps=651000, episode_reward=-404.41 +/- 81.00
Episode length: 48.60 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 318      |
|    time_elapsed    | 2796     |
|    total_timesteps | 651264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=651500, episode_reward=-404.40 +/- 74.03
Episode length: 48.06 +/- 15.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -404        |
| time/                   |             |
|    total_timesteps      | 651500      |
| train/                  |             |
|    approx_kl            | 0.058706995 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.001       |
|    loss                 | 809         |
|    n_updates            | 1953        |
|    policy_gradient_loss | 0.0399      |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=652000, episode_reward=-406.13 +/- 68.19
Episode length: 49.30 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 652000   |
---------------------------------
Eval num_timesteps=652500, episode_reward=-418.84 +/- 63.33
Episode length: 51.14 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 652500   |
---------------------------------
Eval num_timesteps=653000, episode_reward=-414.25 +/- 73.43
Episode length: 52.32 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 319      |
|    time_elapsed    | 2804     |
|    total_timesteps | 653312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=653500, episode_reward=-426.78 +/- 69.13
Episode length: 51.40 +/- 16.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.4        |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 653500      |
| train/                  |             |
|    approx_kl            | 0.019303221 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.428      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.001       |
|    loss                 | 944         |
|    n_updates            | 1954        |
|    policy_gradient_loss | 0.0109      |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=-417.97 +/- 73.32
Episode length: 48.02 +/- 16.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
Eval num_timesteps=654500, episode_reward=-421.03 +/- 60.43
Episode length: 47.60 +/- 11.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 654500   |
---------------------------------
Eval num_timesteps=655000, episode_reward=-425.39 +/- 75.84
Episode length: 51.00 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 320      |
|    time_elapsed    | 2813     |
|    total_timesteps | 655360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.81
Eval num_timesteps=655500, episode_reward=-429.34 +/- 67.45
Episode length: 56.96 +/- 16.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 57         |
|    mean_reward          | -429       |
| time/                   |            |
|    total_timesteps      | 655500     |
| train/                  |            |
|    approx_kl            | 0.20598571 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.535     |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.001      |
|    loss                 | 785        |
|    n_updates            | 1955       |
|    policy_gradient_loss | 0.0615     |
|    value_loss           | 1.8e+03    |
----------------------------------------
Eval num_timesteps=656000, episode_reward=-406.13 +/- 63.75
Episode length: 50.78 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 656000   |
---------------------------------
Eval num_timesteps=656500, episode_reward=-409.99 +/- 72.70
Episode length: 53.14 +/- 18.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 656500   |
---------------------------------
Eval num_timesteps=657000, episode_reward=-438.65 +/- 43.66
Episode length: 54.28 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 321      |
|    time_elapsed    | 2822     |
|    total_timesteps | 657408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.68
Eval num_timesteps=657500, episode_reward=-417.52 +/- 63.18
Episode length: 51.00 +/- 17.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51         |
|    mean_reward          | -418       |
| time/                   |            |
|    total_timesteps      | 657500     |
| train/                  |            |
|    approx_kl            | 0.34172654 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.616     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.001      |
|    loss                 | 776        |
|    n_updates            | 1956       |
|    policy_gradient_loss | 0.0928     |
|    value_loss           | 1.65e+03   |
----------------------------------------
Eval num_timesteps=658000, episode_reward=-402.31 +/- 69.88
Episode length: 48.16 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 658000   |
---------------------------------
Eval num_timesteps=658500, episode_reward=-409.96 +/- 70.97
Episode length: 50.78 +/- 19.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 658500   |
---------------------------------
Eval num_timesteps=659000, episode_reward=-403.97 +/- 70.69
Episode length: 48.74 +/- 17.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 322      |
|    time_elapsed    | 2831     |
|    total_timesteps | 659456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=659500, episode_reward=-402.80 +/- 74.45
Episode length: 49.32 +/- 18.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -403        |
| time/                   |             |
|    total_timesteps      | 659500      |
| train/                  |             |
|    approx_kl            | 0.040984206 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.469      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+03    |
|    n_updates            | 1957        |
|    policy_gradient_loss | 0.0131      |
|    value_loss           | 1.93e+03    |
-----------------------------------------
Eval num_timesteps=660000, episode_reward=-415.47 +/- 65.85
Episode length: 47.90 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=660500, episode_reward=-419.62 +/- 61.42
Episode length: 53.06 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 660500   |
---------------------------------
Eval num_timesteps=661000, episode_reward=-422.80 +/- 57.88
Episode length: 54.68 +/- 20.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=661500, episode_reward=-412.16 +/- 66.23
Episode length: 46.80 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 661500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 323      |
|    time_elapsed    | 2841     |
|    total_timesteps | 661504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=662000, episode_reward=-422.56 +/- 58.44
Episode length: 49.34 +/- 15.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 662000      |
| train/                  |             |
|    approx_kl            | 0.009441398 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.001       |
|    loss                 | 787         |
|    n_updates            | 1958        |
|    policy_gradient_loss | 0.00294     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=662500, episode_reward=-413.36 +/- 57.96
Episode length: 47.04 +/- 14.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 662500   |
---------------------------------
Eval num_timesteps=663000, episode_reward=-425.18 +/- 53.97
Episode length: 49.06 +/- 13.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 663000   |
---------------------------------
Eval num_timesteps=663500, episode_reward=-410.61 +/- 72.76
Episode length: 53.18 +/- 23.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 663500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 324      |
|    time_elapsed    | 2850     |
|    total_timesteps | 663552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=664000, episode_reward=-410.44 +/- 59.82
Episode length: 49.42 +/- 16.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.4        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 664000      |
| train/                  |             |
|    approx_kl            | 0.020174662 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.355      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.001       |
|    loss                 | 787         |
|    n_updates            | 1959        |
|    policy_gradient_loss | 0.00288     |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=664500, episode_reward=-429.31 +/- 73.62
Episode length: 52.68 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 664500   |
---------------------------------
Eval num_timesteps=665000, episode_reward=-416.26 +/- 63.25
Episode length: 47.82 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 665000   |
---------------------------------
Eval num_timesteps=665500, episode_reward=-416.35 +/- 76.41
Episode length: 45.76 +/- 14.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 665500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 325      |
|    time_elapsed    | 2858     |
|    total_timesteps | 665600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=666000, episode_reward=-413.60 +/- 65.80
Episode length: 47.96 +/- 15.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48           |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0074776197 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.315       |
|    explained_variance   | 0.712        |
|    learning_rate        | 0.001        |
|    loss                 | 557          |
|    n_updates            | 1960         |
|    policy_gradient_loss | 0.00445      |
|    value_loss           | 1.65e+03     |
------------------------------------------
Eval num_timesteps=666500, episode_reward=-419.45 +/- 64.06
Episode length: 48.30 +/- 14.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 666500   |
---------------------------------
Eval num_timesteps=667000, episode_reward=-412.36 +/- 64.20
Episode length: 47.80 +/- 19.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 667000   |
---------------------------------
Eval num_timesteps=667500, episode_reward=-403.39 +/- 84.25
Episode length: 49.40 +/- 15.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 667500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 326      |
|    time_elapsed    | 2866     |
|    total_timesteps | 667648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=668000, episode_reward=-419.36 +/- 54.83
Episode length: 47.52 +/- 17.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 668000      |
| train/                  |             |
|    approx_kl            | 0.004689364 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.424      |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.001       |
|    loss                 | 917         |
|    n_updates            | 1961        |
|    policy_gradient_loss | 0.00595     |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=668500, episode_reward=-421.43 +/- 59.39
Episode length: 51.04 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 668500   |
---------------------------------
Eval num_timesteps=669000, episode_reward=-412.27 +/- 64.23
Episode length: 45.12 +/- 15.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 669000   |
---------------------------------
Eval num_timesteps=669500, episode_reward=-400.25 +/- 65.48
Episode length: 51.56 +/- 16.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 669500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 327      |
|    time_elapsed    | 2875     |
|    total_timesteps | 669696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=670000, episode_reward=-399.25 +/- 75.03
Episode length: 51.04 +/- 14.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -399        |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.007041397 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.368      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.001       |
|    loss                 | 795         |
|    n_updates            | 1962        |
|    policy_gradient_loss | 0.000358    |
|    value_loss           | 1.91e+03    |
-----------------------------------------
Eval num_timesteps=670500, episode_reward=-412.91 +/- 68.84
Episode length: 47.62 +/- 15.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 670500   |
---------------------------------
Eval num_timesteps=671000, episode_reward=-391.22 +/- 79.15
Episode length: 47.24 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 671000   |
---------------------------------
Eval num_timesteps=671500, episode_reward=-412.42 +/- 64.42
Episode length: 49.26 +/- 14.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 671500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 328      |
|    time_elapsed    | 2883     |
|    total_timesteps | 671744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=672000, episode_reward=-401.93 +/- 71.66
Episode length: 50.58 +/- 17.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.6        |
|    mean_reward          | -402        |
| time/                   |             |
|    total_timesteps      | 672000      |
| train/                  |             |
|    approx_kl            | 0.005176048 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.308      |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.001       |
|    loss                 | 734         |
|    n_updates            | 1963        |
|    policy_gradient_loss | 0.00429     |
|    value_loss           | 1.99e+03    |
-----------------------------------------
Eval num_timesteps=672500, episode_reward=-406.33 +/- 65.03
Episode length: 51.04 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 672500   |
---------------------------------
Eval num_timesteps=673000, episode_reward=-404.27 +/- 72.85
Episode length: 50.68 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 673000   |
---------------------------------
Eval num_timesteps=673500, episode_reward=-403.69 +/- 68.17
Episode length: 48.14 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 673500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 329      |
|    time_elapsed    | 2892     |
|    total_timesteps | 673792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=674000, episode_reward=-417.10 +/- 69.70
Episode length: 46.30 +/- 16.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.3        |
|    mean_reward          | -417        |
| time/                   |             |
|    total_timesteps      | 674000      |
| train/                  |             |
|    approx_kl            | 0.006874442 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.001       |
|    loss                 | 603         |
|    n_updates            | 1964        |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=674500, episode_reward=-416.84 +/- 62.20
Episode length: 52.32 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 674500   |
---------------------------------
Eval num_timesteps=675000, episode_reward=-421.77 +/- 50.24
Episode length: 44.82 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 675000   |
---------------------------------
Eval num_timesteps=675500, episode_reward=-419.17 +/- 71.15
Episode length: 52.48 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 675500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 330      |
|    time_elapsed    | 2901     |
|    total_timesteps | 675840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=676000, episode_reward=-431.15 +/- 58.51
Episode length: 48.00 +/- 13.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48          |
|    mean_reward          | -431        |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.009095184 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.477      |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.001       |
|    loss                 | 708         |
|    n_updates            | 1965        |
|    policy_gradient_loss | 0.00429     |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=676500, episode_reward=-415.83 +/- 73.81
Episode length: 49.72 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 676500   |
---------------------------------
Eval num_timesteps=677000, episode_reward=-404.59 +/- 88.01
Episode length: 51.20 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 677000   |
---------------------------------
Eval num_timesteps=677500, episode_reward=-425.05 +/- 65.88
Episode length: 51.72 +/- 18.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 677500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 331      |
|    time_elapsed    | 2909     |
|    total_timesteps | 677888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=678000, episode_reward=-406.43 +/- 73.70
Episode length: 47.48 +/- 14.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.5         |
|    mean_reward          | -406         |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0062667294 |
|    clip_fraction        | 0.0681       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.413       |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.001        |
|    loss                 | 941          |
|    n_updates            | 1966         |
|    policy_gradient_loss | 0.00451      |
|    value_loss           | 1.77e+03     |
------------------------------------------
Eval num_timesteps=678500, episode_reward=-411.14 +/- 77.17
Episode length: 49.08 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 678500   |
---------------------------------
Eval num_timesteps=679000, episode_reward=-420.98 +/- 68.87
Episode length: 50.88 +/- 18.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 679000   |
---------------------------------
Eval num_timesteps=679500, episode_reward=-417.91 +/- 74.91
Episode length: 47.26 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 679500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.3     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 332      |
|    time_elapsed    | 2918     |
|    total_timesteps | 679936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=680000, episode_reward=-402.10 +/- 69.03
Episode length: 53.52 +/- 17.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.5        |
|    mean_reward          | -402        |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.011585139 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.445      |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.001       |
|    loss                 | 840         |
|    n_updates            | 1967        |
|    policy_gradient_loss | 0.00542     |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=680500, episode_reward=-385.98 +/- 83.31
Episode length: 51.08 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 680500   |
---------------------------------
Eval num_timesteps=681000, episode_reward=-404.23 +/- 64.59
Episode length: 59.54 +/- 70.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.5     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 681000   |
---------------------------------
Eval num_timesteps=681500, episode_reward=-401.96 +/- 80.83
Episode length: 56.24 +/- 23.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.2     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 681500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.2     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 333      |
|    time_elapsed    | 2927     |
|    total_timesteps | 681984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=682000, episode_reward=-394.26 +/- 99.28
Episode length: 50.28 +/- 18.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.3        |
|    mean_reward          | -394        |
| time/                   |             |
|    total_timesteps      | 682000      |
| train/                  |             |
|    approx_kl            | 0.009380236 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.001       |
|    loss                 | 660         |
|    n_updates            | 1968        |
|    policy_gradient_loss | 0.00849     |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=682500, episode_reward=-401.85 +/- 70.39
Episode length: 50.84 +/- 19.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 682500   |
---------------------------------
Eval num_timesteps=683000, episode_reward=-419.94 +/- 57.51
Episode length: 53.20 +/- 21.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
Eval num_timesteps=683500, episode_reward=-379.96 +/- 73.43
Episode length: 49.82 +/- 24.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 683500   |
---------------------------------
Eval num_timesteps=684000, episode_reward=-417.09 +/- 65.93
Episode length: 50.20 +/- 15.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.1     |
|    ep_rew_mean     | -395     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 334      |
|    time_elapsed    | 2938     |
|    total_timesteps | 684032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=684500, episode_reward=-380.17 +/- 65.77
Episode length: 50.72 +/- 21.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -380        |
| time/                   |             |
|    total_timesteps      | 684500      |
| train/                  |             |
|    approx_kl            | 0.006492518 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.476      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.001       |
|    loss                 | 729         |
|    n_updates            | 1969        |
|    policy_gradient_loss | -0.000371   |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=685000, episode_reward=-391.94 +/- 75.81
Episode length: 54.50 +/- 22.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
Eval num_timesteps=685500, episode_reward=-400.83 +/- 63.63
Episode length: 54.56 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 685500   |
---------------------------------
Eval num_timesteps=686000, episode_reward=-407.21 +/- 77.19
Episode length: 51.10 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.4     |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 335      |
|    time_elapsed    | 2947     |
|    total_timesteps | 686080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=686500, episode_reward=-388.78 +/- 68.24
Episode length: 55.32 +/- 37.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.3        |
|    mean_reward          | -389        |
| time/                   |             |
|    total_timesteps      | 686500      |
| train/                  |             |
|    approx_kl            | 0.007363349 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.001       |
|    loss                 | 627         |
|    n_updates            | 1970        |
|    policy_gradient_loss | 0.00542     |
|    value_loss           | 1.1e+03     |
-----------------------------------------
Eval num_timesteps=687000, episode_reward=-400.99 +/- 56.73
Episode length: 58.26 +/- 28.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 687000   |
---------------------------------
Eval num_timesteps=687500, episode_reward=-391.93 +/- 67.44
Episode length: 57.70 +/- 29.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.7     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 687500   |
---------------------------------
Eval num_timesteps=688000, episode_reward=-398.87 +/- 61.39
Episode length: 50.56 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67       |
|    ep_rew_mean     | -384     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 336      |
|    time_elapsed    | 2956     |
|    total_timesteps | 688128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=688500, episode_reward=-416.60 +/- 55.11
Episode length: 62.62 +/- 68.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 62.6         |
|    mean_reward          | -417         |
| time/                   |              |
|    total_timesteps      | 688500       |
| train/                  |              |
|    approx_kl            | 0.0059882095 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.831       |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.001        |
|    loss                 | 419          |
|    n_updates            | 1971         |
|    policy_gradient_loss | 0.000875     |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=689000, episode_reward=-406.83 +/- 69.48
Episode length: 64.00 +/- 68.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64       |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 689000   |
---------------------------------
Eval num_timesteps=689500, episode_reward=-406.34 +/- 65.31
Episode length: 54.78 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 689500   |
---------------------------------
Eval num_timesteps=690000, episode_reward=-405.35 +/- 66.48
Episode length: 83.64 +/- 112.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.6     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 337      |
|    time_elapsed    | 2967     |
|    total_timesteps | 690176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=690500, episode_reward=-411.73 +/- 69.59
Episode length: 60.02 +/- 68.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60          |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 690500      |
| train/                  |             |
|    approx_kl            | 0.014926538 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.5        |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.001       |
|    loss                 | 835         |
|    n_updates            | 1972        |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 1.65e+03    |
-----------------------------------------
Eval num_timesteps=691000, episode_reward=-421.20 +/- 67.39
Episode length: 51.62 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 691000   |
---------------------------------
Eval num_timesteps=691500, episode_reward=-412.72 +/- 76.75
Episode length: 55.72 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 691500   |
---------------------------------
Eval num_timesteps=692000, episode_reward=-406.63 +/- 67.62
Episode length: 53.58 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 58.8     |
|    ep_rew_mean     | -395     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 338      |
|    time_elapsed    | 2977     |
|    total_timesteps | 692224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=692500, episode_reward=-407.40 +/- 62.86
Episode length: 50.24 +/- 19.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.2        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 692500      |
| train/                  |             |
|    approx_kl            | 0.018268758 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.001       |
|    loss                 | 1.05e+03    |
|    n_updates            | 1973        |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=693000, episode_reward=-399.35 +/- 69.80
Episode length: 53.30 +/- 27.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 693000   |
---------------------------------
Eval num_timesteps=693500, episode_reward=-420.00 +/- 68.52
Episode length: 60.06 +/- 68.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 693500   |
---------------------------------
Eval num_timesteps=694000, episode_reward=-393.78 +/- 79.08
Episode length: 49.44 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.1     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 339      |
|    time_elapsed    | 2986     |
|    total_timesteps | 694272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=694500, episode_reward=-414.18 +/- 66.16
Episode length: 49.78 +/- 16.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -414      |
| time/                   |           |
|    total_timesteps      | 694500    |
| train/                  |           |
|    approx_kl            | 0.0338205 |
|    clip_fraction        | 0.109     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.667    |
|    explained_variance   | 0.689     |
|    learning_rate        | 0.001     |
|    loss                 | 844       |
|    n_updates            | 1974      |
|    policy_gradient_loss | 0.0163    |
|    value_loss           | 1.71e+03  |
---------------------------------------
Eval num_timesteps=695000, episode_reward=-427.10 +/- 65.73
Episode length: 51.42 +/- 16.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
Eval num_timesteps=695500, episode_reward=-414.74 +/- 75.07
Episode length: 50.78 +/- 24.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 695500   |
---------------------------------
Eval num_timesteps=696000, episode_reward=-410.76 +/- 62.21
Episode length: 50.30 +/- 23.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 340      |
|    time_elapsed    | 2994     |
|    total_timesteps | 696320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=696500, episode_reward=-427.53 +/- 69.03
Episode length: 54.36 +/- 20.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.4        |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 696500      |
| train/                  |             |
|    approx_kl            | 0.032753218 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.001       |
|    loss                 | 876         |
|    n_updates            | 1975        |
|    policy_gradient_loss | 0.0106      |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=697000, episode_reward=-411.49 +/- 70.96
Episode length: 51.34 +/- 19.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 697000   |
---------------------------------
Eval num_timesteps=697500, episode_reward=-426.02 +/- 62.18
Episode length: 56.20 +/- 22.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 697500   |
---------------------------------
Eval num_timesteps=698000, episode_reward=-402.16 +/- 79.51
Episode length: 42.88 +/- 11.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.9     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53       |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 341      |
|    time_elapsed    | 3003     |
|    total_timesteps | 698368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=698500, episode_reward=-431.11 +/- 63.37
Episode length: 54.86 +/- 16.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 54.9       |
|    mean_reward          | -431       |
| time/                   |            |
|    total_timesteps      | 698500     |
| train/                  |            |
|    approx_kl            | 0.06182273 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.74      |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.001      |
|    loss                 | 1.06e+03   |
|    n_updates            | 1976       |
|    policy_gradient_loss | 0.0301     |
|    value_loss           | 1.87e+03   |
----------------------------------------
Eval num_timesteps=699000, episode_reward=-431.45 +/- 52.56
Episode length: 52.58 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 699000   |
---------------------------------
Eval num_timesteps=699500, episode_reward=-417.37 +/- 81.73
Episode length: 49.70 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 699500   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-416.37 +/- 67.33
Episode length: 51.18 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 342      |
|    time_elapsed    | 3012     |
|    total_timesteps | 700416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=700500, episode_reward=-407.56 +/- 74.83
Episode length: 48.92 +/- 18.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.9        |
|    mean_reward          | -408        |
| time/                   |             |
|    total_timesteps      | 700500      |
| train/                  |             |
|    approx_kl            | 0.056085333 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.001       |
|    loss                 | 1.06e+03    |
|    n_updates            | 1977        |
|    policy_gradient_loss | 0.0499      |
|    value_loss           | 2.09e+03    |
-----------------------------------------
Eval num_timesteps=701000, episode_reward=-410.40 +/- 77.90
Episode length: 53.98 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 701000   |
---------------------------------
Eval num_timesteps=701500, episode_reward=-415.27 +/- 66.05
Episode length: 51.90 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 701500   |
---------------------------------
Eval num_timesteps=702000, episode_reward=-422.41 +/- 59.37
Episode length: 47.94 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 343      |
|    time_elapsed    | 3021     |
|    total_timesteps | 702464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=702500, episode_reward=-417.52 +/- 67.48
Episode length: 53.00 +/- 17.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53          |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 702500      |
| train/                  |             |
|    approx_kl            | 0.016784964 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.001       |
|    loss                 | 1.17e+03    |
|    n_updates            | 1978        |
|    policy_gradient_loss | 0.00678     |
|    value_loss           | 2.36e+03    |
-----------------------------------------
Eval num_timesteps=703000, episode_reward=-415.15 +/- 66.48
Episode length: 44.84 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 703000   |
---------------------------------
Eval num_timesteps=703500, episode_reward=-416.25 +/- 73.47
Episode length: 52.28 +/- 19.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 703500   |
---------------------------------
Eval num_timesteps=704000, episode_reward=-414.51 +/- 77.07
Episode length: 47.32 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=704500, episode_reward=-431.60 +/- 64.35
Episode length: 50.44 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 704500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -415     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 344      |
|    time_elapsed    | 3031     |
|    total_timesteps | 704512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=705000, episode_reward=-413.14 +/- 64.95
Episode length: 56.16 +/- 18.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.2        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 705000      |
| train/                  |             |
|    approx_kl            | 0.015070029 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.001       |
|    loss                 | 853         |
|    n_updates            | 1979        |
|    policy_gradient_loss | 0.0105      |
|    value_loss           | 1.36e+03    |
-----------------------------------------
Eval num_timesteps=705500, episode_reward=-417.74 +/- 63.54
Episode length: 52.38 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 705500   |
---------------------------------
Eval num_timesteps=706000, episode_reward=-408.18 +/- 61.51
Episode length: 49.18 +/- 14.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 706000   |
---------------------------------
Eval num_timesteps=706500, episode_reward=-411.57 +/- 69.98
Episode length: 49.88 +/- 16.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 706500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 345      |
|    time_elapsed    | 3039     |
|    total_timesteps | 706560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=707000, episode_reward=-410.59 +/- 88.03
Episode length: 50.52 +/- 16.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.5         |
|    mean_reward          | -411         |
| time/                   |              |
|    total_timesteps      | 707000       |
| train/                  |              |
|    approx_kl            | 0.0075942213 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.433       |
|    explained_variance   | 0.706        |
|    learning_rate        | 0.001        |
|    loss                 | 669          |
|    n_updates            | 1980         |
|    policy_gradient_loss | 0.00337      |
|    value_loss           | 1.48e+03     |
------------------------------------------
Eval num_timesteps=707500, episode_reward=-408.74 +/- 68.80
Episode length: 48.88 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 707500   |
---------------------------------
Eval num_timesteps=708000, episode_reward=-412.95 +/- 76.67
Episode length: 49.10 +/- 14.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
Eval num_timesteps=708500, episode_reward=-415.32 +/- 100.83
Episode length: 52.16 +/- 19.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 708500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 346      |
|    time_elapsed    | 3048     |
|    total_timesteps | 708608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.53
Eval num_timesteps=709000, episode_reward=-399.18 +/- 77.49
Episode length: 45.72 +/- 16.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.7      |
|    mean_reward          | -399      |
| time/                   |           |
|    total_timesteps      | 709000    |
| train/                  |           |
|    approx_kl            | 0.2638942 |
|    clip_fraction        | 0.352     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.714    |
|    explained_variance   | 0.68      |
|    learning_rate        | 0.001     |
|    loss                 | 1.24e+03  |
|    n_updates            | 1981      |
|    policy_gradient_loss | 0.179     |
|    value_loss           | 2.52e+03  |
---------------------------------------
Eval num_timesteps=709500, episode_reward=-409.27 +/- 83.17
Episode length: 51.40 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 709500   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-426.69 +/- 64.42
Episode length: 53.08 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
Eval num_timesteps=710500, episode_reward=-429.67 +/- 60.66
Episode length: 52.94 +/- 19.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 710500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 347      |
|    time_elapsed    | 3056     |
|    total_timesteps | 710656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.20
Eval num_timesteps=711000, episode_reward=-421.95 +/- 74.06
Episode length: 47.20 +/- 15.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -422        |
| time/                   |             |
|    total_timesteps      | 711000      |
| train/                  |             |
|    approx_kl            | 0.100524016 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.001       |
|    loss                 | 721         |
|    n_updates            | 1982        |
|    policy_gradient_loss | 0.0269      |
|    value_loss           | 1.75e+03    |
-----------------------------------------
Eval num_timesteps=711500, episode_reward=-408.16 +/- 71.57
Episode length: 51.62 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 711500   |
---------------------------------
Eval num_timesteps=712000, episode_reward=-432.78 +/- 56.58
Episode length: 53.28 +/- 20.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 712000   |
---------------------------------
Eval num_timesteps=712500, episode_reward=-426.75 +/- 62.74
Episode length: 47.68 +/- 15.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 712500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 348      |
|    time_elapsed    | 3065     |
|    total_timesteps | 712704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=713000, episode_reward=-431.59 +/- 61.77
Episode length: 52.82 +/- 16.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.8       |
|    mean_reward          | -432       |
| time/                   |            |
|    total_timesteps      | 713000     |
| train/                  |            |
|    approx_kl            | 0.02967031 |
|    clip_fraction        | 0.0859     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.44      |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.001      |
|    loss                 | 692        |
|    n_updates            | 1983       |
|    policy_gradient_loss | 0.0102     |
|    value_loss           | 1.52e+03   |
----------------------------------------
Eval num_timesteps=713500, episode_reward=-426.20 +/- 65.37
Episode length: 56.28 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 713500   |
---------------------------------
Eval num_timesteps=714000, episode_reward=-399.65 +/- 66.09
Episode length: 49.54 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
Eval num_timesteps=714500, episode_reward=-421.34 +/- 64.20
Episode length: 52.32 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 714500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 349      |
|    time_elapsed    | 3074     |
|    total_timesteps | 714752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=715000, episode_reward=-407.48 +/- 69.60
Episode length: 54.90 +/- 21.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.9        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 715000      |
| train/                  |             |
|    approx_kl            | 0.009347808 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.277      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.001       |
|    loss                 | 1e+03       |
|    n_updates            | 1984        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 1.81e+03    |
-----------------------------------------
Eval num_timesteps=715500, episode_reward=-431.42 +/- 58.63
Episode length: 47.32 +/- 14.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 715500   |
---------------------------------
Eval num_timesteps=716000, episode_reward=-420.56 +/- 66.26
Episode length: 50.62 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 716000   |
---------------------------------
Eval num_timesteps=716500, episode_reward=-414.04 +/- 79.25
Episode length: 48.88 +/- 17.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 716500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 350      |
|    time_elapsed    | 3083     |
|    total_timesteps | 716800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=717000, episode_reward=-396.14 +/- 69.84
Episode length: 51.88 +/- 23.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.9         |
|    mean_reward          | -396         |
| time/                   |              |
|    total_timesteps      | 717000       |
| train/                  |              |
|    approx_kl            | 0.0105877565 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.349       |
|    explained_variance   | 0.714        |
|    learning_rate        | 0.001        |
|    loss                 | 978          |
|    n_updates            | 1985         |
|    policy_gradient_loss | 0.00342      |
|    value_loss           | 1.91e+03     |
------------------------------------------
Eval num_timesteps=717500, episode_reward=-415.83 +/- 79.52
Episode length: 52.10 +/- 19.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 717500   |
---------------------------------
Eval num_timesteps=718000, episode_reward=-416.35 +/- 68.51
Episode length: 50.74 +/- 16.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 718000   |
---------------------------------
Eval num_timesteps=718500, episode_reward=-412.25 +/- 81.67
Episode length: 50.60 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 718500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.2     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 351      |
|    time_elapsed    | 3091     |
|    total_timesteps | 718848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=719000, episode_reward=-403.06 +/- 82.98
Episode length: 52.64 +/- 17.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.6       |
|    mean_reward          | -403       |
| time/                   |            |
|    total_timesteps      | 719000     |
| train/                  |            |
|    approx_kl            | 0.01062406 |
|    clip_fraction        | 0.0359     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.34      |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.001      |
|    loss                 | 1.04e+03   |
|    n_updates            | 1986       |
|    policy_gradient_loss | 0.00174    |
|    value_loss           | 1.7e+03    |
----------------------------------------
Eval num_timesteps=719500, episode_reward=-431.25 +/- 66.53
Episode length: 50.72 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 719500   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-425.44 +/- 61.09
Episode length: 48.20 +/- 12.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=720500, episode_reward=-416.89 +/- 61.89
Episode length: 46.88 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 720500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.1     |
|    ep_rew_mean     | -391     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 352      |
|    time_elapsed    | 3100     |
|    total_timesteps | 720896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=721000, episode_reward=-425.58 +/- 48.37
Episode length: 49.38 +/- 15.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.4        |
|    mean_reward          | -426        |
| time/                   |             |
|    total_timesteps      | 721000      |
| train/                  |             |
|    approx_kl            | 0.006464339 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.001       |
|    loss                 | 796         |
|    n_updates            | 1987        |
|    policy_gradient_loss | 0.00493     |
|    value_loss           | 1.76e+03    |
-----------------------------------------
Eval num_timesteps=721500, episode_reward=-412.81 +/- 56.67
Episode length: 50.66 +/- 14.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 721500   |
---------------------------------
Eval num_timesteps=722000, episode_reward=-431.37 +/- 73.33
Episode length: 46.64 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 722000   |
---------------------------------
Eval num_timesteps=722500, episode_reward=-417.79 +/- 62.71
Episode length: 53.50 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 722500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53       |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 353      |
|    time_elapsed    | 3109     |
|    total_timesteps | 722944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=723000, episode_reward=-433.94 +/- 68.90
Episode length: 49.56 +/- 16.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.6         |
|    mean_reward          | -434         |
| time/                   |              |
|    total_timesteps      | 723000       |
| train/                  |              |
|    approx_kl            | 0.0038584138 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.415       |
|    explained_variance   | 0.6          |
|    learning_rate        | 0.001        |
|    loss                 | 1.17e+03     |
|    n_updates            | 1988         |
|    policy_gradient_loss | 0.00273      |
|    value_loss           | 1.87e+03     |
------------------------------------------
Eval num_timesteps=723500, episode_reward=-408.70 +/- 81.73
Episode length: 51.56 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 723500   |
---------------------------------
Eval num_timesteps=724000, episode_reward=-429.16 +/- 79.99
Episode length: 49.02 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 724000   |
---------------------------------
Eval num_timesteps=724500, episode_reward=-426.80 +/- 52.09
Episode length: 48.12 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 724500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 354      |
|    time_elapsed    | 3117     |
|    total_timesteps | 724992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=725000, episode_reward=-429.13 +/- 57.15
Episode length: 51.92 +/- 18.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.018509148 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.403      |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.001       |
|    loss                 | 1.17e+03    |
|    n_updates            | 1989        |
|    policy_gradient_loss | 0.0109      |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=725500, episode_reward=-428.60 +/- 62.14
Episode length: 49.44 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 725500   |
---------------------------------
Eval num_timesteps=726000, episode_reward=-433.36 +/- 68.25
Episode length: 47.66 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
Eval num_timesteps=726500, episode_reward=-414.19 +/- 80.52
Episode length: 49.38 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 726500   |
---------------------------------
Eval num_timesteps=727000, episode_reward=-412.40 +/- 74.41
Episode length: 44.72 +/- 13.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 355      |
|    time_elapsed    | 3127     |
|    total_timesteps | 727040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=727500, episode_reward=-405.20 +/- 73.89
Episode length: 49.58 +/- 19.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.6       |
|    mean_reward          | -405       |
| time/                   |            |
|    total_timesteps      | 727500     |
| train/                  |            |
|    approx_kl            | 0.00910667 |
|    clip_fraction        | 0.0508     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.489     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.001      |
|    loss                 | 1.17e+03   |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.000404  |
|    value_loss           | 2.06e+03   |
----------------------------------------
Eval num_timesteps=728000, episode_reward=-422.48 +/- 71.50
Episode length: 55.22 +/- 20.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
Eval num_timesteps=728500, episode_reward=-408.19 +/- 72.82
Episode length: 53.76 +/- 19.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 728500   |
---------------------------------
Eval num_timesteps=729000, episode_reward=-409.98 +/- 85.49
Episode length: 47.08 +/- 17.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 356      |
|    time_elapsed    | 3136     |
|    total_timesteps | 729088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=729500, episode_reward=-419.00 +/- 64.92
Episode length: 46.90 +/- 15.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.9       |
|    mean_reward          | -419       |
| time/                   |            |
|    total_timesteps      | 729500     |
| train/                  |            |
|    approx_kl            | 0.01616176 |
|    clip_fraction        | 0.0859     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.555     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.001      |
|    loss                 | 897        |
|    n_updates            | 1991       |
|    policy_gradient_loss | 0.0347     |
|    value_loss           | 2.07e+03   |
----------------------------------------
Eval num_timesteps=730000, episode_reward=-397.35 +/- 93.12
Episode length: 50.10 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 730000   |
---------------------------------
Eval num_timesteps=730500, episode_reward=-412.35 +/- 63.96
Episode length: 56.54 +/- 23.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 730500   |
---------------------------------
Eval num_timesteps=731000, episode_reward=-392.58 +/- 79.19
Episode length: 47.98 +/- 19.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 357      |
|    time_elapsed    | 3145     |
|    total_timesteps | 731136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=731500, episode_reward=-409.55 +/- 70.63
Episode length: 53.32 +/- 19.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.3       |
|    mean_reward          | -410       |
| time/                   |            |
|    total_timesteps      | 731500     |
| train/                  |            |
|    approx_kl            | 0.02480059 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.601     |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.001      |
|    loss                 | 744        |
|    n_updates            | 1992       |
|    policy_gradient_loss | 0.0113     |
|    value_loss           | 1.69e+03   |
----------------------------------------
Eval num_timesteps=732000, episode_reward=-424.70 +/- 69.66
Episode length: 48.22 +/- 14.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
Eval num_timesteps=732500, episode_reward=-422.59 +/- 70.29
Episode length: 47.40 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 732500   |
---------------------------------
Eval num_timesteps=733000, episode_reward=-410.54 +/- 69.71
Episode length: 52.48 +/- 18.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 358      |
|    time_elapsed    | 3154     |
|    total_timesteps | 733184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=733500, episode_reward=-408.75 +/- 89.09
Episode length: 54.60 +/- 16.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.6        |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 733500      |
| train/                  |             |
|    approx_kl            | 0.032965843 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.001       |
|    loss                 | 694         |
|    n_updates            | 1993        |
|    policy_gradient_loss | 0.0168      |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=734000, episode_reward=-425.38 +/- 68.29
Episode length: 53.10 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 734000   |
---------------------------------
Eval num_timesteps=734500, episode_reward=-413.78 +/- 69.55
Episode length: 49.46 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 734500   |
---------------------------------
Eval num_timesteps=735000, episode_reward=-431.89 +/- 57.00
Episode length: 51.04 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -417     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 359      |
|    time_elapsed    | 3162     |
|    total_timesteps | 735232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=735500, episode_reward=-421.38 +/- 70.15
Episode length: 53.32 +/- 17.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.3       |
|    mean_reward          | -421       |
| time/                   |            |
|    total_timesteps      | 735500     |
| train/                  |            |
|    approx_kl            | 0.04058696 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.001      |
|    loss                 | 1.19e+03   |
|    n_updates            | 1994       |
|    policy_gradient_loss | 0.0282     |
|    value_loss           | 2.55e+03   |
----------------------------------------
Eval num_timesteps=736000, episode_reward=-406.27 +/- 93.23
Episode length: 46.36 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 736000   |
---------------------------------
Eval num_timesteps=736500, episode_reward=-410.50 +/- 77.46
Episode length: 50.66 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 736500   |
---------------------------------
Eval num_timesteps=737000, episode_reward=-414.74 +/- 72.02
Episode length: 49.66 +/- 22.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 360      |
|    time_elapsed    | 3171     |
|    total_timesteps | 737280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=737500, episode_reward=-426.75 +/- 75.47
Episode length: 51.94 +/- 18.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 737500      |
| train/                  |             |
|    approx_kl            | 0.030129757 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.599      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.001       |
|    loss                 | 926         |
|    n_updates            | 1995        |
|    policy_gradient_loss | 0.0154      |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=738000, episode_reward=-420.80 +/- 62.28
Episode length: 46.94 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
Eval num_timesteps=738500, episode_reward=-421.39 +/- 71.65
Episode length: 48.20 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 738500   |
---------------------------------
Eval num_timesteps=739000, episode_reward=-418.99 +/- 61.22
Episode length: 45.42 +/- 12.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 361      |
|    time_elapsed    | 3179     |
|    total_timesteps | 739328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=739500, episode_reward=-412.97 +/- 78.23
Episode length: 50.46 +/- 17.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.5        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 739500      |
| train/                  |             |
|    approx_kl            | 0.011613548 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.457      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.001       |
|    loss                 | 1e+03       |
|    n_updates            | 1996        |
|    policy_gradient_loss | 0.00785     |
|    value_loss           | 2.17e+03    |
-----------------------------------------
Eval num_timesteps=740000, episode_reward=-414.20 +/- 57.26
Episode length: 51.68 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=740500, episode_reward=-419.56 +/- 59.06
Episode length: 51.46 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 740500   |
---------------------------------
Eval num_timesteps=741000, episode_reward=-402.79 +/- 88.58
Episode length: 51.22 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 362      |
|    time_elapsed    | 3188     |
|    total_timesteps | 741376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=741500, episode_reward=-412.39 +/- 69.91
Episode length: 51.34 +/- 15.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.3        |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 741500      |
| train/                  |             |
|    approx_kl            | 0.022861393 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.314      |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.001       |
|    loss                 | 762         |
|    n_updates            | 1997        |
|    policy_gradient_loss | 0.018       |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=742000, episode_reward=-409.36 +/- 74.89
Episode length: 47.72 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 742000   |
---------------------------------
Eval num_timesteps=742500, episode_reward=-423.80 +/- 60.24
Episode length: 50.20 +/- 16.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 742500   |
---------------------------------
Eval num_timesteps=743000, episode_reward=-423.18 +/- 66.27
Episode length: 50.06 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 363      |
|    time_elapsed    | 3196     |
|    total_timesteps | 743424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=743500, episode_reward=-412.33 +/- 77.93
Episode length: 52.84 +/- 17.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.8         |
|    mean_reward          | -412         |
| time/                   |              |
|    total_timesteps      | 743500       |
| train/                  |              |
|    approx_kl            | 0.0045681046 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.277       |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 998          |
|    n_updates            | 1998         |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 2.16e+03     |
------------------------------------------
Eval num_timesteps=744000, episode_reward=-403.34 +/- 73.76
Episode length: 49.98 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
Eval num_timesteps=744500, episode_reward=-429.76 +/- 64.15
Episode length: 46.16 +/- 13.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 744500   |
---------------------------------
Eval num_timesteps=745000, episode_reward=-418.39 +/- 55.04
Episode length: 52.18 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 364      |
|    time_elapsed    | 3204     |
|    total_timesteps | 745472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=745500, episode_reward=-417.19 +/- 58.86
Episode length: 47.72 +/- 16.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.7        |
|    mean_reward          | -417        |
| time/                   |             |
|    total_timesteps      | 745500      |
| train/                  |             |
|    approx_kl            | 0.029574802 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.001       |
|    loss                 | 529         |
|    n_updates            | 1999        |
|    policy_gradient_loss | 0.0119      |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=746000, episode_reward=-421.94 +/- 49.27
Episode length: 46.72 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 746000   |
---------------------------------
Eval num_timesteps=746500, episode_reward=-423.75 +/- 73.24
Episode length: 52.20 +/- 20.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 746500   |
---------------------------------
Eval num_timesteps=747000, episode_reward=-418.89 +/- 73.69
Episode length: 49.92 +/- 18.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=747500, episode_reward=-414.79 +/- 73.52
Episode length: 52.50 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 747500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 365      |
|    time_elapsed    | 3215     |
|    total_timesteps | 747520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=748000, episode_reward=-399.00 +/- 82.52
Episode length: 47.54 +/- 16.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.5       |
|    mean_reward          | -399       |
| time/                   |            |
|    total_timesteps      | 748000     |
| train/                  |            |
|    approx_kl            | 0.06969807 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.678     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.001      |
|    loss                 | 738        |
|    n_updates            | 2000       |
|    policy_gradient_loss | 0.0592     |
|    value_loss           | 1.7e+03    |
----------------------------------------
Eval num_timesteps=748500, episode_reward=-425.57 +/- 53.00
Episode length: 56.70 +/- 20.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.7     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 748500   |
---------------------------------
Eval num_timesteps=749000, episode_reward=-403.55 +/- 81.43
Episode length: 52.04 +/- 22.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 749000   |
---------------------------------
Eval num_timesteps=749500, episode_reward=-416.22 +/- 64.09
Episode length: 50.66 +/- 13.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 749500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 366      |
|    time_elapsed    | 3223     |
|    total_timesteps | 749568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=750000, episode_reward=-414.80 +/- 72.29
Episode length: 52.00 +/- 20.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | -415        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.046678316 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.001       |
|    loss                 | 892         |
|    n_updates            | 2001        |
|    policy_gradient_loss | 0.0369      |
|    value_loss           | 2.04e+03    |
-----------------------------------------
Eval num_timesteps=750500, episode_reward=-426.74 +/- 58.89
Episode length: 54.26 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 750500   |
---------------------------------
Eval num_timesteps=751000, episode_reward=-402.20 +/- 83.68
Episode length: 49.58 +/- 17.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 751000   |
---------------------------------
Eval num_timesteps=751500, episode_reward=-429.79 +/- 61.89
Episode length: 47.22 +/- 14.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 751500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 367      |
|    time_elapsed    | 3232     |
|    total_timesteps | 751616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=752000, episode_reward=-436.97 +/- 57.49
Episode length: 46.50 +/- 15.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.5       |
|    mean_reward          | -437       |
| time/                   |            |
|    total_timesteps      | 752000     |
| train/                  |            |
|    approx_kl            | 0.01961069 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.001      |
|    loss                 | 772        |
|    n_updates            | 2002       |
|    policy_gradient_loss | 0.0145     |
|    value_loss           | 1.73e+03   |
----------------------------------------
Eval num_timesteps=752500, episode_reward=-423.80 +/- 67.30
Episode length: 49.18 +/- 15.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 752500   |
---------------------------------
Eval num_timesteps=753000, episode_reward=-402.80 +/- 81.82
Episode length: 45.72 +/- 13.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 753000   |
---------------------------------
Eval num_timesteps=753500, episode_reward=-424.99 +/- 63.58
Episode length: 49.28 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 753500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 368      |
|    time_elapsed    | 3240     |
|    total_timesteps | 753664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=754000, episode_reward=-428.57 +/- 84.28
Episode length: 49.48 +/- 16.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 754000      |
| train/                  |             |
|    approx_kl            | 0.011150651 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.593      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.001       |
|    loss                 | 726         |
|    n_updates            | 2003        |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=754500, episode_reward=-425.60 +/- 79.15
Episode length: 51.68 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 754500   |
---------------------------------
Eval num_timesteps=755000, episode_reward=-412.99 +/- 73.46
Episode length: 51.38 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 755000   |
---------------------------------
Eval num_timesteps=755500, episode_reward=-397.99 +/- 83.06
Episode length: 45.02 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 755500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 369      |
|    time_elapsed    | 3249     |
|    total_timesteps | 755712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.24
Eval num_timesteps=756000, episode_reward=-417.19 +/- 79.87
Episode length: 50.80 +/- 18.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.8       |
|    mean_reward          | -417       |
| time/                   |            |
|    total_timesteps      | 756000     |
| train/                  |            |
|    approx_kl            | 0.06342649 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.58      |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.001      |
|    loss                 | 1.24e+03   |
|    n_updates            | 2004       |
|    policy_gradient_loss | 0.0117     |
|    value_loss           | 1.92e+03   |
----------------------------------------
Eval num_timesteps=756500, episode_reward=-409.19 +/- 78.12
Episode length: 52.82 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 756500   |
---------------------------------
Eval num_timesteps=757000, episode_reward=-421.97 +/- 66.09
Episode length: 47.34 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 757000   |
---------------------------------
Eval num_timesteps=757500, episode_reward=-412.32 +/- 54.70
Episode length: 49.50 +/- 14.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 757500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 370      |
|    time_elapsed    | 3257     |
|    total_timesteps | 757760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.37
Eval num_timesteps=758000, episode_reward=-418.32 +/- 80.94
Episode length: 49.62 +/- 16.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.6       |
|    mean_reward          | -418       |
| time/                   |            |
|    total_timesteps      | 758000     |
| train/                  |            |
|    approx_kl            | 0.18419164 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.735     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.001      |
|    loss                 | 770        |
|    n_updates            | 2005       |
|    policy_gradient_loss | 0.0399     |
|    value_loss           | 1.9e+03    |
----------------------------------------
Eval num_timesteps=758500, episode_reward=-417.77 +/- 69.49
Episode length: 48.26 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 758500   |
---------------------------------
Eval num_timesteps=759000, episode_reward=-426.73 +/- 63.83
Episode length: 51.22 +/- 19.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 759000   |
---------------------------------
Eval num_timesteps=759500, episode_reward=-407.59 +/- 83.57
Episode length: 51.16 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 759500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 371      |
|    time_elapsed    | 3266     |
|    total_timesteps | 759808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=760000, episode_reward=-427.31 +/- 67.03
Episode length: 48.84 +/- 19.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.8        |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.072513156 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.001       |
|    loss                 | 892         |
|    n_updates            | 2006        |
|    policy_gradient_loss | 0.00965     |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=760500, episode_reward=-412.36 +/- 69.61
Episode length: 50.98 +/- 18.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 760500   |
---------------------------------
Eval num_timesteps=761000, episode_reward=-411.11 +/- 73.52
Episode length: 51.72 +/- 22.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 761000   |
---------------------------------
Eval num_timesteps=761500, episode_reward=-438.04 +/- 44.88
Episode length: 47.80 +/- 14.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 761500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 372      |
|    time_elapsed    | 3274     |
|    total_timesteps | 761856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=762000, episode_reward=-407.52 +/- 84.36
Episode length: 46.54 +/- 15.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.5        |
|    mean_reward          | -408        |
| time/                   |             |
|    total_timesteps      | 762000      |
| train/                  |             |
|    approx_kl            | 0.010943371 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.398      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.001       |
|    loss                 | 957         |
|    n_updates            | 2007        |
|    policy_gradient_loss | 0.0043      |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=762500, episode_reward=-396.15 +/- 67.53
Episode length: 48.76 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 762500   |
---------------------------------
Eval num_timesteps=763000, episode_reward=-411.15 +/- 79.67
Episode length: 49.98 +/- 17.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 763000   |
---------------------------------
Eval num_timesteps=763500, episode_reward=-420.78 +/- 72.43
Episode length: 48.46 +/- 14.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 763500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 373      |
|    time_elapsed    | 3283     |
|    total_timesteps | 763904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=764000, episode_reward=-417.80 +/- 69.51
Episode length: 49.22 +/- 15.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 764000      |
| train/                  |             |
|    approx_kl            | 0.010311546 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.001       |
|    loss                 | 1.24e+03    |
|    n_updates            | 2008        |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=764500, episode_reward=-402.17 +/- 80.14
Episode length: 46.76 +/- 13.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 764500   |
---------------------------------
Eval num_timesteps=765000, episode_reward=-404.59 +/- 82.54
Episode length: 44.76 +/- 13.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 765000   |
---------------------------------
Eval num_timesteps=765500, episode_reward=-421.38 +/- 70.62
Episode length: 48.60 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 765500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 374      |
|    time_elapsed    | 3291     |
|    total_timesteps | 765952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=766000, episode_reward=-429.25 +/- 55.07
Episode length: 56.00 +/- 18.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56          |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 766000      |
| train/                  |             |
|    approx_kl            | 0.007180181 |
|    clip_fraction        | 0.0571      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.279      |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.001       |
|    loss                 | 550         |
|    n_updates            | 2009        |
|    policy_gradient_loss | 0.000706    |
|    value_loss           | 1.63e+03    |
-----------------------------------------
Eval num_timesteps=766500, episode_reward=-402.31 +/- 73.08
Episode length: 52.00 +/- 18.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 766500   |
---------------------------------
Eval num_timesteps=767000, episode_reward=-404.85 +/- 74.60
Episode length: 47.56 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 767000   |
---------------------------------
Eval num_timesteps=767500, episode_reward=-419.36 +/- 58.28
Episode length: 47.60 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 767500   |
---------------------------------
Eval num_timesteps=768000, episode_reward=-404.50 +/- 67.59
Episode length: 49.84 +/- 18.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 375      |
|    time_elapsed    | 3301     |
|    total_timesteps | 768000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.21
Eval num_timesteps=768500, episode_reward=-422.85 +/- 54.53
Episode length: 54.32 +/- 16.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 54.3       |
|    mean_reward          | -423       |
| time/                   |            |
|    total_timesteps      | 768500     |
| train/                  |            |
|    approx_kl            | 0.10567459 |
|    clip_fraction        | 0.0273     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.454     |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.001      |
|    loss                 | 646        |
|    n_updates            | 2010       |
|    policy_gradient_loss | 0.0884     |
|    value_loss           | 1.3e+03    |
----------------------------------------
Eval num_timesteps=769000, episode_reward=-410.83 +/- 66.70
Episode length: 46.10 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 769000   |
---------------------------------
Eval num_timesteps=769500, episode_reward=-406.01 +/- 75.01
Episode length: 48.12 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 769500   |
---------------------------------
Eval num_timesteps=770000, episode_reward=-414.10 +/- 67.13
Episode length: 49.96 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 376      |
|    time_elapsed    | 3309     |
|    total_timesteps | 770048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.45
Eval num_timesteps=770500, episode_reward=-408.73 +/- 76.35
Episode length: 49.74 +/- 17.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.7       |
|    mean_reward          | -409       |
| time/                   |            |
|    total_timesteps      | 770500     |
| train/                  |            |
|    approx_kl            | 0.22509661 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.775     |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.001      |
|    loss                 | 811        |
|    n_updates            | 2011       |
|    policy_gradient_loss | 0.0734     |
|    value_loss           | 1.59e+03   |
----------------------------------------
Eval num_timesteps=771000, episode_reward=-399.32 +/- 68.41
Episode length: 47.28 +/- 18.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 771000   |
---------------------------------
Eval num_timesteps=771500, episode_reward=-409.64 +/- 66.07
Episode length: 50.52 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 771500   |
---------------------------------
Eval num_timesteps=772000, episode_reward=-408.79 +/- 72.61
Episode length: 47.66 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 377      |
|    time_elapsed    | 3318     |
|    total_timesteps | 772096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.22
Eval num_timesteps=772500, episode_reward=-422.51 +/- 63.11
Episode length: 52.84 +/- 18.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.8        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 772500      |
| train/                  |             |
|    approx_kl            | 0.107874714 |
|    clip_fraction        | 0.406       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.001       |
|    loss                 | 1.3e+03     |
|    n_updates            | 2012        |
|    policy_gradient_loss | 0.0479      |
|    value_loss           | 2.24e+03    |
-----------------------------------------
Eval num_timesteps=773000, episode_reward=-401.03 +/- 68.60
Episode length: 48.76 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 773000   |
---------------------------------
Eval num_timesteps=773500, episode_reward=-387.86 +/- 79.12
Episode length: 49.70 +/- 15.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 773500   |
---------------------------------
Eval num_timesteps=774000, episode_reward=-410.30 +/- 64.07
Episode length: 47.02 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -419     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 378      |
|    time_elapsed    | 3326     |
|    total_timesteps | 774144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=774500, episode_reward=-406.59 +/- 72.68
Episode length: 48.08 +/- 18.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.1       |
|    mean_reward          | -407       |
| time/                   |            |
|    total_timesteps      | 774500     |
| train/                  |            |
|    approx_kl            | 0.03314115 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.001      |
|    loss                 | 613        |
|    n_updates            | 2013       |
|    policy_gradient_loss | 0.0453     |
|    value_loss           | 1.64e+03   |
----------------------------------------
Eval num_timesteps=775000, episode_reward=-411.78 +/- 72.63
Episode length: 47.80 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 775000   |
---------------------------------
Eval num_timesteps=775500, episode_reward=-419.74 +/- 62.72
Episode length: 51.90 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 775500   |
---------------------------------
Eval num_timesteps=776000, episode_reward=-398.16 +/- 77.11
Episode length: 44.78 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 379      |
|    time_elapsed    | 3335     |
|    total_timesteps | 776192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=776500, episode_reward=-413.44 +/- 72.01
Episode length: 48.82 +/- 13.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.8        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 776500      |
| train/                  |             |
|    approx_kl            | 0.009374304 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.001       |
|    loss                 | 686         |
|    n_updates            | 2014        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 1.63e+03    |
-----------------------------------------
Eval num_timesteps=777000, episode_reward=-399.74 +/- 74.99
Episode length: 44.56 +/- 16.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 777000   |
---------------------------------
Eval num_timesteps=777500, episode_reward=-414.09 +/- 69.51
Episode length: 48.54 +/- 15.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 777500   |
---------------------------------
Eval num_timesteps=778000, episode_reward=-421.55 +/- 68.33
Episode length: 50.22 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 380      |
|    time_elapsed    | 3343     |
|    total_timesteps | 778240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=778500, episode_reward=-414.39 +/- 66.76
Episode length: 47.32 +/- 16.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.3        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 778500      |
| train/                  |             |
|    approx_kl            | 0.017957475 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.001       |
|    loss                 | 1.03e+03    |
|    n_updates            | 2015        |
|    policy_gradient_loss | 0.00287     |
|    value_loss           | 1.87e+03    |
-----------------------------------------
Eval num_timesteps=779000, episode_reward=-387.09 +/- 82.06
Episode length: 43.00 +/- 11.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 779000   |
---------------------------------
Eval num_timesteps=779500, episode_reward=-407.93 +/- 57.59
Episode length: 52.80 +/- 16.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 779500   |
---------------------------------
Eval num_timesteps=780000, episode_reward=-421.49 +/- 60.54
Episode length: 44.70 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.2     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 381      |
|    time_elapsed    | 3351     |
|    total_timesteps | 780288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=780500, episode_reward=-425.43 +/- 54.73
Episode length: 48.74 +/- 12.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 780500      |
| train/                  |             |
|    approx_kl            | 0.046137083 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.662      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.001       |
|    loss                 | 992         |
|    n_updates            | 2016        |
|    policy_gradient_loss | 0.0171      |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=781000, episode_reward=-414.79 +/- 66.13
Episode length: 53.04 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 781000   |
---------------------------------
Eval num_timesteps=781500, episode_reward=-416.49 +/- 58.21
Episode length: 48.28 +/- 18.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 781500   |
---------------------------------
Eval num_timesteps=782000, episode_reward=-404.10 +/- 68.09
Episode length: 46.56 +/- 15.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 382      |
|    time_elapsed    | 3360     |
|    total_timesteps | 782336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.25
Eval num_timesteps=782500, episode_reward=-419.61 +/- 61.74
Episode length: 49.48 +/- 17.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.5       |
|    mean_reward          | -420       |
| time/                   |            |
|    total_timesteps      | 782500     |
| train/                  |            |
|    approx_kl            | 0.12490421 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.001      |
|    loss                 | 1.16e+03   |
|    n_updates            | 2017       |
|    policy_gradient_loss | 0.0483     |
|    value_loss           | 1.91e+03   |
----------------------------------------
Eval num_timesteps=783000, episode_reward=-411.23 +/- 77.03
Episode length: 55.34 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 783000   |
---------------------------------
Eval num_timesteps=783500, episode_reward=-404.81 +/- 68.54
Episode length: 48.54 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 783500   |
---------------------------------
Eval num_timesteps=784000, episode_reward=-410.53 +/- 56.26
Episode length: 46.42 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 383      |
|    time_elapsed    | 3368     |
|    total_timesteps | 784384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.20
Eval num_timesteps=784500, episode_reward=-420.10 +/- 61.03
Episode length: 48.70 +/- 15.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.7       |
|    mean_reward          | -420       |
| time/                   |            |
|    total_timesteps      | 784500     |
| train/                  |            |
|    approx_kl            | 0.09769801 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.001      |
|    loss                 | 877        |
|    n_updates            | 2018       |
|    policy_gradient_loss | 0.0888     |
|    value_loss           | 1.72e+03   |
----------------------------------------
Eval num_timesteps=785000, episode_reward=-412.59 +/- 82.05
Episode length: 50.26 +/- 14.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
Eval num_timesteps=785500, episode_reward=-393.29 +/- 69.81
Episode length: 45.32 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 785500   |
---------------------------------
Eval num_timesteps=786000, episode_reward=-416.10 +/- 66.96
Episode length: 46.14 +/- 14.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.9     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 384      |
|    time_elapsed    | 3376     |
|    total_timesteps | 786432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=786500, episode_reward=-425.25 +/- 63.29
Episode length: 53.52 +/- 17.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.5        |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 786500      |
| train/                  |             |
|    approx_kl            | 0.030630184 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.001       |
|    loss                 | 897         |
|    n_updates            | 2019        |
|    policy_gradient_loss | 0.0185      |
|    value_loss           | 1.99e+03    |
-----------------------------------------
Eval num_timesteps=787000, episode_reward=-419.25 +/- 63.36
Episode length: 50.40 +/- 12.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 787000   |
---------------------------------
Eval num_timesteps=787500, episode_reward=-404.20 +/- 80.72
Episode length: 49.12 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 787500   |
---------------------------------
Eval num_timesteps=788000, episode_reward=-413.81 +/- 56.16
Episode length: 49.90 +/- 13.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.1     |
|    ep_rew_mean     | -405     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 385      |
|    time_elapsed    | 3385     |
|    total_timesteps | 788480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=788500, episode_reward=-408.31 +/- 78.44
Episode length: 52.30 +/- 18.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.3        |
|    mean_reward          | -408        |
| time/                   |             |
|    total_timesteps      | 788500      |
| train/                  |             |
|    approx_kl            | 0.012835323 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.001       |
|    loss                 | 654         |
|    n_updates            | 2020        |
|    policy_gradient_loss | 0.00314     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=789000, episode_reward=-417.55 +/- 73.84
Episode length: 47.12 +/- 13.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 789000   |
---------------------------------
Eval num_timesteps=789500, episode_reward=-416.66 +/- 70.38
Episode length: 50.94 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 789500   |
---------------------------------
Eval num_timesteps=790000, episode_reward=-407.99 +/- 57.11
Episode length: 51.76 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 790000   |
---------------------------------
Eval num_timesteps=790500, episode_reward=-394.93 +/- 71.70
Episode length: 44.96 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 790500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.1     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 386      |
|    time_elapsed    | 3395     |
|    total_timesteps | 790528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=791000, episode_reward=-402.50 +/- 74.92
Episode length: 50.98 +/- 19.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -402        |
| time/                   |             |
|    total_timesteps      | 791000      |
| train/                  |             |
|    approx_kl            | 0.008962651 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.001       |
|    loss                 | 1.06e+03    |
|    n_updates            | 2021        |
|    policy_gradient_loss | 0.000275    |
|    value_loss           | 1.79e+03    |
-----------------------------------------
Eval num_timesteps=791500, episode_reward=-424.41 +/- 63.73
Episode length: 49.18 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 791500   |
---------------------------------
Eval num_timesteps=792000, episode_reward=-423.52 +/- 55.57
Episode length: 49.98 +/- 18.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
Eval num_timesteps=792500, episode_reward=-406.08 +/- 62.64
Episode length: 47.96 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 792500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 387      |
|    time_elapsed    | 3404     |
|    total_timesteps | 792576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=793000, episode_reward=-389.85 +/- 87.24
Episode length: 46.52 +/- 15.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.5        |
|    mean_reward          | -390        |
| time/                   |             |
|    total_timesteps      | 793000      |
| train/                  |             |
|    approx_kl            | 0.020345401 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.001       |
|    loss                 | 561         |
|    n_updates            | 2022        |
|    policy_gradient_loss | 0.017       |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=793500, episode_reward=-415.70 +/- 66.96
Episode length: 49.10 +/- 14.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 793500   |
---------------------------------
Eval num_timesteps=794000, episode_reward=-427.31 +/- 61.01
Episode length: 47.10 +/- 15.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 794000   |
---------------------------------
Eval num_timesteps=794500, episode_reward=-404.93 +/- 78.02
Episode length: 48.92 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 794500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 388      |
|    time_elapsed    | 3412     |
|    total_timesteps | 794624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=795000, episode_reward=-420.49 +/- 61.70
Episode length: 52.74 +/- 14.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.7        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 795000      |
| train/                  |             |
|    approx_kl            | 0.025707018 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.001       |
|    loss                 | 900         |
|    n_updates            | 2023        |
|    policy_gradient_loss | 0.0111      |
|    value_loss           | 1.93e+03    |
-----------------------------------------
Eval num_timesteps=795500, episode_reward=-406.40 +/- 72.42
Episode length: 49.54 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 795500   |
---------------------------------
Eval num_timesteps=796000, episode_reward=-415.76 +/- 56.83
Episode length: 49.74 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 796000   |
---------------------------------
Eval num_timesteps=796500, episode_reward=-425.37 +/- 63.37
Episode length: 50.44 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 796500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 389      |
|    time_elapsed    | 3421     |
|    total_timesteps | 796672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=797000, episode_reward=-402.31 +/- 69.40
Episode length: 48.68 +/- 14.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -402        |
| time/                   |             |
|    total_timesteps      | 797000      |
| train/                  |             |
|    approx_kl            | 0.026812248 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.857      |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.001       |
|    loss                 | 810         |
|    n_updates            | 2024        |
|    policy_gradient_loss | 0.0255      |
|    value_loss           | 1.72e+03    |
-----------------------------------------
Eval num_timesteps=797500, episode_reward=-395.37 +/- 64.36
Episode length: 49.64 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 797500   |
---------------------------------
Eval num_timesteps=798000, episode_reward=-415.45 +/- 60.53
Episode length: 48.16 +/- 16.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
Eval num_timesteps=798500, episode_reward=-401.19 +/- 59.68
Episode length: 45.32 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 798500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -395     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 390      |
|    time_elapsed    | 3429     |
|    total_timesteps | 798720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=799000, episode_reward=-402.91 +/- 62.78
Episode length: 48.06 +/- 16.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -403        |
| time/                   |             |
|    total_timesteps      | 799000      |
| train/                  |             |
|    approx_kl            | 0.021606807 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.001       |
|    loss                 | 785         |
|    n_updates            | 2025        |
|    policy_gradient_loss | 0.0191      |
|    value_loss           | 1.45e+03    |
-----------------------------------------
Eval num_timesteps=799500, episode_reward=-411.61 +/- 70.47
Episode length: 49.30 +/- 15.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 799500   |
---------------------------------
Eval num_timesteps=800000, episode_reward=-398.01 +/- 72.64
Episode length: 47.72 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=800500, episode_reward=-400.20 +/- 59.06
Episode length: 49.24 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 800500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 391      |
|    time_elapsed    | 3438     |
|    total_timesteps | 800768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=801000, episode_reward=-419.69 +/- 70.15
Episode length: 49.42 +/- 16.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.4        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 801000      |
| train/                  |             |
|    approx_kl            | 0.012484262 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.001       |
|    loss                 | 996         |
|    n_updates            | 2026        |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 1.85e+03    |
-----------------------------------------
Eval num_timesteps=801500, episode_reward=-425.65 +/- 54.68
Episode length: 49.58 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 801500   |
---------------------------------
Eval num_timesteps=802000, episode_reward=-409.72 +/- 68.17
Episode length: 46.86 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 802000   |
---------------------------------
Eval num_timesteps=802500, episode_reward=-416.77 +/- 56.82
Episode length: 46.70 +/- 12.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 802500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 392      |
|    time_elapsed    | 3447     |
|    total_timesteps | 802816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=803000, episode_reward=-403.95 +/- 81.55
Episode length: 49.22 +/- 16.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.2       |
|    mean_reward          | -404       |
| time/                   |            |
|    total_timesteps      | 803000     |
| train/                  |            |
|    approx_kl            | 0.01110721 |
|    clip_fraction        | 0.0898     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.835     |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.001      |
|    loss                 | 838        |
|    n_updates            | 2027       |
|    policy_gradient_loss | 0.0275     |
|    value_loss           | 1.67e+03   |
----------------------------------------
Eval num_timesteps=803500, episode_reward=-411.79 +/- 68.32
Episode length: 46.10 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 803500   |
---------------------------------
Eval num_timesteps=804000, episode_reward=-409.48 +/- 63.23
Episode length: 50.64 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
Eval num_timesteps=804500, episode_reward=-415.70 +/- 71.96
Episode length: 50.16 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 804500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 393      |
|    time_elapsed    | 3455     |
|    total_timesteps | 804864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=805000, episode_reward=-414.46 +/- 61.23
Episode length: 49.10 +/- 14.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.1        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 805000      |
| train/                  |             |
|    approx_kl            | 0.042609803 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.001       |
|    loss                 | 605         |
|    n_updates            | 2028        |
|    policy_gradient_loss | 0.0342      |
|    value_loss           | 1.32e+03    |
-----------------------------------------
Eval num_timesteps=805500, episode_reward=-398.70 +/- 61.67
Episode length: 44.54 +/- 14.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 805500   |
---------------------------------
Eval num_timesteps=806000, episode_reward=-414.58 +/- 59.15
Episode length: 48.12 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 806000   |
---------------------------------
Eval num_timesteps=806500, episode_reward=-401.10 +/- 72.16
Episode length: 52.16 +/- 15.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 806500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 394      |
|    time_elapsed    | 3464     |
|    total_timesteps | 806912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.19
Eval num_timesteps=807000, episode_reward=-413.51 +/- 65.25
Episode length: 45.84 +/- 16.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.8       |
|    mean_reward          | -414       |
| time/                   |            |
|    total_timesteps      | 807000     |
| train/                  |            |
|    approx_kl            | 0.09572505 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.914     |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.001      |
|    loss                 | 806        |
|    n_updates            | 2029       |
|    policy_gradient_loss | 0.084      |
|    value_loss           | 1.54e+03   |
----------------------------------------
Eval num_timesteps=807500, episode_reward=-414.52 +/- 65.92
Episode length: 51.82 +/- 20.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 807500   |
---------------------------------
Eval num_timesteps=808000, episode_reward=-392.29 +/- 75.16
Episode length: 52.84 +/- 20.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 808000   |
---------------------------------
Eval num_timesteps=808500, episode_reward=-410.89 +/- 73.41
Episode length: 48.44 +/- 19.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 808500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 395      |
|    time_elapsed    | 3472     |
|    total_timesteps | 808960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=809000, episode_reward=-401.10 +/- 97.75
Episode length: 48.02 +/- 17.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48          |
|    mean_reward          | -401        |
| time/                   |             |
|    total_timesteps      | 809000      |
| train/                  |             |
|    approx_kl            | 0.040051397 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.001       |
|    loss                 | 1.1e+03     |
|    n_updates            | 2030        |
|    policy_gradient_loss | 0.00946     |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=809500, episode_reward=-409.55 +/- 72.03
Episode length: 50.66 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 809500   |
---------------------------------
Eval num_timesteps=810000, episode_reward=-411.92 +/- 66.04
Episode length: 50.94 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
Eval num_timesteps=810500, episode_reward=-404.21 +/- 65.02
Episode length: 47.84 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 810500   |
---------------------------------
Eval num_timesteps=811000, episode_reward=-414.23 +/- 65.36
Episode length: 49.70 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 396      |
|    time_elapsed    | 3482     |
|    total_timesteps | 811008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=811500, episode_reward=-388.58 +/- 77.95
Episode length: 51.14 +/- 17.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -389        |
| time/                   |             |
|    total_timesteps      | 811500      |
| train/                  |             |
|    approx_kl            | 0.016204555 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.001       |
|    loss                 | 835         |
|    n_updates            | 2031        |
|    policy_gradient_loss | 0.00604     |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=812000, episode_reward=-411.01 +/- 59.62
Episode length: 47.94 +/- 19.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 812000   |
---------------------------------
Eval num_timesteps=812500, episode_reward=-412.44 +/- 66.73
Episode length: 51.12 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 812500   |
---------------------------------
Eval num_timesteps=813000, episode_reward=-401.53 +/- 75.40
Episode length: 49.04 +/- 13.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 397      |
|    time_elapsed    | 3491     |
|    total_timesteps | 813056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=813500, episode_reward=-413.29 +/- 58.68
Episode length: 51.54 +/- 21.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.5        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 813500      |
| train/                  |             |
|    approx_kl            | 0.009401453 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.618      |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.001       |
|    loss                 | 828         |
|    n_updates            | 2032        |
|    policy_gradient_loss | 0.00488     |
|    value_loss           | 1.64e+03    |
-----------------------------------------
Eval num_timesteps=814000, episode_reward=-415.79 +/- 61.85
Episode length: 51.66 +/- 21.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 814000   |
---------------------------------
Eval num_timesteps=814500, episode_reward=-418.98 +/- 58.90
Episode length: 47.56 +/- 13.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 814500   |
---------------------------------
Eval num_timesteps=815000, episode_reward=-415.25 +/- 79.77
Episode length: 47.40 +/- 13.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 398      |
|    time_elapsed    | 3499     |
|    total_timesteps | 815104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=815500, episode_reward=-404.24 +/- 65.22
Episode length: 52.06 +/- 15.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.1        |
|    mean_reward          | -404        |
| time/                   |             |
|    total_timesteps      | 815500      |
| train/                  |             |
|    approx_kl            | 0.005265642 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.001       |
|    loss                 | 1.34e+03    |
|    n_updates            | 2033        |
|    policy_gradient_loss | 0.00249     |
|    value_loss           | 1.98e+03    |
-----------------------------------------
Eval num_timesteps=816000, episode_reward=-406.79 +/- 72.65
Episode length: 51.12 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
Eval num_timesteps=816500, episode_reward=-406.81 +/- 66.32
Episode length: 51.70 +/- 14.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 816500   |
---------------------------------
Eval num_timesteps=817000, episode_reward=-420.74 +/- 70.35
Episode length: 44.82 +/- 14.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 399      |
|    time_elapsed    | 3508     |
|    total_timesteps | 817152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=817500, episode_reward=-412.05 +/- 62.19
Episode length: 50.70 +/- 15.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 817500      |
| train/                  |             |
|    approx_kl            | 0.009383387 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.001       |
|    loss                 | 740         |
|    n_updates            | 2034        |
|    policy_gradient_loss | 0.000475    |
|    value_loss           | 1.42e+03    |
-----------------------------------------
Eval num_timesteps=818000, episode_reward=-421.53 +/- 56.05
Episode length: 50.20 +/- 18.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 818000   |
---------------------------------
Eval num_timesteps=818500, episode_reward=-415.15 +/- 57.79
Episode length: 48.90 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 818500   |
---------------------------------
Eval num_timesteps=819000, episode_reward=-400.66 +/- 82.25
Episode length: 47.52 +/- 14.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 400      |
|    time_elapsed    | 3516     |
|    total_timesteps | 819200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=819500, episode_reward=-412.64 +/- 73.54
Episode length: 49.22 +/- 17.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 819500      |
| train/                  |             |
|    approx_kl            | 0.006718781 |
|    clip_fraction        | 0.0592      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.001       |
|    loss                 | 692         |
|    n_updates            | 2035        |
|    policy_gradient_loss | 0.000468    |
|    value_loss           | 1.64e+03    |
-----------------------------------------
Eval num_timesteps=820000, episode_reward=-415.10 +/- 66.81
Episode length: 51.86 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=820500, episode_reward=-396.65 +/- 75.94
Episode length: 49.74 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 820500   |
---------------------------------
Eval num_timesteps=821000, episode_reward=-434.97 +/- 52.76
Episode length: 50.02 +/- 18.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 401      |
|    time_elapsed    | 3525     |
|    total_timesteps | 821248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=821500, episode_reward=-404.95 +/- 62.64
Episode length: 46.78 +/- 17.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.8        |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 821500      |
| train/                  |             |
|    approx_kl            | 0.014263248 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.001       |
|    loss                 | 823         |
|    n_updates            | 2036        |
|    policy_gradient_loss | 0.0259      |
|    value_loss           | 1.66e+03    |
-----------------------------------------
Eval num_timesteps=822000, episode_reward=-401.01 +/- 77.47
Episode length: 46.82 +/- 13.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
Eval num_timesteps=822500, episode_reward=-411.15 +/- 61.31
Episode length: 48.30 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 822500   |
---------------------------------
Eval num_timesteps=823000, episode_reward=-400.23 +/- 61.71
Episode length: 49.72 +/- 13.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 402      |
|    time_elapsed    | 3533     |
|    total_timesteps | 823296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=823500, episode_reward=-398.29 +/- 81.68
Episode length: 47.88 +/- 17.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.9        |
|    mean_reward          | -398        |
| time/                   |             |
|    total_timesteps      | 823500      |
| train/                  |             |
|    approx_kl            | 0.017528636 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.001       |
|    loss                 | 943         |
|    n_updates            | 2037        |
|    policy_gradient_loss | 0.009       |
|    value_loss           | 1.82e+03    |
-----------------------------------------
Eval num_timesteps=824000, episode_reward=-418.17 +/- 65.78
Episode length: 48.78 +/- 14.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 824000   |
---------------------------------
Eval num_timesteps=824500, episode_reward=-392.07 +/- 77.34
Episode length: 44.50 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 824500   |
---------------------------------
Eval num_timesteps=825000, episode_reward=-413.88 +/- 67.60
Episode length: 45.52 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -387     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 403      |
|    time_elapsed    | 3541     |
|    total_timesteps | 825344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=825500, episode_reward=-396.53 +/- 64.60
Episode length: 45.10 +/- 14.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.1        |
|    mean_reward          | -397        |
| time/                   |             |
|    total_timesteps      | 825500      |
| train/                  |             |
|    approx_kl            | 0.024533762 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.001       |
|    loss                 | 831         |
|    n_updates            | 2038        |
|    policy_gradient_loss | 0.00617     |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=826000, episode_reward=-410.84 +/- 68.97
Episode length: 47.18 +/- 14.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 826000   |
---------------------------------
Eval num_timesteps=826500, episode_reward=-392.49 +/- 82.70
Episode length: 50.98 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 826500   |
---------------------------------
Eval num_timesteps=827000, episode_reward=-386.41 +/- 98.68
Episode length: 44.58 +/- 13.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.8     |
|    ep_rew_mean     | -375     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 404      |
|    time_elapsed    | 3549     |
|    total_timesteps | 827392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=827500, episode_reward=-431.59 +/- 59.40
Episode length: 52.58 +/- 16.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.6      |
|    mean_reward          | -432      |
| time/                   |           |
|    total_timesteps      | 827500    |
| train/                  |           |
|    approx_kl            | 0.0652439 |
|    clip_fraction        | 0.32      |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.926    |
|    explained_variance   | 0.716     |
|    learning_rate        | 0.001     |
|    loss                 | 888       |
|    n_updates            | 2039      |
|    policy_gradient_loss | 0.0115    |
|    value_loss           | 2.6e+03   |
---------------------------------------
Eval num_timesteps=828000, episode_reward=-412.39 +/- 68.63
Episode length: 51.42 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
Eval num_timesteps=828500, episode_reward=-437.60 +/- 54.99
Episode length: 52.80 +/- 19.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 828500   |
---------------------------------
Eval num_timesteps=829000, episode_reward=-414.19 +/- 84.45
Episode length: 51.58 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.2     |
|    ep_rew_mean     | -376     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 405      |
|    time_elapsed    | 3558     |
|    total_timesteps | 829440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=829500, episode_reward=-389.60 +/- 76.84
Episode length: 44.68 +/- 14.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44.7       |
|    mean_reward          | -390       |
| time/                   |            |
|    total_timesteps      | 829500     |
| train/                  |            |
|    approx_kl            | 0.05649347 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.771     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.001      |
|    loss                 | 1.04e+03   |
|    n_updates            | 2040       |
|    policy_gradient_loss | 0.0337     |
|    value_loss           | 2.28e+03   |
----------------------------------------
Eval num_timesteps=830000, episode_reward=-402.79 +/- 77.07
Episode length: 51.54 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 830000   |
---------------------------------
Eval num_timesteps=830500, episode_reward=-403.95 +/- 71.46
Episode length: 48.22 +/- 15.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 830500   |
---------------------------------
Eval num_timesteps=831000, episode_reward=-411.80 +/- 68.58
Episode length: 48.98 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.4     |
|    ep_rew_mean     | -374     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 406      |
|    time_elapsed    | 3566     |
|    total_timesteps | 831488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=831500, episode_reward=-434.00 +/- 54.28
Episode length: 53.50 +/- 18.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.5       |
|    mean_reward          | -434       |
| time/                   |            |
|    total_timesteps      | 831500     |
| train/                  |            |
|    approx_kl            | 0.03034693 |
|    clip_fraction        | 0.0586     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.558     |
|    explained_variance   | 0.705      |
|    learning_rate        | 0.001      |
|    loss                 | 1.17e+03   |
|    n_updates            | 2041       |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 1.95e+03   |
----------------------------------------
Eval num_timesteps=832000, episode_reward=-413.59 +/- 82.70
Episode length: 51.64 +/- 21.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=832500, episode_reward=-420.20 +/- 70.97
Episode length: 48.36 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 832500   |
---------------------------------
Eval num_timesteps=833000, episode_reward=-418.97 +/- 66.01
Episode length: 54.48 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 833000   |
---------------------------------
Eval num_timesteps=833500, episode_reward=-408.15 +/- 72.84
Episode length: 44.28 +/- 20.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 833500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.6     |
|    ep_rew_mean     | -377     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 407      |
|    time_elapsed    | 3577     |
|    total_timesteps | 833536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=834000, episode_reward=-408.15 +/- 76.39
Episode length: 51.48 +/- 15.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.5       |
|    mean_reward          | -408       |
| time/                   |            |
|    total_timesteps      | 834000     |
| train/                  |            |
|    approx_kl            | 0.01008777 |
|    clip_fraction        | 0.0352     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.001      |
|    loss                 | 797        |
|    n_updates            | 2042       |
|    policy_gradient_loss | 0.00825    |
|    value_loss           | 1.92e+03   |
----------------------------------------
Eval num_timesteps=834500, episode_reward=-429.73 +/- 74.54
Episode length: 53.58 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 834500   |
---------------------------------
Eval num_timesteps=835000, episode_reward=-429.19 +/- 47.19
Episode length: 53.98 +/- 19.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
Eval num_timesteps=835500, episode_reward=-432.09 +/- 55.77
Episode length: 50.46 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 835500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.3     |
|    ep_rew_mean     | -381     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 408      |
|    time_elapsed    | 3587     |
|    total_timesteps | 835584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=836000, episode_reward=-428.56 +/- 59.13
Episode length: 55.16 +/- 19.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 55.2       |
|    mean_reward          | -429       |
| time/                   |            |
|    total_timesteps      | 836000     |
| train/                  |            |
|    approx_kl            | 0.02015182 |
|    clip_fraction        | 0.0234     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.001      |
|    loss                 | 1.09e+03   |
|    n_updates            | 2043       |
|    policy_gradient_loss | 0.00569    |
|    value_loss           | 2.12e+03   |
----------------------------------------
Eval num_timesteps=836500, episode_reward=-405.20 +/- 74.13
Episode length: 50.38 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 836500   |
---------------------------------
Eval num_timesteps=837000, episode_reward=-419.56 +/- 83.54
Episode length: 47.80 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 837000   |
---------------------------------
Eval num_timesteps=837500, episode_reward=-415.39 +/- 65.79
Episode length: 51.10 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 837500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.9     |
|    ep_rew_mean     | -378     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 409      |
|    time_elapsed    | 3596     |
|    total_timesteps | 837632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.63
Eval num_timesteps=838000, episode_reward=-390.11 +/- 82.00
Episode length: 49.54 +/- 17.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.5       |
|    mean_reward          | -390       |
| time/                   |            |
|    total_timesteps      | 838000     |
| train/                  |            |
|    approx_kl            | 0.31289607 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.691     |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.001      |
|    loss                 | 775        |
|    n_updates            | 2044       |
|    policy_gradient_loss | 0.176      |
|    value_loss           | 1.64e+03   |
----------------------------------------
Eval num_timesteps=838500, episode_reward=-405.31 +/- 70.66
Episode length: 47.84 +/- 14.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 838500   |
---------------------------------
Eval num_timesteps=839000, episode_reward=-395.98 +/- 71.97
Episode length: 49.16 +/- 19.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 839000   |
---------------------------------
Eval num_timesteps=839500, episode_reward=-401.07 +/- 72.74
Episode length: 48.04 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 839500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 410      |
|    time_elapsed    | 3604     |
|    total_timesteps | 839680   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.26
Eval num_timesteps=840000, episode_reward=-407.83 +/- 59.88
Episode length: 46.90 +/- 14.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.9       |
|    mean_reward          | -408       |
| time/                   |            |
|    total_timesteps      | 840000     |
| train/                  |            |
|    approx_kl            | 0.12814106 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.625     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.001      |
|    loss                 | 1.01e+03   |
|    n_updates            | 2045       |
|    policy_gradient_loss | 0.055      |
|    value_loss           | 1.83e+03   |
----------------------------------------
Eval num_timesteps=840500, episode_reward=-395.43 +/- 67.47
Episode length: 48.46 +/- 14.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 840500   |
---------------------------------
Eval num_timesteps=841000, episode_reward=-412.75 +/- 63.48
Episode length: 46.30 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 841000   |
---------------------------------
Eval num_timesteps=841500, episode_reward=-406.73 +/- 64.90
Episode length: 50.68 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 841500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.5     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 411      |
|    time_elapsed    | 3612     |
|    total_timesteps | 841728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=842000, episode_reward=-413.26 +/- 62.13
Episode length: 50.72 +/- 19.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 842000      |
| train/                  |             |
|    approx_kl            | 0.017243724 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.001       |
|    loss                 | 928         |
|    n_updates            | 2046        |
|    policy_gradient_loss | 0.00503     |
|    value_loss           | 1.9e+03     |
-----------------------------------------
Eval num_timesteps=842500, episode_reward=-411.96 +/- 70.50
Episode length: 46.46 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 842500   |
---------------------------------
Eval num_timesteps=843000, episode_reward=-424.77 +/- 55.12
Episode length: 49.34 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 843000   |
---------------------------------
Eval num_timesteps=843500, episode_reward=-390.98 +/- 81.12
Episode length: 49.32 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 843500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 412      |
|    time_elapsed    | 3620     |
|    total_timesteps | 843776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=844000, episode_reward=-417.82 +/- 60.88
Episode length: 47.32 +/- 13.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.3         |
|    mean_reward          | -418         |
| time/                   |              |
|    total_timesteps      | 844000       |
| train/                  |              |
|    approx_kl            | 0.0143075315 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.382       |
|    explained_variance   | 0.736        |
|    learning_rate        | 0.001        |
|    loss                 | 827          |
|    n_updates            | 2047         |
|    policy_gradient_loss | 0.000957     |
|    value_loss           | 1.68e+03     |
------------------------------------------
Eval num_timesteps=844500, episode_reward=-408.94 +/- 72.72
Episode length: 45.48 +/- 14.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 844500   |
---------------------------------
Eval num_timesteps=845000, episode_reward=-429.18 +/- 57.32
Episode length: 50.96 +/- 13.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 845000   |
---------------------------------
Eval num_timesteps=845500, episode_reward=-410.02 +/- 89.99
Episode length: 49.70 +/- 20.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 845500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 413      |
|    time_elapsed    | 3629     |
|    total_timesteps | 845824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=846000, episode_reward=-415.84 +/- 74.85
Episode length: 46.20 +/- 14.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.2        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 846000      |
| train/                  |             |
|    approx_kl            | 0.008761785 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.422      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.001       |
|    loss                 | 742         |
|    n_updates            | 2048        |
|    policy_gradient_loss | 0.00235     |
|    value_loss           | 1.52e+03    |
-----------------------------------------
Eval num_timesteps=846500, episode_reward=-396.33 +/- 68.35
Episode length: 48.92 +/- 16.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 846500   |
---------------------------------
Eval num_timesteps=847000, episode_reward=-429.63 +/- 51.03
Episode length: 48.56 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 847000   |
---------------------------------
Eval num_timesteps=847500, episode_reward=-409.80 +/- 59.15
Episode length: 49.48 +/- 18.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 847500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 414      |
|    time_elapsed    | 3637     |
|    total_timesteps | 847872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=848000, episode_reward=-411.69 +/- 73.59
Episode length: 47.88 +/- 19.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.9        |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 848000      |
| train/                  |             |
|    approx_kl            | 0.014233991 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.001       |
|    loss                 | 1.04e+03    |
|    n_updates            | 2049        |
|    policy_gradient_loss | 0.00262     |
|    value_loss           | 2.11e+03    |
-----------------------------------------
Eval num_timesteps=848500, episode_reward=-405.64 +/- 71.67
Episode length: 49.44 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 848500   |
---------------------------------
Eval num_timesteps=849000, episode_reward=-407.02 +/- 78.21
Episode length: 48.08 +/- 14.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 849000   |
---------------------------------
Eval num_timesteps=849500, episode_reward=-403.40 +/- 76.31
Episode length: 48.34 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 849500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 415      |
|    time_elapsed    | 3645     |
|    total_timesteps | 849920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=850000, episode_reward=-396.61 +/- 92.06
Episode length: 47.30 +/- 16.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.3        |
|    mean_reward          | -397        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.013859276 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.001       |
|    loss                 | 1.1e+03     |
|    n_updates            | 2050        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=850500, episode_reward=-391.87 +/- 80.62
Episode length: 46.84 +/- 16.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 850500   |
---------------------------------
Eval num_timesteps=851000, episode_reward=-403.83 +/- 71.91
Episode length: 47.46 +/- 14.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 851000   |
---------------------------------
Eval num_timesteps=851500, episode_reward=-411.61 +/- 79.88
Episode length: 50.84 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 851500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 416      |
|    time_elapsed    | 3654     |
|    total_timesteps | 851968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=852000, episode_reward=-380.62 +/- 94.64
Episode length: 49.38 +/- 23.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.4       |
|    mean_reward          | -381       |
| time/                   |            |
|    total_timesteps      | 852000     |
| train/                  |            |
|    approx_kl            | 0.05534314 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.001      |
|    loss                 | 848        |
|    n_updates            | 2051       |
|    policy_gradient_loss | 0.0494     |
|    value_loss           | 1.87e+03   |
----------------------------------------
Eval num_timesteps=852500, episode_reward=-396.78 +/- 76.24
Episode length: 46.84 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 852500   |
---------------------------------
Eval num_timesteps=853000, episode_reward=-392.07 +/- 81.56
Episode length: 47.06 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 853000   |
---------------------------------
Eval num_timesteps=853500, episode_reward=-399.83 +/- 66.24
Episode length: 48.68 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 853500   |
---------------------------------
Eval num_timesteps=854000, episode_reward=-393.51 +/- 74.04
Episode length: 44.64 +/- 14.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.9     |
|    ep_rew_mean     | -389     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 417      |
|    time_elapsed    | 3663     |
|    total_timesteps | 854016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=854500, episode_reward=-434.60 +/- 63.57
Episode length: 54.58 +/- 15.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.6        |
|    mean_reward          | -435        |
| time/                   |             |
|    total_timesteps      | 854500      |
| train/                  |             |
|    approx_kl            | 0.064642526 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.001       |
|    loss                 | 1.61e+03    |
|    n_updates            | 2052        |
|    policy_gradient_loss | 0.0601      |
|    value_loss           | 2.92e+03    |
-----------------------------------------
Eval num_timesteps=855000, episode_reward=-396.19 +/- 72.68
Episode length: 46.78 +/- 15.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 855000   |
---------------------------------
Eval num_timesteps=855500, episode_reward=-417.80 +/- 58.85
Episode length: 52.56 +/- 19.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 855500   |
---------------------------------
Eval num_timesteps=856000, episode_reward=-421.40 +/- 70.64
Episode length: 46.52 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.9     |
|    ep_rew_mean     | -382     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 418      |
|    time_elapsed    | 3672     |
|    total_timesteps | 856064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=856500, episode_reward=-408.80 +/- 72.14
Episode length: 47.82 +/- 16.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.8        |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 856500      |
| train/                  |             |
|    approx_kl            | 0.046675906 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.001       |
|    loss                 | 900         |
|    n_updates            | 2053        |
|    policy_gradient_loss | 0.0478      |
|    value_loss           | 2.55e+03    |
-----------------------------------------
Eval num_timesteps=857000, episode_reward=-425.60 +/- 53.67
Episode length: 49.88 +/- 21.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 857000   |
---------------------------------
Eval num_timesteps=857500, episode_reward=-422.00 +/- 64.46
Episode length: 49.00 +/- 16.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 857500   |
---------------------------------
Eval num_timesteps=858000, episode_reward=-408.18 +/- 72.39
Episode length: 47.34 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.4     |
|    ep_rew_mean     | -391     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 419      |
|    time_elapsed    | 3680     |
|    total_timesteps | 858112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=858500, episode_reward=-414.18 +/- 87.46
Episode length: 47.22 +/- 15.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 858500      |
| train/                  |             |
|    approx_kl            | 0.023577083 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.652      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.001       |
|    loss                 | 792         |
|    n_updates            | 2054        |
|    policy_gradient_loss | 0.00614     |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=859000, episode_reward=-405.79 +/- 73.25
Episode length: 49.92 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 859000   |
---------------------------------
Eval num_timesteps=859500, episode_reward=-412.40 +/- 80.00
Episode length: 50.36 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 859500   |
---------------------------------
Eval num_timesteps=860000, episode_reward=-416.60 +/- 64.41
Episode length: 54.12 +/- 13.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.5     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 420      |
|    time_elapsed    | 3689     |
|    total_timesteps | 860160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=860500, episode_reward=-429.20 +/- 62.31
Episode length: 54.34 +/- 23.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.3        |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 860500      |
| train/                  |             |
|    approx_kl            | 0.010054665 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.321      |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.001       |
|    loss                 | 972         |
|    n_updates            | 2055        |
|    policy_gradient_loss | 0.00813     |
|    value_loss           | 1.96e+03    |
-----------------------------------------
Eval num_timesteps=861000, episode_reward=-413.00 +/- 67.05
Episode length: 50.88 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 861000   |
---------------------------------
Eval num_timesteps=861500, episode_reward=-422.00 +/- 63.05
Episode length: 48.36 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 861500   |
---------------------------------
Eval num_timesteps=862000, episode_reward=-405.20 +/- 73.64
Episode length: 50.40 +/- 14.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.5     |
|    ep_rew_mean     | -388     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 421      |
|    time_elapsed    | 3698     |
|    total_timesteps | 862208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=862500, episode_reward=-417.19 +/- 63.28
Episode length: 48.58 +/- 17.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.6         |
|    mean_reward          | -417         |
| time/                   |              |
|    total_timesteps      | 862500       |
| train/                  |              |
|    approx_kl            | 0.0066438136 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.394       |
|    explained_variance   | 0.673        |
|    learning_rate        | 0.001        |
|    loss                 | 730          |
|    n_updates            | 2056         |
|    policy_gradient_loss | 0.00317      |
|    value_loss           | 1.85e+03     |
------------------------------------------
Eval num_timesteps=863000, episode_reward=-397.39 +/- 72.02
Episode length: 48.72 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 863000   |
---------------------------------
Eval num_timesteps=863500, episode_reward=-420.20 +/- 73.95
Episode length: 49.52 +/- 18.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 863500   |
---------------------------------
Eval num_timesteps=864000, episode_reward=-415.99 +/- 78.08
Episode length: 53.04 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.9     |
|    ep_rew_mean     | -378     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 422      |
|    time_elapsed    | 3706     |
|    total_timesteps | 864256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=864500, episode_reward=-423.20 +/- 60.01
Episode length: 49.38 +/- 17.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.4        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 864500      |
| train/                  |             |
|    approx_kl            | 0.023631305 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.001       |
|    loss                 | 930         |
|    n_updates            | 2057        |
|    policy_gradient_loss | 0.006       |
|    value_loss           | 2.24e+03    |
-----------------------------------------
Eval num_timesteps=865000, episode_reward=-417.79 +/- 87.05
Episode length: 49.02 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 865000   |
---------------------------------
Eval num_timesteps=865500, episode_reward=-404.59 +/- 69.00
Episode length: 48.04 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 865500   |
---------------------------------
Eval num_timesteps=866000, episode_reward=-417.20 +/- 77.82
Episode length: 54.34 +/- 19.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39       |
|    ep_rew_mean     | -369     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 423      |
|    time_elapsed    | 3715     |
|    total_timesteps | 866304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=866500, episode_reward=-405.20 +/- 78.38
Episode length: 48.46 +/- 18.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.5        |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 866500      |
| train/                  |             |
|    approx_kl            | 0.008250248 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.001       |
|    loss                 | 1.15e+03    |
|    n_updates            | 2058        |
|    policy_gradient_loss | 0.00638     |
|    value_loss           | 2.72e+03    |
-----------------------------------------
Eval num_timesteps=867000, episode_reward=-423.80 +/- 62.88
Episode length: 48.84 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 867000   |
---------------------------------
Eval num_timesteps=867500, episode_reward=-411.20 +/- 76.94
Episode length: 53.46 +/- 18.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 867500   |
---------------------------------
Eval num_timesteps=868000, episode_reward=-414.19 +/- 67.64
Episode length: 51.90 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | -361     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 424      |
|    time_elapsed    | 3724     |
|    total_timesteps | 868352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=868500, episode_reward=-388.14 +/- 70.27
Episode length: 41.86 +/- 11.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 868500      |
| train/                  |             |
|    approx_kl            | 0.010959909 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.001       |
|    loss                 | 961         |
|    n_updates            | 2059        |
|    policy_gradient_loss | 0.00291     |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=869000, episode_reward=-366.67 +/- 89.58
Episode length: 41.40 +/- 11.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 869000   |
---------------------------------
Eval num_timesteps=869500, episode_reward=-372.52 +/- 79.27
Episode length: 42.52 +/- 13.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 869500   |
---------------------------------
Eval num_timesteps=870000, episode_reward=-397.31 +/- 87.96
Episode length: 41.40 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.9     |
|    ep_rew_mean     | -363     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 425      |
|    time_elapsed    | 3731     |
|    total_timesteps | 870400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=870500, episode_reward=-370.62 +/- 73.68
Episode length: 38.98 +/- 12.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | -371        |
| time/                   |             |
|    total_timesteps      | 870500      |
| train/                  |             |
|    approx_kl            | 0.008281993 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.001       |
|    loss                 | 1.27e+03    |
|    n_updates            | 2060        |
|    policy_gradient_loss | 0.00582     |
|    value_loss           | 2.54e+03    |
-----------------------------------------
Eval num_timesteps=871000, episode_reward=-396.78 +/- 70.33
Episode length: 44.06 +/- 13.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 871000   |
---------------------------------
Eval num_timesteps=871500, episode_reward=-368.33 +/- 87.32
Episode length: 40.26 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.3     |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 871500   |
---------------------------------
Eval num_timesteps=872000, episode_reward=-401.68 +/- 67.76
Episode length: 41.52 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.1     |
|    ep_rew_mean     | -370     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 426      |
|    time_elapsed    | 3738     |
|    total_timesteps | 872448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=872500, episode_reward=-392.18 +/- 86.68
Episode length: 44.76 +/- 15.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44.8         |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 872500       |
| train/                  |              |
|    approx_kl            | 0.0108474735 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.881       |
|    explained_variance   | 0.773        |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+03     |
|    n_updates            | 2061         |
|    policy_gradient_loss | 0.0123       |
|    value_loss           | 1.78e+03     |
------------------------------------------
Eval num_timesteps=873000, episode_reward=-398.98 +/- 65.94
Episode length: 46.26 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 873000   |
---------------------------------
Eval num_timesteps=873500, episode_reward=-402.98 +/- 76.20
Episode length: 46.06 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 873500   |
---------------------------------
Eval num_timesteps=874000, episode_reward=-423.66 +/- 58.07
Episode length: 52.02 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -360     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 427      |
|    time_elapsed    | 3746     |
|    total_timesteps | 874496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=874500, episode_reward=-399.64 +/- 70.23
Episode length: 48.62 +/- 16.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.6       |
|    mean_reward          | -400       |
| time/                   |            |
|    total_timesteps      | 874500     |
| train/                  |            |
|    approx_kl            | 0.01961443 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.001      |
|    loss                 | 875        |
|    n_updates            | 2062       |
|    policy_gradient_loss | -0.00386   |
|    value_loss           | 2.09e+03   |
----------------------------------------
Eval num_timesteps=875000, episode_reward=-410.81 +/- 67.48
Episode length: 46.78 +/- 11.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
Eval num_timesteps=875500, episode_reward=-405.97 +/- 72.52
Episode length: 47.60 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 875500   |
---------------------------------
Eval num_timesteps=876000, episode_reward=-416.57 +/- 55.76
Episode length: 49.92 +/- 19.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
Eval num_timesteps=876500, episode_reward=-391.26 +/- 69.12
Episode length: 47.76 +/- 15.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 876500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.4     |
|    ep_rew_mean     | -373     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 428      |
|    time_elapsed    | 3756     |
|    total_timesteps | 876544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=877000, episode_reward=-421.83 +/- 50.23
Episode length: 46.20 +/- 13.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.2        |
|    mean_reward          | -422        |
| time/                   |             |
|    total_timesteps      | 877000      |
| train/                  |             |
|    approx_kl            | 0.009787107 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.001       |
|    loss                 | 1.51e+03    |
|    n_updates            | 2063        |
|    policy_gradient_loss | -0.000433   |
|    value_loss           | 2.66e+03    |
-----------------------------------------
Eval num_timesteps=877500, episode_reward=-404.88 +/- 77.06
Episode length: 48.82 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 877500   |
---------------------------------
Eval num_timesteps=878000, episode_reward=-390.44 +/- 83.45
Episode length: 44.56 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 878000   |
---------------------------------
Eval num_timesteps=878500, episode_reward=-413.18 +/- 62.59
Episode length: 47.46 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 878500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43       |
|    ep_rew_mean     | -393     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 429      |
|    time_elapsed    | 3764     |
|    total_timesteps | 878592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=879000, episode_reward=-418.35 +/- 71.82
Episode length: 47.94 +/- 15.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.9        |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 879000      |
| train/                  |             |
|    approx_kl            | 0.008005141 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.001       |
|    loss                 | 928         |
|    n_updates            | 2064        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 2.23e+03    |
-----------------------------------------
Eval num_timesteps=879500, episode_reward=-401.64 +/- 68.06
Episode length: 48.28 +/- 13.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 879500   |
---------------------------------
Eval num_timesteps=880000, episode_reward=-398.64 +/- 68.12
Episode length: 49.64 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=880500, episode_reward=-430.54 +/- 63.20
Episode length: 52.88 +/- 16.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 880500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46       |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 430      |
|    time_elapsed    | 3773     |
|    total_timesteps | 880640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.22
Eval num_timesteps=881000, episode_reward=-401.28 +/- 75.88
Episode length: 42.70 +/- 16.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.7       |
|    mean_reward          | -401       |
| time/                   |            |
|    total_timesteps      | 881000     |
| train/                  |            |
|    approx_kl            | 0.05752043 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.619     |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.001      |
|    loss                 | 891        |
|    n_updates            | 2065       |
|    policy_gradient_loss | 0.0348     |
|    value_loss           | 1.69e+03   |
----------------------------------------
Eval num_timesteps=881500, episode_reward=-406.67 +/- 76.86
Episode length: 46.72 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 881500   |
---------------------------------
Eval num_timesteps=882000, episode_reward=-412.38 +/- 71.88
Episode length: 41.84 +/- 12.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
Eval num_timesteps=882500, episode_reward=-402.07 +/- 83.83
Episode length: 45.06 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 882500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 431      |
|    time_elapsed    | 3780     |
|    total_timesteps | 882688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.27
Eval num_timesteps=883000, episode_reward=-395.60 +/- 82.27
Episode length: 50.92 +/- 17.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -396      |
| time/                   |           |
|    total_timesteps      | 883000    |
| train/                  |           |
|    approx_kl            | 0.1372256 |
|    clip_fraction        | 0.371     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.722    |
|    explained_variance   | 0.724     |
|    learning_rate        | 0.001     |
|    loss                 | 1.05e+03  |
|    n_updates            | 2066      |
|    policy_gradient_loss | 0.0738    |
|    value_loss           | 2.26e+03  |
---------------------------------------
Eval num_timesteps=883500, episode_reward=-423.19 +/- 65.47
Episode length: 54.10 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 883500   |
---------------------------------
Eval num_timesteps=884000, episode_reward=-411.80 +/- 60.48
Episode length: 52.84 +/- 19.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 884000   |
---------------------------------
Eval num_timesteps=884500, episode_reward=-434.60 +/- 52.73
Episode length: 50.56 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 884500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | -369     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 432      |
|    time_elapsed    | 3789     |
|    total_timesteps | 884736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=885000, episode_reward=-419.00 +/- 59.12
Episode length: 51.62 +/- 17.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.6        |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 885000      |
| train/                  |             |
|    approx_kl            | 0.049504843 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.001       |
|    loss                 | 1.19e+03    |
|    n_updates            | 2067        |
|    policy_gradient_loss | 0.0148      |
|    value_loss           | 2.15e+03    |
-----------------------------------------
Eval num_timesteps=885500, episode_reward=-420.19 +/- 60.56
Episode length: 48.44 +/- 15.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 885500   |
---------------------------------
Eval num_timesteps=886000, episode_reward=-415.40 +/- 81.22
Episode length: 50.02 +/- 12.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 886000   |
---------------------------------
Eval num_timesteps=886500, episode_reward=-431.00 +/- 64.48
Episode length: 53.76 +/- 21.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 886500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | -382     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 433      |
|    time_elapsed    | 3798     |
|    total_timesteps | 886784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=887000, episode_reward=-416.00 +/- 56.68
Episode length: 49.06 +/- 14.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.1        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 887000      |
| train/                  |             |
|    approx_kl            | 0.014533219 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.394      |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.001       |
|    loss                 | 782         |
|    n_updates            | 2068        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=887500, episode_reward=-410.59 +/- 72.31
Episode length: 49.04 +/- 14.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 887500   |
---------------------------------
Eval num_timesteps=888000, episode_reward=-419.60 +/- 71.50
Episode length: 49.98 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
Eval num_timesteps=888500, episode_reward=-414.20 +/- 66.02
Episode length: 49.32 +/- 14.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 888500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.5     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 434      |
|    time_elapsed    | 3806     |
|    total_timesteps | 888832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=889000, episode_reward=-408.18 +/- 93.26
Episode length: 51.22 +/- 18.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.2        |
|    mean_reward          | -408        |
| time/                   |             |
|    total_timesteps      | 889000      |
| train/                  |             |
|    approx_kl            | 0.012315004 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.435      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.001       |
|    loss                 | 910         |
|    n_updates            | 2069        |
|    policy_gradient_loss | 0.00868     |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=889500, episode_reward=-407.00 +/- 78.85
Episode length: 45.72 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 889500   |
---------------------------------
Eval num_timesteps=890000, episode_reward=-428.00 +/- 59.89
Episode length: 49.38 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
Eval num_timesteps=890500, episode_reward=-397.99 +/- 83.10
Episode length: 48.16 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 890500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 435      |
|    time_elapsed    | 3815     |
|    total_timesteps | 890880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=891000, episode_reward=-430.40 +/- 61.76
Episode length: 50.24 +/- 14.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.2        |
|    mean_reward          | -430        |
| time/                   |             |
|    total_timesteps      | 891000      |
| train/                  |             |
|    approx_kl            | 0.041678365 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.001       |
|    loss                 | 1.25e+03    |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.000774   |
|    value_loss           | 2.52e+03    |
-----------------------------------------
Eval num_timesteps=891500, episode_reward=-423.79 +/- 61.14
Episode length: 53.24 +/- 15.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 891500   |
---------------------------------
Eval num_timesteps=892000, episode_reward=-397.40 +/- 81.83
Episode length: 51.48 +/- 15.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 892000   |
---------------------------------
Eval num_timesteps=892500, episode_reward=-426.20 +/- 61.97
Episode length: 53.22 +/- 20.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 892500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.5     |
|    ep_rew_mean     | -381     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 436      |
|    time_elapsed    | 3824     |
|    total_timesteps | 892928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.23
Eval num_timesteps=893000, episode_reward=-405.85 +/- 83.00
Episode length: 47.02 +/- 28.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47        |
|    mean_reward          | -406      |
| time/                   |           |
|    total_timesteps      | 893000    |
| train/                  |           |
|    approx_kl            | 0.1135074 |
|    clip_fraction        | 0.355     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.876    |
|    explained_variance   | 0.693     |
|    learning_rate        | 0.001     |
|    loss                 | 638       |
|    n_updates            | 2071      |
|    policy_gradient_loss | 0.102     |
|    value_loss           | 1.96e+03  |
---------------------------------------
Eval num_timesteps=893500, episode_reward=-408.59 +/- 69.30
Episode length: 50.20 +/- 14.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 893500   |
---------------------------------
Eval num_timesteps=894000, episode_reward=-404.08 +/- 80.00
Episode length: 48.44 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
Eval num_timesteps=894500, episode_reward=-393.47 +/- 69.82
Episode length: 44.54 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 894500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.3     |
|    ep_rew_mean     | -373     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 437      |
|    time_elapsed    | 3832     |
|    total_timesteps | 894976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=895000, episode_reward=-406.89 +/- 55.90
Episode length: 51.36 +/- 17.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.4        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 895000      |
| train/                  |             |
|    approx_kl            | 0.075302735 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.001       |
|    loss                 | 1.45e+03    |
|    n_updates            | 2072        |
|    policy_gradient_loss | 0.0673      |
|    value_loss           | 2.59e+03    |
-----------------------------------------
Eval num_timesteps=895500, episode_reward=-411.95 +/- 61.43
Episode length: 45.78 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 895500   |
---------------------------------
Eval num_timesteps=896000, episode_reward=-396.82 +/- 77.44
Episode length: 48.18 +/- 15.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=896500, episode_reward=-401.00 +/- 80.79
Episode length: 47.40 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 896500   |
---------------------------------
Eval num_timesteps=897000, episode_reward=-410.38 +/- 66.13
Episode length: 44.80 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | -384     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 438      |
|    time_elapsed    | 3842     |
|    total_timesteps | 897024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=897500, episode_reward=-396.16 +/- 76.61
Episode length: 47.22 +/- 14.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -396        |
| time/                   |             |
|    total_timesteps      | 897500      |
| train/                  |             |
|    approx_kl            | 0.027762594 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.001       |
|    loss                 | 887         |
|    n_updates            | 2073        |
|    policy_gradient_loss | -0.000831   |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=898000, episode_reward=-398.99 +/- 69.93
Episode length: 49.92 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 898000   |
---------------------------------
Eval num_timesteps=898500, episode_reward=-405.85 +/- 81.92
Episode length: 44.74 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 898500   |
---------------------------------
Eval num_timesteps=899000, episode_reward=-411.69 +/- 70.02
Episode length: 47.48 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.5     |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 439      |
|    time_elapsed    | 3850     |
|    total_timesteps | 899072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=899500, episode_reward=-426.87 +/- 53.90
Episode length: 52.48 +/- 16.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.5        |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 899500      |
| train/                  |             |
|    approx_kl            | 0.025402488 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.001       |
|    loss                 | 610         |
|    n_updates            | 2074        |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=900000, episode_reward=-408.40 +/- 69.62
Episode length: 47.70 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=900500, episode_reward=-390.35 +/- 65.28
Episode length: 47.30 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 900500   |
---------------------------------
Eval num_timesteps=901000, episode_reward=-411.56 +/- 67.99
Episode length: 44.80 +/- 14.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 440      |
|    time_elapsed    | 3858     |
|    total_timesteps | 901120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=901500, episode_reward=-418.06 +/- 55.96
Episode length: 47.14 +/- 14.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.1         |
|    mean_reward          | -418         |
| time/                   |              |
|    total_timesteps      | 901500       |
| train/                  |              |
|    approx_kl            | 0.0085620135 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.393       |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.001        |
|    loss                 | 838          |
|    n_updates            | 2075         |
|    policy_gradient_loss | 0.00461      |
|    value_loss           | 1.91e+03     |
------------------------------------------
Eval num_timesteps=902000, episode_reward=-428.28 +/- 53.43
Episode length: 46.10 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 902000   |
---------------------------------
Eval num_timesteps=902500, episode_reward=-386.97 +/- 74.97
Episode length: 50.40 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 902500   |
---------------------------------
Eval num_timesteps=903000, episode_reward=-390.57 +/- 96.55
Episode length: 46.96 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -391     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 441      |
|    time_elapsed    | 3866     |
|    total_timesteps | 903168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=903500, episode_reward=-438.73 +/- 65.26
Episode length: 54.30 +/- 15.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.3        |
|    mean_reward          | -439        |
| time/                   |             |
|    total_timesteps      | 903500      |
| train/                  |             |
|    approx_kl            | 0.005030877 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.001       |
|    loss                 | 617         |
|    n_updates            | 2076        |
|    policy_gradient_loss | 0.00176     |
|    value_loss           | 1.48e+03    |
-----------------------------------------
Eval num_timesteps=904000, episode_reward=-416.36 +/- 74.46
Episode length: 47.58 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 904000   |
---------------------------------
Eval num_timesteps=904500, episode_reward=-409.13 +/- 76.08
Episode length: 50.46 +/- 18.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 904500   |
---------------------------------
Eval num_timesteps=905000, episode_reward=-430.39 +/- 74.61
Episode length: 54.54 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 442      |
|    time_elapsed    | 3875     |
|    total_timesteps | 905216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=905500, episode_reward=-423.53 +/- 58.53
Episode length: 48.86 +/- 13.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.9        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 905500      |
| train/                  |             |
|    approx_kl            | 0.008773433 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.001       |
|    loss                 | 919         |
|    n_updates            | 2077        |
|    policy_gradient_loss | 0.00727     |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=906000, episode_reward=-421.12 +/- 60.45
Episode length: 46.48 +/- 14.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
Eval num_timesteps=906500, episode_reward=-426.61 +/- 65.23
Episode length: 53.60 +/- 19.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 906500   |
---------------------------------
Eval num_timesteps=907000, episode_reward=-415.14 +/- 77.21
Episode length: 48.00 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.1     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 443      |
|    time_elapsed    | 3883     |
|    total_timesteps | 907264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=907500, episode_reward=-408.60 +/- 70.40
Episode length: 48.62 +/- 17.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 907500      |
| train/                  |             |
|    approx_kl            | 0.009474537 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.001       |
|    loss                 | 935         |
|    n_updates            | 2078        |
|    policy_gradient_loss | 0.000328    |
|    value_loss           | 1.76e+03    |
-----------------------------------------
Eval num_timesteps=908000, episode_reward=-406.29 +/- 78.50
Episode length: 49.58 +/- 19.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 908000   |
---------------------------------
Eval num_timesteps=908500, episode_reward=-415.86 +/- 63.89
Episode length: 52.52 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 908500   |
---------------------------------
Eval num_timesteps=909000, episode_reward=-424.80 +/- 68.01
Episode length: 49.98 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.8     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 444      |
|    time_elapsed    | 3892     |
|    total_timesteps | 909312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=909500, episode_reward=-427.76 +/- 68.53
Episode length: 50.66 +/- 16.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 909500      |
| train/                  |             |
|    approx_kl            | 0.024071103 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.849      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.001       |
|    loss                 | 813         |
|    n_updates            | 2079        |
|    policy_gradient_loss | 0.0257      |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=-409.25 +/- 76.80
Episode length: 52.34 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
Eval num_timesteps=910500, episode_reward=-429.62 +/- 55.16
Episode length: 54.26 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 910500   |
---------------------------------
Eval num_timesteps=911000, episode_reward=-429.66 +/- 59.47
Episode length: 50.54 +/- 12.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.3     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 445      |
|    time_elapsed    | 3901     |
|    total_timesteps | 911360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=911500, episode_reward=-396.45 +/- 68.61
Episode length: 35.56 +/- 10.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | -396        |
| time/                   |             |
|    total_timesteps      | 911500      |
| train/                  |             |
|    approx_kl            | 0.029351931 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.001       |
|    loss                 | 1.02e+03    |
|    n_updates            | 2080        |
|    policy_gradient_loss | 0.0304      |
|    value_loss           | 2.13e+03    |
-----------------------------------------
Eval num_timesteps=912000, episode_reward=-344.49 +/- 101.75
Episode length: 33.88 +/- 10.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
Eval num_timesteps=912500, episode_reward=-401.68 +/- 82.32
Episode length: 38.22 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.2     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 912500   |
---------------------------------
Eval num_timesteps=913000, episode_reward=-378.19 +/- 85.69
Episode length: 35.80 +/- 10.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.4     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 446      |
|    time_elapsed    | 3907     |
|    total_timesteps | 913408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=913500, episode_reward=-392.04 +/- 87.50
Episode length: 39.42 +/- 10.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39.4       |
|    mean_reward          | -392       |
| time/                   |            |
|    total_timesteps      | 913500     |
| train/                  |            |
|    approx_kl            | 0.02884012 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.001      |
|    loss                 | 1.44e+03   |
|    n_updates            | 2081       |
|    policy_gradient_loss | 0.0351     |
|    value_loss           | 2.67e+03   |
----------------------------------------
Eval num_timesteps=914000, episode_reward=-387.17 +/- 87.25
Episode length: 41.20 +/- 10.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 914000   |
---------------------------------
Eval num_timesteps=914500, episode_reward=-386.88 +/- 91.55
Episode length: 38.22 +/- 10.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.2     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 914500   |
---------------------------------
Eval num_timesteps=915000, episode_reward=-373.06 +/- 87.97
Episode length: 37.70 +/- 8.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.7     |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.5     |
|    ep_rew_mean     | -369     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 447      |
|    time_elapsed    | 3914     |
|    total_timesteps | 915456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=915500, episode_reward=-422.58 +/- 69.02
Episode length: 49.24 +/- 11.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 915500      |
| train/                  |             |
|    approx_kl            | 0.020794336 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.001       |
|    loss                 | 755         |
|    n_updates            | 2082        |
|    policy_gradient_loss | 0.0242      |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=916000, episode_reward=-400.97 +/- 78.47
Episode length: 53.00 +/- 16.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 916000   |
---------------------------------
Eval num_timesteps=916500, episode_reward=-412.88 +/- 70.21
Episode length: 47.84 +/- 14.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 916500   |
---------------------------------
Eval num_timesteps=917000, episode_reward=-413.17 +/- 76.51
Episode length: 49.98 +/- 18.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=917500, episode_reward=-408.54 +/- 66.78
Episode length: 52.04 +/- 14.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 917500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | -366     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 448      |
|    time_elapsed    | 3925     |
|    total_timesteps | 917504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=918000, episode_reward=-428.36 +/- 69.01
Episode length: 49.26 +/- 15.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 918000      |
| train/                  |             |
|    approx_kl            | 0.011725348 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.99       |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.001       |
|    loss                 | 1.4e+03     |
|    n_updates            | 2083        |
|    policy_gradient_loss | 0.0211      |
|    value_loss           | 2.51e+03    |
-----------------------------------------
Eval num_timesteps=918500, episode_reward=-411.43 +/- 67.64
Episode length: 47.16 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 918500   |
---------------------------------
Eval num_timesteps=919000, episode_reward=-431.21 +/- 74.82
Episode length: 54.72 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 919000   |
---------------------------------
Eval num_timesteps=919500, episode_reward=-412.80 +/- 68.90
Episode length: 47.54 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 919500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | -378     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 449      |
|    time_elapsed    | 3933     |
|    total_timesteps | 919552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=920000, episode_reward=-399.66 +/- 84.09
Episode length: 49.88 +/- 15.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -400        |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.009881607 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.941      |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.001       |
|    loss                 | 716         |
|    n_updates            | 2084        |
|    policy_gradient_loss | 0.00607     |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=920500, episode_reward=-414.37 +/- 83.32
Episode length: 48.28 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 920500   |
---------------------------------
Eval num_timesteps=921000, episode_reward=-423.15 +/- 71.30
Episode length: 49.90 +/- 17.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 921000   |
---------------------------------
Eval num_timesteps=921500, episode_reward=-426.61 +/- 45.69
Episode length: 51.86 +/- 12.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 921500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.8     |
|    ep_rew_mean     | -390     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 450      |
|    time_elapsed    | 3942     |
|    total_timesteps | 921600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=922000, episode_reward=-415.69 +/- 59.92
Episode length: 52.68 +/- 15.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.7        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.009064703 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.967      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.001       |
|    loss                 | 1.02e+03    |
|    n_updates            | 2085        |
|    policy_gradient_loss | 0.0133      |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=922500, episode_reward=-405.04 +/- 70.26
Episode length: 49.96 +/- 19.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 922500   |
---------------------------------
Eval num_timesteps=923000, episode_reward=-401.99 +/- 65.24
Episode length: 46.12 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 923000   |
---------------------------------
Eval num_timesteps=923500, episode_reward=-428.69 +/- 66.36
Episode length: 50.34 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 923500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.8     |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 451      |
|    time_elapsed    | 3951     |
|    total_timesteps | 923648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=924000, episode_reward=-413.19 +/- 76.20
Episode length: 52.24 +/- 15.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.2        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 924000      |
| train/                  |             |
|    approx_kl            | 0.015279302 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.001       |
|    loss                 | 898         |
|    n_updates            | 2086        |
|    policy_gradient_loss | 0.00583     |
|    value_loss           | 1.54e+03    |
-----------------------------------------
Eval num_timesteps=924500, episode_reward=-423.69 +/- 67.38
Episode length: 52.42 +/- 20.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 924500   |
---------------------------------
Eval num_timesteps=925000, episode_reward=-430.96 +/- 59.01
Episode length: 53.92 +/- 19.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
Eval num_timesteps=925500, episode_reward=-424.29 +/- 56.62
Episode length: 52.80 +/- 19.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 925500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.9     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 452      |
|    time_elapsed    | 3960     |
|    total_timesteps | 925696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=926000, episode_reward=-425.60 +/- 56.28
Episode length: 54.30 +/- 17.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 54.3       |
|    mean_reward          | -426       |
| time/                   |            |
|    total_timesteps      | 926000     |
| train/                  |            |
|    approx_kl            | 0.01534898 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.001      |
|    loss                 | 689        |
|    n_updates            | 2087       |
|    policy_gradient_loss | 0.00705    |
|    value_loss           | 1.67e+03   |
----------------------------------------
Eval num_timesteps=926500, episode_reward=-428.58 +/- 56.06
Episode length: 60.52 +/- 17.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 926500   |
---------------------------------
Eval num_timesteps=927000, episode_reward=-432.19 +/- 62.49
Episode length: 53.20 +/- 15.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 927000   |
---------------------------------
Eval num_timesteps=927500, episode_reward=-417.76 +/- 75.21
Episode length: 50.50 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 927500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 453      |
|    time_elapsed    | 3969     |
|    total_timesteps | 927744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=928000, episode_reward=-408.62 +/- 66.60
Episode length: 49.12 +/- 19.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.1       |
|    mean_reward          | -409       |
| time/                   |            |
|    total_timesteps      | 928000     |
| train/                  |            |
|    approx_kl            | 0.01330373 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.001      |
|    loss                 | 870        |
|    n_updates            | 2088       |
|    policy_gradient_loss | 0.0244     |
|    value_loss           | 1.81e+03   |
----------------------------------------
Eval num_timesteps=928500, episode_reward=-435.99 +/- 47.02
Episode length: 55.60 +/- 20.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 928500   |
---------------------------------
Eval num_timesteps=929000, episode_reward=-411.61 +/- 68.74
Episode length: 50.08 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 929000   |
---------------------------------
Eval num_timesteps=929500, episode_reward=-419.16 +/- 66.10
Episode length: 50.16 +/- 13.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 929500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 454      |
|    time_elapsed    | 3977     |
|    total_timesteps | 929792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=930000, episode_reward=-423.71 +/- 76.07
Episode length: 49.88 +/- 13.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.009600795 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.001       |
|    loss                 | 475         |
|    n_updates            | 2089        |
|    policy_gradient_loss | 0.00458     |
|    value_loss           | 985         |
-----------------------------------------
Eval num_timesteps=930500, episode_reward=-411.73 +/- 71.98
Episode length: 46.48 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 930500   |
---------------------------------
Eval num_timesteps=931000, episode_reward=-415.99 +/- 62.71
Episode length: 47.50 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 931000   |
---------------------------------
Eval num_timesteps=931500, episode_reward=-414.03 +/- 68.64
Episode length: 48.94 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 931500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 455      |
|    time_elapsed    | 3986     |
|    total_timesteps | 931840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.28
Eval num_timesteps=932000, episode_reward=-393.93 +/- 75.88
Episode length: 40.18 +/- 9.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40.2       |
|    mean_reward          | -394       |
| time/                   |            |
|    total_timesteps      | 932000     |
| train/                  |            |
|    approx_kl            | 0.09708691 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.001      |
|    loss                 | 816        |
|    n_updates            | 2090       |
|    policy_gradient_loss | 0.078      |
|    value_loss           | 1.74e+03   |
----------------------------------------
Eval num_timesteps=932500, episode_reward=-386.12 +/- 83.80
Episode length: 38.96 +/- 10.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 932500   |
---------------------------------
Eval num_timesteps=933000, episode_reward=-381.91 +/- 76.40
Episode length: 40.72 +/- 11.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.7     |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 933000   |
---------------------------------
Eval num_timesteps=933500, episode_reward=-404.46 +/- 71.64
Episode length: 40.44 +/- 9.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 933500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -395     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 456      |
|    time_elapsed    | 3993     |
|    total_timesteps | 933888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=934000, episode_reward=-395.38 +/- 81.27
Episode length: 43.34 +/- 13.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 43.3       |
|    mean_reward          | -395       |
| time/                   |            |
|    total_timesteps      | 934000     |
| train/                  |            |
|    approx_kl            | 0.02179485 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.001      |
|    loss                 | 818        |
|    n_updates            | 2091       |
|    policy_gradient_loss | 0.00985    |
|    value_loss           | 1.76e+03   |
----------------------------------------
Eval num_timesteps=934500, episode_reward=-403.52 +/- 79.28
Episode length: 42.28 +/- 12.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 934500   |
---------------------------------
Eval num_timesteps=935000, episode_reward=-401.72 +/- 72.94
Episode length: 41.84 +/- 9.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 935000   |
---------------------------------
Eval num_timesteps=935500, episode_reward=-394.10 +/- 84.60
Episode length: 41.86 +/- 11.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 935500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -382     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 457      |
|    time_elapsed    | 4001     |
|    total_timesteps | 935936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=936000, episode_reward=-407.27 +/- 76.22
Episode length: 42.80 +/- 14.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.8        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 936000      |
| train/                  |             |
|    approx_kl            | 0.018022144 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.001       |
|    loss                 | 1.25e+03    |
|    n_updates            | 2092        |
|    policy_gradient_loss | 0.00105     |
|    value_loss           | 2.25e+03    |
-----------------------------------------
Eval num_timesteps=936500, episode_reward=-399.45 +/- 76.07
Episode length: 44.46 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 936500   |
---------------------------------
Eval num_timesteps=937000, episode_reward=-401.20 +/- 73.83
Episode length: 42.20 +/- 9.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 937000   |
---------------------------------
Eval num_timesteps=937500, episode_reward=-397.55 +/- 80.29
Episode length: 42.52 +/- 10.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 937500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 458      |
|    time_elapsed    | 4009     |
|    total_timesteps | 937984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=938000, episode_reward=-406.71 +/- 82.58
Episode length: 46.64 +/- 11.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.6        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 938000      |
| train/                  |             |
|    approx_kl            | 0.010578738 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.001       |
|    loss                 | 1.21e+03    |
|    n_updates            | 2093        |
|    policy_gradient_loss | 0.00512     |
|    value_loss           | 2.52e+03    |
-----------------------------------------
Eval num_timesteps=938500, episode_reward=-406.75 +/- 74.38
Episode length: 47.02 +/- 16.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 938500   |
---------------------------------
Eval num_timesteps=939000, episode_reward=-404.07 +/- 74.78
Episode length: 44.84 +/- 11.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
Eval num_timesteps=939500, episode_reward=-416.74 +/- 67.96
Episode length: 44.08 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 939500   |
---------------------------------
Eval num_timesteps=940000, episode_reward=-412.73 +/- 71.41
Episode length: 48.12 +/- 13.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.8     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 459      |
|    time_elapsed    | 4018     |
|    total_timesteps | 940032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=940500, episode_reward=-424.98 +/- 61.97
Episode length: 50.62 +/- 16.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.6        |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 940500      |
| train/                  |             |
|    approx_kl            | 0.019248033 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.001       |
|    loss                 | 877         |
|    n_updates            | 2094        |
|    policy_gradient_loss | 0.0155      |
|    value_loss           | 1.9e+03     |
-----------------------------------------
Eval num_timesteps=941000, episode_reward=-406.17 +/- 89.49
Episode length: 45.06 +/- 13.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 941000   |
---------------------------------
Eval num_timesteps=941500, episode_reward=-396.53 +/- 76.49
Episode length: 47.66 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 941500   |
---------------------------------
Eval num_timesteps=942000, episode_reward=-415.39 +/- 49.06
Episode length: 49.26 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.7     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 460      |
|    time_elapsed    | 4027     |
|    total_timesteps | 942080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=942500, episode_reward=-406.39 +/- 75.04
Episode length: 49.80 +/- 14.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 942500      |
| train/                  |             |
|    approx_kl            | 0.012654863 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.001       |
|    loss                 | 749         |
|    n_updates            | 2095        |
|    policy_gradient_loss | 0.0237      |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=943000, episode_reward=-422.17 +/- 61.70
Episode length: 53.40 +/- 16.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 943000   |
---------------------------------
Eval num_timesteps=943500, episode_reward=-409.22 +/- 65.33
Episode length: 49.28 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 943500   |
---------------------------------
Eval num_timesteps=944000, episode_reward=-418.02 +/- 59.67
Episode length: 47.92 +/- 14.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.9     |
|    ep_rew_mean     | -394     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 461      |
|    time_elapsed    | 4036     |
|    total_timesteps | 944128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=944500, episode_reward=-407.10 +/- 66.36
Episode length: 47.52 +/- 15.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.5       |
|    mean_reward          | -407       |
| time/                   |            |
|    total_timesteps      | 944500     |
| train/                  |            |
|    approx_kl            | 0.02875267 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.001      |
|    loss                 | 978        |
|    n_updates            | 2096       |
|    policy_gradient_loss | 0.0119     |
|    value_loss           | 1.82e+03   |
----------------------------------------
Eval num_timesteps=945000, episode_reward=-412.83 +/- 78.29
Episode length: 48.94 +/- 18.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 945000   |
---------------------------------
Eval num_timesteps=945500, episode_reward=-404.66 +/- 78.69
Episode length: 46.16 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 945500   |
---------------------------------
Eval num_timesteps=946000, episode_reward=-419.94 +/- 75.93
Episode length: 49.00 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.1     |
|    ep_rew_mean     | -401     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 462      |
|    time_elapsed    | 4044     |
|    total_timesteps | 946176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=946500, episode_reward=-422.95 +/- 61.19
Episode length: 52.08 +/- 17.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.1        |
|    mean_reward          | -423        |
| time/                   |             |
|    total_timesteps      | 946500      |
| train/                  |             |
|    approx_kl            | 0.026075326 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.001       |
|    loss                 | 1.11e+03    |
|    n_updates            | 2097        |
|    policy_gradient_loss | 0.031       |
|    value_loss           | 2.48e+03    |
-----------------------------------------
Eval num_timesteps=947000, episode_reward=-424.98 +/- 63.60
Episode length: 49.94 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 947000   |
---------------------------------
Eval num_timesteps=947500, episode_reward=-403.86 +/- 74.06
Episode length: 51.16 +/- 17.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 947500   |
---------------------------------
Eval num_timesteps=948000, episode_reward=-409.95 +/- 83.36
Episode length: 51.14 +/- 17.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.6     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 463      |
|    time_elapsed    | 4053     |
|    total_timesteps | 948224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=948500, episode_reward=-412.40 +/- 74.40
Episode length: 52.92 +/- 17.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.9       |
|    mean_reward          | -412       |
| time/                   |            |
|    total_timesteps      | 948500     |
| train/                  |            |
|    approx_kl            | 0.00935236 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.001      |
|    loss                 | 808        |
|    n_updates            | 2098       |
|    policy_gradient_loss | 0.0133     |
|    value_loss           | 1.19e+03   |
----------------------------------------
Eval num_timesteps=949000, episode_reward=-418.39 +/- 72.79
Episode length: 52.02 +/- 21.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 949000   |
---------------------------------
Eval num_timesteps=949500, episode_reward=-429.75 +/- 63.65
Episode length: 54.98 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 949500   |
---------------------------------
Eval num_timesteps=950000, episode_reward=-423.80 +/- 61.72
Episode length: 48.66 +/- 15.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.1     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 464      |
|    time_elapsed    | 4062     |
|    total_timesteps | 950272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=950500, episode_reward=-433.22 +/- 60.73
Episode length: 48.24 +/- 18.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.2        |
|    mean_reward          | -433        |
| time/                   |             |
|    total_timesteps      | 950500      |
| train/                  |             |
|    approx_kl            | 0.007960711 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.775      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.001       |
|    loss                 | 908         |
|    n_updates            | 2099        |
|    policy_gradient_loss | 0.000877    |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=951000, episode_reward=-423.19 +/- 66.00
Episode length: 52.06 +/- 20.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 951000   |
---------------------------------
Eval num_timesteps=951500, episode_reward=-402.19 +/- 80.17
Episode length: 50.92 +/- 19.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 951500   |
---------------------------------
Eval num_timesteps=952000, episode_reward=-410.59 +/- 70.30
Episode length: 48.22 +/- 15.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.1     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 465      |
|    time_elapsed    | 4070     |
|    total_timesteps | 952320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.28
Eval num_timesteps=952500, episode_reward=-416.48 +/- 83.58
Episode length: 51.42 +/- 13.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.4       |
|    mean_reward          | -416       |
| time/                   |            |
|    total_timesteps      | 952500     |
| train/                  |            |
|    approx_kl            | 0.07152489 |
|    clip_fraction        | 0.197      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.001      |
|    loss                 | 830        |
|    n_updates            | 2100       |
|    policy_gradient_loss | 0.0373     |
|    value_loss           | 1.46e+03   |
----------------------------------------
Eval num_timesteps=953000, episode_reward=-403.35 +/- 69.78
Episode length: 49.38 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 953000   |
---------------------------------
Eval num_timesteps=953500, episode_reward=-402.80 +/- 69.96
Episode length: 49.12 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 953500   |
---------------------------------
Eval num_timesteps=954000, episode_reward=-425.59 +/- 62.06
Episode length: 48.96 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 466      |
|    time_elapsed    | 4079     |
|    total_timesteps | 954368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.28
Eval num_timesteps=954500, episode_reward=-425.35 +/- 68.95
Episode length: 47.76 +/- 16.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.8       |
|    mean_reward          | -425       |
| time/                   |            |
|    total_timesteps      | 954500     |
| train/                  |            |
|    approx_kl            | 0.13774057 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.953     |
|    explained_variance   | 0.727      |
|    learning_rate        | 0.001      |
|    loss                 | 919        |
|    n_updates            | 2101       |
|    policy_gradient_loss | 0.0468     |
|    value_loss           | 1.53e+03   |
----------------------------------------
Eval num_timesteps=955000, episode_reward=-417.60 +/- 80.38
Episode length: 53.10 +/- 17.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 955000   |
---------------------------------
Eval num_timesteps=955500, episode_reward=-421.90 +/- 56.77
Episode length: 48.88 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 955500   |
---------------------------------
Eval num_timesteps=956000, episode_reward=-415.80 +/- 67.51
Episode length: 51.94 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -424     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 467      |
|    time_elapsed    | 4087     |
|    total_timesteps | 956416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=956500, episode_reward=-401.31 +/- 80.48
Episode length: 49.22 +/- 20.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -401        |
| time/                   |             |
|    total_timesteps      | 956500      |
| train/                  |             |
|    approx_kl            | 0.060056545 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.001       |
|    loss                 | 692         |
|    n_updates            | 2102        |
|    policy_gradient_loss | 0.0197      |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=957000, episode_reward=-412.10 +/- 66.11
Episode length: 49.46 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 957000   |
---------------------------------
Eval num_timesteps=957500, episode_reward=-405.64 +/- 77.56
Episode length: 53.28 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 957500   |
---------------------------------
Eval num_timesteps=958000, episode_reward=-414.63 +/- 89.82
Episode length: 51.58 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -418     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 468      |
|    time_elapsed    | 4096     |
|    total_timesteps | 958464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=958500, episode_reward=-410.29 +/- 64.89
Episode length: 51.26 +/- 15.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.3       |
|    mean_reward          | -410       |
| time/                   |            |
|    total_timesteps      | 958500     |
| train/                  |            |
|    approx_kl            | 0.01630989 |
|    clip_fraction        | 0.0625     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.578     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.001      |
|    loss                 | 974        |
|    n_updates            | 2103       |
|    policy_gradient_loss | 0.00453    |
|    value_loss           | 2.02e+03   |
----------------------------------------
Eval num_timesteps=959000, episode_reward=-406.06 +/- 76.22
Episode length: 48.66 +/- 10.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 959000   |
---------------------------------
Eval num_timesteps=959500, episode_reward=-426.49 +/- 50.60
Episode length: 52.74 +/- 18.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 959500   |
---------------------------------
Eval num_timesteps=960000, episode_reward=-421.20 +/- 70.12
Episode length: 49.28 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=960500, episode_reward=-427.70 +/- 61.80
Episode length: 52.90 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 960500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.1     |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 469      |
|    time_elapsed    | 4107     |
|    total_timesteps | 960512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=961000, episode_reward=-412.97 +/- 56.47
Episode length: 48.06 +/- 14.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 961000      |
| train/                  |             |
|    approx_kl            | 0.009372976 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.001       |
|    loss                 | 785         |
|    n_updates            | 2104        |
|    policy_gradient_loss | 0.0046      |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=961500, episode_reward=-407.97 +/- 74.98
Episode length: 48.54 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 961500   |
---------------------------------
Eval num_timesteps=962000, episode_reward=-394.21 +/- 75.18
Episode length: 49.24 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 962000   |
---------------------------------
Eval num_timesteps=962500, episode_reward=-406.02 +/- 69.94
Episode length: 50.22 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 962500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 470      |
|    time_elapsed    | 4115     |
|    total_timesteps | 962560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=963000, episode_reward=-420.75 +/- 69.56
Episode length: 55.18 +/- 18.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.2        |
|    mean_reward          | -421        |
| time/                   |             |
|    total_timesteps      | 963000      |
| train/                  |             |
|    approx_kl            | 0.012636216 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.001       |
|    loss                 | 463         |
|    n_updates            | 2105        |
|    policy_gradient_loss | 0.0127      |
|    value_loss           | 1.15e+03    |
-----------------------------------------
Eval num_timesteps=963500, episode_reward=-412.40 +/- 65.39
Episode length: 51.52 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 963500   |
---------------------------------
Eval num_timesteps=964000, episode_reward=-409.30 +/- 78.96
Episode length: 50.18 +/- 13.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 964000   |
---------------------------------
Eval num_timesteps=964500, episode_reward=-420.85 +/- 75.11
Episode length: 48.20 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 964500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.1     |
|    ep_rew_mean     | -402     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 471      |
|    time_elapsed    | 4124     |
|    total_timesteps | 964608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=965000, episode_reward=-394.56 +/- 88.07
Episode length: 45.28 +/- 15.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.3        |
|    mean_reward          | -395        |
| time/                   |             |
|    total_timesteps      | 965000      |
| train/                  |             |
|    approx_kl            | 0.073290505 |
|    clip_fraction        | 0.41        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.873      |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.001       |
|    loss                 | 983         |
|    n_updates            | 2106        |
|    policy_gradient_loss | 0.0205      |
|    value_loss           | 1.93e+03    |
-----------------------------------------
Eval num_timesteps=965500, episode_reward=-411.93 +/- 66.86
Episode length: 51.66 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 965500   |
---------------------------------
Eval num_timesteps=966000, episode_reward=-400.21 +/- 61.87
Episode length: 44.80 +/- 12.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
Eval num_timesteps=966500, episode_reward=-414.93 +/- 73.79
Episode length: 47.22 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 966500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 472      |
|    time_elapsed    | 4132     |
|    total_timesteps | 966656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.17
Eval num_timesteps=967000, episode_reward=-419.06 +/- 72.06
Episode length: 53.76 +/- 17.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.8      |
|    mean_reward          | -419      |
| time/                   |           |
|    total_timesteps      | 967000    |
| train/                  |           |
|    approx_kl            | 0.0832381 |
|    clip_fraction        | 0.367     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.975    |
|    explained_variance   | 0.725     |
|    learning_rate        | 0.001     |
|    loss                 | 863       |
|    n_updates            | 2107      |
|    policy_gradient_loss | 0.0367    |
|    value_loss           | 1.76e+03  |
---------------------------------------
Eval num_timesteps=967500, episode_reward=-419.76 +/- 61.44
Episode length: 51.46 +/- 16.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 967500   |
---------------------------------
Eval num_timesteps=968000, episode_reward=-424.12 +/- 64.51
Episode length: 51.22 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 968000   |
---------------------------------
Eval num_timesteps=968500, episode_reward=-415.17 +/- 70.29
Episode length: 48.40 +/- 16.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 968500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -399     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 473      |
|    time_elapsed    | 4141     |
|    total_timesteps | 968704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=969000, episode_reward=-421.82 +/- 63.66
Episode length: 53.58 +/- 20.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.6        |
|    mean_reward          | -422        |
| time/                   |             |
|    total_timesteps      | 969000      |
| train/                  |             |
|    approx_kl            | 0.037682407 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.001       |
|    loss                 | 1.84e+03    |
|    n_updates            | 2108        |
|    policy_gradient_loss | 0.0109      |
|    value_loss           | 2.64e+03    |
-----------------------------------------
Eval num_timesteps=969500, episode_reward=-408.24 +/- 71.52
Episode length: 47.96 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 969500   |
---------------------------------
Eval num_timesteps=970000, episode_reward=-416.89 +/- 72.88
Episode length: 48.56 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 970000   |
---------------------------------
Eval num_timesteps=970500, episode_reward=-427.30 +/- 62.38
Episode length: 46.80 +/- 15.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 970500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 474      |
|    time_elapsed    | 4150     |
|    total_timesteps | 970752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=971000, episode_reward=-427.25 +/- 63.32
Episode length: 52.64 +/- 18.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.6        |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 971000      |
| train/                  |             |
|    approx_kl            | 0.013300197 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.001       |
|    loss                 | 768         |
|    n_updates            | 2109        |
|    policy_gradient_loss | 0.00178     |
|    value_loss           | 1.78e+03    |
-----------------------------------------
Eval num_timesteps=971500, episode_reward=-405.64 +/- 90.27
Episode length: 48.24 +/- 16.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 971500   |
---------------------------------
Eval num_timesteps=972000, episode_reward=-415.91 +/- 70.60
Episode length: 49.20 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
Eval num_timesteps=972500, episode_reward=-417.15 +/- 74.29
Episode length: 51.36 +/- 18.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 972500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.4     |
|    ep_rew_mean     | -403     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 475      |
|    time_elapsed    | 4158     |
|    total_timesteps | 972800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=973000, episode_reward=-409.86 +/- 67.85
Episode length: 49.78 +/- 19.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.8        |
|    mean_reward          | -410        |
| time/                   |             |
|    total_timesteps      | 973000      |
| train/                  |             |
|    approx_kl            | 0.012855828 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.523      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.001       |
|    loss                 | 1.09e+03    |
|    n_updates            | 2110        |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 1.92e+03    |
-----------------------------------------
Eval num_timesteps=973500, episode_reward=-409.39 +/- 76.79
Episode length: 51.76 +/- 16.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 973500   |
---------------------------------
Eval num_timesteps=974000, episode_reward=-412.39 +/- 78.18
Episode length: 49.98 +/- 16.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 974000   |
---------------------------------
Eval num_timesteps=974500, episode_reward=-405.79 +/- 73.24
Episode length: 51.26 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 974500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 476      |
|    time_elapsed    | 4167     |
|    total_timesteps | 974848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=975000, episode_reward=-406.36 +/- 69.57
Episode length: 47.90 +/- 18.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.9        |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 975000      |
| train/                  |             |
|    approx_kl            | 0.004888091 |
|    clip_fraction        | 0.0254      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.398      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.001       |
|    loss                 | 767         |
|    n_updates            | 2111        |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 2.14e+03    |
-----------------------------------------
Eval num_timesteps=975500, episode_reward=-388.40 +/- 88.21
Episode length: 48.84 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 975500   |
---------------------------------
Eval num_timesteps=976000, episode_reward=-431.00 +/- 75.54
Episode length: 47.86 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 976000   |
---------------------------------
Eval num_timesteps=976500, episode_reward=-424.40 +/- 62.23
Episode length: 52.14 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 976500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.4     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 477      |
|    time_elapsed    | 4176     |
|    total_timesteps | 976896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=977000, episode_reward=-412.35 +/- 77.20
Episode length: 55.58 +/- 20.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.6         |
|    mean_reward          | -412         |
| time/                   |              |
|    total_timesteps      | 977000       |
| train/                  |              |
|    approx_kl            | 0.0073078247 |
|    clip_fraction        | 0.0422       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.268       |
|    explained_variance   | 0.68         |
|    learning_rate        | 0.001        |
|    loss                 | 771          |
|    n_updates            | 2112         |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 1.8e+03      |
------------------------------------------
Eval num_timesteps=977500, episode_reward=-425.59 +/- 64.64
Episode length: 53.30 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 977500   |
---------------------------------
Eval num_timesteps=978000, episode_reward=-413.00 +/- 65.98
Episode length: 47.14 +/- 13.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
Eval num_timesteps=978500, episode_reward=-428.00 +/- 69.10
Episode length: 55.16 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 978500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 478      |
|    time_elapsed    | 4185     |
|    total_timesteps | 978944   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=979000, episode_reward=-420.20 +/- 61.16
Episode length: 48.46 +/- 16.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.5        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 979000      |
| train/                  |             |
|    approx_kl            | 0.112439565 |
|    clip_fraction        | 0.062       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.418      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.001       |
|    loss                 | 1.01e+03    |
|    n_updates            | 2114        |
|    policy_gradient_loss | 0.00254     |
|    value_loss           | 1.91e+03    |
-----------------------------------------
Eval num_timesteps=979500, episode_reward=-420.74 +/- 68.35
Episode length: 52.20 +/- 17.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 979500   |
---------------------------------
Eval num_timesteps=980000, episode_reward=-411.12 +/- 75.96
Episode length: 46.52 +/- 14.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=980500, episode_reward=-433.89 +/- 63.42
Episode length: 52.52 +/- 21.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 980500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 479      |
|    time_elapsed    | 4193     |
|    total_timesteps | 980992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=981000, episode_reward=-421.91 +/- 61.22
Episode length: 46.90 +/- 17.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.9        |
|    mean_reward          | -422        |
| time/                   |             |
|    total_timesteps      | 981000      |
| train/                  |             |
|    approx_kl            | 0.009068022 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.001       |
|    loss                 | 1.07e+03    |
|    n_updates            | 2115        |
|    policy_gradient_loss | 0.00314     |
|    value_loss           | 2.08e+03    |
-----------------------------------------
Eval num_timesteps=981500, episode_reward=-419.49 +/- 65.66
Episode length: 52.86 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 981500   |
---------------------------------
Eval num_timesteps=982000, episode_reward=-418.67 +/- 66.31
Episode length: 50.90 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
Eval num_timesteps=982500, episode_reward=-418.35 +/- 63.20
Episode length: 50.22 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 982500   |
---------------------------------
Eval num_timesteps=983000, episode_reward=-415.90 +/- 72.95
Episode length: 53.92 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 480      |
|    time_elapsed    | 4204     |
|    total_timesteps | 983040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.34
Eval num_timesteps=983500, episode_reward=-416.40 +/- 70.43
Episode length: 49.52 +/- 12.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 983500      |
| train/                  |             |
|    approx_kl            | 0.070663005 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.001       |
|    loss                 | 827         |
|    n_updates            | 2116        |
|    policy_gradient_loss | 0.00614     |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=984000, episode_reward=-411.84 +/- 64.16
Episode length: 46.16 +/- 13.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
Eval num_timesteps=984500, episode_reward=-418.26 +/- 62.10
Episode length: 52.00 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 984500   |
---------------------------------
Eval num_timesteps=985000, episode_reward=-403.79 +/- 67.08
Episode length: 48.82 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -385     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 481      |
|    time_elapsed    | 4212     |
|    total_timesteps | 985088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=985500, episode_reward=-416.22 +/- 63.23
Episode length: 54.28 +/- 17.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.3        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 985500      |
| train/                  |             |
|    approx_kl            | 0.037731998 |
|    clip_fraction        | 0.375       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.001       |
|    loss                 | 1e+03       |
|    n_updates            | 2117        |
|    policy_gradient_loss | 0.0239      |
|    value_loss           | 2.54e+03    |
-----------------------------------------
Eval num_timesteps=986000, episode_reward=-411.36 +/- 64.63
Episode length: 45.70 +/- 14.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 986000   |
---------------------------------
Eval num_timesteps=986500, episode_reward=-387.70 +/- 66.50
Episode length: 52.44 +/- 22.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 986500   |
---------------------------------
Eval num_timesteps=987000, episode_reward=-408.36 +/- 63.15
Episode length: 50.00 +/- 15.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.8     |
|    ep_rew_mean     | -378     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 482      |
|    time_elapsed    | 4221     |
|    total_timesteps | 987136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=987500, episode_reward=-387.57 +/- 75.69
Episode length: 41.42 +/- 10.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.4        |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 987500      |
| train/                  |             |
|    approx_kl            | 0.045133617 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.001       |
|    loss                 | 941         |
|    n_updates            | 2118        |
|    policy_gradient_loss | 0.0328      |
|    value_loss           | 2.33e+03    |
-----------------------------------------
Eval num_timesteps=988000, episode_reward=-377.05 +/- 79.74
Episode length: 44.24 +/- 14.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 988000   |
---------------------------------
Eval num_timesteps=988500, episode_reward=-385.58 +/- 85.26
Episode length: 41.30 +/- 11.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 988500   |
---------------------------------
Eval num_timesteps=989000, episode_reward=-425.33 +/- 65.35
Episode length: 42.16 +/- 13.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43       |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 483      |
|    time_elapsed    | 4229     |
|    total_timesteps | 989184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=989500, episode_reward=-379.11 +/- 80.91
Episode length: 46.38 +/- 16.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.4       |
|    mean_reward          | -379       |
| time/                   |            |
|    total_timesteps      | 989500     |
| train/                  |            |
|    approx_kl            | 0.03821115 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.001      |
|    loss                 | 722        |
|    n_updates            | 2119       |
|    policy_gradient_loss | 0.0152     |
|    value_loss           | 2.04e+03   |
----------------------------------------
Eval num_timesteps=990000, episode_reward=-408.82 +/- 81.41
Episode length: 47.04 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
Eval num_timesteps=990500, episode_reward=-402.90 +/- 80.21
Episode length: 47.72 +/- 13.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 990500   |
---------------------------------
Eval num_timesteps=991000, episode_reward=-417.30 +/- 70.23
Episode length: 49.00 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | -409     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 484      |
|    time_elapsed    | 4237     |
|    total_timesteps | 991232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=991500, episode_reward=-418.15 +/- 59.97
Episode length: 49.16 +/- 16.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | -418        |
| time/                   |             |
|    total_timesteps      | 991500      |
| train/                  |             |
|    approx_kl            | 0.017944267 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.001       |
|    loss                 | 848         |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.0184      |
|    value_loss           | 2.04e+03    |
-----------------------------------------
Eval num_timesteps=992000, episode_reward=-416.90 +/- 60.88
Episode length: 48.04 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
Eval num_timesteps=992500, episode_reward=-423.55 +/- 59.18
Episode length: 46.80 +/- 15.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 992500   |
---------------------------------
Eval num_timesteps=993000, episode_reward=-418.79 +/- 64.14
Episode length: 46.58 +/- 15.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 485      |
|    time_elapsed    | 4245     |
|    total_timesteps | 993280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=993500, episode_reward=-424.06 +/- 56.65
Episode length: 45.46 +/- 16.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.5        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 993500      |
| train/                  |             |
|    approx_kl            | 0.023752743 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.696       |
|    learning_rate        | 0.001       |
|    loss                 | 734         |
|    n_updates            | 2121        |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=994000, episode_reward=-412.08 +/- 75.75
Episode length: 49.38 +/- 14.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 994000   |
---------------------------------
Eval num_timesteps=994500, episode_reward=-402.20 +/- 64.97
Episode length: 46.30 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 994500   |
---------------------------------
Eval num_timesteps=995000, episode_reward=-423.72 +/- 68.36
Episode length: 46.24 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -416     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 486      |
|    time_elapsed    | 4254     |
|    total_timesteps | 995328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=995500, episode_reward=-420.42 +/- 58.87
Episode length: 51.46 +/- 16.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.5        |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 995500      |
| train/                  |             |
|    approx_kl            | 0.013541323 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.966      |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.001       |
|    loss                 | 871         |
|    n_updates            | 2122        |
|    policy_gradient_loss | 0.015       |
|    value_loss           | 2.01e+03    |
-----------------------------------------
Eval num_timesteps=996000, episode_reward=-416.26 +/- 61.16
Episode length: 51.08 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=996500, episode_reward=-428.98 +/- 60.39
Episode length: 48.90 +/- 15.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 996500   |
---------------------------------
Eval num_timesteps=997000, episode_reward=-413.28 +/- 82.20
Episode length: 46.12 +/- 15.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 487      |
|    time_elapsed    | 4262     |
|    total_timesteps | 997376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=997500, episode_reward=-432.37 +/- 52.21
Episode length: 54.92 +/- 19.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 54.9       |
|    mean_reward          | -432       |
| time/                   |            |
|    total_timesteps      | 997500     |
| train/                  |            |
|    approx_kl            | 0.04272541 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.961     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.001      |
|    loss                 | 841        |
|    n_updates            | 2123       |
|    policy_gradient_loss | 0.0269     |
|    value_loss           | 1.8e+03    |
----------------------------------------
Eval num_timesteps=998000, episode_reward=-428.50 +/- 70.44
Episode length: 56.02 +/- 17.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 998000   |
---------------------------------
Eval num_timesteps=998500, episode_reward=-414.28 +/- 76.57
Episode length: 48.64 +/- 14.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 998500   |
---------------------------------
Eval num_timesteps=999000, episode_reward=-411.14 +/- 60.76
Episode length: 49.14 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 488      |
|    time_elapsed    | 4271     |
|    total_timesteps | 999424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=999500, episode_reward=-415.64 +/- 74.28
Episode length: 48.66 +/- 20.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 999500      |
| train/                  |             |
|    approx_kl            | 0.035454344 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.001       |
|    loss                 | 960         |
|    n_updates            | 2124        |
|    policy_gradient_loss | 0.0153      |
|    value_loss           | 1.65e+03    |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=-393.46 +/- 71.35
Episode length: 45.46 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1000500, episode_reward=-409.24 +/- 70.85
Episode length: 47.32 +/- 18.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 1000500  |
---------------------------------
Eval num_timesteps=1001000, episode_reward=-435.02 +/- 58.88
Episode length: 55.64 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 1001000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.9     |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 489      |
|    time_elapsed    | 4279     |
|    total_timesteps | 1001472  |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-stop-3-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 0 due to reaching max kl: 0.03
