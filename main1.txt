2024-12-04 01:47:20.215582: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-12-04 01:47:25.777701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-04 01:47:32.899156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-499.96 +/- 90.40
Episode length: 52.96 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-529.70 +/- 62.53
Episode length: 53.60 +/- 15.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-517.43 +/- 84.67
Episode length: 47.90 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-525.40 +/- 72.08
Episode length: 51.88 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | -342     |
| time/              |          |
|    fps             | 50       |
|    iterations      | 1        |
|    time_elapsed    | 40       |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=920.40 +/- 732.72
Episode length: 36.00 +/- 6.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 920          |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0040879264 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -2.08        |
|    explained_variance   | -4.98e-05    |
|    learning_rate        | 0.001        |
|    loss                 | 1.32e+03     |
|    n_updates            | 1            |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 1.86e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=759.07 +/- 653.76
Episode length: 34.74 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=862.40 +/- 704.48
Episode length: 35.84 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=739.97 +/- 629.48
Episode length: 35.22 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.3     |
|    ep_rew_mean     | -343     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 2        |
|    time_elapsed    | 74       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=781.17 +/- 709.04
Episode length: 34.60 +/- 7.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 781         |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.009128341 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.000154    |
|    learning_rate        | 0.001       |
|    loss                 | 2.44e+03    |
|    n_updates            | 2           |
|    policy_gradient_loss | -0.00369    |
|    value_loss           | 4.41e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=695.08 +/- 643.71
Episode length: 33.74 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=814.95 +/- 675.36
Episode length: 35.00 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=966.59 +/- 686.04
Episode length: 37.02 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.7     |
|    ep_rew_mean     | -340     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 3        |
|    time_elapsed    | 103      |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=6500, episode_reward=834.72 +/- 650.04
Episode length: 36.34 +/- 5.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.3        |
|    mean_reward          | 835         |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.014685276 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | -0.000268   |
|    learning_rate        | 0.001       |
|    loss                 | 1.18e+03    |
|    n_updates            | 3           |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 2.47e+03    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=857.46 +/- 703.89
Episode length: 35.60 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=731.35 +/- 601.30
Episode length: 34.86 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=971.32 +/- 729.61
Episode length: 36.72 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 54       |
|    iterations      | 4        |
|    time_elapsed    | 150      |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=8500, episode_reward=802.59 +/- 664.46
Episode length: 35.08 +/- 5.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 803         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.016753213 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.000401    |
|    learning_rate        | 0.001       |
|    loss                 | 2.2e+03     |
|    n_updates            | 4           |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 4.72e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=920.50 +/- 715.21
Episode length: 35.76 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=784.38 +/- 686.04
Episode length: 34.42 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=888.84 +/- 623.09
Episode length: 37.18 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.3     |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 5        |
|    time_elapsed    | 191      |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=10500, episode_reward=812.48 +/- 680.80
Episode length: 34.78 +/- 6.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.8       |
|    mean_reward          | 812        |
| time/                   |            |
|    total_timesteps      | 10500      |
| train/                  |            |
|    approx_kl            | 0.03144583 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.00167    |
|    learning_rate        | 0.001      |
|    loss                 | 4.62e+03   |
|    n_updates            | 5          |
|    policy_gradient_loss | -0.00559   |
|    value_loss           | 9.58e+03   |
----------------------------------------
Eval num_timesteps=11000, episode_reward=778.92 +/- 580.67
Episode length: 36.10 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=873.40 +/- 622.03
Episode length: 36.40 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=647.42 +/- 525.94
Episode length: 33.64 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.6     |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 6        |
|    time_elapsed    | 229      |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=12500, episode_reward=992.49 +/- 739.51
Episode length: 36.22 +/- 6.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.2       |
|    mean_reward          | 992        |
| time/                   |            |
|    total_timesteps      | 12500      |
| train/                  |            |
|    approx_kl            | 0.04313818 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.00197    |
|    learning_rate        | 0.001      |
|    loss                 | 3.68e+03   |
|    n_updates            | 6          |
|    policy_gradient_loss | -0.00208   |
|    value_loss           | 7.03e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=13000, episode_reward=908.61 +/- 702.38
Episode length: 35.98 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=887.58 +/- 724.77
Episode length: 35.04 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=781.90 +/- 642.52
Episode length: 35.38 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 92.1     |
| time/              |          |
|    fps             | 53       |
|    iterations      | 7        |
|    time_elapsed    | 267      |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=14500, episode_reward=857.30 +/- 638.63
Episode length: 36.60 +/- 5.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.6        |
|    mean_reward          | 857         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.015126353 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.626      |
|    explained_variance   | 0.00241     |
|    learning_rate        | 0.001       |
|    loss                 | 4.21e+03    |
|    n_updates            | 7           |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 8.02e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=703.61 +/- 621.96
Episode length: 33.82 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=875.99 +/- 699.23
Episode length: 35.68 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=732.56 +/- 657.37
Episode length: 34.40 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 52       |
|    iterations      | 8        |
|    time_elapsed    | 309      |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=16500, episode_reward=833.01 +/- 715.67
Episode length: 34.98 +/- 6.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35         |
|    mean_reward          | 833        |
| time/                   |            |
|    total_timesteps      | 16500      |
| train/                  |            |
|    approx_kl            | 0.01713717 |
|    clip_fraction        | 0.0521     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.00048    |
|    learning_rate        | 0.001      |
|    loss                 | 1.71e+04   |
|    n_updates            | 8          |
|    policy_gradient_loss | -0.00913   |
|    value_loss           | 3.61e+04   |
----------------------------------------
Eval num_timesteps=17000, episode_reward=1002.24 +/- 721.62
Episode length: 36.80 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
New best mean reward!
Eval num_timesteps=17500, episode_reward=890.03 +/- 735.74
Episode length: 34.94 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=653.65 +/- 519.91
Episode length: 34.78 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 504      |
| time/              |          |
|    fps             | 53       |
|    iterations      | 9        |
|    time_elapsed    | 346      |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=950.79 +/- 709.14
Episode length: 36.30 +/- 7.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 951          |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.0045425547 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.162       |
|    explained_variance   | -8.3e-05     |
|    learning_rate        | 0.001        |
|    loss                 | 1.05e+04     |
|    n_updates            | 9            |
|    policy_gradient_loss | 0.0017       |
|    value_loss           | 2e+04        |
------------------------------------------
Eval num_timesteps=19000, episode_reward=854.18 +/- 709.12
Episode length: 35.18 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=761.41 +/- 681.28
Episode length: 34.72 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=725.44 +/- 593.06
Episode length: 34.92 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 537      |
| time/              |          |
|    fps             | 54       |
|    iterations      | 10       |
|    time_elapsed    | 375      |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=903.80 +/- 715.06
Episode length: 35.56 +/- 5.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 904          |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0035078588 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0936      |
|    explained_variance   | -0.000202    |
|    learning_rate        | 0.001        |
|    loss                 | 1.06e+04     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 2.44e+04     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=997.75 +/- 739.06
Episode length: 36.30 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=1003.43 +/- 744.38
Episode length: 36.48 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
New best mean reward!
Eval num_timesteps=22000, episode_reward=994.81 +/- 771.82
Episode length: 35.74 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=896.04 +/- 717.59
Episode length: 36.10 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 675      |
| time/              |          |
|    fps             | 55       |
|    iterations      | 11       |
|    time_elapsed    | 408      |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=23000, episode_reward=929.38 +/- 709.22
Episode length: 35.90 +/- 6.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 929         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.005118402 |
|    clip_fraction        | 0.00313     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0145     |
|    explained_variance   | -0.000631   |
|    learning_rate        | 0.001       |
|    loss                 | 8.3e+03     |
|    n_updates            | 11          |
|    policy_gradient_loss | 0.000253    |
|    value_loss           | 3.44e+04    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=786.99 +/- 695.08
Episode length: 33.66 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=740.71 +/- 586.84
Episode length: 35.14 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=806.37 +/- 654.94
Episode length: 35.56 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 666      |
| time/              |          |
|    fps             | 56       |
|    iterations      | 12       |
|    time_elapsed    | 437      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=888.33 +/- 722.76
Episode length: 34.78 +/- 7.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 888         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.004901825 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0763     |
|    explained_variance   | -0.000469   |
|    learning_rate        | 0.001       |
|    loss                 | 2.7e+04     |
|    n_updates            | 15          |
|    policy_gradient_loss | -0.000276   |
|    value_loss           | 2.46e+04    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=906.41 +/- 690.51
Episode length: 36.36 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=710.77 +/- 588.23
Episode length: 34.86 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=803.03 +/- 697.00
Episode length: 34.46 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.1     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 57       |
|    iterations      | 13       |
|    time_elapsed    | 465      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=27000, episode_reward=863.76 +/- 709.54
Episode length: 35.02 +/- 7.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 864         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.012540676 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.958      |
|    explained_variance   | 0.000295    |
|    learning_rate        | 0.001       |
|    loss                 | 3.17e+03    |
|    n_updates            | 16          |
|    policy_gradient_loss | 0.0263      |
|    value_loss           | 7.04e+03    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=749.46 +/- 643.93
Episode length: 33.96 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=821.84 +/- 670.45
Episode length: 35.46 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=859.37 +/- 666.40
Episode length: 35.74 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.6     |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 14       |
|    time_elapsed    | 491      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=714.34 +/- 594.36
Episode length: 35.06 +/- 5.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 714         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.010955818 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.000481    |
|    learning_rate        | 0.001       |
|    loss                 | 3.61e+03    |
|    n_updates            | 17          |
|    policy_gradient_loss | 0.00452     |
|    value_loss           | 6.39e+03    |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=986.26 +/- 728.83
Episode length: 37.14 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=899.26 +/- 699.25
Episode length: 35.46 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=705.64 +/- 582.11
Episode length: 34.78 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.9     |
|    ep_rew_mean     | 6.22     |
| time/              |          |
|    fps             | 59       |
|    iterations      | 15       |
|    time_elapsed    | 520      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=31000, episode_reward=929.76 +/- 731.47
Episode length: 35.64 +/- 6.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.6       |
|    mean_reward          | 930        |
| time/                   |            |
|    total_timesteps      | 31000      |
| train/                  |            |
|    approx_kl            | 0.00968229 |
|    clip_fraction        | 0.0578     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.000703   |
|    learning_rate        | 0.001      |
|    loss                 | 3.37e+03   |
|    n_updates            | 18         |
|    policy_gradient_loss | 0.00776    |
|    value_loss           | 6.53e+03   |
----------------------------------------
Eval num_timesteps=31500, episode_reward=891.64 +/- 664.52
Episode length: 36.46 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=799.55 +/- 625.22
Episode length: 35.58 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=877.48 +/- 713.07
Episode length: 35.18 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.7     |
|    ep_rew_mean     | -64.6    |
| time/              |          |
|    fps             | 59       |
|    iterations      | 16       |
|    time_elapsed    | 553      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=955.63 +/- 716.12
Episode length: 35.76 +/- 6.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 956         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.007559684 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.000988    |
|    learning_rate        | 0.001       |
|    loss                 | 4.38e+03    |
|    n_updates            | 19          |
|    policy_gradient_loss | -0.00886    |
|    value_loss           | 6.75e+03    |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=850.71 +/- 681.67
Episode length: 35.32 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=990.10 +/- 723.96
Episode length: 36.44 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=660.86 +/- 567.07
Episode length: 33.94 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | -32.9    |
| time/              |          |
|    fps             | 59       |
|    iterations      | 17       |
|    time_elapsed    | 586      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=907.34 +/- 723.04
Episode length: 35.66 +/- 6.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.7        |
|    mean_reward          | 907         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.004562624 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.00148     |
|    learning_rate        | 0.001       |
|    loss                 | 2.57e+03    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 5.29e+03    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=810.36 +/- 691.54
Episode length: 34.82 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=735.05 +/- 646.14
Episode length: 34.42 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=664.16 +/- 595.01
Episode length: 34.32 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.6     |
|    ep_rew_mean     | -2.36    |
| time/              |          |
|    fps             | 58       |
|    iterations      | 18       |
|    time_elapsed    | 627      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=37000, episode_reward=782.35 +/- 731.72
Episode length: 33.58 +/- 7.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 782         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.012533614 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.00161     |
|    learning_rate        | 0.001       |
|    loss                 | 3e+03       |
|    n_updates            | 21          |
|    policy_gradient_loss | -0.00245    |
|    value_loss           | 6.27e+03    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=809.62 +/- 683.74
Episode length: 34.62 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=946.25 +/- 734.18
Episode length: 36.06 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=724.39 +/- 587.06
Episode length: 35.02 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.7     |
|    ep_rew_mean     | 29       |
| time/              |          |
|    fps             | 59       |
|    iterations      | 19       |
|    time_elapsed    | 654      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=39000, episode_reward=878.68 +/- 676.06
Episode length: 36.00 +/- 5.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 879         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.014967367 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.986      |
|    explained_variance   | 0.00275     |
|    learning_rate        | 0.001       |
|    loss                 | 3.04e+03    |
|    n_updates            | 22          |
|    policy_gradient_loss | -0.00399    |
|    value_loss           | 7.51e+03    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=922.59 +/- 667.32
Episode length: 36.36 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=692.14 +/- 598.96
Episode length: 34.10 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=758.13 +/- 630.32
Episode length: 34.98 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.6     |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 20       |
|    time_elapsed    | 686      |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=41000, episode_reward=630.15 +/- 540.59
Episode length: 33.90 +/- 5.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.9        |
|    mean_reward          | 630         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.014916485 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.00195     |
|    learning_rate        | 0.001       |
|    loss                 | 4.37e+03    |
|    n_updates            | 23          |
|    policy_gradient_loss | 0.00842     |
|    value_loss           | 7.96e+03    |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=792.18 +/- 683.05
Episode length: 34.54 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=809.34 +/- 673.07
Episode length: 34.98 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=873.39 +/- 712.91
Episode length: 35.10 +/- 7.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=826.16 +/- 722.23
Episode length: 33.80 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 249      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 21       |
|    time_elapsed    | 721      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=43500, episode_reward=990.48 +/- 713.66
Episode length: 37.26 +/- 6.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.3        |
|    mean_reward          | 990         |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.009165462 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.489      |
|    explained_variance   | 0.00269     |
|    learning_rate        | 0.001       |
|    loss                 | 1.2e+04     |
|    n_updates            | 24          |
|    policy_gradient_loss | 0.000308    |
|    value_loss           | 1.58e+04    |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=867.19 +/- 735.28
Episode length: 34.64 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=694.97 +/- 609.48
Episode length: 34.38 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=844.42 +/- 675.97
Episode length: 35.56 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 22       |
|    time_elapsed    | 757      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=45500, episode_reward=886.75 +/- 705.15
Episode length: 36.10 +/- 6.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.1        |
|    mean_reward          | 887         |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.039700326 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.236      |
|    explained_variance   | 0.00209     |
|    learning_rate        | 0.001       |
|    loss                 | 4.9e+03     |
|    n_updates            | 25          |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 1.08e+04    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=890.38 +/- 736.95
Episode length: 35.54 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=824.14 +/- 739.51
Episode length: 34.46 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=668.59 +/- 569.40
Episode length: 34.86 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 520      |
| time/              |          |
|    fps             | 58       |
|    iterations      | 23       |
|    time_elapsed    | 800      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=47500, episode_reward=827.61 +/- 637.11
Episode length: 35.90 +/- 6.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 828         |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.029197767 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0251     |
|    explained_variance   | 0.000832    |
|    learning_rate        | 0.001       |
|    loss                 | 2.49e+04    |
|    n_updates            | 26          |
|    policy_gradient_loss | 0.0163      |
|    value_loss           | 2.83e+04    |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=961.10 +/- 742.69
Episode length: 36.54 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=791.17 +/- 677.68
Episode length: 35.16 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=813.46 +/- 681.71
Episode length: 35.10 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 710      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 24       |
|    time_elapsed    | 832      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=761.41 +/- 650.04
Episode length: 34.98 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 49500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.72e-07 |
|    explained_variance   | 0.00131   |
|    learning_rate        | 0.001     |
|    loss                 | 1.06e+04  |
|    n_updates            | 36        |
|    policy_gradient_loss | 1.02e-06  |
|    value_loss           | 3.03e+04  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=906.40 +/- 707.69
Episode length: 36.12 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=970.96 +/- 765.22
Episode length: 35.78 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=812.61 +/- 605.23
Episode length: 36.00 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 25       |
|    time_elapsed    | 864      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=1064.10 +/- 726.11
Episode length: 37.82 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.8      |
|    mean_reward          | 1.06e+03  |
| time/                   |           |
|    total_timesteps      | 51500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.59e-16 |
|    explained_variance   | 0.0163    |
|    learning_rate        | 0.001     |
|    loss                 | 1.57e+04  |
|    n_updates            | 46        |
|    policy_gradient_loss | 3.71e-10  |
|    value_loss           | 2.53e+04  |
---------------------------------------
New best mean reward!
Eval num_timesteps=52000, episode_reward=769.17 +/- 665.94
Episode length: 34.34 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=760.69 +/- 667.95
Episode length: 34.34 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=750.21 +/- 672.85
Episode length: 34.32 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 59       |
|    iterations      | 26       |
|    time_elapsed    | 894      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=834.12 +/- 702.53
Episode length: 35.36 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 53500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.65e-19 |
|    explained_variance   | 0.00348   |
|    learning_rate        | 0.001     |
|    loss                 | 7.38e+03  |
|    n_updates            | 56        |
|    policy_gradient_loss | 7.8e-10   |
|    value_loss           | 2.47e+04  |
---------------------------------------
Eval num_timesteps=54000, episode_reward=860.26 +/- 678.07
Episode length: 35.76 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=897.46 +/- 700.13
Episode length: 34.98 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=749.98 +/- 649.31
Episode length: 34.80 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 27       |
|    time_elapsed    | 919      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=794.39 +/- 656.98
Episode length: 35.08 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 794       |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.69e-14 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.001     |
|    loss                 | 2.4e+04   |
|    n_updates            | 66        |
|    policy_gradient_loss | 1.64e-10  |
|    value_loss           | 3.29e+04  |
---------------------------------------
Eval num_timesteps=56000, episode_reward=684.45 +/- 614.49
Episode length: 34.18 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=668.55 +/- 557.07
Episode length: 34.02 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=859.21 +/- 655.25
Episode length: 36.22 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 28       |
|    time_elapsed    | 944      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=777.52 +/- 600.89
Episode length: 35.76 +/- 5.53
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.8     |
|    mean_reward          | 778      |
| time/                   |          |
|    total_timesteps      | 57500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.4e-16 |
|    explained_variance   | 0.0039   |
|    learning_rate        | 0.001    |
|    loss                 | 1.08e+04 |
|    n_updates            | 76       |
|    policy_gradient_loss | 8.13e-10 |
|    value_loss           | 3.15e+04 |
--------------------------------------
Eval num_timesteps=58000, episode_reward=770.13 +/- 639.44
Episode length: 34.94 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=739.77 +/- 630.36
Episode length: 34.88 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=850.74 +/- 711.00
Episode length: 35.22 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 29       |
|    time_elapsed    | 970      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=711.90 +/- 590.83
Episode length: 34.84 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 712       |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.3e-13  |
|    explained_variance   | 0.00126   |
|    learning_rate        | 0.001     |
|    loss                 | 3.14e+04  |
|    n_updates            | 86        |
|    policy_gradient_loss | -7.86e-10 |
|    value_loss           | 4.41e+04  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=743.90 +/- 641.06
Episode length: 34.62 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=820.39 +/- 689.15
Episode length: 34.76 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=697.97 +/- 647.55
Episode length: 33.50 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 728      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 30       |
|    time_elapsed    | 996      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=781.32 +/- 683.35
Episode length: 34.72 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 781       |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-15 |
|    explained_variance   | 0.00315   |
|    learning_rate        | 0.001     |
|    loss                 | 8.68e+03  |
|    n_updates            | 96        |
|    policy_gradient_loss | -1.73e-10 |
|    value_loss           | 2.62e+04  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=862.12 +/- 690.78
Episode length: 35.34 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=794.28 +/- 651.30
Episode length: 34.32 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=972.77 +/- 744.66
Episode length: 36.40 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 31       |
|    time_elapsed    | 1024     |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=764.89 +/- 667.91
Episode length: 34.30 +/- 7.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.45e-13 |
|    explained_variance   | 0.00701   |
|    learning_rate        | 0.001     |
|    loss                 | 2.24e+04  |
|    n_updates            | 106       |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 5.5e+04   |
---------------------------------------
Eval num_timesteps=64000, episode_reward=678.81 +/- 567.77
Episode length: 34.48 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=814.83 +/- 631.14
Episode length: 36.06 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=1014.27 +/- 718.99
Episode length: 37.30 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=763.27 +/- 643.68
Episode length: 34.90 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 32       |
|    time_elapsed    | 1055     |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=886.84 +/- 717.50
Episode length: 35.32 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 887       |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.66e-15 |
|    explained_variance   | 0.00323   |
|    learning_rate        | 0.001     |
|    loss                 | 1.67e+04  |
|    n_updates            | 116       |
|    policy_gradient_loss | 2.36e-10  |
|    value_loss           | 3.02e+04  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=858.71 +/- 703.84
Episode length: 35.72 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=878.17 +/- 765.64
Episode length: 34.54 +/- 8.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=735.65 +/- 614.57
Episode length: 34.60 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 33       |
|    time_elapsed    | 1081     |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=928.71 +/- 803.62
Episode length: 34.82 +/- 8.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 929       |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.7e-13  |
|    explained_variance   | 0.00988   |
|    learning_rate        | 0.001     |
|    loss                 | 3.35e+04  |
|    n_updates            | 126       |
|    policy_gradient_loss | -6.64e-10 |
|    value_loss           | 6.14e+04  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=784.75 +/- 641.01
Episode length: 35.22 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=796.54 +/- 664.95
Episode length: 34.46 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=830.27 +/- 721.51
Episode length: 34.18 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 885      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 34       |
|    time_elapsed    | 1123     |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=719.29 +/- 614.52
Episode length: 34.80 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 719       |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.27e-16 |
|    explained_variance   | 0.00347   |
|    learning_rate        | 0.001     |
|    loss                 | 2.54e+04  |
|    n_updates            | 136       |
|    policy_gradient_loss | 4.07e-10  |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=713.56 +/- 608.32
Episode length: 34.50 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=851.12 +/- 734.20
Episode length: 34.54 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=885.33 +/- 739.73
Episode length: 35.20 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 35       |
|    time_elapsed    | 1169     |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=749.92 +/- 680.08
Episode length: 34.60 +/- 7.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 750       |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-14 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.001     |
|    loss                 | 3.22e+04  |
|    n_updates            | 146       |
|    policy_gradient_loss | -4.95e-09 |
|    value_loss           | 7.35e+04  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=779.57 +/- 672.29
Episode length: 34.90 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=662.89 +/- 547.24
Episode length: 34.62 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=969.69 +/- 693.97
Episode length: 36.56 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 36       |
|    time_elapsed    | 1207     |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=754.32 +/- 681.28
Episode length: 34.40 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 754       |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.01e-17 |
|    explained_variance   | 0.00319   |
|    learning_rate        | 0.001     |
|    loss                 | 1.28e+04  |
|    n_updates            | 156       |
|    policy_gradient_loss | -3.29e-10 |
|    value_loss           | 2.97e+04  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=776.82 +/- 676.77
Episode length: 34.92 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=935.61 +/- 742.99
Episode length: 35.56 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=843.35 +/- 671.11
Episode length: 35.68 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 37       |
|    time_elapsed    | 1234     |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=614.75 +/- 574.43
Episode length: 33.34 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 615       |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.92e-16 |
|    explained_variance   | 0.00383   |
|    learning_rate        | 0.001     |
|    loss                 | 4.23e+04  |
|    n_updates            | 166       |
|    policy_gradient_loss | 3.63e-09  |
|    value_loss           | 7.98e+04  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=772.33 +/- 654.06
Episode length: 34.80 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=1019.81 +/- 724.88
Episode length: 37.20 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=665.64 +/- 668.08
Episode length: 32.70 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 38       |
|    time_elapsed    | 1259     |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=616.66 +/- 611.91
Episode length: 33.18 +/- 7.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.2      |
|    mean_reward          | 617       |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.45e-18 |
|    explained_variance   | 0.00381   |
|    learning_rate        | 0.001     |
|    loss                 | 8.69e+03  |
|    n_updates            | 176       |
|    policy_gradient_loss | -3.91e-09 |
|    value_loss           | 3.44e+04  |
---------------------------------------
Eval num_timesteps=78500, episode_reward=725.20 +/- 600.91
Episode length: 34.64 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=943.99 +/- 695.26
Episode length: 36.54 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=974.57 +/- 717.73
Episode length: 36.50 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 39       |
|    time_elapsed    | 1304     |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=828.13 +/- 672.14
Episode length: 35.44 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.15e-17 |
|    explained_variance   | 0.00621   |
|    learning_rate        | 0.001     |
|    loss                 | 4.19e+04  |
|    n_updates            | 186       |
|    policy_gradient_loss | -5.53e-10 |
|    value_loss           | 8.07e+04  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=930.99 +/- 694.17
Episode length: 36.06 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=1031.11 +/- 714.33
Episode length: 37.20 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=762.48 +/- 733.10
Episode length: 33.68 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 40       |
|    time_elapsed    | 1350     |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=718.12 +/- 642.49
Episode length: 33.74 +/- 7.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.7     |
|    mean_reward          | 718      |
| time/                   |          |
|    total_timesteps      | 82000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.2e-19 |
|    explained_variance   | 0.00441  |
|    learning_rate        | 0.001    |
|    loss                 | 1.03e+04 |
|    n_updates            | 196      |
|    policy_gradient_loss | 5.08e-10 |
|    value_loss           | 2.72e+04 |
--------------------------------------
Eval num_timesteps=82500, episode_reward=888.52 +/- 706.41
Episode length: 35.86 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=855.89 +/- 693.92
Episode length: 35.20 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=846.34 +/- 613.32
Episode length: 36.74 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 41       |
|    time_elapsed    | 1381     |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=751.60 +/- 616.43
Episode length: 35.12 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 752       |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.52e-18 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.001     |
|    loss                 | 2.9e+04   |
|    n_updates            | 206       |
|    policy_gradient_loss | 2.85e-09  |
|    value_loss           | 7.35e+04  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=931.37 +/- 720.95
Episode length: 35.80 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=773.82 +/- 766.78
Episode length: 33.04 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=793.52 +/- 655.34
Episode length: 35.14 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=775.46 +/- 653.53
Episode length: 34.58 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 60       |
|    iterations      | 42       |
|    time_elapsed    | 1416     |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=791.36 +/- 643.27
Episode length: 35.14 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 791       |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-20 |
|    explained_variance   | 0.00489   |
|    learning_rate        | 0.001     |
|    loss                 | 1.01e+04  |
|    n_updates            | 216       |
|    policy_gradient_loss | -2.61e-09 |
|    value_loss           | 3.09e+04  |
---------------------------------------
Eval num_timesteps=87000, episode_reward=847.13 +/- 687.20
Episode length: 34.96 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=908.29 +/- 689.27
Episode length: 37.06 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=858.82 +/- 708.30
Episode length: 35.16 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 43       |
|    time_elapsed    | 1441     |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=740.85 +/- 610.25
Episode length: 35.06 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 741       |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-18 |
|    explained_variance   | 0.00476   |
|    learning_rate        | 0.001     |
|    loss                 | 4.68e+04  |
|    n_updates            | 226       |
|    policy_gradient_loss | -4.34e-09 |
|    value_loss           | 7.81e+04  |
---------------------------------------
Eval num_timesteps=89000, episode_reward=910.38 +/- 700.70
Episode length: 35.60 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=948.94 +/- 714.23
Episode length: 35.84 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=705.62 +/- 654.22
Episode length: 33.44 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 44       |
|    time_elapsed    | 1466     |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=852.90 +/- 673.77
Episode length: 35.92 +/- 5.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 90500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.46e-21 |
|    explained_variance   | 0.00543   |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+04  |
|    n_updates            | 236       |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 2.89e+04  |
---------------------------------------
Eval num_timesteps=91000, episode_reward=869.94 +/- 671.42
Episode length: 36.02 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=1022.42 +/- 695.10
Episode length: 37.90 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.9     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=932.38 +/- 729.83
Episode length: 36.02 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 61       |
|    iterations      | 45       |
|    time_elapsed    | 1492     |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=892.81 +/- 715.49
Episode length: 35.80 +/- 6.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 893       |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.18e-19 |
|    explained_variance   | 0.0183    |
|    learning_rate        | 0.001     |
|    loss                 | 2.79e+04  |
|    n_updates            | 246       |
|    policy_gradient_loss | -1.86e-09 |
|    value_loss           | 6.84e+04  |
---------------------------------------
Eval num_timesteps=93000, episode_reward=915.15 +/- 699.33
Episode length: 36.36 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=871.80 +/- 725.41
Episode length: 34.94 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=706.36 +/- 615.72
Episode length: 34.34 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 963      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 46       |
|    time_elapsed    | 1517     |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=694.53 +/- 598.50
Episode length: 34.22 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 695       |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.33e-21 |
|    explained_variance   | 0.00614   |
|    learning_rate        | 0.001     |
|    loss                 | 3.05e+04  |
|    n_updates            | 256       |
|    policy_gradient_loss | 7.63e-10  |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=864.14 +/- 673.78
Episode length: 36.40 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=686.52 +/- 566.97
Episode length: 34.36 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=961.82 +/- 686.26
Episode length: 37.26 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 47       |
|    time_elapsed    | 1543     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=811.55 +/- 678.17
Episode length: 34.68 +/- 5.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.94e-19 |
|    explained_variance   | -0.00162  |
|    learning_rate        | 0.001     |
|    loss                 | 2.75e+04  |
|    n_updates            | 266       |
|    policy_gradient_loss | 2.06e-09  |
|    value_loss           | 7e+04     |
---------------------------------------
Eval num_timesteps=97000, episode_reward=867.98 +/- 701.30
Episode length: 35.52 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=992.60 +/- 773.75
Episode length: 36.04 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 993      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=672.12 +/- 583.20
Episode length: 34.20 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 48       |
|    time_elapsed    | 1568     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=918.07 +/- 741.48
Episode length: 35.78 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 918       |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.28e-21 |
|    explained_variance   | 0.00568   |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+04  |
|    n_updates            | 276       |
|    policy_gradient_loss | 3.48e-09  |
|    value_loss           | 2.76e+04  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=801.57 +/- 648.14
Episode length: 35.16 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=848.19 +/- 680.69
Episode length: 35.50 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=1052.68 +/- 729.96
Episode length: 37.42 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 62       |
|    iterations      | 49       |
|    time_elapsed    | 1594     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=935.00 +/- 732.93
Episode length: 36.00 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 935       |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-19 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.001     |
|    loss                 | 2.92e+04  |
|    n_updates            | 286       |
|    policy_gradient_loss | -4.48e-10 |
|    value_loss           | 5.89e+04  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=770.21 +/- 605.03
Episode length: 35.52 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=763.57 +/- 703.16
Episode length: 34.38 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=825.75 +/- 670.07
Episode length: 35.68 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 50       |
|    time_elapsed    | 1619     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=800.83 +/- 667.44
Episode length: 34.42 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.03e-22 |
|    explained_variance   | 0.00376   |
|    learning_rate        | 0.001     |
|    loss                 | 1.57e+04  |
|    n_updates            | 296       |
|    policy_gradient_loss | 6.82e-10  |
|    value_loss           | 2.16e+04  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=889.62 +/- 717.35
Episode length: 35.46 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=856.79 +/- 641.81
Episode length: 36.14 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=764.27 +/- 676.86
Episode length: 34.24 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 51       |
|    time_elapsed    | 1644     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=708.28 +/- 618.21
Episode length: 34.66 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 708       |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.15e-19 |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.001     |
|    loss                 | 2.23e+04  |
|    n_updates            | 306       |
|    policy_gradient_loss | 2.6e-10   |
|    value_loss           | 5.83e+04  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=867.76 +/- 736.38
Episode length: 35.34 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=641.56 +/- 647.52
Episode length: 32.14 +/- 7.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.1     |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=761.76 +/- 648.00
Episode length: 34.68 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 52       |
|    time_elapsed    | 1669     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=936.12 +/- 754.46
Episode length: 35.98 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 936       |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.11e-22 |
|    explained_variance   | 0.00571   |
|    learning_rate        | 0.001     |
|    loss                 | 1.22e+04  |
|    n_updates            | 316       |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 2.41e+04  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=726.96 +/- 630.83
Episode length: 34.18 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=821.96 +/- 712.98
Episode length: 34.46 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=960.17 +/- 736.20
Episode length: 35.96 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=941.98 +/- 731.23
Episode length: 35.92 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 816      |
| time/              |          |
|    fps             | 63       |
|    iterations      | 53       |
|    time_elapsed    | 1699     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=840.85 +/- 639.39
Episode length: 35.96 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.76e-19 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.001     |
|    loss                 | 2.92e+04  |
|    n_updates            | 326       |
|    policy_gradient_loss | 2.23e-09  |
|    value_loss           | 5.53e+04  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=968.42 +/- 776.86
Episode length: 35.80 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=855.18 +/- 671.92
Episode length: 36.26 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=579.94 +/- 502.73
Episode length: 33.62 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 580      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 54       |
|    time_elapsed    | 1724     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=765.43 +/- 635.96
Episode length: 34.66 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.61e-21 |
|    explained_variance   | 0.00448   |
|    learning_rate        | 0.001     |
|    loss                 | 5.75e+03  |
|    n_updates            | 336       |
|    policy_gradient_loss | -5.76e-10 |
|    value_loss           | 2.16e+04  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=889.92 +/- 705.77
Episode length: 35.66 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=795.98 +/- 623.01
Episode length: 36.12 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=934.63 +/- 710.62
Episode length: 36.22 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 55       |
|    time_elapsed    | 1750     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=827.36 +/- 667.51
Episode length: 35.60 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.95e-19 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.001     |
|    loss                 | 2.67e+04  |
|    n_updates            | 346       |
|    policy_gradient_loss | -4.37e-11 |
|    value_loss           | 5.11e+04  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=832.73 +/- 706.14
Episode length: 34.20 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=719.25 +/- 673.20
Episode length: 33.30 +/- 8.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=932.60 +/- 701.70
Episode length: 35.78 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 56       |
|    time_elapsed    | 1775     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=950.16 +/- 752.22
Episode length: 36.02 +/- 7.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 950       |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-21 |
|    explained_variance   | 0.00472   |
|    learning_rate        | 0.001     |
|    loss                 | 5.65e+03  |
|    n_updates            | 356       |
|    policy_gradient_loss | 8.34e-10  |
|    value_loss           | 2.35e+04  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=857.27 +/- 737.40
Episode length: 35.04 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=877.43 +/- 731.73
Episode length: 35.28 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=743.79 +/- 681.86
Episode length: 34.16 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 703      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 57       |
|    time_elapsed    | 1811     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=798.70 +/- 623.35
Episode length: 35.50 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.35e-19 |
|    explained_variance   | -0.00132  |
|    learning_rate        | 0.001     |
|    loss                 | 2.51e+04  |
|    n_updates            | 366       |
|    policy_gradient_loss | -2.21e-09 |
|    value_loss           | 5.28e+04  |
---------------------------------------
Eval num_timesteps=117500, episode_reward=772.15 +/- 606.20
Episode length: 35.38 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=847.67 +/- 636.71
Episode length: 36.20 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=790.30 +/- 661.41
Episode length: 34.98 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 734      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 58       |
|    time_elapsed    | 1837     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=911.11 +/- 671.81
Episode length: 36.86 +/- 5.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 911       |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.56e-21 |
|    explained_variance   | 0.00752   |
|    learning_rate        | 0.001     |
|    loss                 | 4.43e+03  |
|    n_updates            | 376       |
|    policy_gradient_loss | -6.52e-10 |
|    value_loss           | 2.41e+04  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=832.39 +/- 650.84
Episode length: 36.12 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=667.02 +/- 648.84
Episode length: 32.30 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.3     |
|    mean_reward     | 667      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=662.20 +/- 556.46
Episode length: 34.10 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 59       |
|    time_elapsed    | 1863     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=799.92 +/- 685.58
Episode length: 35.18 +/- 7.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 800       |
| time/                   |           |
|    total_timesteps      | 121000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.87e-18 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.001     |
|    loss                 | 2.42e+04  |
|    n_updates            | 386       |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 4.39e+04  |
---------------------------------------
Eval num_timesteps=121500, episode_reward=853.50 +/- 717.89
Episode length: 35.14 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=912.45 +/- 681.26
Episode length: 35.94 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=898.27 +/- 619.09
Episode length: 36.92 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 60       |
|    time_elapsed    | 1890     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=854.33 +/- 666.21
Episode length: 35.80 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 854       |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-20 |
|    explained_variance   | 0.00431   |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+04  |
|    n_updates            | 396       |
|    policy_gradient_loss | -8.09e-10 |
|    value_loss           | 2.07e+04  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=798.92 +/- 662.87
Episode length: 35.68 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=974.31 +/- 730.78
Episode length: 36.54 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=958.47 +/- 683.74
Episode length: 37.04 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 61       |
|    time_elapsed    | 1916     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=976.87 +/- 789.67
Episode length: 35.42 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 977       |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.15e-18 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.001     |
|    loss                 | 1.93e+04  |
|    n_updates            | 406       |
|    policy_gradient_loss | 4.9e-10   |
|    value_loss           | 4.1e+04   |
---------------------------------------
Eval num_timesteps=125500, episode_reward=1040.55 +/- 721.89
Episode length: 36.78 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=993.80 +/- 714.00
Episode length: 36.72 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=792.95 +/- 615.70
Episode length: 35.80 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 62       |
|    time_elapsed    | 1942     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=874.87 +/- 643.59
Episode length: 36.38 +/- 5.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.74e-20 |
|    explained_variance   | 0.00448   |
|    learning_rate        | 0.001     |
|    loss                 | 8.46e+03  |
|    n_updates            | 416       |
|    policy_gradient_loss | -4.95e-10 |
|    value_loss           | 2.32e+04  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=732.61 +/- 592.67
Episode length: 35.38 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=703.20 +/- 633.09
Episode length: 34.24 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=835.33 +/- 637.22
Episode length: 36.26 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=763.58 +/- 680.80
Episode length: 34.18 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 63       |
|    time_elapsed    | 1972     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=908.54 +/- 735.32
Episode length: 35.74 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 909       |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.81e-17 |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+04  |
|    n_updates            | 426       |
|    policy_gradient_loss | 1.73e-09  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=725.93 +/- 598.68
Episode length: 34.88 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=883.88 +/- 689.05
Episode length: 36.06 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=768.68 +/- 624.89
Episode length: 35.54 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 64       |
|    time_elapsed    | 1997     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=988.01 +/- 752.49
Episode length: 35.84 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 988       |
| time/                   |           |
|    total_timesteps      | 131500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.17e-20 |
|    explained_variance   | 0.00818   |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+04  |
|    n_updates            | 436       |
|    policy_gradient_loss | 1.28e-09  |
|    value_loss           | 3.05e+04  |
---------------------------------------
Eval num_timesteps=132000, episode_reward=896.17 +/- 691.33
Episode length: 35.82 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=995.89 +/- 754.50
Episode length: 36.42 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=820.67 +/- 708.54
Episode length: 34.40 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 953      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 65       |
|    time_elapsed    | 2023     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=852.55 +/- 704.36
Episode length: 35.20 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.73e-17 |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.001     |
|    loss                 | 1.31e+04  |
|    n_updates            | 446       |
|    policy_gradient_loss | 7.42e-10  |
|    value_loss           | 3.57e+04  |
---------------------------------------
Eval num_timesteps=134000, episode_reward=623.55 +/- 602.02
Episode length: 33.04 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=936.99 +/- 732.38
Episode length: 35.78 +/- 7.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=878.63 +/- 667.47
Episode length: 36.00 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 966      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 66       |
|    time_elapsed    | 2068     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=863.65 +/- 658.28
Episode length: 36.38 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 864       |
| time/                   |           |
|    total_timesteps      | 135500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-19 |
|    explained_variance   | 0.00773   |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+04  |
|    n_updates            | 456       |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 3.14e+04  |
---------------------------------------
Eval num_timesteps=136000, episode_reward=928.08 +/- 703.58
Episode length: 36.08 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=796.22 +/- 680.64
Episode length: 35.06 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=825.37 +/- 627.68
Episode length: 36.18 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 819      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 67       |
|    time_elapsed    | 2095     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=779.60 +/- 655.42
Episode length: 35.02 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.14e-17 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 2.99e+04  |
|    n_updates            | 466       |
|    policy_gradient_loss | -3.42e-09 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=807.51 +/- 734.20
Episode length: 34.26 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=895.28 +/- 733.99
Episode length: 35.20 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=904.84 +/- 712.70
Episode length: 35.74 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 68       |
|    time_elapsed    | 2123     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=1021.84 +/- 732.02
Episode length: 36.48 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.95e-19 |
|    explained_variance   | 0.00602   |
|    learning_rate        | 0.001     |
|    loss                 | 2.53e+04  |
|    n_updates            | 476       |
|    policy_gradient_loss | -2.75e-10 |
|    value_loss           | 2.45e+04  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=713.32 +/- 632.73
Episode length: 34.08 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=795.06 +/- 649.59
Episode length: 35.54 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=881.86 +/- 715.58
Episode length: 35.50 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 69       |
|    time_elapsed    | 2153     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=779.67 +/- 684.96
Episode length: 33.96 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 141500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.86e-16 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 1.73e+04  |
|    n_updates            | 486       |
|    policy_gradient_loss | 3.29e-09  |
|    value_loss           | 3.83e+04  |
---------------------------------------
Eval num_timesteps=142000, episode_reward=723.51 +/- 657.62
Episode length: 34.14 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=745.44 +/- 629.44
Episode length: 35.18 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=842.71 +/- 733.54
Episode length: 35.12 +/- 7.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 899      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 70       |
|    time_elapsed    | 2178     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=908.00 +/- 719.26
Episode length: 36.52 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 908       |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.94e-19 |
|    explained_variance   | 0.00666   |
|    learning_rate        | 0.001     |
|    loss                 | 5.65e+03  |
|    n_updates            | 496       |
|    policy_gradient_loss | 7.31e-10  |
|    value_loss           | 3.3e+04   |
---------------------------------------
Eval num_timesteps=144000, episode_reward=744.75 +/- 629.13
Episode length: 34.86 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=849.25 +/- 661.19
Episode length: 35.82 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=900.58 +/- 681.05
Episode length: 36.26 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 71       |
|    time_elapsed    | 2218     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=861.09 +/- 690.49
Episode length: 34.90 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.56e-16 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+04  |
|    n_updates            | 506       |
|    policy_gradient_loss | 4.45e-10  |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=924.50 +/- 744.75
Episode length: 35.16 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=772.09 +/- 652.13
Episode length: 35.30 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=1010.54 +/- 666.54
Episode length: 37.76 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 72       |
|    time_elapsed    | 2249     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=861.09 +/- 739.00
Episode length: 34.68 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.79e-19 |
|    explained_variance   | 0.00732   |
|    learning_rate        | 0.001     |
|    loss                 | 1.82e+04  |
|    n_updates            | 516       |
|    policy_gradient_loss | -1.43e-09 |
|    value_loss           | 2.74e+04  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=978.88 +/- 704.76
Episode length: 36.86 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=664.41 +/- 558.97
Episode length: 34.70 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=762.70 +/- 631.94
Episode length: 34.76 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=880.90 +/- 745.70
Episode length: 34.90 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 73       |
|    time_elapsed    | 2285     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=922.51 +/- 702.33
Episode length: 36.10 +/- 5.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 923       |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.98e-16 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.001     |
|    loss                 | 1.34e+04  |
|    n_updates            | 526       |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 3.49e+04  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=734.53 +/- 584.55
Episode length: 35.30 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=790.39 +/- 605.04
Episode length: 35.56 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=962.72 +/- 725.89
Episode length: 35.82 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 869      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 74       |
|    time_elapsed    | 2310     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=945.59 +/- 689.88
Episode length: 36.52 +/- 5.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 946       |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.35e-18 |
|    explained_variance   | 0.00556   |
|    learning_rate        | 0.001     |
|    loss                 | 2.04e+04  |
|    n_updates            | 536       |
|    policy_gradient_loss | -1.68e-09 |
|    value_loss           | 3.07e+04  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=714.26 +/- 556.03
Episode length: 35.18 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=857.33 +/- 686.44
Episode length: 34.86 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=760.85 +/- 684.44
Episode length: 33.80 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 75       |
|    time_elapsed    | 2335     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=649.60 +/- 579.49
Episode length: 33.52 +/- 7.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 650       |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.28e-15 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.001     |
|    loss                 | 1.77e+04  |
|    n_updates            | 546       |
|    policy_gradient_loss | 1.66e-09  |
|    value_loss           | 3.3e+04   |
---------------------------------------
Eval num_timesteps=154500, episode_reward=826.30 +/- 608.95
Episode length: 36.00 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=850.82 +/- 719.37
Episode length: 34.62 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=746.52 +/- 679.22
Episode length: 33.92 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 76       |
|    time_elapsed    | 2362     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=815.57 +/- 719.55
Episode length: 34.12 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.07e-18 |
|    explained_variance   | 0.00606   |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+04  |
|    n_updates            | 556       |
|    policy_gradient_loss | -1.24e-09 |
|    value_loss           | 2.48e+04  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=731.66 +/- 640.26
Episode length: 33.52 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=835.12 +/- 695.75
Episode length: 35.22 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=692.30 +/- 670.92
Episode length: 33.68 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 77       |
|    time_elapsed    | 2387     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=841.49 +/- 667.90
Episode length: 35.34 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.32e-15 |
|    explained_variance   | 0.0131    |
|    learning_rate        | 0.001     |
|    loss                 | 1.68e+04  |
|    n_updates            | 566       |
|    policy_gradient_loss | 3.07e-10  |
|    value_loss           | 3.29e+04  |
---------------------------------------
Eval num_timesteps=158500, episode_reward=871.08 +/- 736.16
Episode length: 35.00 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=825.49 +/- 652.15
Episode length: 35.84 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=1004.33 +/- 705.97
Episode length: 37.02 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 78       |
|    time_elapsed    | 2413     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=781.12 +/- 646.19
Episode length: 35.16 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 781       |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.97e-18 |
|    explained_variance   | 0.00364   |
|    learning_rate        | 0.001     |
|    loss                 | 9.93e+03  |
|    n_updates            | 576       |
|    policy_gradient_loss | 4.69e-10  |
|    value_loss           | 2.3e+04   |
---------------------------------------
Eval num_timesteps=160500, episode_reward=850.36 +/- 714.34
Episode length: 35.30 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=995.75 +/- 727.83
Episode length: 36.32 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=696.45 +/- 653.11
Episode length: 33.52 +/- 7.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 79       |
|    time_elapsed    | 2438     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=915.58 +/- 695.41
Episode length: 36.42 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 916       |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.05e-15 |
|    explained_variance   | 0.0148    |
|    learning_rate        | 0.001     |
|    loss                 | 8.35e+03  |
|    n_updates            | 586       |
|    policy_gradient_loss | -2.17e-09 |
|    value_loss           | 3.08e+04  |
---------------------------------------
Eval num_timesteps=162500, episode_reward=740.09 +/- 569.03
Episode length: 35.88 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=691.33 +/- 608.50
Episode length: 34.14 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=767.84 +/- 675.21
Episode length: 34.26 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 80       |
|    time_elapsed    | 2467     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=785.37 +/- 677.52
Episode length: 34.22 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-17 |
|    explained_variance   | 0.006     |
|    learning_rate        | 0.001     |
|    loss                 | 2.33e+04  |
|    n_updates            | 596       |
|    policy_gradient_loss | -2.01e-09 |
|    value_loss           | 2.59e+04  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=803.88 +/- 653.89
Episode length: 35.24 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=845.15 +/- 674.69
Episode length: 35.78 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=720.58 +/- 581.21
Episode length: 35.08 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 698      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 81       |
|    time_elapsed    | 2493     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=899.12 +/- 696.83
Episode length: 35.68 +/- 6.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 899       |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.74e-15 |
|    explained_variance   | 0.00341   |
|    learning_rate        | 0.001     |
|    loss                 | 1.53e+04  |
|    n_updates            | 606       |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 3.03e+04  |
---------------------------------------
Eval num_timesteps=166500, episode_reward=755.25 +/- 636.71
Episode length: 35.32 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=682.31 +/- 571.64
Episode length: 34.94 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=929.02 +/- 730.42
Episode length: 35.76 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 737      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 82       |
|    time_elapsed    | 2525     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=722.23 +/- 687.49
Episode length: 33.86 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 722       |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-17 |
|    explained_variance   | 0.00565   |
|    learning_rate        | 0.001     |
|    loss                 | 2.96e+04  |
|    n_updates            | 616       |
|    policy_gradient_loss | -1.37e-10 |
|    value_loss           | 2.76e+04  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=1069.36 +/- 802.50
Episode length: 36.02 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
New best mean reward!
Eval num_timesteps=169000, episode_reward=691.14 +/- 598.86
Episode length: 33.96 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=1002.25 +/- 774.87
Episode length: 35.48 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 83       |
|    time_elapsed    | 2555     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=973.46 +/- 713.73
Episode length: 36.14 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 973       |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-14 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.001     |
|    loss                 | 8.07e+03  |
|    n_updates            | 626       |
|    policy_gradient_loss | 1.37e-10  |
|    value_loss           | 2.92e+04  |
---------------------------------------
Eval num_timesteps=170500, episode_reward=903.38 +/- 741.07
Episode length: 35.46 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=842.55 +/- 694.96
Episode length: 35.12 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=866.85 +/- 697.46
Episode length: 35.26 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=917.97 +/- 723.11
Episode length: 35.84 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 84       |
|    time_elapsed    | 2585     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=937.13 +/- 749.78
Episode length: 35.44 +/- 6.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 937       |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.21e-17 |
|    explained_variance   | 0.00573   |
|    learning_rate        | 0.001     |
|    loss                 | 1.71e+04  |
|    n_updates            | 636       |
|    policy_gradient_loss | -4.58e-10 |
|    value_loss           | 2.71e+04  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=764.42 +/- 719.21
Episode length: 33.80 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=957.93 +/- 660.46
Episode length: 37.50 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=943.30 +/- 705.61
Episode length: 36.24 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 85       |
|    time_elapsed    | 2610     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=814.36 +/- 716.38
Episode length: 34.22 +/- 7.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.2     |
|    mean_reward          | 814      |
| time/                   |          |
|    total_timesteps      | 174500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.6e-14 |
|    explained_variance   | 0.0143   |
|    learning_rate        | 0.001    |
|    loss                 | 8.62e+03 |
|    n_updates            | 646      |
|    policy_gradient_loss | 2.04e-10 |
|    value_loss           | 2.84e+04 |
--------------------------------------
Eval num_timesteps=175000, episode_reward=917.91 +/- 705.02
Episode length: 36.60 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=796.12 +/- 661.17
Episode length: 35.48 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=792.67 +/- 602.73
Episode length: 36.00 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 86       |
|    time_elapsed    | 2636     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=853.22 +/- 687.20
Episode length: 35.30 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.05e-16 |
|    explained_variance   | 0.00678   |
|    learning_rate        | 0.001     |
|    loss                 | 1.56e+04  |
|    n_updates            | 656       |
|    policy_gradient_loss | -1.57e-09 |
|    value_loss           | 2.72e+04  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=1010.05 +/- 750.77
Episode length: 36.56 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=851.67 +/- 654.17
Episode length: 35.92 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=987.83 +/- 706.54
Episode length: 37.34 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 936      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 87       |
|    time_elapsed    | 2662     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=789.60 +/- 661.93
Episode length: 35.32 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 790       |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.99e-14 |
|    explained_variance   | 0.0144    |
|    learning_rate        | 0.001     |
|    loss                 | 1.24e+04  |
|    n_updates            | 666       |
|    policy_gradient_loss | 2.37e-10  |
|    value_loss           | 2.86e+04  |
---------------------------------------
Eval num_timesteps=179000, episode_reward=1096.92 +/- 773.20
Episode length: 37.00 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
New best mean reward!
Eval num_timesteps=179500, episode_reward=774.33 +/- 640.67
Episode length: 35.18 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=988.00 +/- 707.74
Episode length: 36.64 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 88       |
|    time_elapsed    | 2688     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=819.32 +/- 613.39
Episode length: 36.18 +/- 5.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 819       |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-16 |
|    explained_variance   | 0.00705   |
|    learning_rate        | 0.001     |
|    loss                 | 9.15e+03  |
|    n_updates            | 676       |
|    policy_gradient_loss | 1.15e-09  |
|    value_loss           | 3.43e+04  |
---------------------------------------
Eval num_timesteps=181000, episode_reward=844.53 +/- 708.21
Episode length: 35.14 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=860.43 +/- 744.82
Episode length: 34.40 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=951.98 +/- 718.23
Episode length: 36.72 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 904      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 89       |
|    time_elapsed    | 2713     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=881.55 +/- 690.67
Episode length: 35.84 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 882       |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.33e-14 |
|    explained_variance   | 0.00957   |
|    learning_rate        | 0.001     |
|    loss                 | 1.3e+04   |
|    n_updates            | 686       |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=183000, episode_reward=832.05 +/- 711.09
Episode length: 34.44 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=618.41 +/- 565.30
Episode length: 33.10 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=796.58 +/- 696.27
Episode length: 34.52 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 90       |
|    time_elapsed    | 2738     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=840.11 +/- 670.08
Episode length: 35.92 +/- 6.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 840       |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.91e-16 |
|    explained_variance   | 0.00624   |
|    learning_rate        | 0.001     |
|    loss                 | 1.08e+04  |
|    n_updates            | 696       |
|    policy_gradient_loss | -8.24e-10 |
|    value_loss           | 3.27e+04  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=1037.44 +/- 723.46
Episode length: 37.36 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=992.52 +/- 709.87
Episode length: 36.50 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 993      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=944.19 +/- 726.17
Episode length: 36.12 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 91       |
|    time_elapsed    | 2765     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=897.29 +/- 705.47
Episode length: 35.90 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 897       |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.16e-14 |
|    explained_variance   | 0.00804   |
|    learning_rate        | 0.001     |
|    loss                 | 1.12e+04  |
|    n_updates            | 706       |
|    policy_gradient_loss | 1.67e-10  |
|    value_loss           | 2.74e+04  |
---------------------------------------
Eval num_timesteps=187000, episode_reward=813.05 +/- 736.37
Episode length: 34.04 +/- 8.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=824.78 +/- 639.82
Episode length: 35.58 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=858.40 +/- 714.45
Episode length: 34.58 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 627      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 92       |
|    time_elapsed    | 2790     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=809.76 +/- 693.38
Episode length: 34.60 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 810       |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.68e-16 |
|    explained_variance   | 0.00357   |
|    learning_rate        | 0.001     |
|    loss                 | 3.62e+03  |
|    n_updates            | 716       |
|    policy_gradient_loss | 8.51e-10  |
|    value_loss           | 1.62e+04  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=801.43 +/- 618.34
Episode length: 35.48 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=857.83 +/- 708.88
Episode length: 35.72 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=842.45 +/- 664.58
Episode length: 35.30 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 93       |
|    time_elapsed    | 2816     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=733.22 +/- 656.03
Episode length: 34.46 +/- 7.21
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.5     |
|    mean_reward          | 733      |
| time/                   |          |
|    total_timesteps      | 190500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.2e-13 |
|    explained_variance   | 0.0134   |
|    learning_rate        | 0.001    |
|    loss                 | 1.44e+04 |
|    n_updates            | 726      |
|    policy_gradient_loss | 2.16e-09 |
|    value_loss           | 2.71e+04 |
--------------------------------------
Eval num_timesteps=191000, episode_reward=738.97 +/- 633.79
Episode length: 34.38 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=831.02 +/- 643.99
Episode length: 35.64 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=857.30 +/- 701.28
Episode length: 35.66 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=808.14 +/- 665.84
Episode length: 35.80 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 881      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 94       |
|    time_elapsed    | 2847     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=786.65 +/- 647.71
Episode length: 34.86 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.24e-16 |
|    explained_variance   | 0.00616   |
|    learning_rate        | 0.001     |
|    loss                 | 1.91e+04  |
|    n_updates            | 736       |
|    policy_gradient_loss | 1.95e-10  |
|    value_loss           | 3.08e+04  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=714.35 +/- 596.28
Episode length: 34.78 +/- 7.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=772.63 +/- 645.00
Episode length: 34.56 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=739.83 +/- 623.20
Episode length: 35.16 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 95       |
|    time_elapsed    | 2883     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=895.06 +/- 698.57
Episode length: 36.04 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 895       |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.72e-13 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+04  |
|    n_updates            | 746       |
|    policy_gradient_loss | -4.92e-10 |
|    value_loss           | 2.73e+04  |
---------------------------------------
Eval num_timesteps=195500, episode_reward=811.90 +/- 659.16
Episode length: 35.20 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=797.99 +/- 716.69
Episode length: 33.96 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=895.36 +/- 689.53
Episode length: 35.54 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 96       |
|    time_elapsed    | 2916     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=890.54 +/- 662.24
Episode length: 36.72 +/- 5.29
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.7     |
|    mean_reward          | 891      |
| time/                   |          |
|    total_timesteps      | 197000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.2e-15 |
|    explained_variance   | 0.00778  |
|    learning_rate        | 0.001    |
|    loss                 | 1.14e+04 |
|    n_updates            | 756      |
|    policy_gradient_loss | 9.6e-10  |
|    value_loss           | 3.2e+04  |
--------------------------------------
Eval num_timesteps=197500, episode_reward=729.14 +/- 654.47
Episode length: 34.14 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=904.31 +/- 691.29
Episode length: 35.96 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=870.89 +/- 688.12
Episode length: 35.48 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 67       |
|    iterations      | 97       |
|    time_elapsed    | 2962     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=879.28 +/- 702.35
Episode length: 35.66 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 879       |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.84e-13 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+04  |
|    n_updates            | 766       |
|    policy_gradient_loss | -4.66e-11 |
|    value_loss           | 2.92e+04  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=762.66 +/- 615.46
Episode length: 35.36 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=1010.83 +/- 702.74
Episode length: 37.04 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=825.04 +/- 675.99
Episode length: 34.74 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 98       |
|    time_elapsed    | 2997     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=854.43 +/- 690.53
Episode length: 35.18 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 854       |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.38e-15 |
|    explained_variance   | 0.00486   |
|    learning_rate        | 0.001     |
|    loss                 | 9.28e+03  |
|    n_updates            | 776       |
|    policy_gradient_loss | 2.03e-09  |
|    value_loss           | 2.14e+04  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=685.83 +/- 623.16
Episode length: 34.04 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=823.90 +/- 676.83
Episode length: 35.08 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=894.87 +/- 640.01
Episode length: 36.64 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 99       |
|    time_elapsed    | 3035     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=692.61 +/- 570.15
Episode length: 34.84 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 693       |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.54e-13 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.001     |
|    loss                 | 1.15e+04  |
|    n_updates            | 786       |
|    policy_gradient_loss | 4.39e-10  |
|    value_loss           | 2.48e+04  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=837.94 +/- 623.24
Episode length: 36.18 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=765.03 +/- 678.02
Episode length: 34.52 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=669.91 +/- 572.19
Episode length: 34.20 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 100      |
|    time_elapsed    | 3065     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=970.59 +/- 726.39
Episode length: 36.36 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 971       |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.84e-15 |
|    explained_variance   | 0.00708   |
|    learning_rate        | 0.001     |
|    loss                 | 1.75e+04  |
|    n_updates            | 796       |
|    policy_gradient_loss | 7.32e-10  |
|    value_loss           | 3.37e+04  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=727.14 +/- 586.61
Episode length: 35.14 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=579.62 +/- 551.40
Episode length: 32.54 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.5     |
|    mean_reward     | 580      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=705.95 +/- 622.22
Episode length: 33.90 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 858      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 101      |
|    time_elapsed    | 3092     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=691.75 +/- 610.50
Episode length: 34.56 +/- 6.57
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.6     |
|    mean_reward          | 692      |
| time/                   |          |
|    total_timesteps      | 207000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.5e-12 |
|    explained_variance   | 0.0134   |
|    learning_rate        | 0.001    |
|    loss                 | 2.32e+04 |
|    n_updates            | 806      |
|    policy_gradient_loss | 6.29e-10 |
|    value_loss           | 2.74e+04 |
--------------------------------------
Eval num_timesteps=207500, episode_reward=849.51 +/- 715.16
Episode length: 35.28 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=837.85 +/- 729.91
Episode length: 34.78 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=762.44 +/- 705.72
Episode length: 34.00 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 815      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 102      |
|    time_elapsed    | 3136     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=839.02 +/- 662.29
Episode length: 35.76 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.25e-15 |
|    explained_variance   | 0.00695   |
|    learning_rate        | 0.001     |
|    loss                 | 9.25e+03  |
|    n_updates            | 816       |
|    policy_gradient_loss | 5.02e-10  |
|    value_loss           | 3.25e+04  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=820.10 +/- 670.94
Episode length: 35.56 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=792.79 +/- 685.24
Episode length: 34.66 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=910.85 +/- 711.19
Episode length: 36.16 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 103      |
|    time_elapsed    | 3181     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=915.14 +/- 745.39
Episode length: 35.46 +/- 7.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 915       |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.93e-12 |
|    explained_variance   | 0.0159    |
|    learning_rate        | 0.001     |
|    loss                 | 1.84e+04  |
|    n_updates            | 826       |
|    policy_gradient_loss | -4.51e-10 |
|    value_loss           | 2.73e+04  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=881.00 +/- 664.59
Episode length: 35.84 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=800.75 +/- 679.78
Episode length: 34.54 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=982.03 +/- 757.94
Episode length: 35.66 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 104      |
|    time_elapsed    | 3227     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=896.14 +/- 696.01
Episode length: 35.68 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 896       |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-14 |
|    explained_variance   | 0.00625   |
|    learning_rate        | 0.001     |
|    loss                 | 2.07e+04  |
|    n_updates            | 836       |
|    policy_gradient_loss | -3.68e-10 |
|    value_loss           | 2.8e+04   |
---------------------------------------
Eval num_timesteps=213500, episode_reward=792.43 +/- 652.58
Episode length: 35.24 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=829.67 +/- 709.14
Episode length: 34.68 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=941.47 +/- 736.04
Episode length: 35.96 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=872.21 +/- 712.07
Episode length: 35.30 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 831      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 105      |
|    time_elapsed    | 3262     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=789.60 +/- 643.51
Episode length: 35.14 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 790       |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.09e-12 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.001     |
|    loss                 | 1.22e+04  |
|    n_updates            | 846       |
|    policy_gradient_loss | 1.46e-09  |
|    value_loss           | 2.57e+04  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=832.16 +/- 681.82
Episode length: 35.66 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=812.20 +/- 633.46
Episode length: 35.98 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=801.27 +/- 648.10
Episode length: 35.54 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 106      |
|    time_elapsed    | 3290     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=727.35 +/- 591.47
Episode length: 34.66 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 727       |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-14 |
|    explained_variance   | 0.00689   |
|    learning_rate        | 0.001     |
|    loss                 | 8.63e+03  |
|    n_updates            | 856       |
|    policy_gradient_loss | 5.6e-10   |
|    value_loss           | 3.17e+04  |
---------------------------------------
Eval num_timesteps=218000, episode_reward=878.06 +/- 730.64
Episode length: 34.94 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=738.45 +/- 664.48
Episode length: 34.34 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=866.68 +/- 698.01
Episode length: 35.72 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 911      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 107      |
|    time_elapsed    | 3316     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=812.47 +/- 671.67
Episode length: 35.06 +/- 5.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.13e-30 |
|    explained_variance   | 0.0107    |
|    learning_rate        | 0.001     |
|    loss                 | 1.21e+04  |
|    n_updates            | 866       |
|    policy_gradient_loss | -7.83e-10 |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=220000, episode_reward=790.97 +/- 663.98
Episode length: 35.20 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=933.03 +/- 721.28
Episode length: 36.40 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=774.97 +/- 649.67
Episode length: 34.66 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 108      |
|    time_elapsed    | 3359     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=857.64 +/- 697.64
Episode length: 35.52 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 858       |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-24 |
|    explained_variance   | 0.00355   |
|    learning_rate        | 0.001     |
|    loss                 | 3.83e+04  |
|    n_updates            | 876       |
|    policy_gradient_loss | -4.64e-10 |
|    value_loss           | 7.98e+04  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=901.51 +/- 741.01
Episode length: 35.72 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=659.23 +/- 564.29
Episode length: 33.84 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=859.44 +/- 713.78
Episode length: 35.70 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 109      |
|    time_elapsed    | 3394     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=748.04 +/- 692.08
Episode length: 33.68 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 748       |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.77e-27 |
|    explained_variance   | 0.0154    |
|    learning_rate        | 0.001     |
|    loss                 | 6.3e+03   |
|    n_updates            | 886       |
|    policy_gradient_loss | 1.91e-10  |
|    value_loss           | 2.44e+04  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=706.52 +/- 650.78
Episode length: 33.76 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=785.88 +/- 612.87
Episode length: 35.40 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=789.95 +/- 640.53
Episode length: 34.94 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 722      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 110      |
|    time_elapsed    | 3430     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=855.43 +/- 691.64
Episode length: 35.78 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.47e-24 |
|    explained_variance   | -0.0049   |
|    learning_rate        | 0.001     |
|    loss                 | 4.33e+04  |
|    n_updates            | 896       |
|    policy_gradient_loss | -8.66e-10 |
|    value_loss           | 7.48e+04  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=814.56 +/- 709.83
Episode length: 34.62 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=844.15 +/- 699.49
Episode length: 35.52 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=778.15 +/- 664.15
Episode length: 34.64 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 701      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 111      |
|    time_elapsed    | 3457     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=728.07 +/- 601.97
Episode length: 35.04 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 728       |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.05e-10 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.001     |
|    loss                 | 6.95e+03  |
|    n_updates            | 906       |
|    policy_gradient_loss | 1.21e-09  |
|    value_loss           | 2.01e+04  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=988.43 +/- 747.79
Episode length: 36.34 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=830.82 +/- 672.05
Episode length: 35.66 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=724.88 +/- 611.06
Episode length: 34.70 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 112      |
|    time_elapsed    | 3499     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=1010.48 +/- 768.77
Episode length: 36.70 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.91e-12 |
|    explained_variance   | 0.00824   |
|    learning_rate        | 0.001     |
|    loss                 | 9.19e+03  |
|    n_updates            | 916       |
|    policy_gradient_loss | 1.16e-10  |
|    value_loss           | 3.18e+04  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=934.94 +/- 673.93
Episode length: 37.00 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=888.72 +/- 727.51
Episode length: 35.72 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=883.57 +/- 706.81
Episode length: 35.88 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 113      |
|    time_elapsed    | 3533     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=760.49 +/- 653.20
Episode length: 34.82 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 760       |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-26 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.001     |
|    loss                 | 9.7e+03   |
|    n_updates            | 926       |
|    policy_gradient_loss | -4.07e-11 |
|    value_loss           | 2.3e+04   |
---------------------------------------
Eval num_timesteps=232000, episode_reward=780.36 +/- 677.43
Episode length: 35.30 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=955.38 +/- 719.75
Episode length: 36.56 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=861.36 +/- 653.64
Episode length: 37.18 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 737      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 114      |
|    time_elapsed    | 3560     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=920.79 +/- 697.09
Episode length: 36.24 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 921       |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-23 |
|    explained_variance   | -0.00316  |
|    learning_rate        | 0.001     |
|    loss                 | 2.68e+04  |
|    n_updates            | 936       |
|    policy_gradient_loss | 3.08e-09  |
|    value_loss           | 6.82e+04  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=707.70 +/- 622.64
Episode length: 34.60 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=969.06 +/- 716.69
Episode length: 36.60 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=764.99 +/- 603.41
Episode length: 35.36 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=761.92 +/- 651.10
Episode length: 34.22 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 115      |
|    time_elapsed    | 3589     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=767.65 +/- 644.82
Episode length: 34.44 +/- 7.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.31e-10 |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+04  |
|    n_updates            | 946       |
|    policy_gradient_loss | -2.3e-10  |
|    value_loss           | 2.54e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=850.88 +/- 679.69
Episode length: 35.66 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=667.21 +/- 633.57
Episode length: 33.70 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 667      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=768.32 +/- 647.96
Episode length: 34.98 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 116      |
|    time_elapsed    | 3614     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=804.65 +/- 675.35
Episode length: 34.74 +/- 5.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-12 |
|    explained_variance   | 0.0027    |
|    learning_rate        | 0.001     |
|    loss                 | 5.95e+03  |
|    n_updates            | 956       |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 1.98e+04  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=766.51 +/- 720.49
Episode length: 33.72 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=792.10 +/- 668.44
Episode length: 35.06 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=860.49 +/- 665.92
Episode length: 36.26 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 117      |
|    time_elapsed    | 3639     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=829.22 +/- 709.11
Episode length: 34.64 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.19e-10 |
|    explained_variance   | 0.0157    |
|    learning_rate        | 0.001     |
|    loss                 | 1.83e+04  |
|    n_updates            | 966       |
|    policy_gradient_loss | 8e-11     |
|    value_loss           | 2.55e+04  |
---------------------------------------
Eval num_timesteps=240500, episode_reward=1026.02 +/- 775.55
Episode length: 36.60 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=811.36 +/- 653.65
Episode length: 35.50 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=836.12 +/- 726.55
Episode length: 34.66 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 118      |
|    time_elapsed    | 3664     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=768.87 +/- 665.52
Episode length: 34.84 +/- 6.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 769       |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.33e-12 |
|    explained_variance   | 0.00667   |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+04  |
|    n_updates            | 976       |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 2.98e+04  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=799.43 +/- 680.30
Episode length: 34.62 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=791.59 +/- 639.89
Episode length: 35.50 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=915.20 +/- 702.33
Episode length: 36.20 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 119      |
|    time_elapsed    | 3689     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=739.55 +/- 608.67
Episode length: 34.96 +/- 6.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 740       |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-25 |
|    explained_variance   | 0.0161    |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+04  |
|    n_updates            | 986       |
|    policy_gradient_loss | -4.5e-10  |
|    value_loss           | 2.85e+04  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=811.28 +/- 605.05
Episode length: 36.18 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=813.07 +/- 696.97
Episode length: 34.82 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=788.43 +/- 674.43
Episode length: 34.94 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 855      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 120      |
|    time_elapsed    | 3714     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=722.12 +/- 636.07
Episode length: 33.84 +/- 7.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 722       |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.28e-22 |
|    explained_variance   | 0.0211    |
|    learning_rate        | 0.001     |
|    loss                 | 2.96e+04  |
|    n_updates            | 996       |
|    policy_gradient_loss | 2.06e-09  |
|    value_loss           | 5.54e+04  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=859.00 +/- 664.21
Episode length: 36.24 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=793.58 +/- 680.39
Episode length: 34.74 +/- 7.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=906.01 +/- 672.50
Episode length: 36.90 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 121      |
|    time_elapsed    | 3740     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=823.46 +/- 669.87
Episode length: 35.00 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 823       |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.85e-24 |
|    explained_variance   | 0.015     |
|    learning_rate        | 0.001     |
|    loss                 | 1.21e+04  |
|    n_updates            | 1006      |
|    policy_gradient_loss | -5.06e-10 |
|    value_loss           | 2.54e+04  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=781.04 +/- 676.83
Episode length: 34.40 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=810.04 +/- 640.75
Episode length: 35.64 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=806.38 +/- 664.44
Episode length: 35.58 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 122      |
|    time_elapsed    | 3781     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=652.22 +/- 585.26
Episode length: 33.50 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 652       |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.18e-22 |
|    explained_variance   | 0.0173    |
|    learning_rate        | 0.001     |
|    loss                 | 2.52e+04  |
|    n_updates            | 1016      |
|    policy_gradient_loss | -2.11e-09 |
|    value_loss           | 5.19e+04  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=858.58 +/- 680.01
Episode length: 35.92 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=754.89 +/- 661.83
Episode length: 33.96 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=995.53 +/- 706.26
Episode length: 36.64 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 123      |
|    time_elapsed    | 3809     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=765.94 +/- 616.67
Episode length: 35.70 +/- 5.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 766       |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.4e-10  |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 1.51e+04  |
|    n_updates            | 1026      |
|    policy_gradient_loss | -1.35e-10 |
|    value_loss           | 2.05e+04  |
---------------------------------------
Eval num_timesteps=252500, episode_reward=844.82 +/- 702.03
Episode length: 34.96 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=842.68 +/- 642.83
Episode length: 35.94 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=924.29 +/- 720.60
Episode length: 35.76 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 124      |
|    time_elapsed    | 3836     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=826.98 +/- 685.83
Episode length: 35.36 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.91e-12 |
|    explained_variance   | 0.00767   |
|    learning_rate        | 0.001     |
|    loss                 | 9.76e+03  |
|    n_updates            | 1036      |
|    policy_gradient_loss | -1.5e-09  |
|    value_loss           | 3.02e+04  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=799.20 +/- 702.64
Episode length: 34.26 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=751.66 +/- 651.64
Episode length: 34.66 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=857.42 +/- 683.10
Episode length: 35.62 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=856.60 +/- 732.90
Episode length: 34.68 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 125      |
|    time_elapsed    | 3875     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=827.03 +/- 662.16
Episode length: 35.38 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.21e-24 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.001     |
|    loss                 | 1.63e+04  |
|    n_updates            | 1046      |
|    policy_gradient_loss | 9.31e-10  |
|    value_loss           | 2.4e+04   |
---------------------------------------
Eval num_timesteps=257000, episode_reward=1023.02 +/- 690.96
Episode length: 37.98 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38       |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=789.46 +/- 699.34
Episode length: 34.30 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=890.00 +/- 664.33
Episode length: 36.16 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 126      |
|    time_elapsed    | 3911     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=711.03 +/- 585.04
Episode length: 34.44 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 711       |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.29e-20 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.001     |
|    loss                 | 1.81e+04  |
|    n_updates            | 1056      |
|    policy_gradient_loss | -2.9e-09  |
|    value_loss           | 5.19e+04  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=748.53 +/- 691.90
Episode length: 33.86 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=962.72 +/- 732.27
Episode length: 36.00 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=930.26 +/- 707.34
Episode length: 36.52 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 127      |
|    time_elapsed    | 3946     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=878.29 +/- 680.82
Episode length: 35.58 +/- 5.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.98e-23 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.001     |
|    loss                 | 9.81e+03  |
|    n_updates            | 1066      |
|    policy_gradient_loss | 1.51e-10  |
|    value_loss           | 2.58e+04  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=819.67 +/- 636.80
Episode length: 35.76 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=673.27 +/- 524.25
Episode length: 35.14 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=842.96 +/- 702.60
Episode length: 35.38 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 128      |
|    time_elapsed    | 3980     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=830.74 +/- 726.11
Episode length: 34.46 +/- 7.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-20 |
|    explained_variance   | 0.022     |
|    learning_rate        | 0.001     |
|    loss                 | 2.32e+04  |
|    n_updates            | 1076      |
|    policy_gradient_loss | 6.47e-09  |
|    value_loss           | 4.82e+04  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=843.69 +/- 709.65
Episode length: 35.10 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=762.42 +/- 665.22
Episode length: 33.98 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=756.26 +/- 669.60
Episode length: 34.00 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 943      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 129      |
|    time_elapsed    | 4006     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=860.62 +/- 663.84
Episode length: 36.18 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.91e-23 |
|    explained_variance   | 0.0172    |
|    learning_rate        | 0.001     |
|    loss                 | 5.02e+03  |
|    n_updates            | 1086      |
|    policy_gradient_loss | 3.03e-10  |
|    value_loss           | 2.63e+04  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=861.50 +/- 702.31
Episode length: 35.24 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=804.55 +/- 677.80
Episode length: 35.16 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=765.92 +/- 681.15
Episode length: 34.12 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 910      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 130      |
|    time_elapsed    | 4033     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=833.60 +/- 634.49
Episode length: 36.36 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.52e-20 |
|    explained_variance   | 0.0194    |
|    learning_rate        | 0.001     |
|    loss                 | 2.79e+04  |
|    n_updates            | 1096      |
|    policy_gradient_loss | -5.92e-10 |
|    value_loss           | 4.56e+04  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=982.22 +/- 772.02
Episode length: 35.38 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=836.73 +/- 662.26
Episode length: 35.28 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=853.39 +/- 673.40
Episode length: 35.24 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 131      |
|    time_elapsed    | 4060     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=758.29 +/- 610.61
Episode length: 35.32 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.62e-23 |
|    explained_variance   | 0.0204    |
|    learning_rate        | 0.001     |
|    loss                 | 8.32e+03  |
|    n_updates            | 1106      |
|    policy_gradient_loss | 4.03e-10  |
|    value_loss           | 2.79e+04  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=808.42 +/- 656.82
Episode length: 35.26 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=818.45 +/- 684.02
Episode length: 35.14 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=740.66 +/- 633.15
Episode length: 34.42 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 985      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 132      |
|    time_elapsed    | 4087     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=748.12 +/- 643.85
Episode length: 34.78 +/- 6.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 748       |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.74e-20 |
|    explained_variance   | 0.028     |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+04  |
|    n_updates            | 1116      |
|    policy_gradient_loss | -1.86e-09 |
|    value_loss           | 4.38e+04  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=706.31 +/- 551.17
Episode length: 35.34 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=646.45 +/- 552.81
Episode length: 33.68 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 646      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=985.64 +/- 744.73
Episode length: 36.06 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 938      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 133      |
|    time_elapsed    | 4115     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=875.18 +/- 656.31
Episode length: 35.84 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-22 |
|    explained_variance   | 0.0169    |
|    learning_rate        | 0.001     |
|    loss                 | 1.6e+04   |
|    n_updates            | 1126      |
|    policy_gradient_loss | 3.61e-10  |
|    value_loss           | 2.75e+04  |
---------------------------------------
Eval num_timesteps=273000, episode_reward=861.29 +/- 698.25
Episode length: 35.22 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=643.32 +/- 594.67
Episode length: 33.36 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 643      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=736.53 +/- 632.41
Episode length: 34.72 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | 984      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 134      |
|    time_elapsed    | 4153     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=812.44 +/- 743.41
Episode length: 33.48 +/- 7.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.05e-20 |
|    explained_variance   | 0.0286    |
|    learning_rate        | 0.001     |
|    loss                 | 1.89e+04  |
|    n_updates            | 1136      |
|    policy_gradient_loss | 3.93e-11  |
|    value_loss           | 4.21e+04  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=864.86 +/- 726.84
Episode length: 34.86 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=795.28 +/- 700.92
Episode length: 34.36 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=973.56 +/- 738.17
Episode length: 36.54 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 919      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 135      |
|    time_elapsed    | 4186     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=852.45 +/- 718.74
Episode length: 34.90 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.46e-09 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.001     |
|    loss                 | 1.19e+04  |
|    n_updates            | 1146      |
|    policy_gradient_loss | 1.98e-10  |
|    value_loss           | 2.77e+04  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=814.64 +/- 644.82
Episode length: 35.82 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=746.26 +/- 651.22
Episode length: 34.24 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=872.51 +/- 674.01
Episode length: 36.06 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=967.73 +/- 685.84
Episode length: 36.70 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 136      |
|    time_elapsed    | 4220     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=778.48 +/- 642.50
Episode length: 34.90 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 778       |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.23e-11 |
|    explained_variance   | 0.00758   |
|    learning_rate        | 0.001     |
|    loss                 | 1.34e+04  |
|    n_updates            | 1156      |
|    policy_gradient_loss | -1.03e-09 |
|    value_loss           | 2.68e+04  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=952.15 +/- 693.13
Episode length: 36.62 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=767.12 +/- 638.45
Episode length: 35.36 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=654.66 +/- 541.00
Episode length: 34.10 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 893      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 137      |
|    time_elapsed    | 4248     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=598.14 +/- 413.11
Episode length: 35.42 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 598       |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.62e-22 |
|    explained_variance   | 0.0189    |
|    learning_rate        | 0.001     |
|    loss                 | 1.11e+04  |
|    n_updates            | 1166      |
|    policy_gradient_loss | 4.25e-10  |
|    value_loss           | 2.8e+04   |
---------------------------------------
Eval num_timesteps=281500, episode_reward=737.55 +/- 672.52
Episode length: 33.88 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=702.16 +/- 636.68
Episode length: 33.74 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=822.00 +/- 697.58
Episode length: 35.12 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 138      |
|    time_elapsed    | 4293     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=797.33 +/- 640.48
Episode length: 35.48 +/- 5.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 797       |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.89e-19 |
|    explained_variance   | 0.0231    |
|    learning_rate        | 0.001     |
|    loss                 | 2.6e+04   |
|    n_updates            | 1176      |
|    policy_gradient_loss | 4.52e-09  |
|    value_loss           | 4.3e+04   |
---------------------------------------
Eval num_timesteps=283500, episode_reward=853.90 +/- 666.05
Episode length: 35.50 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=853.81 +/- 708.16
Episode length: 34.74 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=730.08 +/- 625.01
Episode length: 33.90 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 911      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 139      |
|    time_elapsed    | 4327     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=738.50 +/- 613.69
Episode length: 34.74 +/- 6.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 739       |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-21 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.001     |
|    loss                 | 8.4e+03   |
|    n_updates            | 1186      |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 2.43e+04  |
---------------------------------------
Eval num_timesteps=285500, episode_reward=915.53 +/- 710.84
Episode length: 36.36 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=658.62 +/- 572.04
Episode length: 33.82 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=834.51 +/- 686.29
Episode length: 36.04 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 924      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 140      |
|    time_elapsed    | 4362     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=817.15 +/- 683.26
Episode length: 34.52 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.17e-19 |
|    explained_variance   | 0.0249    |
|    learning_rate        | 0.001     |
|    loss                 | 2.1e+04   |
|    n_updates            | 1196      |
|    policy_gradient_loss | -1.48e-09 |
|    value_loss           | 4.26e+04  |
---------------------------------------
Eval num_timesteps=287500, episode_reward=872.35 +/- 724.67
Episode length: 35.16 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=835.10 +/- 648.17
Episode length: 35.52 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=812.37 +/- 610.84
Episode length: 36.38 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 141      |
|    time_elapsed    | 4402     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=906.05 +/- 696.80
Episode length: 35.98 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.32e-21 |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.001     |
|    loss                 | 1.5e+04   |
|    n_updates            | 1206      |
|    policy_gradient_loss | 2.81e-10  |
|    value_loss           | 2.34e+04  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=798.86 +/- 677.08
Episode length: 35.00 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=826.62 +/- 687.65
Episode length: 35.20 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=728.54 +/- 609.64
Episode length: 34.76 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 142      |
|    time_elapsed    | 4442     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=823.47 +/- 705.35
Episode length: 34.88 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 823       |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.4e-18  |
|    explained_variance   | 0.0154    |
|    learning_rate        | 0.001     |
|    loss                 | 2.23e+04  |
|    n_updates            | 1216      |
|    policy_gradient_loss | -3.23e-10 |
|    value_loss           | 4.55e+04  |
---------------------------------------
Eval num_timesteps=291500, episode_reward=670.24 +/- 601.26
Episode length: 34.00 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=807.54 +/- 698.35
Episode length: 35.18 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=834.85 +/- 688.23
Episode length: 35.18 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 143      |
|    time_elapsed    | 4469     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=745.59 +/- 622.66
Episode length: 34.62 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 746       |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.82e-21 |
|    explained_variance   | 0.0166    |
|    learning_rate        | 0.001     |
|    loss                 | 1.22e+04  |
|    n_updates            | 1226      |
|    policy_gradient_loss | 5.82e-10  |
|    value_loss           | 2.39e+04  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=731.79 +/- 658.33
Episode length: 33.86 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=677.76 +/- 613.82
Episode length: 33.96 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=854.02 +/- 706.08
Episode length: 35.24 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 910      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 144      |
|    time_elapsed    | 4496     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=840.80 +/- 763.50
Episode length: 34.00 +/- 8.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.84e-18 |
|    explained_variance   | 0.026     |
|    learning_rate        | 0.001     |
|    loss                 | 2.16e+04  |
|    n_updates            | 1236      |
|    policy_gradient_loss | -7.28e-10 |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=295500, episode_reward=911.39 +/- 716.41
Episode length: 36.10 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=709.53 +/- 598.60
Episode length: 34.40 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=718.57 +/- 635.11
Episode length: 34.26 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 979      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 145      |
|    time_elapsed    | 4523     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=749.36 +/- 594.56
Episode length: 35.34 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 749       |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-20 |
|    explained_variance   | 0.0191    |
|    learning_rate        | 0.001     |
|    loss                 | 1.54e+04  |
|    n_updates            | 1246      |
|    policy_gradient_loss | 4.51e-11  |
|    value_loss           | 2.7e+04   |
---------------------------------------
Eval num_timesteps=297500, episode_reward=951.93 +/- 767.85
Episode length: 35.86 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=977.22 +/- 776.47
Episode length: 35.78 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 977      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=808.58 +/- 614.03
Episode length: 36.68 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=854.68 +/- 633.23
Episode length: 36.06 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 146      |
|    time_elapsed    | 4555     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=834.19 +/- 664.74
Episode length: 35.56 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.55e-18 |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.001     |
|    loss                 | 2.23e+04  |
|    n_updates            | 1256      |
|    policy_gradient_loss | 1.62e-09  |
|    value_loss           | 3.99e+04  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=915.28 +/- 723.42
Episode length: 35.52 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=702.33 +/- 634.30
Episode length: 34.24 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=826.55 +/- 669.32
Episode length: 35.72 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 147      |
|    time_elapsed    | 4585     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=900.37 +/- 679.85
Episode length: 36.54 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 900       |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-08 |
|    explained_variance   | 0.00606   |
|    learning_rate        | 0.001     |
|    loss                 | 9.22e+03  |
|    n_updates            | 1266      |
|    policy_gradient_loss | 5.65e-10  |
|    value_loss           | 1.91e+04  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=847.05 +/- 659.09
Episode length: 35.36 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=872.68 +/- 752.96
Episode length: 34.74 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=801.75 +/- 681.52
Episode length: 34.72 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 689      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 148      |
|    time_elapsed    | 4627     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=872.72 +/- 718.72
Episode length: 35.62 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 873       |
| time/                   |           |
|    total_timesteps      | 303500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-10 |
|    explained_variance   | 0.00567   |
|    learning_rate        | 0.001     |
|    loss                 | 4.4e+03   |
|    n_updates            | 1276      |
|    policy_gradient_loss | 1.93e-09  |
|    value_loss           | 2.24e+04  |
---------------------------------------
Eval num_timesteps=304000, episode_reward=635.22 +/- 606.61
Episode length: 32.68 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=743.55 +/- 598.45
Episode length: 35.20 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=808.58 +/- 678.18
Episode length: 34.80 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 149      |
|    time_elapsed    | 4670     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=838.73 +/- 685.16
Episode length: 35.74 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.66e-09 |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.001     |
|    loss                 | 6e+03     |
|    n_updates            | 1286      |
|    policy_gradient_loss | -8.73e-11 |
|    value_loss           | 2.4e+04   |
---------------------------------------
Eval num_timesteps=306000, episode_reward=925.28 +/- 733.35
Episode length: 35.14 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=774.71 +/- 679.40
Episode length: 34.32 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=777.34 +/- 679.04
Episode length: 34.88 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 150      |
|    time_elapsed    | 4701     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=718.77 +/- 594.70
Episode length: 34.84 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 719       |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.52e-12 |
|    explained_variance   | 0.00675   |
|    learning_rate        | 0.001     |
|    loss                 | 1.24e+04  |
|    n_updates            | 1296      |
|    policy_gradient_loss | -4.67e-10 |
|    value_loss           | 2.01e+04  |
---------------------------------------
Eval num_timesteps=308000, episode_reward=943.45 +/- 663.72
Episode length: 37.24 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=799.08 +/- 690.24
Episode length: 34.82 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=863.17 +/- 738.19
Episode length: 35.08 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 151      |
|    time_elapsed    | 4727     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=779.50 +/- 679.03
Episode length: 34.38 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 779       |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-17 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.001     |
|    loss                 | 1.57e+04  |
|    n_updates            | 1306      |
|    policy_gradient_loss | -5.24e-11 |
|    value_loss           | 2.36e+04  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=780.23 +/- 682.45
Episode length: 34.72 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=732.06 +/- 688.24
Episode length: 34.10 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=720.25 +/- 601.16
Episode length: 34.54 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 152      |
|    time_elapsed    | 4759     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=817.23 +/- 632.24
Episode length: 35.42 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 311500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.38e-20 |
|    explained_variance   | 0.0189    |
|    learning_rate        | 0.001     |
|    loss                 | 1.75e+04  |
|    n_updates            | 1316      |
|    policy_gradient_loss | -2.65e-10 |
|    value_loss           | 2.79e+04  |
---------------------------------------
Eval num_timesteps=312000, episode_reward=882.92 +/- 711.61
Episode length: 35.08 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=861.59 +/- 681.51
Episode length: 35.72 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=945.85 +/- 719.93
Episode length: 36.04 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 153      |
|    time_elapsed    | 4786     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=785.58 +/- 605.39
Episode length: 35.78 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.38e-17 |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.001     |
|    loss                 | 2.26e+04  |
|    n_updates            | 1326      |
|    policy_gradient_loss | -7.51e-10 |
|    value_loss           | 4.36e+04  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=853.41 +/- 688.80
Episode length: 35.30 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=935.37 +/- 765.34
Episode length: 35.32 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=880.37 +/- 744.22
Episode length: 34.90 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 154      |
|    time_elapsed    | 4821     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=752.26 +/- 633.92
Episode length: 34.58 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 752       |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.43e-19 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.001     |
|    loss                 | 1.75e+04  |
|    n_updates            | 1336      |
|    policy_gradient_loss | -6.69e-10 |
|    value_loss           | 2.35e+04  |
---------------------------------------
Eval num_timesteps=316000, episode_reward=875.58 +/- 617.65
Episode length: 37.40 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=706.48 +/- 749.38
Episode length: 32.48 +/- 8.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.5     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=1046.68 +/- 718.14
Episode length: 36.52 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 155      |
|    time_elapsed    | 4851     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=595.68 +/- 526.07
Episode length: 33.52 +/- 6.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 596       |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.79e-16 |
|    explained_variance   | 0.0161    |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+04  |
|    n_updates            | 1346      |
|    policy_gradient_loss | 9.9e-10   |
|    value_loss           | 3.45e+04  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=657.43 +/- 600.61
Episode length: 33.30 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 657      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=900.94 +/- 718.83
Episode length: 36.08 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=765.34 +/- 621.76
Episode length: 35.42 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 156      |
|    time_elapsed    | 4880     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=1010.20 +/- 745.47
Episode length: 36.10 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.95e-08 |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.001     |
|    loss                 | 1.44e+04  |
|    n_updates            | 1356      |
|    policy_gradient_loss | 2.62e-11  |
|    value_loss           | 2.15e+04  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=826.82 +/- 669.31
Episode length: 36.04 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=684.32 +/- 570.99
Episode length: 34.26 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=978.64 +/- 718.02
Episode length: 36.86 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=791.09 +/- 646.30
Episode length: 35.36 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 719      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 157      |
|    time_elapsed    | 4915     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=882.60 +/- 738.63
Episode length: 35.18 +/- 7.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.48e-10 |
|    explained_variance   | 0.00627   |
|    learning_rate        | 0.001     |
|    loss                 | 3.73e+03  |
|    n_updates            | 1366      |
|    policy_gradient_loss | -1e-09    |
|    value_loss           | 2.5e+04   |
---------------------------------------
Eval num_timesteps=322500, episode_reward=948.87 +/- 744.07
Episode length: 35.52 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=928.70 +/- 693.21
Episode length: 36.56 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=881.95 +/- 684.76
Episode length: 35.74 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 728      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 158      |
|    time_elapsed    | 4940     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=1014.14 +/- 739.98
Episode length: 36.76 +/- 5.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.25e-19 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.001     |
|    loss                 | 1.53e+04  |
|    n_updates            | 1376      |
|    policy_gradient_loss | 1.89e-09  |
|    value_loss           | 2.42e+04  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=817.21 +/- 705.46
Episode length: 34.54 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=901.20 +/- 737.16
Episode length: 35.36 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=623.54 +/- 544.77
Episode length: 33.54 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 159      |
|    time_elapsed    | 4979     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=787.27 +/- 685.25
Episode length: 34.48 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16e-16 |
|    explained_variance   | 0.017     |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+04  |
|    n_updates            | 1386      |
|    policy_gradient_loss | 9.4e-10   |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=738.76 +/- 632.12
Episode length: 34.54 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=921.25 +/- 650.48
Episode length: 37.00 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=823.10 +/- 664.92
Episode length: 35.98 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 816      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 160      |
|    time_elapsed    | 5023     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=885.38 +/- 733.87
Episode length: 35.04 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 885       |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.16e-19 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.001     |
|    loss                 | 1.71e+04  |
|    n_updates            | 1396      |
|    policy_gradient_loss | -1.6e-11  |
|    value_loss           | 2.47e+04  |
---------------------------------------
Eval num_timesteps=328500, episode_reward=805.67 +/- 685.37
Episode length: 34.32 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=809.15 +/- 648.85
Episode length: 35.58 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=912.92 +/- 711.03
Episode length: 35.58 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 914      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 161      |
|    time_elapsed    | 5069     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=902.89 +/- 717.49
Episode length: 35.60 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 903       |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.09e-16 |
|    explained_variance   | 0.0309    |
|    learning_rate        | 0.001     |
|    loss                 | 1.26e+04  |
|    n_updates            | 1406      |
|    policy_gradient_loss | 1.45e-09  |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=778.58 +/- 628.76
Episode length: 35.12 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=787.62 +/- 693.21
Episode length: 34.36 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=815.61 +/- 694.19
Episode length: 34.86 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 912      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 162      |
|    time_elapsed    | 5110     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=703.03 +/- 613.10
Episode length: 34.14 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 703       |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.53e-19 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.001     |
|    loss                 | 1.08e+04  |
|    n_updates            | 1416      |
|    policy_gradient_loss | 6.48e-10  |
|    value_loss           | 2.47e+04  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=727.87 +/- 679.26
Episode length: 33.72 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=956.88 +/- 780.51
Episode length: 35.18 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=703.26 +/- 581.79
Episode length: 34.80 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 163      |
|    time_elapsed    | 5136     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=728.31 +/- 621.33
Episode length: 34.30 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 728       |
| time/                   |           |
|    total_timesteps      | 334000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.72e-16 |
|    explained_variance   | 0.0183    |
|    learning_rate        | 0.001     |
|    loss                 | 1.88e+04  |
|    n_updates            | 1426      |
|    policy_gradient_loss | 2.41e-09  |
|    value_loss           | 3.5e+04   |
---------------------------------------
Eval num_timesteps=334500, episode_reward=958.14 +/- 721.52
Episode length: 36.40 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=872.75 +/- 743.43
Episode length: 34.74 +/- 7.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=889.24 +/- 689.87
Episode length: 36.24 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 164      |
|    time_elapsed    | 5163     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=944.93 +/- 742.45
Episode length: 36.10 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 945       |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.34e-18 |
|    explained_variance   | 0.0148    |
|    learning_rate        | 0.001     |
|    loss                 | 8.75e+03  |
|    n_updates            | 1436      |
|    policy_gradient_loss | 8.03e-10  |
|    value_loss           | 2.52e+04  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=808.74 +/- 680.51
Episode length: 34.56 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=925.60 +/- 669.36
Episode length: 36.64 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=729.93 +/- 681.27
Episode length: 33.06 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 165      |
|    time_elapsed    | 5202     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=668.05 +/- 628.93
Episode length: 32.46 +/- 8.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.5      |
|    mean_reward          | 668       |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.8e-15  |
|    explained_variance   | 0.0231    |
|    learning_rate        | 0.001     |
|    loss                 | 2.04e+04  |
|    n_updates            | 1446      |
|    policy_gradient_loss | -8.25e-10 |
|    value_loss           | 3.11e+04  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=910.29 +/- 702.19
Episode length: 36.12 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=805.59 +/- 714.84
Episode length: 34.62 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=856.35 +/- 693.73
Episode length: 35.40 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 166      |
|    time_elapsed    | 5230     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=904.76 +/- 680.72
Episode length: 36.74 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 905       |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.19e-18 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.001     |
|    loss                 | 5.32e+03  |
|    n_updates            | 1456      |
|    policy_gradient_loss | 4.51e-10  |
|    value_loss           | 2.18e+04  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=1020.06 +/- 756.80
Episode length: 36.68 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=632.82 +/- 601.70
Episode length: 32.64 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=969.91 +/- 707.56
Episode length: 36.36 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=738.23 +/- 592.14
Episode length: 35.24 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 167      |
|    time_elapsed    | 5275     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=832.65 +/- 711.51
Episode length: 34.48 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 833       |
| time/                   |           |
|    total_timesteps      | 342500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.98e-15 |
|    explained_variance   | 0.0238    |
|    learning_rate        | 0.001     |
|    loss                 | 1.37e+04  |
|    n_updates            | 1466      |
|    policy_gradient_loss | -2.06e-09 |
|    value_loss           | 3.21e+04  |
---------------------------------------
Eval num_timesteps=343000, episode_reward=764.20 +/- 676.22
Episode length: 34.24 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=620.34 +/- 569.61
Episode length: 32.90 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=521.96 +/- 607.51
Episode length: 31.06 +/- 8.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.1     |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 168      |
|    time_elapsed    | 5301     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=754.25 +/- 589.95
Episode length: 35.34 +/- 5.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 754       |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.96e-17 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.001     |
|    loss                 | 1.45e+04  |
|    n_updates            | 1476      |
|    policy_gradient_loss | -8.57e-10 |
|    value_loss           | 2.37e+04  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=843.35 +/- 697.04
Episode length: 35.48 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=849.40 +/- 691.22
Episode length: 35.42 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=836.28 +/- 665.84
Episode length: 35.70 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 169      |
|    time_elapsed    | 5330     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=845.05 +/- 683.85
Episode length: 35.62 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 845       |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.35e-15 |
|    explained_variance   | 0.0293    |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+04  |
|    n_updates            | 1486      |
|    policy_gradient_loss | -6.98e-11 |
|    value_loss           | 3.07e+04  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=802.12 +/- 591.87
Episode length: 36.00 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=855.92 +/- 658.34
Episode length: 35.86 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=605.08 +/- 528.56
Episode length: 33.54 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 605      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 170      |
|    time_elapsed    | 5360     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=734.94 +/- 604.27
Episode length: 34.84 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 735       |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.15e-17 |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.001     |
|    loss                 | 1.16e+04  |
|    n_updates            | 1496      |
|    policy_gradient_loss | -4.05e-10 |
|    value_loss           | 2.42e+04  |
---------------------------------------
Eval num_timesteps=349000, episode_reward=843.81 +/- 634.51
Episode length: 36.18 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=700.13 +/- 613.82
Episode length: 34.52 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=689.34 +/- 657.13
Episode length: 33.24 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 171      |
|    time_elapsed    | 5388     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=768.27 +/- 649.52
Episode length: 34.50 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.32e-14 |
|    explained_variance   | 0.027     |
|    learning_rate        | 0.001     |
|    loss                 | 2.14e+04  |
|    n_updates            | 1506      |
|    policy_gradient_loss | -1.12e-09 |
|    value_loss           | 2.79e+04  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=778.98 +/- 636.89
Episode length: 35.44 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=887.73 +/- 680.48
Episode length: 36.18 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=877.70 +/- 736.56
Episode length: 35.20 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 172      |
|    time_elapsed    | 5415     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=813.71 +/- 641.46
Episode length: 35.74 +/- 5.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.03e-17 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 9.66e+03  |
|    n_updates            | 1516      |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 2.58e+04  |
---------------------------------------
Eval num_timesteps=353000, episode_reward=962.46 +/- 740.07
Episode length: 35.44 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=781.13 +/- 625.04
Episode length: 35.38 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=809.77 +/- 650.69
Episode length: 35.82 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 173      |
|    time_elapsed    | 5445     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=791.26 +/- 656.34
Episode length: 35.02 +/- 6.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 791       |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.71e-14 |
|    explained_variance   | 0.023     |
|    learning_rate        | 0.001     |
|    loss                 | 1.8e+04   |
|    n_updates            | 1526      |
|    policy_gradient_loss | 3.06e-10  |
|    value_loss           | 2.92e+04  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=812.19 +/- 685.55
Episode length: 35.08 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=917.68 +/- 705.91
Episode length: 35.78 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=890.21 +/- 705.47
Episode length: 35.26 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 856      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 174      |
|    time_elapsed    | 5471     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=865.52 +/- 692.89
Episode length: 35.40 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 866       |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-16 |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+04  |
|    n_updates            | 1536      |
|    policy_gradient_loss | -2.36e-10 |
|    value_loss           | 3.01e+04  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=853.03 +/- 716.79
Episode length: 34.82 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=842.28 +/- 654.76
Episode length: 36.24 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=852.72 +/- 656.34
Episode length: 36.08 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 175      |
|    time_elapsed    | 5498     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=756.53 +/- 615.27
Episode length: 35.60 +/- 5.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 757       |
| time/                   |           |
|    total_timesteps      | 358500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.68e-14 |
|    explained_variance   | 0.0195    |
|    learning_rate        | 0.001     |
|    loss                 | 1.05e+04  |
|    n_updates            | 1546      |
|    policy_gradient_loss | 8.41e-10  |
|    value_loss           | 2.85e+04  |
---------------------------------------
Eval num_timesteps=359000, episode_reward=830.14 +/- 651.89
Episode length: 35.54 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=791.27 +/- 712.20
Episode length: 34.38 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=771.24 +/- 679.32
Episode length: 34.50 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 176      |
|    time_elapsed    | 5525     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=806.31 +/- 664.91
Episode length: 35.48 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 806       |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.19e-16 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.001     |
|    loss                 | 9.16e+03  |
|    n_updates            | 1556      |
|    policy_gradient_loss | -3.32e-10 |
|    value_loss           | 2.53e+04  |
---------------------------------------
Eval num_timesteps=361000, episode_reward=912.91 +/- 704.91
Episode length: 36.36 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=937.64 +/- 754.87
Episode length: 35.52 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=755.12 +/- 615.93
Episode length: 35.26 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 177      |
|    time_elapsed    | 5554     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=753.08 +/- 622.77
Episode length: 35.00 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 753       |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.94e-14 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.001     |
|    loss                 | 7.26e+03  |
|    n_updates            | 1566      |
|    policy_gradient_loss | -2.86e-09 |
|    value_loss           | 2.91e+04  |
---------------------------------------
Eval num_timesteps=363000, episode_reward=749.35 +/- 675.27
Episode length: 33.86 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=735.44 +/- 624.57
Episode length: 34.76 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=982.89 +/- 706.71
Episode length: 36.36 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=836.93 +/- 658.56
Episode length: 35.48 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 178      |
|    time_elapsed    | 5600     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=808.33 +/- 722.87
Episode length: 34.26 +/- 7.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 808       |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-16 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.001     |
|    loss                 | 1.51e+04  |
|    n_updates            | 1576      |
|    policy_gradient_loss | -7.97e-10 |
|    value_loss           | 2.22e+04  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=868.56 +/- 718.95
Episode length: 35.48 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=875.47 +/- 704.46
Episode length: 35.66 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=763.62 +/- 600.29
Episode length: 35.68 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 179      |
|    time_elapsed    | 5644     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=754.84 +/- 662.42
Episode length: 34.34 +/- 7.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.72e-13 |
|    explained_variance   | 0.0238    |
|    learning_rate        | 0.001     |
|    loss                 | 8.49e+03  |
|    n_updates            | 1586      |
|    policy_gradient_loss | -7.92e-10 |
|    value_loss           | 2.86e+04  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=996.16 +/- 738.39
Episode length: 36.80 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=800.04 +/- 721.19
Episode length: 34.08 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=882.36 +/- 669.44
Episode length: 35.94 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 64       |
|    iterations      | 180      |
|    time_elapsed    | 5675     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=692.23 +/- 559.42
Episode length: 35.12 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 692       |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.39e-16 |
|    explained_variance   | 0.00964   |
|    learning_rate        | 0.001     |
|    loss                 | 1.14e+04  |
|    n_updates            | 1596      |
|    policy_gradient_loss | 7.8e-10   |
|    value_loss           | 2.03e+04  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=970.81 +/- 749.99
Episode length: 36.22 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=723.55 +/- 589.20
Episode length: 34.98 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=799.26 +/- 675.28
Episode length: 34.44 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 867      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 181      |
|    time_elapsed    | 5701     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=1006.84 +/- 732.88
Episode length: 36.22 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 371000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.53e-13 |
|    explained_variance   | 0.0271    |
|    learning_rate        | 0.001     |
|    loss                 | 2.74e+04  |
|    n_updates            | 1606      |
|    policy_gradient_loss | -8.41e-10 |
|    value_loss           | 2.93e+04  |
---------------------------------------
Eval num_timesteps=371500, episode_reward=705.16 +/- 594.05
Episode length: 34.42 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=914.00 +/- 741.47
Episode length: 34.94 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=838.67 +/- 718.75
Episode length: 34.58 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 182      |
|    time_elapsed    | 5729     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=801.57 +/- 631.35
Episode length: 35.10 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 373000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-15 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.001     |
|    loss                 | 1.04e+04  |
|    n_updates            | 1616      |
|    policy_gradient_loss | 3.59e-10  |
|    value_loss           | 2.47e+04  |
---------------------------------------
Eval num_timesteps=373500, episode_reward=739.08 +/- 659.29
Episode length: 34.22 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=797.03 +/- 652.08
Episode length: 35.34 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=682.22 +/- 576.22
Episode length: 34.26 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 183      |
|    time_elapsed    | 5756     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=754.77 +/- 661.32
Episode length: 34.30 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 375000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.67e-13 |
|    explained_variance   | 0.0277    |
|    learning_rate        | 0.001     |
|    loss                 | 1.56e+04  |
|    n_updates            | 1626      |
|    policy_gradient_loss | -7.19e-10 |
|    value_loss           | 2.79e+04  |
---------------------------------------
Eval num_timesteps=375500, episode_reward=1168.91 +/- 768.83
Episode length: 36.78 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
New best mean reward!
Eval num_timesteps=376000, episode_reward=636.82 +/- 552.08
Episode length: 34.14 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=902.85 +/- 741.50
Episode length: 35.44 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.2     |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    fps             | 65       |
|    iterations      | 184      |
|    time_elapsed    | 5782     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=801.78 +/- 702.18
Episode length: 34.24 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.77e-15 |
|    explained_variance   | 0.0151    |
|    learning_rate        | 0.001     |
|    loss                 | 3.05e+04  |
|    n_updates            | 1636      |
|    policy_gradient_loss | 8.29e-10  |
|    value_loss           | 3.4e+04   |
---------------------------------------
Eval num_timesteps=377500, episode_reward=775.76 +/- 642.06
Episode length: 35.16 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=794.07 +/- 701.65
Episode length: 34.36 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=838.19 +/- 636.62
Episode length: 36.40 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 924      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 185      |
|    time_elapsed    | 5810     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=834.67 +/- 652.10
Episode length: 35.62 +/- 5.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 835       |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.63e-12 |
|    explained_variance   | 0.0218    |
|    learning_rate        | 0.001     |
|    loss                 | 8.73e+03  |
|    n_updates            | 1646      |
|    policy_gradient_loss | 8.15e-11  |
|    value_loss           | 2.81e+04  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=775.04 +/- 627.05
Episode length: 34.82 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=982.05 +/- 726.74
Episode length: 36.80 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=753.39 +/- 648.35
Episode length: 34.80 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 186      |
|    time_elapsed    | 5840     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=942.65 +/- 698.32
Episode length: 36.84 +/- 5.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 943       |
| time/                   |           |
|    total_timesteps      | 381000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.84e-15 |
|    explained_variance   | 0.0126    |
|    learning_rate        | 0.001     |
|    loss                 | 4.34e+03  |
|    n_updates            | 1656      |
|    policy_gradient_loss | 8.79e-10  |
|    value_loss           | 2.33e+04  |
---------------------------------------
Eval num_timesteps=381500, episode_reward=827.95 +/- 671.72
Episode length: 35.42 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=854.82 +/- 720.94
Episode length: 35.34 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=832.59 +/- 690.23
Episode length: 35.36 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 187      |
|    time_elapsed    | 5868     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=795.77 +/- 680.92
Episode length: 34.78 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 796       |
| time/                   |           |
|    total_timesteps      | 383000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.17e-12 |
|    explained_variance   | 0.0192    |
|    learning_rate        | 0.001     |
|    loss                 | 1.47e+04  |
|    n_updates            | 1666      |
|    policy_gradient_loss | 3.86e-10  |
|    value_loss           | 2.61e+04  |
---------------------------------------
Eval num_timesteps=383500, episode_reward=935.74 +/- 738.59
Episode length: 36.16 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=759.47 +/- 637.41
Episode length: 34.18 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=870.92 +/- 776.66
Episode length: 34.60 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=781.05 +/- 632.44
Episode length: 35.38 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 933      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 188      |
|    time_elapsed    | 5900     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=903.53 +/- 682.60
Episode length: 36.62 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 904       |
| time/                   |           |
|    total_timesteps      | 385500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-14 |
|    explained_variance   | 0.0153    |
|    learning_rate        | 0.001     |
|    loss                 | 1.31e+04  |
|    n_updates            | 1676      |
|    policy_gradient_loss | -5.94e-10 |
|    value_loss           | 3.43e+04  |
---------------------------------------
Eval num_timesteps=386000, episode_reward=808.36 +/- 676.71
Episode length: 34.76 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=833.76 +/- 737.61
Episode length: 34.68 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=843.26 +/- 738.65
Episode length: 34.52 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 189      |
|    time_elapsed    | 5927     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=702.30 +/- 619.10
Episode length: 34.36 +/- 7.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 702       |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.59e-12 |
|    explained_variance   | 0.0216    |
|    learning_rate        | 0.001     |
|    loss                 | 1.85e+04  |
|    n_updates            | 1686      |
|    policy_gradient_loss | 5.85e-10  |
|    value_loss           | 2.69e+04  |
---------------------------------------
Eval num_timesteps=388000, episode_reward=726.81 +/- 607.81
Episode length: 34.98 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=840.34 +/- 653.86
Episode length: 36.22 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=800.72 +/- 714.70
Episode length: 34.44 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 190      |
|    time_elapsed    | 5953     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=781.55 +/- 669.72
Episode length: 34.72 +/- 7.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 389500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.72e-14 |
|    explained_variance   | 0.00852   |
|    learning_rate        | 0.001     |
|    loss                 | 8.55e+03  |
|    n_updates            | 1696      |
|    policy_gradient_loss | 8.29e-10  |
|    value_loss           | 1.96e+04  |
---------------------------------------
Eval num_timesteps=390000, episode_reward=1042.53 +/- 678.88
Episode length: 37.66 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.7     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=842.33 +/- 672.38
Episode length: 35.48 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=676.71 +/- 563.24
Episode length: 34.68 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 191      |
|    time_elapsed    | 5980     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=821.15 +/- 706.44
Episode length: 34.74 +/- 6.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 821       |
| time/                   |           |
|    total_timesteps      | 391500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.79e-12 |
|    explained_variance   | 0.0188    |
|    learning_rate        | 0.001     |
|    loss                 | 1.52e+04  |
|    n_updates            | 1706      |
|    policy_gradient_loss | -6.23e-10 |
|    value_loss           | 2.56e+04  |
---------------------------------------
Eval num_timesteps=392000, episode_reward=874.45 +/- 772.56
Episode length: 34.26 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=631.64 +/- 579.40
Episode length: 33.96 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=668.91 +/- 591.54
Episode length: 34.38 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 879      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 192      |
|    time_elapsed    | 6007     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=694.76 +/- 629.00
Episode length: 34.10 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 695       |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.89e-14 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.001     |
|    loss                 | 2.54e+04  |
|    n_updates            | 1716      |
|    policy_gradient_loss | -7.57e-10 |
|    value_loss           | 3.04e+04  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=853.86 +/- 722.14
Episode length: 35.16 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=846.47 +/- 694.00
Episode length: 35.56 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=797.80 +/- 634.38
Episode length: 35.30 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 193      |
|    time_elapsed    | 6034     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=903.82 +/- 710.38
Episode length: 35.94 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 904       |
| time/                   |           |
|    total_timesteps      | 395500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.97e-11 |
|    explained_variance   | 0.0234    |
|    learning_rate        | 0.001     |
|    loss                 | 1.61e+04  |
|    n_updates            | 1726      |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 2.91e+04  |
---------------------------------------
Eval num_timesteps=396000, episode_reward=882.10 +/- 703.01
Episode length: 35.38 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=883.80 +/- 692.60
Episode length: 36.04 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=634.35 +/- 544.51
Episode length: 33.50 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 634      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 194      |
|    time_elapsed    | 6061     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=839.08 +/- 690.02
Episode length: 35.60 +/- 5.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 397500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.53e-14 |
|    explained_variance   | 0.0131    |
|    learning_rate        | 0.001     |
|    loss                 | 9.05e+03  |
|    n_updates            | 1736      |
|    policy_gradient_loss | 5.46e-10  |
|    value_loss           | 2.73e+04  |
---------------------------------------
Eval num_timesteps=398000, episode_reward=878.23 +/- 711.94
Episode length: 35.82 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=923.11 +/- 711.49
Episode length: 36.14 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=847.99 +/- 625.49
Episode length: 36.34 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 195      |
|    time_elapsed    | 6088     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=829.35 +/- 704.58
Episode length: 34.52 +/- 7.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.25e-11 |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.001     |
|    loss                 | 9.84e+03  |
|    n_updates            | 1746      |
|    policy_gradient_loss | 3.16e-10  |
|    value_loss           | 2.35e+04  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=1017.53 +/- 731.51
Episode length: 36.52 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=882.59 +/- 726.44
Episode length: 35.70 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=933.45 +/- 741.86
Episode length: 36.00 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 196      |
|    time_elapsed    | 6113     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=927.86 +/- 704.13
Episode length: 36.44 +/- 5.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 928       |
| time/                   |           |
|    total_timesteps      | 401500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.38e-13 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.001     |
|    loss                 | 1.09e+04  |
|    n_updates            | 1756      |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 2.88e+04  |
---------------------------------------
Eval num_timesteps=402000, episode_reward=685.03 +/- 600.63
Episode length: 34.00 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=908.48 +/- 716.18
Episode length: 35.82 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=963.84 +/- 731.14
Episode length: 35.82 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 197      |
|    time_elapsed    | 6138     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=825.90 +/- 666.86
Episode length: 35.12 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 403500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.29e-11 |
|    explained_variance   | 0.0209    |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+04  |
|    n_updates            | 1766      |
|    policy_gradient_loss | 5.91e-10  |
|    value_loss           | 2.23e+04  |
---------------------------------------
Eval num_timesteps=404000, episode_reward=830.13 +/- 704.26
Episode length: 35.20 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=1030.06 +/- 730.85
Episode length: 36.72 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=910.15 +/- 698.21
Episode length: 36.28 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=873.35 +/- 670.25
Episode length: 35.22 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 198      |
|    time_elapsed    | 6175     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=817.19 +/- 654.67
Episode length: 35.42 +/- 6.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 406000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.48e-13 |
|    explained_variance   | 0.00966   |
|    learning_rate        | 0.001     |
|    loss                 | 9.35e+03  |
|    n_updates            | 1776      |
|    policy_gradient_loss | 4.86e-10  |
|    value_loss           | 2.41e+04  |
---------------------------------------
Eval num_timesteps=406500, episode_reward=762.28 +/- 658.86
Episode length: 33.88 +/- 7.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=853.19 +/- 669.63
Episode length: 35.54 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=871.84 +/- 682.06
Episode length: 36.08 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 844      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 199      |
|    time_elapsed    | 6207     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=780.08 +/- 612.19
Episode length: 35.28 +/- 5.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 408000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.25e-11 |
|    explained_variance   | 0.0217    |
|    learning_rate        | 0.001     |
|    loss                 | 1.64e+04  |
|    n_updates            | 1786      |
|    policy_gradient_loss | -6.81e-10 |
|    value_loss           | 2.54e+04  |
---------------------------------------
Eval num_timesteps=408500, episode_reward=755.97 +/- 665.44
Episode length: 34.38 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=836.82 +/- 690.14
Episode length: 35.54 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=823.21 +/- 714.20
Episode length: 34.14 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 200      |
|    time_elapsed    | 6233     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=863.39 +/- 729.29
Episode length: 35.72 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 863       |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.54e-13 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 5.27e+03  |
|    n_updates            | 1796      |
|    policy_gradient_loss | 7.97e-10  |
|    value_loss           | 2.56e+04  |
---------------------------------------
Eval num_timesteps=410500, episode_reward=998.13 +/- 756.08
Episode length: 35.90 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=861.89 +/- 635.82
Episode length: 36.08 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=734.70 +/- 578.48
Episode length: 35.30 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 201      |
|    time_elapsed    | 6265     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=852.34 +/- 661.58
Episode length: 35.86 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 412000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-10 |
|    explained_variance   | 0.0276    |
|    learning_rate        | 0.001     |
|    loss                 | 1.12e+04  |
|    n_updates            | 1806      |
|    policy_gradient_loss | -3.73e-10 |
|    value_loss           | 2.82e+04  |
---------------------------------------
Eval num_timesteps=412500, episode_reward=771.03 +/- 659.89
Episode length: 34.70 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=775.04 +/- 638.74
Episode length: 35.08 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=729.52 +/- 652.48
Episode length: 34.36 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 923      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 202      |
|    time_elapsed    | 6306     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=973.32 +/- 733.26
Episode length: 36.44 +/- 7.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 973       |
| time/                   |           |
|    total_timesteps      | 414000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.84e-13 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.001     |
|    loss                 | 1.95e+04  |
|    n_updates            | 1816      |
|    policy_gradient_loss | 3.49e-11  |
|    value_loss           | 2.95e+04  |
---------------------------------------
Eval num_timesteps=414500, episode_reward=770.42 +/- 646.79
Episode length: 34.48 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=712.61 +/- 611.24
Episode length: 33.86 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=806.11 +/- 672.02
Episode length: 34.72 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 203      |
|    time_elapsed    | 6342     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=922.22 +/- 733.95
Episode length: 35.50 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 922       |
| time/                   |           |
|    total_timesteps      | 416000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.92e-10 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.001     |
|    loss                 | 1.51e+04  |
|    n_updates            | 1826      |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 2.47e+04  |
---------------------------------------
Eval num_timesteps=416500, episode_reward=951.39 +/- 720.91
Episode length: 36.28 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=792.07 +/- 637.71
Episode length: 35.72 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=940.82 +/- 708.02
Episode length: 36.32 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 204      |
|    time_elapsed    | 6369     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=687.67 +/- 620.47
Episode length: 33.94 +/- 7.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 688       |
| time/                   |           |
|    total_timesteps      | 418000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.25e-12 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.001     |
|    loss                 | 1.26e+04  |
|    n_updates            | 1836      |
|    policy_gradient_loss | 1.66e-09  |
|    value_loss           | 3.4e+04   |
---------------------------------------
Eval num_timesteps=418500, episode_reward=973.06 +/- 769.93
Episode length: 35.62 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=834.38 +/- 703.28
Episode length: 34.92 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=869.59 +/- 706.93
Episode length: 35.04 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 205      |
|    time_elapsed    | 6394     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=930.81 +/- 717.00
Episode length: 35.88 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 931       |
| time/                   |           |
|    total_timesteps      | 420000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.93e-10 |
|    explained_variance   | 0.0151    |
|    learning_rate        | 0.001     |
|    loss                 | 1.34e+04  |
|    n_updates            | 1846      |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 2.44e+04  |
---------------------------------------
Eval num_timesteps=420500, episode_reward=810.97 +/- 570.50
Episode length: 36.64 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=935.75 +/- 725.66
Episode length: 35.82 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=887.17 +/- 701.28
Episode length: 36.24 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 206      |
|    time_elapsed    | 6420     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=993.21 +/- 725.88
Episode length: 36.50 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 993       |
| time/                   |           |
|    total_timesteps      | 422000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-12 |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.001     |
|    loss                 | 2.6e+04   |
|    n_updates            | 1856      |
|    policy_gradient_loss | -3.54e-10 |
|    value_loss           | 3.4e+04   |
---------------------------------------
Eval num_timesteps=422500, episode_reward=1077.11 +/- 734.61
Episode length: 36.84 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=868.22 +/- 682.60
Episode length: 35.56 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=907.79 +/- 757.30
Episode length: 34.68 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 932      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 207      |
|    time_elapsed    | 6465     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=784.41 +/- 663.43
Episode length: 34.64 +/- 7.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 424000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.95e-10 |
|    explained_variance   | 0.0219    |
|    learning_rate        | 0.001     |
|    loss                 | 1.09e+04  |
|    n_updates            | 1866      |
|    policy_gradient_loss | -1.03e-09 |
|    value_loss           | 2.4e+04   |
---------------------------------------
Eval num_timesteps=424500, episode_reward=766.35 +/- 675.03
Episode length: 33.92 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=786.15 +/- 661.95
Episode length: 34.78 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=806.59 +/- 700.35
Episode length: 33.88 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 208      |
|    time_elapsed    | 6498     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=927.24 +/- 744.61
Episode length: 35.10 +/- 7.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 426000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-12 |
|    explained_variance   | 0.0113    |
|    learning_rate        | 0.001     |
|    loss                 | 1.13e+04  |
|    n_updates            | 1876      |
|    policy_gradient_loss | 9.9e-11   |
|    value_loss           | 2.62e+04  |
---------------------------------------
Eval num_timesteps=426500, episode_reward=853.81 +/- 709.89
Episode length: 35.32 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=861.60 +/- 725.80
Episode length: 34.88 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=907.21 +/- 719.78
Episode length: 35.76 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=769.55 +/- 662.42
Episode length: 34.54 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 722      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 209      |
|    time_elapsed    | 6536     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=1063.02 +/- 741.59
Episode length: 37.50 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.5      |
|    mean_reward          | 1.06e+03  |
| time/                   |           |
|    total_timesteps      | 428500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.39e-10 |
|    explained_variance   | 0.0161    |
|    learning_rate        | 0.001     |
|    loss                 | 7.86e+03  |
|    n_updates            | 1886      |
|    policy_gradient_loss | 3.91e-10  |
|    value_loss           | 2.31e+04  |
---------------------------------------
Eval num_timesteps=429000, episode_reward=828.62 +/- 681.40
Episode length: 35.76 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=988.51 +/- 738.71
Episode length: 36.06 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=668.42 +/- 589.28
Episode length: 34.36 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 210      |
|    time_elapsed    | 6564     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=723.88 +/- 626.36
Episode length: 34.56 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 430500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.47e-12 |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.001     |
|    loss                 | 1.11e+04  |
|    n_updates            | 1896      |
|    policy_gradient_loss | -5.09e-10 |
|    value_loss           | 2.99e+04  |
---------------------------------------
Eval num_timesteps=431000, episode_reward=674.74 +/- 591.13
Episode length: 34.36 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=829.82 +/- 730.86
Episode length: 34.04 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=812.88 +/- 657.06
Episode length: 35.08 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 960      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 211      |
|    time_elapsed    | 6590     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=948.74 +/- 709.59
Episode length: 36.38 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 949       |
| time/                   |           |
|    total_timesteps      | 432500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.86e-20 |
|    explained_variance   | 0.0199    |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+04  |
|    n_updates            | 1906      |
|    policy_gradient_loss | 2.09e-09  |
|    value_loss           | 2.71e+04  |
---------------------------------------
Eval num_timesteps=433000, episode_reward=957.65 +/- 699.49
Episode length: 37.04 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=828.56 +/- 672.52
Episode length: 35.46 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=779.02 +/- 609.68
Episode length: 35.34 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 898      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 212      |
|    time_elapsed    | 6616     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=748.41 +/- 649.34
Episode length: 34.76 +/- 7.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 748       |
| time/                   |           |
|    total_timesteps      | 434500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.8e-17  |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.001     |
|    loss                 | 1.99e+04  |
|    n_updates            | 1916      |
|    policy_gradient_loss | -2.43e-09 |
|    value_loss           | 4.01e+04  |
---------------------------------------
Eval num_timesteps=435000, episode_reward=962.53 +/- 754.11
Episode length: 36.52 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=800.84 +/- 659.60
Episode length: 35.56 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=824.79 +/- 673.77
Episode length: 35.06 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 213      |
|    time_elapsed    | 6643     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=892.21 +/- 737.39
Episode length: 35.44 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 892       |
| time/                   |           |
|    total_timesteps      | 436500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.43e-10 |
|    explained_variance   | 0.018     |
|    learning_rate        | 0.001     |
|    loss                 | 1.21e+04  |
|    n_updates            | 1926      |
|    policy_gradient_loss | -3.67e-10 |
|    value_loss           | 2.17e+04  |
---------------------------------------
Eval num_timesteps=437000, episode_reward=816.82 +/- 656.27
Episode length: 35.26 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=894.19 +/- 729.25
Episode length: 35.18 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=958.90 +/- 725.42
Episode length: 36.50 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 214      |
|    time_elapsed    | 6673     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=896.30 +/- 716.84
Episode length: 35.60 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 896       |
| time/                   |           |
|    total_timesteps      | 438500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.65e-12 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.001     |
|    loss                 | 1.51e+04  |
|    n_updates            | 1936      |
|    policy_gradient_loss | 6.14e-10  |
|    value_loss           | 2.51e+04  |
---------------------------------------
Eval num_timesteps=439000, episode_reward=922.56 +/- 717.23
Episode length: 35.92 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=695.20 +/- 643.12
Episode length: 33.76 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=903.54 +/- 727.67
Episode length: 35.54 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 215      |
|    time_elapsed    | 6705     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=849.86 +/- 736.49
Episode length: 34.72 +/- 6.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 850       |
| time/                   |           |
|    total_timesteps      | 440500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-09 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.001     |
|    loss                 | 1.51e+04  |
|    n_updates            | 1946      |
|    policy_gradient_loss | -3.62e-10 |
|    value_loss           | 2.11e+04  |
---------------------------------------
Eval num_timesteps=441000, episode_reward=959.05 +/- 757.22
Episode length: 35.46 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=937.49 +/- 722.95
Episode length: 36.10 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=759.15 +/- 663.89
Episode length: 34.24 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 809      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 216      |
|    time_elapsed    | 6733     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=823.94 +/- 654.83
Episode length: 35.06 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 824       |
| time/                   |           |
|    total_timesteps      | 442500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.66e-12 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+04  |
|    n_updates            | 1956      |
|    policy_gradient_loss | -9.63e-10 |
|    value_loss           | 2.7e+04   |
---------------------------------------
Eval num_timesteps=443000, episode_reward=920.20 +/- 721.98
Episode length: 35.38 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=947.18 +/- 734.38
Episode length: 35.80 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=724.16 +/- 655.29
Episode length: 33.96 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 217      |
|    time_elapsed    | 6760     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=727.61 +/- 668.32
Episode length: 34.50 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 728       |
| time/                   |           |
|    total_timesteps      | 444500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.93e-19 |
|    explained_variance   | 0.0203    |
|    learning_rate        | 0.001     |
|    loss                 | 1.04e+04  |
|    n_updates            | 1966      |
|    policy_gradient_loss | 1.48e-09  |
|    value_loss           | 2.36e+04  |
---------------------------------------
Eval num_timesteps=445000, episode_reward=739.54 +/- 683.44
Episode length: 34.20 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=756.10 +/- 675.82
Episode length: 33.88 +/- 7.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=1072.42 +/- 744.58
Episode length: 37.24 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 218      |
|    time_elapsed    | 6785     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=792.87 +/- 647.91
Episode length: 35.08 +/- 6.21
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.1     |
|    mean_reward          | 793      |
| time/                   |          |
|    total_timesteps      | 446500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.2e-16 |
|    explained_variance   | 0.0228   |
|    learning_rate        | 0.001    |
|    loss                 | 2.27e+04 |
|    n_updates            | 1976     |
|    policy_gradient_loss | 1.19e-10 |
|    value_loss           | 3.72e+04 |
--------------------------------------
Eval num_timesteps=447000, episode_reward=839.51 +/- 728.17
Episode length: 34.28 +/- 7.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=861.74 +/- 746.39
Episode length: 33.90 +/- 8.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=788.59 +/- 609.22
Episode length: 35.88 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=829.03 +/- 685.16
Episode length: 35.92 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 884      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 219      |
|    time_elapsed    | 6815     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=955.38 +/- 756.00
Episode length: 35.26 +/- 7.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 955       |
| time/                   |           |
|    total_timesteps      | 449000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.07e-19 |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.001     |
|    loss                 | 1.6e+04   |
|    n_updates            | 1986      |
|    policy_gradient_loss | 3e-10     |
|    value_loss           | 2.71e+04  |
---------------------------------------
Eval num_timesteps=449500, episode_reward=917.46 +/- 734.57
Episode length: 35.72 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=806.09 +/- 695.62
Episode length: 34.40 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=851.72 +/- 674.61
Episode length: 35.80 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 220      |
|    time_elapsed    | 6840     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=795.51 +/- 628.36
Episode length: 35.28 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 796       |
| time/                   |           |
|    total_timesteps      | 451000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.27e-16 |
|    explained_variance   | 0.0239    |
|    learning_rate        | 0.001     |
|    loss                 | 1.98e+04  |
|    n_updates            | 1996      |
|    policy_gradient_loss | -7.28e-11 |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=451500, episode_reward=890.64 +/- 645.36
Episode length: 36.74 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=684.10 +/- 579.38
Episode length: 34.08 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=883.56 +/- 694.15
Episode length: 36.20 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 221      |
|    time_elapsed    | 6865     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=1074.64 +/- 731.00
Episode length: 37.54 +/- 5.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.5      |
|    mean_reward          | 1.07e+03  |
| time/                   |           |
|    total_timesteps      | 453000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.83e-18 |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.001     |
|    loss                 | 3.07e+03  |
|    n_updates            | 2006      |
|    policy_gradient_loss | 1.38e-11  |
|    value_loss           | 2.46e+04  |
---------------------------------------
Eval num_timesteps=453500, episode_reward=867.55 +/- 703.89
Episode length: 35.34 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=809.41 +/- 637.55
Episode length: 35.94 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=833.89 +/- 667.01
Episode length: 35.32 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 65       |
|    iterations      | 222      |
|    time_elapsed    | 6891     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=824.15 +/- 659.82
Episode length: 35.52 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 824       |
| time/                   |           |
|    total_timesteps      | 455000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.44e-16 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.001     |
|    loss                 | 1.61e+04  |
|    n_updates            | 2016      |
|    policy_gradient_loss | -8.28e-10 |
|    value_loss           | 3.57e+04  |
---------------------------------------
Eval num_timesteps=455500, episode_reward=762.09 +/- 629.17
Episode length: 35.36 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=825.44 +/- 629.30
Episode length: 35.96 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=712.70 +/- 633.74
Episode length: 34.14 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 223      |
|    time_elapsed    | 6916     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=849.06 +/- 744.34
Episode length: 34.08 +/- 7.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 849       |
| time/                   |           |
|    total_timesteps      | 457000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.56e-18 |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.001     |
|    loss                 | 1.43e+04  |
|    n_updates            | 2026      |
|    policy_gradient_loss | 3.64e-11  |
|    value_loss           | 3e+04     |
---------------------------------------
Eval num_timesteps=457500, episode_reward=867.29 +/- 698.49
Episode length: 36.26 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=965.30 +/- 691.61
Episode length: 36.56 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=843.57 +/- 673.04
Episode length: 35.52 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 224      |
|    time_elapsed    | 6942     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=789.20 +/- 611.93
Episode length: 36.36 +/- 5.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 459000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.48e-16 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+04  |
|    n_updates            | 2036      |
|    policy_gradient_loss | -2.89e-09 |
|    value_loss           | 3.29e+04  |
---------------------------------------
Eval num_timesteps=459500, episode_reward=814.82 +/- 695.19
Episode length: 34.74 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=889.11 +/- 727.24
Episode length: 35.34 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=823.39 +/- 673.44
Episode length: 35.62 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 225      |
|    time_elapsed    | 6968     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=784.68 +/- 643.63
Episode length: 35.36 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 461000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.47e-18 |
|    explained_variance   | 0.0218    |
|    learning_rate        | 0.001     |
|    loss                 | 1.4e+04   |
|    n_updates            | 2046      |
|    policy_gradient_loss | 3.03e-10  |
|    value_loss           | 2.77e+04  |
---------------------------------------
Eval num_timesteps=461500, episode_reward=830.33 +/- 643.45
Episode length: 35.90 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=960.39 +/- 711.35
Episode length: 36.56 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=802.32 +/- 664.79
Episode length: 35.32 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 963      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 226      |
|    time_elapsed    | 6994     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=705.30 +/- 604.27
Episode length: 34.70 +/- 7.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 705       |
| time/                   |           |
|    total_timesteps      | 463000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-15 |
|    explained_variance   | 0.0246    |
|    learning_rate        | 0.001     |
|    loss                 | 1.47e+04  |
|    n_updates            | 2056      |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 3.18e+04  |
---------------------------------------
Eval num_timesteps=463500, episode_reward=779.65 +/- 682.18
Episode length: 34.66 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=763.30 +/- 631.24
Episode length: 34.36 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=676.98 +/- 561.31
Episode length: 34.26 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 227      |
|    time_elapsed    | 7020     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=743.26 +/- 588.74
Episode length: 35.28 +/- 6.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 465000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.18e-09 |
|    explained_variance   | 0.0126    |
|    learning_rate        | 0.001     |
|    loss                 | 1.24e+04  |
|    n_updates            | 2066      |
|    policy_gradient_loss | 9.17e-11  |
|    value_loss           | 2.1e+04   |
---------------------------------------
Eval num_timesteps=465500, episode_reward=781.19 +/- 625.52
Episode length: 35.58 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=879.76 +/- 665.69
Episode length: 36.36 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=839.25 +/- 672.17
Episode length: 35.38 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 913      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 228      |
|    time_elapsed    | 7046     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=841.48 +/- 707.89
Episode length: 34.94 +/- 7.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 467000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16e-11 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.001     |
|    loss                 | 2.32e+04  |
|    n_updates            | 2076      |
|    policy_gradient_loss | 5.68e-10  |
|    value_loss           | 3.28e+04  |
---------------------------------------
Eval num_timesteps=467500, episode_reward=974.29 +/- 694.65
Episode length: 37.18 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=868.31 +/- 727.70
Episode length: 34.46 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=848.22 +/- 758.29
Episode length: 34.00 +/- 7.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 229      |
|    time_elapsed    | 7071     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=1086.82 +/- 766.79
Episode length: 36.98 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 1.09e+03  |
| time/                   |           |
|    total_timesteps      | 469000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.36e-18 |
|    explained_variance   | 0.0174    |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+04  |
|    n_updates            | 2086      |
|    policy_gradient_loss | -4.83e-10 |
|    value_loss           | 2.36e+04  |
---------------------------------------
Eval num_timesteps=469500, episode_reward=758.06 +/- 672.08
Episode length: 34.42 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=777.62 +/- 585.09
Episode length: 35.82 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=823.49 +/- 668.73
Episode length: 35.84 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=976.62 +/- 725.09
Episode length: 37.00 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 977      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 907      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 230      |
|    time_elapsed    | 7102     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=830.65 +/- 703.53
Episode length: 34.68 +/- 6.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 471500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.08e-15 |
|    explained_variance   | 0.0294    |
|    learning_rate        | 0.001     |
|    loss                 | 1.1e+04   |
|    n_updates            | 2096      |
|    policy_gradient_loss | -2.65e-09 |
|    value_loss           | 3.06e+04  |
---------------------------------------
Eval num_timesteps=472000, episode_reward=815.37 +/- 636.46
Episode length: 36.08 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=946.81 +/- 679.90
Episode length: 37.16 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=747.98 +/- 631.93
Episode length: 34.66 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 231      |
|    time_elapsed    | 7127     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=988.56 +/- 745.43
Episode length: 35.92 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 989       |
| time/                   |           |
|    total_timesteps      | 473500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.2e-09  |
|    explained_variance   | 0.00838   |
|    learning_rate        | 0.001     |
|    loss                 | 4.38e+03  |
|    n_updates            | 2106      |
|    policy_gradient_loss | -9.75e-10 |
|    value_loss           | 1.69e+04  |
---------------------------------------
Eval num_timesteps=474000, episode_reward=1046.92 +/- 763.81
Episode length: 36.58 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=829.10 +/- 696.63
Episode length: 34.86 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=873.60 +/- 722.41
Episode length: 35.28 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 232      |
|    time_elapsed    | 7152     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=909.22 +/- 709.86
Episode length: 35.94 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 909       |
| time/                   |           |
|    total_timesteps      | 475500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.82e-11 |
|    explained_variance   | 0.00987   |
|    learning_rate        | 0.001     |
|    loss                 | 2.41e+04  |
|    n_updates            | 2116      |
|    policy_gradient_loss | -4.13e-10 |
|    value_loss           | 2.95e+04  |
---------------------------------------
Eval num_timesteps=476000, episode_reward=769.12 +/- 615.01
Episode length: 35.42 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=726.95 +/- 636.41
Episode length: 34.46 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=755.17 +/- 656.71
Episode length: 33.88 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 899      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 233      |
|    time_elapsed    | 7178     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=688.69 +/- 604.56
Episode length: 34.16 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 689       |
| time/                   |           |
|    total_timesteps      | 477500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.02e-18 |
|    explained_variance   | 0.0209    |
|    learning_rate        | 0.001     |
|    loss                 | 1.27e+04  |
|    n_updates            | 2126      |
|    policy_gradient_loss | -1.04e-09 |
|    value_loss           | 2.98e+04  |
---------------------------------------
Eval num_timesteps=478000, episode_reward=795.28 +/- 685.80
Episode length: 35.30 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=766.78 +/- 572.96
Episode length: 35.82 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=836.05 +/- 724.93
Episode length: 34.90 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 234      |
|    time_elapsed    | 7204     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=768.03 +/- 667.61
Episode length: 34.06 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 479500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.84e-15 |
|    explained_variance   | 0.0179    |
|    learning_rate        | 0.001     |
|    loss                 | 1.54e+04  |
|    n_updates            | 2136      |
|    policy_gradient_loss | 1.75e-11  |
|    value_loss           | 3.24e+04  |
---------------------------------------
Eval num_timesteps=480000, episode_reward=1007.04 +/- 740.17
Episode length: 35.94 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=888.04 +/- 650.27
Episode length: 36.66 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=867.04 +/- 723.05
Episode length: 34.58 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 235      |
|    time_elapsed    | 7229     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=916.40 +/- 696.28
Episode length: 36.12 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 916       |
| time/                   |           |
|    total_timesteps      | 481500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35e-17 |
|    explained_variance   | 0.0179    |
|    learning_rate        | 0.001     |
|    loss                 | 1.55e+04  |
|    n_updates            | 2146      |
|    policy_gradient_loss | -5.85e-10 |
|    value_loss           | 2.68e+04  |
---------------------------------------
Eval num_timesteps=482000, episode_reward=726.35 +/- 595.71
Episode length: 34.90 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=967.69 +/- 743.44
Episode length: 36.26 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=964.37 +/- 758.46
Episode length: 35.70 +/- 8.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 964      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 907      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 236      |
|    time_elapsed    | 7255     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=927.54 +/- 750.63
Episode length: 35.24 +/- 7.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 928       |
| time/                   |           |
|    total_timesteps      | 483500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.41e-15 |
|    explained_variance   | 0.0282    |
|    learning_rate        | 0.001     |
|    loss                 | 1.81e+04  |
|    n_updates            | 2156      |
|    policy_gradient_loss | -3.55e-10 |
|    value_loss           | 3.32e+04  |
---------------------------------------
Eval num_timesteps=484000, episode_reward=769.15 +/- 688.00
Episode length: 34.84 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=728.40 +/- 633.20
Episode length: 34.96 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=769.89 +/- 637.63
Episode length: 35.10 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 237      |
|    time_elapsed    | 7280     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=801.31 +/- 694.15
Episode length: 34.86 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 485500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.06e-17 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.001     |
|    loss                 | 1.37e+04  |
|    n_updates            | 2166      |
|    policy_gradient_loss | 9.95e-10  |
|    value_loss           | 2.26e+04  |
---------------------------------------
Eval num_timesteps=486000, episode_reward=808.20 +/- 614.11
Episode length: 35.70 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=840.83 +/- 682.89
Episode length: 35.68 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=668.87 +/- 610.52
Episode length: 33.86 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 238      |
|    time_elapsed    | 7305     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=947.02 +/- 692.79
Episode length: 36.80 +/- 5.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 947       |
| time/                   |           |
|    total_timesteps      | 487500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.49e-14 |
|    explained_variance   | 0.0154    |
|    learning_rate        | 0.001     |
|    loss                 | 2.6e+04   |
|    n_updates            | 2176      |
|    policy_gradient_loss | -5.21e-10 |
|    value_loss           | 3.15e+04  |
---------------------------------------
Eval num_timesteps=488000, episode_reward=880.16 +/- 709.41
Episode length: 35.76 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=752.67 +/- 661.32
Episode length: 34.42 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=849.55 +/- 682.11
Episode length: 35.18 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 239      |
|    time_elapsed    | 7330     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=842.00 +/- 710.43
Episode length: 34.72 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 842       |
| time/                   |           |
|    total_timesteps      | 489500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.21e-17 |
|    explained_variance   | 0.0138    |
|    learning_rate        | 0.001     |
|    loss                 | 1.4e+04   |
|    n_updates            | 2186      |
|    policy_gradient_loss | -6.98e-11 |
|    value_loss           | 2.23e+04  |
---------------------------------------
Eval num_timesteps=490000, episode_reward=901.51 +/- 689.00
Episode length: 36.18 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=903.31 +/- 666.17
Episode length: 36.84 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=832.88 +/- 684.23
Episode length: 34.92 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=790.58 +/- 641.79
Episode length: 35.26 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 240      |
|    time_elapsed    | 7360     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=853.03 +/- 750.01
Episode length: 34.56 +/- 7.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 492000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.19e-14 |
|    explained_variance   | 0.0223    |
|    learning_rate        | 0.001     |
|    loss                 | 1.45e+04  |
|    n_updates            | 2196      |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 3.14e+04  |
---------------------------------------
Eval num_timesteps=492500, episode_reward=902.84 +/- 699.58
Episode length: 36.06 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=822.45 +/- 700.48
Episode length: 35.16 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=716.16 +/- 614.47
Episode length: 33.82 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 241      |
|    time_elapsed    | 7385     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=874.92 +/- 671.37
Episode length: 36.12 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 494000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.61e-08 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.001     |
|    loss                 | 8.72e+03  |
|    n_updates            | 2206      |
|    policy_gradient_loss | 1.73e-09  |
|    value_loss           | 2.39e+04  |
---------------------------------------
Eval num_timesteps=494500, episode_reward=937.33 +/- 710.75
Episode length: 35.94 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=833.88 +/- 691.96
Episode length: 35.30 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=787.33 +/- 577.59
Episode length: 36.56 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 242      |
|    time_elapsed    | 7410     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=874.87 +/- 700.41
Episode length: 35.90 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 496000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-10 |
|    explained_variance   | 0.00815   |
|    learning_rate        | 0.001     |
|    loss                 | 1.06e+04  |
|    n_updates            | 2216      |
|    policy_gradient_loss | 9.75e-11  |
|    value_loss           | 2.32e+04  |
---------------------------------------
Eval num_timesteps=496500, episode_reward=796.73 +/- 682.18
Episode length: 34.76 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=807.78 +/- 663.29
Episode length: 35.08 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=864.07 +/- 718.40
Episode length: 34.96 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 243      |
|    time_elapsed    | 7436     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=839.20 +/- 710.44
Episode length: 34.82 +/- 7.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.8     |
|    mean_reward          | 839      |
| time/                   |          |
|    total_timesteps      | 498000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.6e-08 |
|    explained_variance   | 0.014    |
|    learning_rate        | 0.001    |
|    loss                 | 4.82e+03 |
|    n_updates            | 2226     |
|    policy_gradient_loss | 3.84e-10 |
|    value_loss           | 2.23e+04 |
--------------------------------------
Eval num_timesteps=498500, episode_reward=822.01 +/- 692.01
Episode length: 34.74 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=970.48 +/- 705.05
Episode length: 36.74 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=791.03 +/- 707.35
Episode length: 34.30 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 244      |
|    time_elapsed    | 7461     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=761.84 +/- 669.13
Episode length: 34.28 +/- 7.14
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.3     |
|    mean_reward          | 762      |
| time/                   |          |
|    total_timesteps      | 500000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.5e-10 |
|    explained_variance   | 0.011    |
|    learning_rate        | 0.001    |
|    loss                 | 1.2e+04  |
|    n_updates            | 2236     |
|    policy_gradient_loss | 5.05e-10 |
|    value_loss           | 2.8e+04  |
--------------------------------------
Eval num_timesteps=500500, episode_reward=820.18 +/- 644.67
Episode length: 35.74 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=906.01 +/- 717.42
Episode length: 35.76 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=975.08 +/- 729.18
Episode length: 36.04 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 66       |
|    iterations      | 245      |
|    time_elapsed    | 7489     |
|    total_timesteps | 501760   |
---------------------------------
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-stop-2-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
