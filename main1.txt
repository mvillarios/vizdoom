/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-516.14 +/- 79.29
Episode length: 48.86 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-517.33 +/- 62.89
Episode length: 49.64 +/- 12.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | -390     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=918.92 +/- 773.63
Episode length: 35.04 +/- 7.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 919         |
| time/                   |             |
|    total_timesteps      | 1500        |
| train/                  |             |
|    approx_kl            | 0.009981565 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 6.74e-05    |
|    learning_rate        | 0.0005      |
|    loss                 | 2.84e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 6.63e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=1001.59 +/- 736.10
Episode length: 36.20 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | -361     |
| time/              |          |
|    fps             | 259      |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-16.42 +/- 126.49
Episode length: 30.90 +/- 7.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.9        |
|    mean_reward          | -16.4       |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.013643275 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.000708    |
|    learning_rate        | 0.0005      |
|    loss                 | 2.63e+03    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 7.12e+03    |
-----------------------------------------
Eval num_timesteps=3000, episode_reward=4.32 +/- 157.33
Episode length: 33.86 +/- 11.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 4.32     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | -348     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 3        |
|    time_elapsed    | 11       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-164.24 +/- 207.43
Episode length: 59.10 +/- 29.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.1        |
|    mean_reward          | -164        |
| time/                   |             |
|    total_timesteps      | 3500        |
| train/                  |             |
|    approx_kl            | 0.016920388 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.000831    |
|    learning_rate        | 0.0005      |
|    loss                 | 2.34e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0311     |
|    value_loss           | 5.57e+03    |
-----------------------------------------
Eval num_timesteps=4000, episode_reward=-149.88 +/- 153.14
Episode length: 46.70 +/- 21.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | -309     |
| time/              |          |
|    fps             | 260      |
|    iterations      | 4        |
|    time_elapsed    | 15       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-309.38 +/- 155.40
Episode length: 70.36 +/- 53.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 70.4       |
|    mean_reward          | -309       |
| time/                   |            |
|    total_timesteps      | 4500       |
| train/                  |            |
|    approx_kl            | 0.03632569 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.99      |
|    explained_variance   | 0.000208   |
|    learning_rate        | 0.0005     |
|    loss                 | 4.26e+03   |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0292    |
|    value_loss           | 9.22e+03   |
----------------------------------------
Eval num_timesteps=5000, episode_reward=-279.28 +/- 191.80
Episode length: 81.10 +/- 58.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.1     |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.4     |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 5120     |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.06
Eval num_timesteps=5500, episode_reward=-105.49 +/- 161.59
Episode length: 33.32 +/- 11.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.3        |
|    mean_reward          | -105        |
| time/                   |             |
|    total_timesteps      | 5500        |
| train/                  |             |
|    approx_kl            | 0.032383423 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -0.00174    |
|    learning_rate        | 0.0005      |
|    loss                 | 5.79e+03    |
|    n_updates            | 49          |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 1.02e+04    |
-----------------------------------------
Eval num_timesteps=6000, episode_reward=-77.05 +/- 168.59
Episode length: 34.92 +/- 11.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | -77.1    |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.9     |
|    ep_rew_mean     | -287     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 6        |
|    time_elapsed    | 25       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=6500, episode_reward=4.69 +/- 153.14
Episode length: 31.16 +/- 7.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.2        |
|    mean_reward          | 4.69        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.026198322 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.00014     |
|    learning_rate        | 0.0005      |
|    loss                 | 7.59e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.0179      |
|    value_loss           | 1.21e+04    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=-28.12 +/- 132.13
Episode length: 29.36 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.4     |
|    mean_reward     | -28.1    |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.3     |
|    ep_rew_mean     | -250     |
| time/              |          |
|    fps             | 253      |
|    iterations      | 7        |
|    time_elapsed    | 28       |
|    total_timesteps | 7168     |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=7500, episode_reward=-4.68 +/- 147.43
Episode length: 29.26 +/- 7.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.3       |
|    mean_reward          | -4.68      |
| time/                   |            |
|    total_timesteps      | 7500       |
| train/                  |            |
|    approx_kl            | 0.06872773 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | 0.00031    |
|    learning_rate        | 0.0005     |
|    loss                 | 4.76e+03   |
|    n_updates            | 53         |
|    policy_gradient_loss | 0.00842    |
|    value_loss           | 9.74e+03   |
----------------------------------------
Eval num_timesteps=8000, episode_reward=-19.74 +/- 115.91
Episode length: 29.32 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.3     |
|    mean_reward     | -19.7    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61       |
|    ep_rew_mean     | -220     |
| time/              |          |
|    fps             | 263      |
|    iterations      | 8        |
|    time_elapsed    | 31       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=8500, episode_reward=65.54 +/- 149.64
Episode length: 27.78 +/- 4.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.8        |
|    mean_reward          | 65.5        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.035261482 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | -0.00693    |
|    learning_rate        | 0.0005      |
|    loss                 | 6.55e+03    |
|    n_updates            | 54          |
|    policy_gradient_loss | 0.00572     |
|    value_loss           | 1.41e+04    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=62.62 +/- 162.47
Episode length: 28.44 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.4     |
|    mean_reward     | 62.6     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.8     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 9        |
|    time_elapsed    | 33       |
|    total_timesteps | 9216     |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.09
Eval num_timesteps=9500, episode_reward=643.28 +/- 540.43
Episode length: 33.94 +/- 6.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.9        |
|    mean_reward          | 643         |
| time/                   |             |
|    total_timesteps      | 9500        |
| train/                  |             |
|    approx_kl            | 0.036301583 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | -0.00319    |
|    learning_rate        | 0.0005      |
|    loss                 | 4.87e+03    |
|    n_updates            | 58          |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 1.03e+04    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=956.35 +/- 734.48
Episode length: 36.06 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | -75.3    |
| time/              |          |
|    fps             | 274      |
|    iterations      | 10       |
|    time_elapsed    | 37       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=10500, episode_reward=943.47 +/- 736.47
Episode length: 35.62 +/- 6.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 943         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.025281342 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.00323     |
|    learning_rate        | 0.0005      |
|    loss                 | 7.77e+03    |
|    n_updates            | 59          |
|    policy_gradient_loss | -0.00155    |
|    value_loss           | 1.51e+04    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=795.71 +/- 635.67
Episode length: 35.24 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.6     |
|    ep_rew_mean     | 38.5     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 11       |
|    time_elapsed    | 40       |
|    total_timesteps | 11264    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.07
Eval num_timesteps=11500, episode_reward=748.24 +/- 562.49
Episode length: 36.06 +/- 5.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.1        |
|    mean_reward          | 748         |
| time/                   |             |
|    total_timesteps      | 11500       |
| train/                  |             |
|    approx_kl            | 0.021728871 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.00651     |
|    learning_rate        | 0.0005      |
|    loss                 | 8.27e+03    |
|    n_updates            | 64          |
|    policy_gradient_loss | -0.00878    |
|    value_loss           | 1.91e+04    |
-----------------------------------------
Eval num_timesteps=12000, episode_reward=843.39 +/- 728.00
Episode length: 34.96 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.3     |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 12       |
|    time_elapsed    | 44       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=812.44 +/- 647.50
Episode length: 35.78 +/- 6.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 812         |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.011240838 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.000598    |
|    learning_rate        | 0.0005      |
|    loss                 | 1.85e+04    |
|    n_updates            | 74          |
|    policy_gradient_loss | -0.00624    |
|    value_loss           | 3.52e+04    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=731.13 +/- 599.45
Episode length: 35.02 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.9     |
|    ep_rew_mean     | 292      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 13       |
|    time_elapsed    | 47       |
|    total_timesteps | 13312    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.08
Eval num_timesteps=13500, episode_reward=762.14 +/- 658.27
Episode length: 34.68 +/- 7.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 762         |
| time/                   |             |
|    total_timesteps      | 13500       |
| train/                  |             |
|    approx_kl            | 0.043163825 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.000257    |
|    learning_rate        | 0.0005      |
|    loss                 | 5.49e+04    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00564    |
|    value_loss           | 9e+04       |
-----------------------------------------
Eval num_timesteps=14000, episode_reward=668.65 +/- 563.35
Episode length: 34.82 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.5     |
|    ep_rew_mean     | 447      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 14       |
|    time_elapsed    | 50       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=14500, episode_reward=816.46 +/- 600.28
Episode length: 35.90 +/- 5.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 816         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.033875544 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | -0.000209   |
|    learning_rate        | 0.0005      |
|    loss                 | 7.37e+04    |
|    n_updates            | 82          |
|    policy_gradient_loss | 0.00342     |
|    value_loss           | 1.23e+05    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=878.66 +/- 719.43
Episode length: 35.80 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 676      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 15       |
|    time_elapsed    | 54       |
|    total_timesteps | 15360    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=15500, episode_reward=703.80 +/- 573.58
Episode length: 35.00 +/- 6.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 704         |
| time/                   |             |
|    total_timesteps      | 15500       |
| train/                  |             |
|    approx_kl            | 0.085890755 |
|    clip_fraction        | 0.00368     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0061     |
|    explained_variance   | 1.55e-05    |
|    learning_rate        | 0.0005      |
|    loss                 | 9.98e+04    |
|    n_updates            | 85          |
|    policy_gradient_loss | -0.000427   |
|    value_loss           | 1.98e+05    |
-----------------------------------------
Eval num_timesteps=16000, episode_reward=621.40 +/- 494.92
Episode length: 34.24 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 621      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 16       |
|    time_elapsed    | 57       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=609.31 +/- 613.91
Episode length: 32.86 +/- 6.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.9      |
|    mean_reward          | 609       |
| time/                   |           |
|    total_timesteps      | 16500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000194 |
|    explained_variance   | -9.66e-05 |
|    learning_rate        | 0.0005    |
|    loss                 | 6.54e+04  |
|    n_updates            | 95        |
|    policy_gradient_loss | -2.31e-07 |
|    value_loss           | 1.29e+05  |
---------------------------------------
Eval num_timesteps=17000, episode_reward=741.60 +/- 646.47
Episode length: 33.90 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 17       |
|    time_elapsed    | 60       |
|    total_timesteps | 17408    |
---------------------------------
Eval num_timesteps=17500, episode_reward=724.93 +/- 669.60
Episode length: 33.82 +/- 7.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 725       |
| time/                   |           |
|    total_timesteps      | 17500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.07e-06 |
|    explained_variance   | 4.35e-05  |
|    learning_rate        | 0.0005    |
|    loss                 | 7.33e+04  |
|    n_updates            | 105       |
|    policy_gradient_loss | -3.01e-07 |
|    value_loss           | 1.36e+05  |
---------------------------------------
Eval num_timesteps=18000, episode_reward=879.35 +/- 679.18
Episode length: 36.14 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 18       |
|    time_elapsed    | 64       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=836.88 +/- 706.56
Episode length: 35.30 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 837       |
| time/                   |           |
|    total_timesteps      | 18500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.13e-08 |
|    explained_variance   | 0.000513  |
|    learning_rate        | 0.0005    |
|    loss                 | 1.08e+05  |
|    n_updates            | 115       |
|    policy_gradient_loss | 3.75e-09  |
|    value_loss           | 2.12e+05  |
---------------------------------------
Eval num_timesteps=19000, episode_reward=789.58 +/- 728.50
Episode length: 33.66 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 860      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 19       |
|    time_elapsed    | 67       |
|    total_timesteps | 19456    |
---------------------------------
Eval num_timesteps=19500, episode_reward=905.55 +/- 728.81
Episode length: 34.92 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 19500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-13 |
|    explained_variance   | 0.000603  |
|    learning_rate        | 0.0005    |
|    loss                 | 9.7e+04   |
|    n_updates            | 125       |
|    policy_gradient_loss | 8.64e-09  |
|    value_loss           | 1.74e+05  |
---------------------------------------
Eval num_timesteps=20000, episode_reward=768.91 +/- 642.87
Episode length: 34.92 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 899      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 20       |
|    time_elapsed    | 71       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=649.17 +/- 636.97
Episode length: 32.82 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.8      |
|    mean_reward          | 649       |
| time/                   |           |
|    total_timesteps      | 20500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.97e-23 |
|    explained_variance   | 0.00229   |
|    learning_rate        | 0.0005    |
|    loss                 | 7.87e+04  |
|    n_updates            | 135       |
|    policy_gradient_loss | 1.89e-09  |
|    value_loss           | 1.38e+05  |
---------------------------------------
Eval num_timesteps=21000, episode_reward=861.28 +/- 715.06
Episode length: 35.42 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=849.73 +/- 700.50
Episode length: 35.20 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 943      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 21       |
|    time_elapsed    | 75       |
|    total_timesteps | 21504    |
---------------------------------
Eval num_timesteps=22000, episode_reward=824.85 +/- 706.38
Episode length: 34.66 +/- 7.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 825       |
| time/                   |           |
|    total_timesteps      | 22000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.81e-39 |
|    explained_variance   | -0.000942 |
|    learning_rate        | 0.0005    |
|    loss                 | 6.58e+04  |
|    n_updates            | 145       |
|    policy_gradient_loss | 3.63e-09  |
|    value_loss           | 1.35e+05  |
---------------------------------------
Eval num_timesteps=22500, episode_reward=908.65 +/- 687.86
Episode length: 35.90 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 922      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 22       |
|    time_elapsed    | 79       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=892.91 +/- 680.01
Episode length: 35.96 +/- 6.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36       |
|    mean_reward          | 893      |
| time/                   |          |
|    total_timesteps      | 23000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00684 |
|    learning_rate        | 0.0005   |
|    loss                 | 4.8e+04  |
|    n_updates            | 155      |
|    policy_gradient_loss | 2.19e-09 |
|    value_loss           | 1.08e+05 |
--------------------------------------
Eval num_timesteps=23500, episode_reward=840.89 +/- 705.26
Episode length: 35.06 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 23       |
|    time_elapsed    | 82       |
|    total_timesteps | 23552    |
---------------------------------
Eval num_timesteps=24000, episode_reward=802.54 +/- 629.66
Episode length: 35.76 +/- 6.64
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.8     |
|    mean_reward          | 803      |
| time/                   |          |
|    total_timesteps      | 24000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0137  |
|    learning_rate        | 0.0005   |
|    loss                 | 1.16e+05 |
|    n_updates            | 165      |
|    policy_gradient_loss | 2.33e-09 |
|    value_loss           | 2.39e+05 |
--------------------------------------
Eval num_timesteps=24500, episode_reward=970.75 +/- 754.10
Episode length: 36.24 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 24       |
|    time_elapsed    | 86       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=786.75 +/- 640.19
Episode length: 34.72 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.00507  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.75e+04  |
|    n_updates            | 175       |
|    policy_gradient_loss | -1.86e-10 |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=25500, episode_reward=926.83 +/- 718.05
Episode length: 36.30 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 25       |
|    time_elapsed    | 89       |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=26000, episode_reward=882.74 +/- 743.71
Episode length: 34.98 +/- 7.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 26000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.0302   |
|    learning_rate        | 0.0005    |
|    loss                 | 1.16e+05  |
|    n_updates            | 185       |
|    policy_gradient_loss | -2.79e-09 |
|    value_loss           | 2.31e+05  |
---------------------------------------
Eval num_timesteps=26500, episode_reward=732.90 +/- 605.98
Episode length: 34.90 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 26       |
|    time_elapsed    | 93       |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=796.95 +/- 661.04
Episode length: 35.14 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 797       |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.82e-35 |
|    explained_variance   | -0.00249  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.15e+04  |
|    n_updates            | 195       |
|    policy_gradient_loss | 1.34e-09  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=27500, episode_reward=821.17 +/- 690.82
Episode length: 34.54 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 27       |
|    time_elapsed    | 96       |
|    total_timesteps | 27648    |
---------------------------------
Eval num_timesteps=28000, episode_reward=781.99 +/- 716.92
Episode length: 34.28 +/- 7.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 28000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.26e-37 |
|    explained_variance   | 0.00177   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.09e+04  |
|    n_updates            | 205       |
|    policy_gradient_loss | 9.08e-10  |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=28500, episode_reward=826.53 +/- 649.59
Episode length: 35.52 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 889      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 28       |
|    time_elapsed    | 100      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=788.77 +/- 633.76
Episode length: 35.50 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.74e-35 |
|    explained_variance   | -0.00564  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.38e+04  |
|    n_updates            | 215       |
|    policy_gradient_loss | 1.55e-09  |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=29500, episode_reward=799.03 +/- 708.38
Episode length: 35.14 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 29       |
|    time_elapsed    | 103      |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=903.14 +/- 692.17
Episode length: 35.84 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 903       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.56e-37 |
|    explained_variance   | 0.00111   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.89e+04  |
|    n_updates            | 225       |
|    policy_gradient_loss | 1.21e-09  |
|    value_loss           | 8.24e+04  |
---------------------------------------
Eval num_timesteps=30500, episode_reward=801.36 +/- 694.25
Episode length: 34.50 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 30       |
|    time_elapsed    | 107      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=837.95 +/- 761.40
Episode length: 34.30 +/- 8.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 31000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.46e-35 |
|    explained_variance   | -0.00364  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.11e+04  |
|    n_updates            | 235       |
|    policy_gradient_loss | 9.9e-10   |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=31500, episode_reward=957.94 +/- 700.25
Episode length: 36.60 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 31       |
|    time_elapsed    | 110      |
|    total_timesteps | 31744    |
---------------------------------
Eval num_timesteps=32000, episode_reward=831.21 +/- 658.12
Episode length: 35.18 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 32000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.08e-36 |
|    explained_variance   | 0.00252   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.79e+04  |
|    n_updates            | 245       |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 1.31e+05  |
---------------------------------------
Eval num_timesteps=32500, episode_reward=874.57 +/- 750.49
Episode length: 34.28 +/- 8.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 32       |
|    time_elapsed    | 114      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=701.16 +/- 614.16
Episode length: 34.36 +/- 6.16
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.4     |
|    mean_reward          | 701      |
| time/                   |          |
|    total_timesteps      | 33000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00876 |
|    learning_rate        | 0.0005   |
|    loss                 | 5.57e+04 |
|    n_updates            | 255      |
|    policy_gradient_loss | 1.32e-09 |
|    value_loss           | 1.08e+05 |
--------------------------------------
Eval num_timesteps=33500, episode_reward=810.52 +/- 702.87
Episode length: 34.58 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 33       |
|    time_elapsed    | 117      |
|    total_timesteps | 33792    |
---------------------------------
Eval num_timesteps=34000, episode_reward=700.76 +/- 581.49
Episode length: 35.22 +/- 5.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 34000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.0188   |
|    learning_rate        | 0.0005    |
|    loss                 | 1.02e+05  |
|    n_updates            | 265       |
|    policy_gradient_loss | -4.17e-09 |
|    value_loss           | 2.4e+05   |
---------------------------------------
Eval num_timesteps=34500, episode_reward=679.09 +/- 567.27
Episode length: 34.06 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 896      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 34       |
|    time_elapsed    | 120      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=865.94 +/- 731.55
Episode length: 35.14 +/- 7.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.1     |
|    mean_reward          | 866      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9.4e-34 |
|    explained_variance   | -0.00382 |
|    learning_rate        | 0.0005   |
|    loss                 | 5.05e+04 |
|    n_updates            | 275      |
|    policy_gradient_loss | 1.36e-09 |
|    value_loss           | 1.1e+05  |
--------------------------------------
Eval num_timesteps=35500, episode_reward=917.55 +/- 709.30
Episode length: 35.60 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 855      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 35       |
|    time_elapsed    | 124      |
|    total_timesteps | 35840    |
---------------------------------
Eval num_timesteps=36000, episode_reward=876.97 +/- 672.48
Episode length: 35.84 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 36000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.45e-35 |
|    explained_variance   | -0.000255 |
|    learning_rate        | 0.0005    |
|    loss                 | 5.59e+04  |
|    n_updates            | 285       |
|    policy_gradient_loss | -3.28e-09 |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=36500, episode_reward=863.21 +/- 697.03
Episode length: 35.54 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 36       |
|    time_elapsed    | 127      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=765.83 +/- 709.41
Episode length: 34.62 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 766       |
| time/                   |           |
|    total_timesteps      | 37000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-33 |
|    explained_variance   | -0.0021   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.81e+04  |
|    n_updates            | 295       |
|    policy_gradient_loss | 5.06e-10  |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=37500, episode_reward=1035.04 +/- 784.64
Episode length: 36.64 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 37       |
|    time_elapsed    | 131      |
|    total_timesteps | 37888    |
---------------------------------
Eval num_timesteps=38000, episode_reward=827.77 +/- 686.24
Episode length: 35.34 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 38000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.18e-35 |
|    explained_variance   | 0.00357   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.26e+04  |
|    n_updates            | 305       |
|    policy_gradient_loss | -1.08e-09 |
|    value_loss           | 9.4e+04   |
---------------------------------------
Eval num_timesteps=38500, episode_reward=766.41 +/- 615.55
Episode length: 34.64 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 38       |
|    time_elapsed    | 134      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=942.26 +/- 693.93
Episode length: 36.52 +/- 5.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 942       |
| time/                   |           |
|    total_timesteps      | 39000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-33 |
|    explained_variance   | -0.00638  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.58e+04  |
|    n_updates            | 315       |
|    policy_gradient_loss | 1.65e-09  |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=39500, episode_reward=922.18 +/- 727.74
Episode length: 35.64 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 39       |
|    time_elapsed    | 138      |
|    total_timesteps | 39936    |
---------------------------------
Eval num_timesteps=40000, episode_reward=852.31 +/- 726.40
Episode length: 34.90 +/- 7.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.16e-35 |
|    explained_variance   | 0.00068   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.92e+04  |
|    n_updates            | 325       |
|    policy_gradient_loss | 1.53e-09  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=40500, episode_reward=768.16 +/- 652.86
Episode length: 34.36 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 40       |
|    time_elapsed    | 141      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=847.94 +/- 715.27
Episode length: 35.22 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 848       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.47e-33 |
|    explained_variance   | -0.00289  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.28e+04  |
|    n_updates            | 335       |
|    policy_gradient_loss | -7.28e-10 |
|    value_loss           | 1.26e+05  |
---------------------------------------
Eval num_timesteps=41500, episode_reward=752.11 +/- 638.81
Episode length: 34.52 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 41       |
|    time_elapsed    | 145      |
|    total_timesteps | 41984    |
---------------------------------
Eval num_timesteps=42000, episode_reward=667.78 +/- 622.01
Episode length: 33.50 +/- 7.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 668       |
| time/                   |           |
|    total_timesteps      | 42000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.11e-35 |
|    explained_variance   | 0.00187   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.12e+04  |
|    n_updates            | 345       |
|    policy_gradient_loss | -1.29e-09 |
|    value_loss           | 9.82e+04  |
---------------------------------------
Eval num_timesteps=42500, episode_reward=844.24 +/- 656.00
Episode length: 35.44 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=712.04 +/- 559.74
Episode length: 35.44 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 42       |
|    time_elapsed    | 149      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=812.13 +/- 704.23
Episode length: 34.84 +/- 6.66
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.8     |
|    mean_reward          | 812      |
| time/                   |          |
|    total_timesteps      | 43500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00543 |
|    learning_rate        | 0.0005   |
|    loss                 | 6.65e+04 |
|    n_updates            | 355      |
|    policy_gradient_loss | 9.95e-10 |
|    value_loss           | 1.22e+05 |
--------------------------------------
Eval num_timesteps=44000, episode_reward=773.77 +/- 674.53
Episode length: 34.88 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 43       |
|    time_elapsed    | 153      |
|    total_timesteps | 44032    |
---------------------------------
Eval num_timesteps=44500, episode_reward=700.33 +/- 640.08
Episode length: 33.70 +/- 7.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 700       |
| time/                   |           |
|    total_timesteps      | 44500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.0203   |
|    learning_rate        | 0.0005    |
|    loss                 | 1.35e+05  |
|    n_updates            | 365       |
|    policy_gradient_loss | -1.58e-09 |
|    value_loss           | 2.78e+05  |
---------------------------------------
Eval num_timesteps=45000, episode_reward=820.89 +/- 642.86
Episode length: 35.38 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 44       |
|    time_elapsed    | 156      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=745.66 +/- 617.02
Episode length: 34.46 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 746       |
| time/                   |           |
|    total_timesteps      | 45500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.15e-32 |
|    explained_variance   | -0.00799  |
|    learning_rate        | 0.0005    |
|    loss                 | 4.84e+04  |
|    n_updates            | 375       |
|    policy_gradient_loss | -1.54e-09 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=46000, episode_reward=831.38 +/- 638.85
Episode length: 35.92 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 45       |
|    time_elapsed    | 160      |
|    total_timesteps | 46080    |
---------------------------------
Eval num_timesteps=46500, episode_reward=871.51 +/- 636.38
Episode length: 36.28 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 872       |
| time/                   |           |
|    total_timesteps      | 46500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.67e-33 |
|    explained_variance   | 0.000675  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.85e+04  |
|    n_updates            | 385       |
|    policy_gradient_loss | -5.25e-09 |
|    value_loss           | 9.81e+04  |
---------------------------------------
Eval num_timesteps=47000, episode_reward=770.66 +/- 618.31
Episode length: 35.36 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 46       |
|    time_elapsed    | 163      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=832.30 +/- 695.42
Episode length: 35.58 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 832       |
| time/                   |           |
|    total_timesteps      | 47500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.79e-32 |
|    explained_variance   | -0.00275  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.25e+04  |
|    n_updates            | 395       |
|    policy_gradient_loss | -1.13e-09 |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=48000, episode_reward=918.18 +/- 726.33
Episode length: 35.58 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 827      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 47       |
|    time_elapsed    | 167      |
|    total_timesteps | 48128    |
---------------------------------
Eval num_timesteps=48500, episode_reward=833.68 +/- 638.83
Episode length: 35.70 +/- 5.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 48500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.48e-34 |
|    explained_variance   | -0.00308  |
|    learning_rate        | 0.0005    |
|    loss                 | 4.53e+04  |
|    n_updates            | 405       |
|    policy_gradient_loss | 1.66e-09  |
|    value_loss           | 8.98e+04  |
---------------------------------------
Eval num_timesteps=49000, episode_reward=820.00 +/- 653.14
Episode length: 35.72 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 48       |
|    time_elapsed    | 170      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=832.18 +/- 662.46
Episode length: 35.76 +/- 6.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.8     |
|    mean_reward          | 832      |
| time/                   |          |
|    total_timesteps      | 49500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4.1e-32 |
|    explained_variance   | -0.00857 |
|    learning_rate        | 0.0005   |
|    loss                 | 6.12e+04 |
|    n_updates            | 415      |
|    policy_gradient_loss | 1.54e-09 |
|    value_loss           | 1.04e+05 |
--------------------------------------
Eval num_timesteps=50000, episode_reward=769.05 +/- 654.35
Episode length: 34.92 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 745      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 49       |
|    time_elapsed    | 174      |
|    total_timesteps | 50176    |
---------------------------------
Eval num_timesteps=50500, episode_reward=837.63 +/- 609.92
Episode length: 36.92 +/- 4.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 50500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-33 |
|    explained_variance   | 0.00306   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.51e+04  |
|    n_updates            | 425       |
|    policy_gradient_loss | 1.64e-09  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=51000, episode_reward=804.88 +/- 655.46
Episode length: 35.14 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 797      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 50       |
|    time_elapsed    | 177      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=851.94 +/- 647.59
Episode length: 36.04 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 51500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.00259  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.43e+04  |
|    n_updates            | 435       |
|    policy_gradient_loss | -7.16e-10 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=52000, episode_reward=898.56 +/- 657.99
Episode length: 36.26 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 867      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 51       |
|    time_elapsed    | 181      |
|    total_timesteps | 52224    |
---------------------------------
Eval num_timesteps=52500, episode_reward=857.10 +/- 669.95
Episode length: 35.60 +/- 6.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.6     |
|    mean_reward          | 857      |
| time/                   |          |
|    total_timesteps      | 52500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0221  |
|    learning_rate        | 0.0005   |
|    loss                 | 9.47e+04 |
|    n_updates            | 445      |
|    policy_gradient_loss | 1.05e-09 |
|    value_loss           | 2.33e+05 |
--------------------------------------
Eval num_timesteps=53000, episode_reward=995.38 +/- 748.03
Episode length: 35.88 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 925      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 52       |
|    time_elapsed    | 184      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=831.00 +/- 684.65
Episode length: 35.22 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 53500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.11e-32 |
|    explained_variance   | -0.00475  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.21e+04  |
|    n_updates            | 455       |
|    policy_gradient_loss | -1.83e-09 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=54000, episode_reward=814.31 +/- 655.81
Episode length: 35.28 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 911      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 53       |
|    time_elapsed    | 188      |
|    total_timesteps | 54272    |
---------------------------------
Eval num_timesteps=54500, episode_reward=689.33 +/- 553.91
Episode length: 34.66 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 689       |
| time/                   |           |
|    total_timesteps      | 54500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.14e-33 |
|    explained_variance   | 0.00214   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.16e+04  |
|    n_updates            | 465       |
|    policy_gradient_loss | 1.05e-10  |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=55000, episode_reward=809.70 +/- 706.16
Episode length: 34.84 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 54       |
|    time_elapsed    | 191      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=855.12 +/- 665.47
Episode length: 35.84 +/- 6.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.46e-32 |
|    explained_variance   | -0.00573  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.21e+04  |
|    n_updates            | 475       |
|    policy_gradient_loss | 1.28e-10  |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=56000, episode_reward=891.60 +/- 664.34
Episode length: 36.28 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 751      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 55       |
|    time_elapsed    | 195      |
|    total_timesteps | 56320    |
---------------------------------
Eval num_timesteps=56500, episode_reward=788.29 +/- 676.30
Episode length: 34.52 +/- 6.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 788       |
| time/                   |           |
|    total_timesteps      | 56500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.76e-33 |
|    explained_variance   | -0.000712 |
|    learning_rate        | 0.0005    |
|    loss                 | 3.28e+04  |
|    n_updates            | 485       |
|    policy_gradient_loss | 1.57e-09  |
|    value_loss           | 6.34e+04  |
---------------------------------------
Eval num_timesteps=57000, episode_reward=791.55 +/- 653.17
Episode length: 35.06 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 816      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 56       |
|    time_elapsed    | 198      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=1031.16 +/- 695.34
Episode length: 37.10 +/- 5.88
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 37.1     |
|    mean_reward          | 1.03e+03 |
| time/                   |          |
|    total_timesteps      | 57500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.2e-31 |
|    explained_variance   | -0.00118 |
|    learning_rate        | 0.0005   |
|    loss                 | 6.97e+04 |
|    n_updates            | 495      |
|    policy_gradient_loss | 3.18e-09 |
|    value_loss           | 1.2e+05  |
--------------------------------------
Eval num_timesteps=58000, episode_reward=827.32 +/- 719.48
Episode length: 34.78 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 57       |
|    time_elapsed    | 202      |
|    total_timesteps | 58368    |
---------------------------------
Eval num_timesteps=58500, episode_reward=797.76 +/- 576.55
Episode length: 36.44 +/- 5.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 798       |
| time/                   |           |
|    total_timesteps      | 58500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.4e-33  |
|    explained_variance   | -0.000417 |
|    learning_rate        | 0.0005    |
|    loss                 | 4.73e+04  |
|    n_updates            | 505       |
|    policy_gradient_loss | -1.64e-09 |
|    value_loss           | 8.89e+04  |
---------------------------------------
Eval num_timesteps=59000, episode_reward=902.43 +/- 724.66
Episode length: 35.76 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 885      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 58       |
|    time_elapsed    | 205      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=779.17 +/- 675.20
Episode length: 35.06 +/- 6.22
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.1     |
|    mean_reward          | 779      |
| time/                   |          |
|    total_timesteps      | 59500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.000368 |
|    learning_rate        | 0.0005   |
|    loss                 | 5.98e+04 |
|    n_updates            | 515      |
|    policy_gradient_loss | 9.02e-10 |
|    value_loss           | 1.15e+05 |
--------------------------------------
Eval num_timesteps=60000, episode_reward=819.05 +/- 648.33
Episode length: 35.82 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 942      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 59       |
|    time_elapsed    | 209      |
|    total_timesteps | 60416    |
---------------------------------
Eval num_timesteps=60500, episode_reward=763.26 +/- 630.88
Episode length: 34.84 +/- 6.27
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.8     |
|    mean_reward          | 763      |
| time/                   |          |
|    total_timesteps      | 60500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0213  |
|    learning_rate        | 0.0005   |
|    loss                 | 1.14e+05 |
|    n_updates            | 525      |
|    policy_gradient_loss | 5.2e-09  |
|    value_loss           | 2.41e+05 |
--------------------------------------
Eval num_timesteps=61000, episode_reward=904.45 +/- 724.54
Episode length: 35.54 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 939      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 60       |
|    time_elapsed    | 212      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=933.86 +/- 738.67
Episode length: 35.66 +/- 7.08
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.7     |
|    mean_reward          | 934      |
| time/                   |          |
|    total_timesteps      | 61500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00374 |
|    learning_rate        | 0.0005   |
|    loss                 | 5.07e+04 |
|    n_updates            | 535      |
|    policy_gradient_loss | 1.46e-09 |
|    value_loss           | 1.02e+05 |
--------------------------------------
Eval num_timesteps=62000, episode_reward=788.24 +/- 607.02
Episode length: 35.36 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 901      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 61       |
|    time_elapsed    | 216      |
|    total_timesteps | 62464    |
---------------------------------
Eval num_timesteps=62500, episode_reward=854.94 +/- 661.38
Episode length: 36.02 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 62500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.0258   |
|    learning_rate        | 0.0005    |
|    loss                 | 1.39e+05  |
|    n_updates            | 545       |
|    policy_gradient_loss | -6.15e-09 |
|    value_loss           | 2.9e+05   |
---------------------------------------
Eval num_timesteps=63000, episode_reward=997.94 +/- 727.10
Episode length: 36.66 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 62       |
|    time_elapsed    | 219      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=781.90 +/- 629.12
Episode length: 35.38 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.91e-29 |
|    explained_variance   | 0.000618  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.56e+04  |
|    n_updates            | 555       |
|    policy_gradient_loss | 9.84e-10  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=782.83 +/- 632.42
Episode length: 35.84 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=668.00 +/- 602.67
Episode length: 33.44 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 63       |
|    time_elapsed    | 224      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=1023.89 +/- 753.52
Episode length: 36.60 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 65000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-30 |
|    explained_variance   | 0.00395   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.97e+04  |
|    n_updates            | 565       |
|    policy_gradient_loss | 4.27e-09  |
|    value_loss           | 1.29e+05  |
---------------------------------------
Eval num_timesteps=65500, episode_reward=1050.12 +/- 759.64
Episode length: 36.28 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 64       |
|    time_elapsed    | 227      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=880.09 +/- 672.51
Episode length: 36.36 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 880       |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.75e-30 |
|    explained_variance   | -0.00753  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.99e+04  |
|    n_updates            | 575       |
|    policy_gradient_loss | 1.75e-11  |
|    value_loss           | 1.18e+05  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=1043.97 +/- 749.75
Episode length: 36.46 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 65       |
|    time_elapsed    | 231      |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=867.54 +/- 686.73
Episode length: 35.74 +/- 5.64
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.7     |
|    mean_reward          | 868      |
| time/                   |          |
|    total_timesteps      | 67000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.7e-31 |
|    explained_variance   | 0.00286  |
|    learning_rate        | 0.0005   |
|    loss                 | 6.81e+04 |
|    n_updates            | 585      |
|    policy_gradient_loss | 1.83e-09 |
|    value_loss           | 1.46e+05 |
--------------------------------------
Eval num_timesteps=67500, episode_reward=826.44 +/- 734.27
Episode length: 34.72 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 66       |
|    time_elapsed    | 234      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=742.81 +/- 635.32
Episode length: 35.32 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.17e-30 |
|    explained_variance   | -0.00304  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.19e+04  |
|    n_updates            | 595       |
|    policy_gradient_loss | -1.07e-09 |
|    value_loss           | 1.24e+05  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=703.05 +/- 643.03
Episode length: 33.80 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 67       |
|    time_elapsed    | 238      |
|    total_timesteps | 68608    |
---------------------------------
Eval num_timesteps=69000, episode_reward=879.49 +/- 665.73
Episode length: 36.60 +/- 5.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 879       |
| time/                   |           |
|    total_timesteps      | 69000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.68e-31 |
|    explained_variance   | 0.000213  |
|    learning_rate        | 0.0005    |
|    loss                 | 3.66e+04  |
|    n_updates            | 605       |
|    policy_gradient_loss | -8.27e-10 |
|    value_loss           | 8.33e+04  |
---------------------------------------
Eval num_timesteps=69500, episode_reward=813.37 +/- 634.40
Episode length: 35.78 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 68       |
|    time_elapsed    | 241      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=783.12 +/- 649.72
Episode length: 34.46 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 783       |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-29 |
|    explained_variance   | -0.000819 |
|    learning_rate        | 0.0005    |
|    loss                 | 4.98e+04  |
|    n_updates            | 615       |
|    policy_gradient_loss | -6.4e-10  |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=790.97 +/- 654.41
Episode length: 35.08 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 69       |
|    time_elapsed    | 244      |
|    total_timesteps | 70656    |
---------------------------------
Eval num_timesteps=71000, episode_reward=734.43 +/- 693.07
Episode length: 33.26 +/- 7.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 71000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.06e-31 |
|    explained_variance   | 0.00635   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.96e+04  |
|    n_updates            | 625       |
|    policy_gradient_loss | 2.25e-09  |
|    value_loss           | 9.15e+04  |
---------------------------------------
Eval num_timesteps=71500, episode_reward=852.62 +/- 656.18
Episode length: 36.40 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 854      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 70       |
|    time_elapsed    | 248      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=659.37 +/- 587.64
Episode length: 34.02 +/- 6.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 659       |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00348   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.95e+04  |
|    n_updates            | 635       |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=716.07 +/- 654.50
Episode length: 34.16 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 71       |
|    time_elapsed    | 251      |
|    total_timesteps | 72704    |
---------------------------------
Eval num_timesteps=73000, episode_reward=720.70 +/- 616.68
Episode length: 34.46 +/- 5.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.5     |
|    mean_reward          | 721      |
| time/                   |          |
|    total_timesteps      | 73000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0121  |
|    learning_rate        | 0.0005   |
|    loss                 | 1.43e+05 |
|    n_updates            | 645      |
|    policy_gradient_loss | 4.01e-09 |
|    value_loss           | 2.85e+05 |
--------------------------------------
Eval num_timesteps=73500, episode_reward=676.93 +/- 605.94
Episode length: 34.28 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 72       |
|    time_elapsed    | 255      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=777.23 +/- 695.22
Episode length: 33.76 +/- 7.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 777       |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.28e-29 |
|    explained_variance   | -0.00533  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.77e+04  |
|    n_updates            | 655       |
|    policy_gradient_loss | 2.36e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=815.42 +/- 633.58
Episode length: 35.62 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 73       |
|    time_elapsed    | 258      |
|    total_timesteps | 74752    |
---------------------------------
Eval num_timesteps=75000, episode_reward=700.62 +/- 640.44
Episode length: 33.78 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 75000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.03e-31 |
|    explained_variance   | 0.00287   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.22e+04  |
|    n_updates            | 665       |
|    policy_gradient_loss | 2.33e-09  |
|    value_loss           | 1.18e+05  |
---------------------------------------
Eval num_timesteps=75500, episode_reward=897.22 +/- 712.46
Episode length: 35.72 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 74       |
|    time_elapsed    | 261      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=820.38 +/- 707.57
Episode length: 34.22 +/- 7.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 820       |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.41e-29 |
|    explained_variance   | -0.00806  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.12e+04  |
|    n_updates            | 675       |
|    policy_gradient_loss | 2.47e-09  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=76500, episode_reward=866.43 +/- 718.27
Episode length: 35.64 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 75       |
|    time_elapsed    | 265      |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=808.14 +/- 667.07
Episode length: 35.22 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 808       |
| time/                   |           |
|    total_timesteps      | 77000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-30 |
|    explained_variance   | 0.00253   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.23e+04  |
|    n_updates            | 685       |
|    policy_gradient_loss | 1.13e-09  |
|    value_loss           | 1.3e+05   |
---------------------------------------
Eval num_timesteps=77500, episode_reward=877.36 +/- 718.66
Episode length: 35.56 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 831      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 76       |
|    time_elapsed    | 268      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=815.70 +/- 670.93
Episode length: 35.06 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.76e-29 |
|    explained_variance   | 0.000278  |
|    learning_rate        | 0.0005    |
|    loss                 | 4.89e+04  |
|    n_updates            | 695       |
|    policy_gradient_loss | -9.9e-10  |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=78500, episode_reward=901.39 +/- 711.03
Episode length: 35.46 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 77       |
|    time_elapsed    | 272      |
|    total_timesteps | 78848    |
---------------------------------
Eval num_timesteps=79000, episode_reward=826.68 +/- 654.82
Episode length: 35.50 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 79000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.76e-30 |
|    explained_variance   | 0.0033    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.21e+04  |
|    n_updates            | 705       |
|    policy_gradient_loss | 2.47e-09  |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=79500, episode_reward=869.30 +/- 690.18
Episode length: 35.62 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 78       |
|    time_elapsed    | 275      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=671.23 +/- 578.27
Episode length: 33.30 +/- 6.87
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.3     |
|    mean_reward          | 671      |
| time/                   |          |
|    total_timesteps      | 80000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.000688 |
|    learning_rate        | 0.0005   |
|    loss                 | 5.17e+04 |
|    n_updates            | 715      |
|    policy_gradient_loss | 6.64e-10 |
|    value_loss           | 1.1e+05  |
--------------------------------------
Eval num_timesteps=80500, episode_reward=703.42 +/- 653.40
Episode length: 33.88 +/- 8.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 79       |
|    time_elapsed    | 278      |
|    total_timesteps | 80896    |
---------------------------------
Eval num_timesteps=81000, episode_reward=704.85 +/- 641.69
Episode length: 34.30 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 705       |
| time/                   |           |
|    total_timesteps      | 81000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.00567  |
|    learning_rate        | 0.0005    |
|    loss                 | 1.07e+05  |
|    n_updates            | 725       |
|    policy_gradient_loss | -3.85e-09 |
|    value_loss           | 2.24e+05  |
---------------------------------------
Eval num_timesteps=81500, episode_reward=895.35 +/- 695.05
Episode length: 36.32 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 919      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 80       |
|    time_elapsed    | 282      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=899.60 +/- 726.15
Episode length: 35.88 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 900       |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00259   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.14e+04  |
|    n_updates            | 735       |
|    policy_gradient_loss | -1.51e-09 |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=82500, episode_reward=881.15 +/- 717.72
Episode length: 35.54 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 81       |
|    time_elapsed    | 285      |
|    total_timesteps | 82944    |
---------------------------------
Eval num_timesteps=83000, episode_reward=765.14 +/- 609.33
Episode length: 34.78 +/- 5.82
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.8     |
|    mean_reward          | 765      |
| time/                   |          |
|    total_timesteps      | 83000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0126  |
|    learning_rate        | 0.0005   |
|    loss                 | 1.54e+05 |
|    n_updates            | 745      |
|    policy_gradient_loss | 3.91e-09 |
|    value_loss           | 2.68e+05 |
--------------------------------------
Eval num_timesteps=83500, episode_reward=761.32 +/- 628.13
Episode length: 35.00 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 82       |
|    time_elapsed    | 289      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=876.33 +/- 691.31
Episode length: 35.84 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 876       |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-28 |
|    explained_variance   | -0.000617 |
|    learning_rate        | 0.0005    |
|    loss                 | 6.63e+04  |
|    n_updates            | 755       |
|    policy_gradient_loss | -7.97e-10 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=802.19 +/- 680.57
Episode length: 34.76 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 83       |
|    time_elapsed    | 292      |
|    total_timesteps | 84992    |
---------------------------------
Eval num_timesteps=85000, episode_reward=729.57 +/- 687.77
Episode length: 33.30 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 730       |
| time/                   |           |
|    total_timesteps      | 85000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.27e-30 |
|    explained_variance   | -4.65e-05 |
|    learning_rate        | 0.0005    |
|    loss                 | 6.1e+04   |
|    n_updates            | 765       |
|    policy_gradient_loss | 4.66e-09  |
|    value_loss           | 1.37e+05  |
---------------------------------------
Eval num_timesteps=85500, episode_reward=930.69 +/- 742.92
Episode length: 35.22 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=759.16 +/- 646.67
Episode length: 34.54 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 84       |
|    time_elapsed    | 297      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=780.75 +/- 647.25
Episode length: 35.44 +/- 5.57
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.4     |
|    mean_reward          | 781      |
| time/                   |          |
|    total_timesteps      | 86500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00271  |
|    learning_rate        | 0.0005   |
|    loss                 | 5.96e+04 |
|    n_updates            | 775      |
|    policy_gradient_loss | 5.7e-10  |
|    value_loss           | 1.24e+05 |
--------------------------------------
Eval num_timesteps=87000, episode_reward=852.99 +/- 669.71
Episode length: 36.18 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 901      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 85       |
|    time_elapsed    | 300      |
|    total_timesteps | 87040    |
---------------------------------
Eval num_timesteps=87500, episode_reward=678.98 +/- 572.75
Episode length: 34.72 +/- 6.70
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.7     |
|    mean_reward          | 679      |
| time/                   |          |
|    total_timesteps      | 87500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0109  |
|    learning_rate        | 0.0005   |
|    loss                 | 1.03e+05 |
|    n_updates            | 785      |
|    policy_gradient_loss | 5.28e-09 |
|    value_loss           | 2.05e+05 |
--------------------------------------
Eval num_timesteps=88000, episode_reward=822.05 +/- 675.28
Episode length: 35.66 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 907      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 86       |
|    time_elapsed    | 304      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=831.19 +/- 692.02
Episode length: 35.12 +/- 6.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.00102  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.23e+04  |
|    n_updates            | 795       |
|    policy_gradient_loss | -2.86e-09 |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=89000, episode_reward=694.42 +/- 635.85
Episode length: 33.56 +/- 7.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 87       |
|    time_elapsed    | 307      |
|    total_timesteps | 89088    |
---------------------------------
Eval num_timesteps=89500, episode_reward=737.87 +/- 689.97
Episode length: 33.50 +/- 7.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 738       |
| time/                   |           |
|    total_timesteps      | 89500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.0104   |
|    learning_rate        | 0.0005    |
|    loss                 | 1.03e+05  |
|    n_updates            | 805       |
|    policy_gradient_loss | -5.11e-09 |
|    value_loss           | 2.15e+05  |
---------------------------------------
Eval num_timesteps=90000, episode_reward=854.05 +/- 692.50
Episode length: 35.40 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 88       |
|    time_elapsed    | 311      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=877.04 +/- 697.42
Episode length: 35.78 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 90500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.57e-27 |
|    explained_variance   | 0.00183   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.72e+04  |
|    n_updates            | 815       |
|    policy_gradient_loss | -7.33e-10 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=91000, episode_reward=790.84 +/- 665.06
Episode length: 35.18 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 89       |
|    time_elapsed    | 314      |
|    total_timesteps | 91136    |
---------------------------------
Eval num_timesteps=91500, episode_reward=828.94 +/- 664.31
Episode length: 36.00 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 91500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-28 |
|    explained_variance   | 0.00192   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.86e+04  |
|    n_updates            | 825       |
|    policy_gradient_loss | -2.27e-09 |
|    value_loss           | 8.54e+04  |
---------------------------------------
Eval num_timesteps=92000, episode_reward=966.21 +/- 667.98
Episode length: 37.12 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 90       |
|    time_elapsed    | 318      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=825.61 +/- 611.86
Episode length: 36.46 +/- 5.99
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.5     |
|    mean_reward          | 826      |
| time/                   |          |
|    total_timesteps      | 92500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00052 |
|    learning_rate        | 0.0005   |
|    loss                 | 5.15e+04 |
|    n_updates            | 835      |
|    policy_gradient_loss | 2.22e-09 |
|    value_loss           | 1.07e+05 |
--------------------------------------
Eval num_timesteps=93000, episode_reward=924.19 +/- 653.13
Episode length: 36.90 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 858      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 91       |
|    time_elapsed    | 321      |
|    total_timesteps | 93184    |
---------------------------------
Eval num_timesteps=93500, episode_reward=857.02 +/- 702.62
Episode length: 34.94 +/- 6.36
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.9     |
|    mean_reward          | 857      |
| time/                   |          |
|    total_timesteps      | 93500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0052  |
|    learning_rate        | 0.0005   |
|    loss                 | 9.02e+04 |
|    n_updates            | 845      |
|    policy_gradient_loss | 2.62e-10 |
|    value_loss           | 1.75e+05 |
--------------------------------------
Eval num_timesteps=94000, episode_reward=876.05 +/- 632.54
Episode length: 36.96 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 92       |
|    time_elapsed    | 325      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=734.45 +/- 641.06
Episode length: 34.52 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | -0.00196  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.13e+04  |
|    n_updates            | 855       |
|    policy_gradient_loss | -1.28e-10 |
|    value_loss           | 9.88e+04  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=767.70 +/- 637.99
Episode length: 35.24 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 93       |
|    time_elapsed    | 328      |
|    total_timesteps | 95232    |
---------------------------------
Eval num_timesteps=95500, episode_reward=882.86 +/- 707.09
Episode length: 35.28 +/- 6.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.3     |
|    mean_reward          | 883      |
| time/                   |          |
|    total_timesteps      | 95500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0106  |
|    learning_rate        | 0.0005   |
|    loss                 | 1.09e+05 |
|    n_updates            | 865      |
|    policy_gradient_loss | 8.15e-10 |
|    value_loss           | 2.06e+05 |
--------------------------------------
Eval num_timesteps=96000, episode_reward=875.65 +/- 691.25
Episode length: 35.90 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 94       |
|    time_elapsed    | 332      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=719.85 +/- 645.13
Episode length: 34.04 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 720       |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.38e-27 |
|    explained_variance   | 0.00196   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.29e+04  |
|    n_updates            | 875       |
|    policy_gradient_loss | 6.17e-10  |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=860.63 +/- 665.32
Episode length: 36.10 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 95       |
|    time_elapsed    | 335      |
|    total_timesteps | 97280    |
---------------------------------
Eval num_timesteps=97500, episode_reward=847.41 +/- 695.08
Episode length: 35.30 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 847       |
| time/                   |           |
|    total_timesteps      | 97500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.12e-29 |
|    explained_variance   | 0.00791   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.95e+04  |
|    n_updates            | 885       |
|    policy_gradient_loss | -7.57e-10 |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=98000, episode_reward=993.69 +/- 723.31
Episode length: 36.60 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.4     |
|    ep_rew_mean     | 718      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 96       |
|    time_elapsed    | 339      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=825.24 +/- 685.24
Episode length: 35.24 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 825       |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.44e-27 |
|    explained_variance   | -0.00432  |
|    learning_rate        | 0.0005    |
|    loss                 | 4.73e+04  |
|    n_updates            | 895       |
|    policy_gradient_loss | 9.43e-10  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=783.92 +/- 634.56
Episode length: 35.18 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.6     |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 97       |
|    time_elapsed    | 342      |
|    total_timesteps | 99328    |
---------------------------------
Eval num_timesteps=99500, episode_reward=771.64 +/- 632.40
Episode length: 34.94 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 772       |
| time/                   |           |
|    total_timesteps      | 99500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.78e-29 |
|    explained_variance   | 0.000461  |
|    learning_rate        | 0.0005    |
|    loss                 | 4.37e+04  |
|    n_updates            | 905       |
|    policy_gradient_loss | -7.86e-10 |
|    value_loss           | 9.17e+04  |
---------------------------------------
Eval num_timesteps=100000, episode_reward=862.34 +/- 719.96
Episode length: 35.12 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | 694      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 98       |
|    time_elapsed    | 345      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=771.16 +/- 639.83
Episode length: 34.98 +/- 5.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 771       |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.5e-27  |
|    explained_variance   | -0.000377 |
|    learning_rate        | 0.0005    |
|    loss                 | 5.56e+04  |
|    n_updates            | 915       |
|    policy_gradient_loss | -7.92e-10 |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=101000, episode_reward=803.62 +/- 625.16
Episode length: 36.44 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 99       |
|    time_elapsed    | 349      |
|    total_timesteps | 101376   |
---------------------------------
Eval num_timesteps=101500, episode_reward=790.25 +/- 702.57
Episode length: 34.42 +/- 7.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 790       |
| time/                   |           |
|    total_timesteps      | 101500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.15e-29 |
|    explained_variance   | 0.003     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.93e+04  |
|    n_updates            | 925       |
|    policy_gradient_loss | -1.16e-11 |
|    value_loss           | 1.41e+05  |
---------------------------------------
Eval num_timesteps=102000, episode_reward=729.31 +/- 600.79
Episode length: 35.12 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 100      |
|    time_elapsed    | 352      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=769.20 +/- 658.93
Episode length: 34.82 +/- 6.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 769       |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.62e-27 |
|    explained_variance   | 0.00136   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.18e+04  |
|    n_updates            | 935       |
|    policy_gradient_loss | -5.82e-11 |
|    value_loss           | 9.79e+04  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=915.49 +/- 753.05
Episode length: 34.58 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 101      |
|    time_elapsed    | 356      |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=939.32 +/- 672.76
Episode length: 36.36 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 939       |
| time/                   |           |
|    total_timesteps      | 103500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.11e-29 |
|    explained_variance   | 0.00262   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.74e+04  |
|    n_updates            | 945       |
|    policy_gradient_loss | -6.17e-10 |
|    value_loss           | 9.43e+04  |
---------------------------------------
Eval num_timesteps=104000, episode_reward=805.46 +/- 709.48
Episode length: 34.04 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 102      |
|    time_elapsed    | 359      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=876.10 +/- 655.01
Episode length: 36.72 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 876       |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00402   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.95e+04  |
|    n_updates            | 955       |
|    policy_gradient_loss | -3.96e-10 |
|    value_loss           | 8.98e+04  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=975.08 +/- 759.06
Episode length: 35.80 +/- 7.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 728      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 103      |
|    time_elapsed    | 363      |
|    total_timesteps | 105472   |
---------------------------------
Eval num_timesteps=105500, episode_reward=720.30 +/- 598.81
Episode length: 35.34 +/- 6.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.3     |
|    mean_reward          | 720      |
| time/                   |          |
|    total_timesteps      | 105500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0073  |
|    learning_rate        | 0.0005   |
|    loss                 | 1.03e+05 |
|    n_updates            | 965      |
|    policy_gradient_loss | 1.33e-09 |
|    value_loss           | 1.99e+05 |
--------------------------------------
Eval num_timesteps=106000, episode_reward=942.72 +/- 716.13
Episode length: 36.64 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 104      |
|    time_elapsed    | 366      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=916.24 +/- 708.98
Episode length: 36.08 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 916       |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00405   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.38e+04  |
|    n_updates            | 975       |
|    policy_gradient_loss | -5.47e-10 |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=820.57 +/- 701.77
Episode length: 35.56 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=886.24 +/- 716.64
Episode length: 35.52 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 105      |
|    time_elapsed    | 371      |
|    total_timesteps | 107520   |
---------------------------------
Eval num_timesteps=108000, episode_reward=809.38 +/- 701.12
Episode length: 34.42 +/- 6.31
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.4     |
|    mean_reward          | 809      |
| time/                   |          |
|    total_timesteps      | 108000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00381 |
|    learning_rate        | 0.0005   |
|    loss                 | 9.07e+04 |
|    n_updates            | 985      |
|    policy_gradient_loss | 4.83e-09 |
|    value_loss           | 1.74e+05 |
--------------------------------------
Eval num_timesteps=108500, episode_reward=856.82 +/- 660.10
Episode length: 36.08 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 106      |
|    time_elapsed    | 375      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=799.31 +/- 645.81
Episode length: 35.48 +/- 6.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.5     |
|    mean_reward          | 799      |
| time/                   |          |
|    total_timesteps      | 109000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00297  |
|    learning_rate        | 0.0005   |
|    loss                 | 4.95e+04 |
|    n_updates            | 995      |
|    policy_gradient_loss | 1.05e-10 |
|    value_loss           | 1.02e+05 |
--------------------------------------
Eval num_timesteps=109500, episode_reward=837.02 +/- 618.17
Episode length: 36.44 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 107      |
|    time_elapsed    | 378      |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=882.52 +/- 736.45
Episode length: 35.26 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 110000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.1e-46  |
|    explained_variance   | -0.00166  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.78e+04  |
|    n_updates            | 1005      |
|    policy_gradient_loss | -9.66e-10 |
|    value_loss           | 1.55e+05  |
---------------------------------------
Eval num_timesteps=110500, episode_reward=748.10 +/- 642.59
Episode length: 34.58 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 108      |
|    time_elapsed    | 381      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=774.67 +/- 641.96
Episode length: 35.26 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 775       |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-26 |
|    explained_variance   | -0.00209  |
|    learning_rate        | 0.0005    |
|    loss                 | 4.45e+04  |
|    n_updates            | 1015      |
|    policy_gradient_loss | -1.02e-09 |
|    value_loss           | 8.91e+04  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=989.31 +/- 722.99
Episode length: 35.88 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 109      |
|    time_elapsed    | 385      |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=765.17 +/- 624.67
Episode length: 35.08 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 112000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.21e-28 |
|    explained_variance   | -0.000165 |
|    learning_rate        | 0.0005    |
|    loss                 | 3.92e+04  |
|    n_updates            | 1025      |
|    policy_gradient_loss | 2.88e-09  |
|    value_loss           | 8.38e+04  |
---------------------------------------
Eval num_timesteps=112500, episode_reward=696.58 +/- 570.59
Episode length: 34.44 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 729      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 110      |
|    time_elapsed    | 388      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=800.92 +/- 630.04
Episode length: 35.48 +/- 5.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.75e-47 |
|    explained_variance   | 0.00441   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.33e+04  |
|    n_updates            | 1035      |
|    policy_gradient_loss | 1.65e-09  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=987.70 +/- 742.57
Episode length: 36.26 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 111      |
|    time_elapsed    | 392      |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=763.39 +/- 679.39
Episode length: 34.46 +/- 6.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 763       |
| time/                   |           |
|    total_timesteps      | 114000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.61e-44 |
|    explained_variance   | -0.00846  |
|    learning_rate        | 0.0005    |
|    loss                 | 9.7e+04   |
|    n_updates            | 1045      |
|    policy_gradient_loss | 2.02e-09  |
|    value_loss           | 1.68e+05  |
---------------------------------------
Eval num_timesteps=114500, episode_reward=733.16 +/- 592.79
Episode length: 35.38 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 112      |
|    time_elapsed    | 395      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=862.82 +/- 673.25
Episode length: 35.46 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 863       |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.28e-46 |
|    explained_variance   | 0.00506   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.19e+04  |
|    n_updates            | 1055      |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 8.78e+04  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=686.23 +/- 629.72
Episode length: 33.88 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 867      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 113      |
|    time_elapsed    | 399      |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=824.06 +/- 679.49
Episode length: 35.38 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 824       |
| time/                   |           |
|    total_timesteps      | 116000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.39e-44 |
|    explained_variance   | 0.00336   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.86e+04  |
|    n_updates            | 1065      |
|    policy_gradient_loss | 5.88e-10  |
|    value_loss           | 1.33e+05  |
---------------------------------------
Eval num_timesteps=116500, episode_reward=880.85 +/- 634.89
Episode length: 36.66 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 114      |
|    time_elapsed    | 402      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=685.35 +/- 615.92
Episode length: 34.20 +/- 6.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 685       |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.32e-26 |
|    explained_variance   | 0.00596   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.11e+04  |
|    n_updates            | 1075      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 8.93e+04  |
---------------------------------------
Eval num_timesteps=117500, episode_reward=792.89 +/- 672.88
Episode length: 34.66 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 115      |
|    time_elapsed    | 406      |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=872.19 +/- 710.43
Episode length: 35.44 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 872       |
| time/                   |           |
|    total_timesteps      | 118000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.25e-27 |
|    explained_variance   | 0.000785  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.64e+04  |
|    n_updates            | 1085      |
|    policy_gradient_loss | -2e-09    |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=118500, episode_reward=699.08 +/- 614.98
Episode length: 34.48 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 116      |
|    time_elapsed    | 409      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=769.34 +/- 688.22
Episode length: 34.16 +/- 7.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 769       |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.33e-25 |
|    explained_variance   | -0.000524 |
|    learning_rate        | 0.0005    |
|    loss                 | 3.95e+04  |
|    n_updates            | 1095      |
|    policy_gradient_loss | -8.15e-11 |
|    value_loss           | 9.51e+04  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=902.69 +/- 730.08
Episode length: 35.44 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 117      |
|    time_elapsed    | 412      |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=728.31 +/- 668.33
Episode length: 34.00 +/- 7.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 728       |
| time/                   |           |
|    total_timesteps      | 120000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.03e-27 |
|    explained_variance   | 0.00378   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.74e+04  |
|    n_updates            | 1105      |
|    policy_gradient_loss | 1.64e-09  |
|    value_loss           | 9.39e+04  |
---------------------------------------
Eval num_timesteps=120500, episode_reward=882.76 +/- 682.26
Episode length: 36.26 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 722      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 118      |
|    time_elapsed    | 416      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=868.98 +/- 760.58
Episode length: 34.34 +/- 7.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 869       |
| time/                   |           |
|    total_timesteps      | 121000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.95e-25 |
|    explained_variance   | 0.00729   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.7e+04   |
|    n_updates            | 1115      |
|    policy_gradient_loss | -2.42e-09 |
|    value_loss           | 8.42e+04  |
---------------------------------------
Eval num_timesteps=121500, episode_reward=965.49 +/- 753.99
Episode length: 35.76 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 119      |
|    time_elapsed    | 419      |
|    total_timesteps | 121856   |
---------------------------------
Eval num_timesteps=122000, episode_reward=822.94 +/- 702.80
Episode length: 34.72 +/- 7.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.7     |
|    mean_reward          | 823      |
| time/                   |          |
|    total_timesteps      | 122000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4e-27   |
|    explained_variance   | 0.00264  |
|    learning_rate        | 0.0005   |
|    loss                 | 5.25e+04 |
|    n_updates            | 1125     |
|    policy_gradient_loss | 2.53e-09 |
|    value_loss           | 1.2e+05  |
--------------------------------------
Eval num_timesteps=122500, episode_reward=867.04 +/- 723.20
Episode length: 34.62 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 120      |
|    time_elapsed    | 423      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=947.14 +/- 737.86
Episode length: 35.56 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 947       |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.18e-44 |
|    explained_variance   | 0.00439   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.28e+04  |
|    n_updates            | 1135      |
|    policy_gradient_loss | -3.49e-11 |
|    value_loss           | 8.12e+04  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=896.93 +/- 685.84
Episode length: 35.64 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 121      |
|    time_elapsed    | 426      |
|    total_timesteps | 123904   |
---------------------------------
Eval num_timesteps=124000, episode_reward=1145.29 +/- 755.44
Episode length: 37.84 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.8      |
|    mean_reward          | 1.15e+03  |
| time/                   |           |
|    total_timesteps      | 124000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-41 |
|    explained_variance   | 0.00352   |
|    learning_rate        | 0.0005    |
|    loss                 | 9.6e+04   |
|    n_updates            | 1145      |
|    policy_gradient_loss | 9.43e-10  |
|    value_loss           | 1.6e+05   |
---------------------------------------
New best mean reward!
Eval num_timesteps=124500, episode_reward=777.37 +/- 637.50
Episode length: 35.14 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 122      |
|    time_elapsed    | 430      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=723.82 +/- 608.36
Episode length: 34.80 +/- 6.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.74e-43 |
|    explained_variance   | 0.0033    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.86e+04  |
|    n_updates            | 1155      |
|    policy_gradient_loss | -1.84e-09 |
|    value_loss           | 9.29e+04  |
---------------------------------------
Eval num_timesteps=125500, episode_reward=949.10 +/- 753.42
Episode length: 35.24 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 716      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 123      |
|    time_elapsed    | 433      |
|    total_timesteps | 125952   |
---------------------------------
Eval num_timesteps=126000, episode_reward=651.98 +/- 597.88
Episode length: 33.36 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 652       |
| time/                   |           |
|    total_timesteps      | 126000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.73e-41 |
|    explained_variance   | -0.00327  |
|    learning_rate        | 0.0005    |
|    loss                 | 7.54e+04  |
|    n_updates            | 1165      |
|    policy_gradient_loss | 3.31e-09  |
|    value_loss           | 1.66e+05  |
---------------------------------------
Eval num_timesteps=126500, episode_reward=745.86 +/- 635.98
Episode length: 35.32 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 124      |
|    time_elapsed    | 437      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=732.69 +/- 637.93
Episode length: 34.64 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 733       |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.29e-43 |
|    explained_variance   | 0.00204   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.31e+04  |
|    n_updates            | 1175      |
|    policy_gradient_loss | -5.47e-10 |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=741.80 +/- 673.45
Episode length: 34.22 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=811.37 +/- 691.14
Episode length: 35.46 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 695      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 125      |
|    time_elapsed    | 441      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=847.45 +/- 713.65
Episode length: 35.60 +/- 7.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 847       |
| time/                   |           |
|    total_timesteps      | 128500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.45e-41 |
|    explained_variance   | -0.00301  |
|    learning_rate        | 0.0005    |
|    loss                 | 6e+04     |
|    n_updates            | 1185      |
|    policy_gradient_loss | -2.44e-10 |
|    value_loss           | 1.56e+05  |
---------------------------------------
Eval num_timesteps=129000, episode_reward=973.55 +/- 735.60
Episode length: 35.70 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.3     |
|    ep_rew_mean     | 659      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 126      |
|    time_elapsed    | 445      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=802.35 +/- 689.31
Episode length: 35.24 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.59e-25 |
|    explained_variance   | -0.0018   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.14e+04  |
|    n_updates            | 1195      |
|    policy_gradient_loss | -9.9e-10  |
|    value_loss           | 9.07e+04  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=818.29 +/- 698.85
Episode length: 34.32 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 693      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 127      |
|    time_elapsed    | 448      |
|    total_timesteps | 130048   |
---------------------------------
Eval num_timesteps=130500, episode_reward=754.98 +/- 628.87
Episode length: 35.18 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 130500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.29e-26 |
|    explained_variance   | 0.00418   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.84e+04  |
|    n_updates            | 1205      |
|    policy_gradient_loss | 5.3e-10   |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=131000, episode_reward=745.76 +/- 638.07
Episode length: 34.38 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 128      |
|    time_elapsed    | 452      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=928.14 +/- 703.89
Episode length: 36.56 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 928       |
| time/                   |           |
|    total_timesteps      | 131500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.33e-42 |
|    explained_variance   | 0.000581  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.86e+04  |
|    n_updates            | 1215      |
|    policy_gradient_loss | 3.97e-09  |
|    value_loss           | 1.18e+05  |
---------------------------------------
Eval num_timesteps=132000, episode_reward=691.97 +/- 595.55
Episode length: 33.86 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 129      |
|    time_elapsed    | 455      |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=765.03 +/- 608.64
Episode length: 35.48 +/- 5.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 132500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.89e-40 |
|    explained_variance   | 0.00234   |
|    learning_rate        | 0.0005    |
|    loss                 | 7.02e+04  |
|    n_updates            | 1225      |
|    policy_gradient_loss | 1.01e-09  |
|    value_loss           | 1.44e+05  |
---------------------------------------
Eval num_timesteps=133000, episode_reward=682.69 +/- 598.55
Episode length: 33.76 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 130      |
|    time_elapsed    | 459      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=872.50 +/- 680.59
Episode length: 36.44 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 873       |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.22e-41 |
|    explained_variance   | 0.00306   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.21e+04  |
|    n_updates            | 1235      |
|    policy_gradient_loss | 7.39e-10  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=134000, episode_reward=791.73 +/- 667.66
Episode length: 34.56 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 131      |
|    time_elapsed    | 462      |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=797.46 +/- 633.32
Episode length: 35.62 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 797       |
| time/                   |           |
|    total_timesteps      | 134500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.27e-39 |
|    explained_variance   | -0.00423  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.19e+04  |
|    n_updates            | 1245      |
|    policy_gradient_loss | -5.88e-10 |
|    value_loss           | 1.26e+05  |
---------------------------------------
Eval num_timesteps=135000, episode_reward=920.12 +/- 707.45
Episode length: 35.78 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 903      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 132      |
|    time_elapsed    | 466      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=972.52 +/- 772.83
Episode length: 35.70 +/- 7.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 973       |
| time/                   |           |
|    total_timesteps      | 135500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.28e-41 |
|    explained_variance   | -0.000309 |
|    learning_rate        | 0.0005    |
|    loss                 | 5.97e+04  |
|    n_updates            | 1255      |
|    policy_gradient_loss | 2.81e-09  |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=136000, episode_reward=989.70 +/- 772.73
Episode length: 35.84 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 890      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 133      |
|    time_elapsed    | 469      |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=800.25 +/- 701.01
Episode length: 34.24 +/- 6.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 800       |
| time/                   |           |
|    total_timesteps      | 136500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.42e-39 |
|    explained_variance   | -0.00049  |
|    learning_rate        | 0.0005    |
|    loss                 | 8.43e+04  |
|    n_updates            | 1265      |
|    policy_gradient_loss | 1.28e-10  |
|    value_loss           | 1.38e+05  |
---------------------------------------
Eval num_timesteps=137000, episode_reward=1036.87 +/- 707.63
Episode length: 36.58 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 134      |
|    time_elapsed    | 473      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=944.80 +/- 758.74
Episode length: 34.66 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 945       |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.22e-40 |
|    explained_variance   | 0.00141   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.75e+04  |
|    n_updates            | 1275      |
|    policy_gradient_loss | 5.24e-11  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=789.41 +/- 674.70
Episode length: 34.64 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 135      |
|    time_elapsed    | 476      |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=765.02 +/- 669.87
Episode length: 34.12 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 138500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.4e-38  |
|    explained_variance   | -8.88e-05 |
|    learning_rate        | 0.0005    |
|    loss                 | 5.58e+04  |
|    n_updates            | 1285      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=139000, episode_reward=1006.05 +/- 754.07
Episode length: 36.24 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 816      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 136      |
|    time_elapsed    | 480      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=893.93 +/- 727.86
Episode length: 35.60 +/- 6.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 894       |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.22e-40 |
|    explained_variance   | 0.00336   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.05e+04  |
|    n_updates            | 1295      |
|    policy_gradient_loss | -1.78e-09 |
|    value_loss           | 9.84e+04  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=881.36 +/- 720.58
Episode length: 34.76 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 137      |
|    time_elapsed    | 483      |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=965.28 +/- 716.21
Episode length: 36.52 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 965       |
| time/                   |           |
|    total_timesteps      | 140500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.84e-38 |
|    explained_variance   | -0.00371  |
|    learning_rate        | 0.0005    |
|    loss                 | 7.21e+04  |
|    n_updates            | 1305      |
|    policy_gradient_loss | -1.56e-09 |
|    value_loss           | 1.28e+05  |
---------------------------------------
Eval num_timesteps=141000, episode_reward=753.62 +/- 631.44
Episode length: 35.20 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 138      |
|    time_elapsed    | 487      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=1001.07 +/- 724.23
Episode length: 36.88 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 141500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-39 |
|    explained_variance   | -0.00196  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.25e+04  |
|    n_updates            | 1315      |
|    policy_gradient_loss | -2.25e-09 |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=142000, episode_reward=831.37 +/- 670.81
Episode length: 35.50 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 139      |
|    time_elapsed    | 490      |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=781.60 +/- 691.88
Episode length: 34.50 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 142500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-37 |
|    explained_variance   | 0.00373   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.07e+04  |
|    n_updates            | 1325      |
|    policy_gradient_loss | -2.75e-09 |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=143000, episode_reward=794.44 +/- 651.41
Episode length: 35.10 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 140      |
|    time_elapsed    | 493      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=802.27 +/- 680.07
Episode length: 34.66 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-25 |
|    explained_variance   | 0.000519  |
|    learning_rate        | 0.0005    |
|    loss                 | 3.33e+04  |
|    n_updates            | 1335      |
|    policy_gradient_loss | 5.12e-10  |
|    value_loss           | 7.36e+04  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=944.92 +/- 712.68
Episode length: 36.12 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 141      |
|    time_elapsed    | 497      |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=735.06 +/- 620.46
Episode length: 35.06 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 735       |
| time/                   |           |
|    total_timesteps      | 144500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.97e-27 |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.23e+04  |
|    n_updates            | 1345      |
|    policy_gradient_loss | -3.09e-09 |
|    value_loss           | 7.05e+04  |
---------------------------------------
Eval num_timesteps=145000, episode_reward=884.98 +/- 719.68
Episode length: 35.36 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 142      |
|    time_elapsed    | 500      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=825.81 +/- 663.77
Episode length: 35.62 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.55e-27 |
|    explained_variance   | 0.00287   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.56e+04  |
|    n_updates            | 1355      |
|    policy_gradient_loss | -9.08e-10 |
|    value_loss           | 9.96e+04  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=956.69 +/- 702.58
Episode length: 36.16 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 143      |
|    time_elapsed    | 504      |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=927.00 +/- 676.31
Episode length: 36.48 +/- 5.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 146500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.28e-29 |
|    explained_variance   | 0.00705   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.93e+04  |
|    n_updates            | 1365      |
|    policy_gradient_loss | 3.42e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=147000, episode_reward=862.30 +/- 703.56
Episode length: 34.98 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 144      |
|    time_elapsed    | 507      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=680.50 +/- 619.50
Episode length: 33.90 +/- 7.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 680       |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-26 |
|    explained_variance   | 0.00535   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.64e+04  |
|    n_updates            | 1375      |
|    policy_gradient_loss | -1.3e-09  |
|    value_loss           | 8.32e+04  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=957.29 +/- 756.83
Episode length: 35.46 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 145      |
|    time_elapsed    | 511      |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=787.81 +/- 675.83
Episode length: 35.00 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 788       |
| time/                   |           |
|    total_timesteps      | 148500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.14e-28 |
|    explained_variance   | 0.00246   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.78e+04  |
|    n_updates            | 1385      |
|    policy_gradient_loss | 2.06e-09  |
|    value_loss           | 7.75e+04  |
---------------------------------------
Eval num_timesteps=149000, episode_reward=698.16 +/- 634.89
Episode length: 34.06 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=691.79 +/- 617.49
Episode length: 33.74 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 146      |
|    time_elapsed    | 515      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=954.17 +/- 654.37
Episode length: 36.98 +/- 5.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 954       |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.08e-44 |
|    explained_variance   | 0.00375   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.89e+04  |
|    n_updates            | 1395      |
|    policy_gradient_loss | 3.49e-11  |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=768.58 +/- 696.84
Episode length: 34.08 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 147      |
|    time_elapsed    | 519      |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=797.18 +/- 630.00
Episode length: 35.76 +/- 6.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 797       |
| time/                   |           |
|    total_timesteps      | 151000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.49e-41 |
|    explained_variance   | 0.00754   |
|    learning_rate        | 0.0005    |
|    loss                 | 7.85e+04  |
|    n_updates            | 1405      |
|    policy_gradient_loss | 1.89e-09  |
|    value_loss           | 1.38e+05  |
---------------------------------------
Eval num_timesteps=151500, episode_reward=808.68 +/- 675.41
Episode length: 34.78 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 148      |
|    time_elapsed    | 522      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=898.96 +/- 754.50
Episode length: 35.00 +/- 7.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 899       |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-25 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.83e+04  |
|    n_updates            | 1415      |
|    policy_gradient_loss | 1.68e-09  |
|    value_loss           | 8.11e+04  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=773.04 +/- 678.48
Episode length: 34.20 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 149      |
|    time_elapsed    | 526      |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=740.60 +/- 610.44
Episode length: 34.78 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 741       |
| time/                   |           |
|    total_timesteps      | 153000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.28e-27 |
|    explained_variance   | 0.0062    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.74e+04  |
|    n_updates            | 1425      |
|    policy_gradient_loss | 2.85e-10  |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=153500, episode_reward=999.59 +/- 710.18
Episode length: 36.82 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 150      |
|    time_elapsed    | 529      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=750.87 +/- 624.09
Episode length: 35.06 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 751       |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.89e-42 |
|    explained_variance   | 0.00449   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.11e+04  |
|    n_updates            | 1435      |
|    policy_gradient_loss | -5.01e-10 |
|    value_loss           | 1.12e+05  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=941.86 +/- 713.64
Episode length: 36.20 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 151      |
|    time_elapsed    | 533      |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=820.67 +/- 701.20
Episode length: 34.60 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 821       |
| time/                   |           |
|    total_timesteps      | 155000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55e-39 |
|    explained_variance   | 0.00228   |
|    learning_rate        | 0.0005    |
|    loss                 | 7.89e+04  |
|    n_updates            | 1445      |
|    policy_gradient_loss | -4.07e-10 |
|    value_loss           | 1.39e+05  |
---------------------------------------
Eval num_timesteps=155500, episode_reward=729.38 +/- 616.36
Episode length: 34.60 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 152      |
|    time_elapsed    | 536      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=788.73 +/- 618.96
Episode length: 35.58 +/- 5.80
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.6     |
|    mean_reward          | 789      |
| time/                   |          |
|    total_timesteps      | 156000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.8e-41 |
|    explained_variance   | 0.00715  |
|    learning_rate        | 0.0005   |
|    loss                 | 5.67e+04 |
|    n_updates            | 1455     |
|    policy_gradient_loss | 5.08e-09 |
|    value_loss           | 1.14e+05 |
--------------------------------------
Eval num_timesteps=156500, episode_reward=799.04 +/- 642.67
Episode length: 35.10 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 962      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 153      |
|    time_elapsed    | 540      |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=890.32 +/- 650.63
Episode length: 36.34 +/- 5.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 890       |
| time/                   |           |
|    total_timesteps      | 157000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-38 |
|    explained_variance   | 0.00308   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.29e+04  |
|    n_updates            | 1465      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 1.19e+05  |
---------------------------------------
Eval num_timesteps=157500, episode_reward=734.00 +/- 628.79
Episode length: 34.12 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 943      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 154      |
|    time_elapsed    | 543      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=614.14 +/- 523.66
Episode length: 33.40 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 614       |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.75e-40 |
|    explained_variance   | 0.0062    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.68e+04  |
|    n_updates            | 1475      |
|    policy_gradient_loss | 1.83e-09  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=158500, episode_reward=874.29 +/- 711.85
Episode length: 35.88 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 155      |
|    time_elapsed    | 546      |
|    total_timesteps | 158720   |
---------------------------------
Eval num_timesteps=159000, episode_reward=699.50 +/- 597.95
Episode length: 34.68 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 700       |
| time/                   |           |
|    total_timesteps      | 159000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.58e-38 |
|    explained_variance   | 0.00513   |
|    learning_rate        | 0.0005    |
|    loss                 | 8.32e+04  |
|    n_updates            | 1485      |
|    policy_gradient_loss | -1e-09    |
|    value_loss           | 1.51e+05  |
---------------------------------------
Eval num_timesteps=159500, episode_reward=697.24 +/- 607.40
Episode length: 35.28 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 819      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 156      |
|    time_elapsed    | 550      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=811.53 +/- 653.41
Episode length: 35.00 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.65e-39 |
|    explained_variance   | 0.00232   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.42e+04  |
|    n_updates            | 1495      |
|    policy_gradient_loss | -1.92e-09 |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=160500, episode_reward=920.84 +/- 693.71
Episode length: 36.44 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 685      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 157      |
|    time_elapsed    | 553      |
|    total_timesteps | 160768   |
---------------------------------
Eval num_timesteps=161000, episode_reward=937.78 +/- 729.97
Episode length: 35.84 +/- 6.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 938       |
| time/                   |           |
|    total_timesteps      | 161000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.34e-37 |
|    explained_variance   | 0.0104    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.71e+04  |
|    n_updates            | 1505      |
|    policy_gradient_loss | 3.23e-09  |
|    value_loss           | 1.36e+05  |
---------------------------------------
Eval num_timesteps=161500, episode_reward=777.11 +/- 636.50
Episode length: 35.20 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 158      |
|    time_elapsed    | 557      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=914.27 +/- 768.90
Episode length: 35.44 +/- 7.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 914       |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.42e-39 |
|    explained_variance   | 0.00565   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.49e+04  |
|    n_updates            | 1515      |
|    policy_gradient_loss | 4.92e-09  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=162500, episode_reward=698.58 +/- 629.21
Episode length: 33.90 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 159      |
|    time_elapsed    | 560      |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=875.38 +/- 698.99
Episode length: 35.62 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 163000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.91e-37 |
|    explained_variance   | 0.0126    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.2e+04   |
|    n_updates            | 1525      |
|    policy_gradient_loss | -3.49e-10 |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=163500, episode_reward=876.29 +/- 711.71
Episode length: 35.06 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 929      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 160      |
|    time_elapsed    | 564      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=877.99 +/- 687.64
Episode length: 35.32 +/- 6.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3e-38    |
|    explained_variance   | 0.00226   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.77e+04  |
|    n_updates            | 1535      |
|    policy_gradient_loss | -1.32e-09 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=772.65 +/- 630.74
Episode length: 34.92 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 914      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 161      |
|    time_elapsed    | 567      |
|    total_timesteps | 164864   |
---------------------------------
Eval num_timesteps=165000, episode_reward=918.82 +/- 748.55
Episode length: 35.18 +/- 7.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 919       |
| time/                   |           |
|    total_timesteps      | 165000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.86e-36 |
|    explained_variance   | 0.00991   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.2e+04   |
|    n_updates            | 1545      |
|    policy_gradient_loss | 2.33e-10  |
|    value_loss           | 1.3e+05   |
---------------------------------------
Eval num_timesteps=165500, episode_reward=895.00 +/- 733.25
Episode length: 35.00 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 976      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 162      |
|    time_elapsed    | 570      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=720.00 +/- 656.57
Episode length: 34.00 +/- 6.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 720       |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.53e-38 |
|    explained_variance   | 0.00696   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.08e+04  |
|    n_updates            | 1555      |
|    policy_gradient_loss | 2.66e-09  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=166500, episode_reward=945.63 +/- 708.59
Episode length: 36.64 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 163      |
|    time_elapsed    | 574      |
|    total_timesteps | 166912   |
---------------------------------
Eval num_timesteps=167000, episode_reward=902.59 +/- 722.51
Episode length: 35.18 +/- 7.07
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.2     |
|    mean_reward          | 903      |
| time/                   |          |
|    total_timesteps      | 167000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -7.4e-36 |
|    explained_variance   | 0.00631  |
|    learning_rate        | 0.0005   |
|    loss                 | 5.67e+04 |
|    n_updates            | 1565     |
|    policy_gradient_loss | 3.32e-10 |
|    value_loss           | 1.16e+05 |
--------------------------------------
Eval num_timesteps=167500, episode_reward=678.71 +/- 616.67
Episode length: 34.36 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 164      |
|    time_elapsed    | 577      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=949.38 +/- 763.04
Episode length: 35.84 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 949       |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.03e-37 |
|    explained_variance   | 0.00224   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.94e+04  |
|    n_updates            | 1575      |
|    policy_gradient_loss | 3.32e-10  |
|    value_loss           | 8.24e+04  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=850.37 +/- 650.44
Episode length: 36.20 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 165      |
|    time_elapsed    | 581      |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=771.00 +/- 615.74
Episode length: 35.42 +/- 5.99
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.4     |
|    mean_reward          | 771      |
| time/                   |          |
|    total_timesteps      | 169000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5e-35   |
|    explained_variance   | 0.00715  |
|    learning_rate        | 0.0005   |
|    loss                 | 5.98e+04 |
|    n_updates            | 1585     |
|    policy_gradient_loss | 6.75e-10 |
|    value_loss           | 1.23e+05 |
--------------------------------------
Eval num_timesteps=169500, episode_reward=921.45 +/- 687.89
Episode length: 36.82 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 166      |
|    time_elapsed    | 584      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=799.48 +/- 657.82
Episode length: 35.06 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-36 |
|    explained_variance   | 0.000383  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.14e+04  |
|    n_updates            | 1595      |
|    policy_gradient_loss | -9.08e-10 |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=170500, episode_reward=763.72 +/- 617.88
Episode length: 35.18 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=818.61 +/- 630.54
Episode length: 35.68 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 167      |
|    time_elapsed    | 589      |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=881.71 +/- 715.04
Episode length: 35.54 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 882       |
| time/                   |           |
|    total_timesteps      | 171500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.56e-35 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.64e+04  |
|    n_updates            | 1605      |
|    policy_gradient_loss | -2.75e-09 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=172000, episode_reward=863.98 +/- 656.42
Episode length: 35.86 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 168      |
|    time_elapsed    | 593      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=713.70 +/- 628.38
Episode length: 34.14 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 714       |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.95e-36 |
|    explained_variance   | 0.00218   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.78e+04  |
|    n_updates            | 1615      |
|    policy_gradient_loss | -1.37e-09 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=931.88 +/- 710.29
Episode length: 36.70 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 169      |
|    time_elapsed    | 596      |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=733.34 +/- 632.89
Episode length: 34.80 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 733       |
| time/                   |           |
|    total_timesteps      | 173500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-34 |
|    explained_variance   | 0.00625   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.68e+04  |
|    n_updates            | 1625      |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=174000, episode_reward=887.56 +/- 637.56
Episode length: 36.16 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 170      |
|    time_elapsed    | 599      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=1075.52 +/- 703.15
Episode length: 36.92 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 1.08e+03  |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.87e-36 |
|    explained_variance   | 0.00931   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.82e+04  |
|    n_updates            | 1635      |
|    policy_gradient_loss | -1.51e-09 |
|    value_loss           | 9.61e+04  |
---------------------------------------
Eval num_timesteps=175000, episode_reward=689.24 +/- 676.01
Episode length: 33.32 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 171      |
|    time_elapsed    | 603      |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=789.21 +/- 716.08
Episode length: 33.54 +/- 8.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 175500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.55e-34 |
|    explained_variance   | 0.00583   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.64e+04  |
|    n_updates            | 1645      |
|    policy_gradient_loss | -4.66e-11 |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=176000, episode_reward=858.22 +/- 703.16
Episode length: 34.94 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 710      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 172      |
|    time_elapsed    | 606      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=790.27 +/- 600.40
Episode length: 35.80 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 790       |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.03e-36 |
|    explained_variance   | 0.0055    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.87e+04  |
|    n_updates            | 1655      |
|    policy_gradient_loss | -9.84e-10 |
|    value_loss           | 9.36e+04  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=678.83 +/- 568.42
Episode length: 34.32 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 173      |
|    time_elapsed    | 610      |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=597.22 +/- 534.06
Episode length: 34.02 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 597       |
| time/                   |           |
|    total_timesteps      | 177500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.73e-34 |
|    explained_variance   | 0.00562   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.12e+04  |
|    n_updates            | 1665      |
|    policy_gradient_loss | -3.14e-09 |
|    value_loss           | 1.13e+05  |
---------------------------------------
Eval num_timesteps=178000, episode_reward=976.91 +/- 729.36
Episode length: 36.48 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 977      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 174      |
|    time_elapsed    | 613      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=791.69 +/- 646.94
Episode length: 35.24 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 792       |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.09e-35 |
|    explained_variance   | 0.00225   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.05e+04  |
|    n_updates            | 1675      |
|    policy_gradient_loss | -6.17e-10 |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=179000, episode_reward=866.44 +/- 667.11
Episode length: 35.86 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 727      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 175      |
|    time_elapsed    | 617      |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=806.03 +/- 682.81
Episode length: 34.96 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 806       |
| time/                   |           |
|    total_timesteps      | 179500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.76e-33 |
|    explained_variance   | 0.00356   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.06e+04  |
|    n_updates            | 1685      |
|    policy_gradient_loss | 1.58e-09  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=180000, episode_reward=721.92 +/- 610.73
Episode length: 34.34 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 176      |
|    time_elapsed    | 620      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=739.49 +/- 633.04
Episode length: 34.70 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 739       |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.29e-34 |
|    explained_variance   | 0.00532   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.1e+04   |
|    n_updates            | 1695      |
|    policy_gradient_loss | 1.72e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=181000, episode_reward=781.18 +/- 692.61
Episode length: 34.08 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 177      |
|    time_elapsed    | 624      |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=1031.51 +/- 783.58
Episode length: 36.06 +/- 7.15
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.1     |
|    mean_reward          | 1.03e+03 |
| time/                   |          |
|    total_timesteps      | 181500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00745  |
|    learning_rate        | 0.0005   |
|    loss                 | 5.53e+04 |
|    n_updates            | 1705     |
|    policy_gradient_loss | 3.35e-09 |
|    value_loss           | 1.06e+05 |
--------------------------------------
Eval num_timesteps=182000, episode_reward=765.49 +/- 647.08
Episode length: 34.98 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 178      |
|    time_elapsed    | 627      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=957.20 +/- 720.68
Episode length: 36.14 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 957       |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.18e-39 |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.0005    |
|    loss                 | 8.75e+04  |
|    n_updates            | 1715      |
|    policy_gradient_loss | 4.13e-09  |
|    value_loss           | 1.66e+05  |
---------------------------------------
Eval num_timesteps=183000, episode_reward=725.94 +/- 682.14
Episode length: 33.44 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 179      |
|    time_elapsed    | 631      |
|    total_timesteps | 183296   |
---------------------------------
Eval num_timesteps=183500, episode_reward=711.36 +/- 682.97
Episode length: 32.94 +/- 7.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.9      |
|    mean_reward          | 711       |
| time/                   |           |
|    total_timesteps      | 183500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.36e-17 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.97e+04  |
|    n_updates            | 1725      |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 9.28e+04  |
---------------------------------------
Eval num_timesteps=184000, episode_reward=891.19 +/- 712.13
Episode length: 35.78 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 180      |
|    time_elapsed    | 634      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=777.15 +/- 676.82
Episode length: 33.88 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 777       |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-18 |
|    explained_variance   | 0.000757  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.49e+04  |
|    n_updates            | 1735      |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 1.24e+05  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=828.92 +/- 633.35
Episode length: 35.74 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 181      |
|    time_elapsed    | 637      |
|    total_timesteps | 185344   |
---------------------------------
Eval num_timesteps=185500, episode_reward=920.29 +/- 674.02
Episode length: 36.36 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 920       |
| time/                   |           |
|    total_timesteps      | 185500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.74e-30 |
|    explained_variance   | 0.00549   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.07e+04  |
|    n_updates            | 1745      |
|    policy_gradient_loss | 2.68e-10  |
|    value_loss           | 9.61e+04  |
---------------------------------------
Eval num_timesteps=186000, episode_reward=933.55 +/- 744.34
Episode length: 35.54 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 182      |
|    time_elapsed    | 641      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=852.83 +/- 687.81
Episode length: 35.58 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.44e-32 |
|    explained_variance   | 0.00545   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.54e+04  |
|    n_updates            | 1755      |
|    policy_gradient_loss | -3.68e-09 |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=187000, episode_reward=906.71 +/- 723.94
Episode length: 36.12 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 183      |
|    time_elapsed    | 644      |
|    total_timesteps | 187392   |
---------------------------------
Eval num_timesteps=187500, episode_reward=855.48 +/- 655.46
Episode length: 35.82 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 187500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.09e-32 |
|    explained_variance   | 0.00705   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.85e+04  |
|    n_updates            | 1765      |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 1.28e+05  |
---------------------------------------
Eval num_timesteps=188000, episode_reward=880.98 +/- 731.81
Episode length: 35.42 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 184      |
|    time_elapsed    | 648      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=761.37 +/- 617.72
Episode length: 35.60 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.11e-33 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.04e+04  |
|    n_updates            | 1775      |
|    policy_gradient_loss | -2.47e-09 |
|    value_loss           | 8.35e+04  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=849.06 +/- 688.39
Episode length: 35.36 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 642      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 185      |
|    time_elapsed    | 651      |
|    total_timesteps | 189440   |
---------------------------------
Eval num_timesteps=189500, episode_reward=786.97 +/- 658.17
Episode length: 34.88 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 189500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.66e-32 |
|    explained_variance   | 0.0317    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.31e+04  |
|    n_updates            | 1785      |
|    policy_gradient_loss | -4.91e-09 |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=190000, episode_reward=936.17 +/- 687.14
Episode length: 36.76 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 729      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 186      |
|    time_elapsed    | 655      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=723.76 +/- 630.33
Episode length: 34.46 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 190500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-33 |
|    explained_variance   | 0.00975   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.24e+04  |
|    n_updates            | 1795      |
|    policy_gradient_loss | 8.38e-10  |
|    value_loss           | 9.37e+04  |
---------------------------------------
Eval num_timesteps=191000, episode_reward=761.64 +/- 701.77
Episode length: 34.16 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 187      |
|    time_elapsed    | 658      |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=982.94 +/- 744.92
Episode length: 35.58 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 983       |
| time/                   |           |
|    total_timesteps      | 191500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.85e-32 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.04e+04  |
|    n_updates            | 1805      |
|    policy_gradient_loss | 1.43e-09  |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=192000, episode_reward=995.23 +/- 769.35
Episode length: 36.28 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=772.60 +/- 648.17
Episode length: 34.68 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 188      |
|    time_elapsed    | 663      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=740.29 +/- 617.56
Episode length: 34.48 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 740       |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.21e-34 |
|    explained_variance   | 0.00622   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.47e+04  |
|    n_updates            | 1815      |
|    policy_gradient_loss | -1.7e-09  |
|    value_loss           | 9.89e+04  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=964.88 +/- 716.88
Episode length: 36.34 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 816      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 189      |
|    time_elapsed    | 667      |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=821.67 +/- 609.87
Episode length: 35.70 +/- 6.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 822       |
| time/                   |           |
|    total_timesteps      | 194000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00914   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.84e+04  |
|    n_updates            | 1825      |
|    policy_gradient_loss | -6.69e-10 |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=194500, episode_reward=784.05 +/- 657.14
Episode length: 35.20 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 190      |
|    time_elapsed    | 670      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=838.58 +/- 643.56
Episode length: 36.68 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-39 |
|    explained_variance   | 0.0225    |
|    learning_rate        | 0.0005    |
|    loss                 | 8.23e+04  |
|    n_updates            | 1835      |
|    policy_gradient_loss | -1.49e-09 |
|    value_loss           | 1.69e+05  |
---------------------------------------
Eval num_timesteps=195500, episode_reward=709.85 +/- 612.31
Episode length: 34.84 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 191      |
|    time_elapsed    | 674      |
|    total_timesteps | 195584   |
---------------------------------
Eval num_timesteps=196000, episode_reward=900.24 +/- 740.29
Episode length: 35.44 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 900       |
| time/                   |           |
|    total_timesteps      | 196000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-40 |
|    explained_variance   | 0.00786   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.78e+04  |
|    n_updates            | 1845      |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 9.54e+04  |
---------------------------------------
Eval num_timesteps=196500, episode_reward=811.52 +/- 709.14
Episode length: 34.96 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 192      |
|    time_elapsed    | 677      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=772.97 +/- 640.33
Episode length: 34.88 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.81e-40 |
|    explained_variance   | 0.0228    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.94e+04  |
|    n_updates            | 1855      |
|    policy_gradient_loss | -2.27e-09 |
|    value_loss           | 1.47e+05  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=738.73 +/- 622.83
Episode length: 34.96 +/- 7.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 193      |
|    time_elapsed    | 681      |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=700.72 +/- 592.82
Episode length: 34.48 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 198000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.58e-25 |
|    explained_variance   | 0.0146    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.02e+04  |
|    n_updates            | 1865      |
|    policy_gradient_loss | 9.43e-10  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=198500, episode_reward=1084.40 +/- 762.16
Episode length: 36.88 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 194      |
|    time_elapsed    | 684      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=757.70 +/- 639.14
Episode length: 35.04 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.96e-27 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.58e+04  |
|    n_updates            | 1875      |
|    policy_gradient_loss | -5.12e-10 |
|    value_loss           | 9.24e+04  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=622.12 +/- 585.49
Episode length: 32.96 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 195      |
|    time_elapsed    | 687      |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=695.62 +/- 671.78
Episode length: 33.82 +/- 7.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 696       |
| time/                   |           |
|    total_timesteps      | 200000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-26 |
|    explained_variance   | 0.03      |
|    learning_rate        | 0.0005    |
|    loss                 | 5.9e+04   |
|    n_updates            | 1885      |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 8.35e+04  |
---------------------------------------
Eval num_timesteps=200500, episode_reward=666.74 +/- 663.97
Episode length: 33.08 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 667      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 196      |
|    time_elapsed    | 691      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=1019.73 +/- 734.98
Episode length: 36.56 +/- 5.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-28 |
|    explained_variance   | 0.00271   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.89e+04  |
|    n_updates            | 1895      |
|    policy_gradient_loss | 1.99e-09  |
|    value_loss           | 1.41e+05  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=967.48 +/- 745.33
Episode length: 36.30 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 197      |
|    time_elapsed    | 694      |
|    total_timesteps | 201728   |
---------------------------------
Eval num_timesteps=202000, episode_reward=801.09 +/- 688.41
Episode length: 34.86 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 202000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.72e-27 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.71e+04  |
|    n_updates            | 1905      |
|    policy_gradient_loss | 2.76e-09  |
|    value_loss           | 9.11e+04  |
---------------------------------------
Eval num_timesteps=202500, episode_reward=732.05 +/- 660.65
Episode length: 34.72 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 198      |
|    time_elapsed    | 698      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=860.32 +/- 658.23
Episode length: 35.74 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 860       |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-28 |
|    explained_variance   | 0.00816   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.96e+04  |
|    n_updates            | 1915      |
|    policy_gradient_loss | 1.26e-09  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=1137.99 +/- 727.29
Episode length: 38.26 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.3     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 199      |
|    time_elapsed    | 701      |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=915.88 +/- 707.04
Episode length: 35.76 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 916       |
| time/                   |           |
|    total_timesteps      | 204000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-26 |
|    explained_variance   | 0.0295    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.74e+04  |
|    n_updates            | 1925      |
|    policy_gradient_loss | 1.52e-09  |
|    value_loss           | 9.18e+04  |
---------------------------------------
Eval num_timesteps=204500, episode_reward=917.66 +/- 693.09
Episode length: 35.98 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 725      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 200      |
|    time_elapsed    | 705      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=811.77 +/- 702.03
Episode length: 34.22 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.94e-28 |
|    explained_variance   | 0.0225    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.41e+04  |
|    n_updates            | 1935      |
|    policy_gradient_loss | 3.26e-10  |
|    value_loss           | 8.18e+04  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=673.56 +/- 589.76
Episode length: 34.16 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 201      |
|    time_elapsed    | 708      |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=981.92 +/- 735.96
Episode length: 36.28 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 982       |
| time/                   |           |
|    total_timesteps      | 206000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.11e-41 |
|    explained_variance   | 0.0208    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.25e+04  |
|    n_updates            | 1945      |
|    policy_gradient_loss | -1.29e-09 |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=206500, episode_reward=720.43 +/- 594.25
Episode length: 35.14 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 202      |
|    time_elapsed    | 712      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=806.62 +/- 665.77
Episode length: 35.06 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-37 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.73e+04  |
|    n_updates            | 1955      |
|    policy_gradient_loss | 4.66e-11  |
|    value_loss           | 1.42e+05  |
---------------------------------------
Eval num_timesteps=207500, episode_reward=907.19 +/- 737.08
Episode length: 35.46 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 203      |
|    time_elapsed    | 715      |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=785.68 +/- 665.00
Episode length: 34.90 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 208000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.08e-39 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.08e+04  |
|    n_updates            | 1965      |
|    policy_gradient_loss | -2.14e-09 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=208500, episode_reward=789.41 +/- 620.55
Episode length: 35.58 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 204      |
|    time_elapsed    | 718      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=896.81 +/- 756.44
Episode length: 34.92 +/- 7.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.9     |
|    mean_reward          | 897      |
| time/                   |          |
|    total_timesteps      | 209000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.9e-37 |
|    explained_variance   | 0.0179   |
|    learning_rate        | 0.0005   |
|    loss                 | 7.38e+04 |
|    n_updates            | 1975     |
|    policy_gradient_loss | 3.85e-09 |
|    value_loss           | 1.51e+05 |
--------------------------------------
Eval num_timesteps=209500, episode_reward=857.58 +/- 681.92
Episode length: 35.74 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 205      |
|    time_elapsed    | 722      |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=832.69 +/- 691.74
Episode length: 35.28 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 833       |
| time/                   |           |
|    total_timesteps      | 210000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.96e-25 |
|    explained_variance   | 0.0209    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.51e+04  |
|    n_updates            | 1985      |
|    policy_gradient_loss | 1.21e-09  |
|    value_loss           | 9.44e+04  |
---------------------------------------
Eval num_timesteps=210500, episode_reward=918.14 +/- 699.98
Episode length: 35.96 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 206      |
|    time_elapsed    | 725      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=957.37 +/- 720.82
Episode length: 36.44 +/- 5.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 957       |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.38e-27 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.46e+04  |
|    n_updates            | 1995      |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 8.91e+04  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=951.24 +/- 764.76
Episode length: 35.30 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 207      |
|    time_elapsed    | 729      |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=850.43 +/- 716.05
Episode length: 34.82 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 850       |
| time/                   |           |
|    total_timesteps      | 212000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.11e-26 |
|    explained_variance   | 0.0252    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.66e+04  |
|    n_updates            | 2005      |
|    policy_gradient_loss | 5.65e-10  |
|    value_loss           | 9.28e+04  |
---------------------------------------
Eval num_timesteps=212500, episode_reward=714.30 +/- 643.97
Episode length: 33.88 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 208      |
|    time_elapsed    | 732      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=953.66 +/- 674.32
Episode length: 37.46 +/- 5.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.5      |
|    mean_reward          | 954       |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.21e-27 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.52e+04  |
|    n_updates            | 2015      |
|    policy_gradient_loss | -5.01e-10 |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=213500, episode_reward=742.06 +/- 604.91
Episode length: 35.16 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=672.48 +/- 616.34
Episode length: 33.80 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 209      |
|    time_elapsed    | 737      |
|    total_timesteps | 214016   |
---------------------------------
Eval num_timesteps=214500, episode_reward=737.88 +/- 641.25
Episode length: 34.76 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 738       |
| time/                   |           |
|    total_timesteps      | 214500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.21e-25 |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.12e+04  |
|    n_updates            | 2025      |
|    policy_gradient_loss | -1.98e-09 |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=215000, episode_reward=687.77 +/- 546.59
Episode length: 34.98 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 210      |
|    time_elapsed    | 740      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=648.48 +/- 533.67
Episode length: 34.36 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 648       |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.25e-27 |
|    explained_variance   | 0.0169    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.11e+04  |
|    n_updates            | 2035      |
|    policy_gradient_loss | 2.57e-09  |
|    value_loss           | 1.19e+05  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=893.09 +/- 711.99
Episode length: 35.64 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 211      |
|    time_elapsed    | 744      |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=785.83 +/- 691.80
Episode length: 34.34 +/- 7.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 216500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.43e-38 |
|    explained_variance   | 0.0224    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.24e+04  |
|    n_updates            | 2045      |
|    policy_gradient_loss | -9.43e-10 |
|    value_loss           | 9.62e+04  |
---------------------------------------
Eval num_timesteps=217000, episode_reward=913.57 +/- 736.92
Episode length: 35.48 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 212      |
|    time_elapsed    | 747      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=970.49 +/- 714.98
Episode length: 36.54 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 970       |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-26 |
|    explained_variance   | 0.0225    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.78e+04  |
|    n_updates            | 2055      |
|    policy_gradient_loss | 2.33e-10  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=218000, episode_reward=905.03 +/- 724.15
Episode length: 35.54 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 213      |
|    time_elapsed    | 751      |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=794.72 +/- 627.12
Episode length: 35.56 +/- 5.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 795       |
| time/                   |           |
|    total_timesteps      | 218500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.47e-28 |
|    explained_variance   | 0.0277    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.57e+04  |
|    n_updates            | 2065      |
|    policy_gradient_loss | -1.69e-09 |
|    value_loss           | 8.61e+04  |
---------------------------------------
Eval num_timesteps=219000, episode_reward=726.82 +/- 614.83
Episode length: 34.74 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 214      |
|    time_elapsed    | 754      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=1082.01 +/- 723.44
Episode length: 37.42 +/- 4.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.4      |
|    mean_reward          | 1.08e+03  |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.86e-28 |
|    explained_variance   | 0.0259    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.06e+04  |
|    n_updates            | 2075      |
|    policy_gradient_loss | 2.33e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=220000, episode_reward=846.84 +/- 725.58
Episode length: 35.14 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 215      |
|    time_elapsed    | 758      |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=869.19 +/- 739.11
Episode length: 34.72 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 869       |
| time/                   |           |
|    total_timesteps      | 220500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.08e-30 |
|    explained_variance   | 0.00931   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.21e+04  |
|    n_updates            | 2085      |
|    policy_gradient_loss | 2.98e-09  |
|    value_loss           | 9.38e+04  |
---------------------------------------
Eval num_timesteps=221000, episode_reward=988.68 +/- 732.21
Episode length: 35.94 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 216      |
|    time_elapsed    | 761      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=923.83 +/- 719.66
Episode length: 36.20 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 924       |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.52e-28 |
|    explained_variance   | 0.0195    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.17e+04  |
|    n_updates            | 2095      |
|    policy_gradient_loss | 3.03e-10  |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=862.77 +/- 717.89
Episode length: 35.30 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 217      |
|    time_elapsed    | 765      |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=821.87 +/- 722.47
Episode length: 34.10 +/- 7.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 822       |
| time/                   |           |
|    total_timesteps      | 222500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.57e-19 |
|    explained_variance   | 0.0382    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.12e+04  |
|    n_updates            | 2105      |
|    policy_gradient_loss | 3.67e-10  |
|    value_loss           | 6.51e+04  |
---------------------------------------
Eval num_timesteps=223000, episode_reward=909.18 +/- 687.21
Episode length: 36.02 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 218      |
|    time_elapsed    | 768      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=847.07 +/- 620.67
Episode length: 36.44 +/- 5.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 847       |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.91e-20 |
|    explained_variance   | 0.000946  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.22e+04  |
|    n_updates            | 2115      |
|    policy_gradient_loss | -2.26e-09 |
|    value_loss           | 9.94e+04  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=808.63 +/- 643.91
Episode length: 34.96 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 219      |
|    time_elapsed    | 772      |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=683.29 +/- 579.31
Episode length: 34.26 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 683       |
| time/                   |           |
|    total_timesteps      | 224500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.79e-30 |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.57e+04  |
|    n_updates            | 2125      |
|    policy_gradient_loss | -1.4e-09  |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=225000, episode_reward=982.29 +/- 714.51
Episode length: 36.62 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 220      |
|    time_elapsed    | 775      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=897.52 +/- 688.80
Episode length: 35.36 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 898       |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.34e-28 |
|    explained_variance   | 0.0375    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.91e+04  |
|    n_updates            | 2135      |
|    policy_gradient_loss | 4.6e-09   |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=660.53 +/- 583.49
Episode length: 33.84 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 729      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 221      |
|    time_elapsed    | 779      |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=768.45 +/- 637.84
Episode length: 35.12 +/- 5.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 226500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.05e-29 |
|    explained_variance   | 0.0398    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.24e+04  |
|    n_updates            | 2145      |
|    policy_gradient_loss | 1.86e-10  |
|    value_loss           | 7.51e+04  |
---------------------------------------
Eval num_timesteps=227000, episode_reward=820.62 +/- 633.86
Episode length: 35.26 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 222      |
|    time_elapsed    | 782      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=843.10 +/- 714.61
Episode length: 34.44 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 843       |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.44e-35 |
|    explained_variance   | 0.025     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.1e+04   |
|    n_updates            | 2155      |
|    policy_gradient_loss | 2.15e-10  |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=874.73 +/- 705.98
Episode length: 35.82 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 223      |
|    time_elapsed    | 786      |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=771.76 +/- 644.23
Episode length: 34.70 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 772       |
| time/                   |           |
|    total_timesteps      | 228500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.63e-33 |
|    explained_variance   | 0.0661    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.35e+04  |
|    n_updates            | 2165      |
|    policy_gradient_loss | -1.21e-09 |
|    value_loss           | 1.27e+05  |
---------------------------------------
Eval num_timesteps=229000, episode_reward=715.09 +/- 657.05
Episode length: 33.56 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 986      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 224      |
|    time_elapsed    | 789      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=831.46 +/- 646.04
Episode length: 35.32 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-34 |
|    explained_variance   | 0.00873   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.62e+04  |
|    n_updates            | 2175      |
|    policy_gradient_loss | 3.17e-09  |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=735.64 +/- 683.24
Episode length: 34.02 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37       |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 290      |
|    iterations      | 225      |
|    time_elapsed    | 793      |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=816.71 +/- 643.38
Episode length: 35.90 +/- 5.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 230500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.27e-33 |
|    explained_variance   | 0.061     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.01e+04  |
|    n_updates            | 2185      |
|    policy_gradient_loss | 1.36e-09  |
|    value_loss           | 1.27e+05  |
---------------------------------------
Eval num_timesteps=231000, episode_reward=845.22 +/- 664.60
Episode length: 36.28 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 879      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 226      |
|    time_elapsed    | 796      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=952.26 +/- 730.54
Episode length: 36.18 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 952       |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.47e-22 |
|    explained_variance   | 0.0337    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.28e+04  |
|    n_updates            | 2195      |
|    policy_gradient_loss | 1.52e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=232000, episode_reward=777.07 +/- 701.08
Episode length: 34.10 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 227      |
|    time_elapsed    | 799      |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=735.47 +/- 719.73
Episode length: 33.32 +/- 8.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 735       |
| time/                   |           |
|    total_timesteps      | 232500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.14e-23 |
|    explained_variance   | 0.0341    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.96e+04  |
|    n_updates            | 2205      |
|    policy_gradient_loss | 6e-09     |
|    value_loss           | 9.55e+04  |
---------------------------------------
Eval num_timesteps=233000, episode_reward=866.10 +/- 658.59
Episode length: 36.28 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 786      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 228      |
|    time_elapsed    | 803      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=770.64 +/- 652.94
Episode length: 34.56 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 771       |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-34 |
|    explained_variance   | 0.00691   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.27e+04  |
|    n_updates            | 2215      |
|    policy_gradient_loss | -1.13e-09 |
|    value_loss           | 9.95e+04  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=838.63 +/- 677.06
Episode length: 35.74 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 229      |
|    time_elapsed    | 806      |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=896.26 +/- 738.46
Episode length: 35.00 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 896       |
| time/                   |           |
|    total_timesteps      | 234500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.36e-32 |
|    explained_variance   | 0.0416    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.43e+04  |
|    n_updates            | 2225      |
|    policy_gradient_loss | -1.12e-09 |
|    value_loss           | 1.37e+05  |
---------------------------------------
Eval num_timesteps=235000, episode_reward=981.61 +/- 739.43
Episode length: 36.50 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=685.48 +/- 625.63
Episode length: 33.38 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 230      |
|    time_elapsed    | 811      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=837.83 +/- 629.97
Episode length: 36.28 +/- 5.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-33 |
|    explained_variance   | 0.0372    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.28e+04  |
|    n_updates            | 2235      |
|    policy_gradient_loss | 4.07e-10  |
|    value_loss           | 8.87e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=848.56 +/- 672.46
Episode length: 35.42 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 231      |
|    time_elapsed    | 815      |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=823.25 +/- 742.56
Episode length: 33.98 +/- 7.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 823       |
| time/                   |           |
|    total_timesteps      | 237000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.71e-29 |
|    explained_variance   | 0.0217    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.92e+04  |
|    n_updates            | 2245      |
|    policy_gradient_loss | -6.81e-10 |
|    value_loss           | 1.35e+05  |
---------------------------------------
Eval num_timesteps=237500, episode_reward=818.93 +/- 681.31
Episode length: 35.34 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 232      |
|    time_elapsed    | 818      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=781.58 +/- 688.24
Episode length: 34.78 +/- 7.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.36e-18 |
|    explained_variance   | 0.0343    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.89e+04  |
|    n_updates            | 2255      |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 9.66e+04  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=731.62 +/- 694.09
Episode length: 33.82 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 233      |
|    time_elapsed    | 821      |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=948.35 +/- 738.96
Episode length: 35.70 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 948       |
| time/                   |           |
|    total_timesteps      | 239000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-18 |
|    explained_variance   | 0.0016    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.44e+04  |
|    n_updates            | 2265      |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 1.58e+05  |
---------------------------------------
Eval num_timesteps=239500, episode_reward=786.00 +/- 682.94
Episode length: 35.10 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 888      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 234      |
|    time_elapsed    | 825      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=734.43 +/- 667.47
Episode length: 33.92 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.53e-32 |
|    explained_variance   | 0.0301    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.22e+04  |
|    n_updates            | 2275      |
|    policy_gradient_loss | -2.17e-09 |
|    value_loss           | 9.48e+04  |
---------------------------------------
Eval num_timesteps=240500, episode_reward=770.34 +/- 682.89
Episode length: 34.24 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 235      |
|    time_elapsed    | 828      |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=845.60 +/- 692.00
Episode length: 35.02 +/- 7.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 846       |
| time/                   |           |
|    total_timesteps      | 241000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.59e-30 |
|    explained_variance   | 0.054     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.83e+04  |
|    n_updates            | 2285      |
|    policy_gradient_loss | 2.99e-09  |
|    value_loss           | 1.19e+05  |
---------------------------------------
Eval num_timesteps=241500, episode_reward=736.41 +/- 617.57
Episode length: 34.94 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 236      |
|    time_elapsed    | 832      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=778.99 +/- 615.52
Episode length: 35.54 +/- 5.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 779       |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.44e-32 |
|    explained_variance   | -0.000569 |
|    learning_rate        | 0.0005    |
|    loss                 | 5.54e+04  |
|    n_updates            | 2295      |
|    policy_gradient_loss | 1.86e-10  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=873.90 +/- 729.56
Episode length: 34.66 +/- 7.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 237      |
|    time_elapsed    | 835      |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=818.57 +/- 682.37
Episode length: 34.70 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 819       |
| time/                   |           |
|    total_timesteps      | 243000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.06e-30 |
|    explained_variance   | 0.0595    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.47e+04  |
|    n_updates            | 2305      |
|    policy_gradient_loss | 2.36e-09  |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=243500, episode_reward=668.60 +/- 562.94
Episode length: 34.06 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 238      |
|    time_elapsed    | 839      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=800.16 +/- 661.89
Episode length: 34.90 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 800       |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.44e-32 |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.0005    |
|    loss                 | 6.43e+04  |
|    n_updates            | 2315      |
|    policy_gradient_loss | 2.19e-09  |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=622.03 +/- 544.64
Episode length: 33.76 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 933      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 239      |
|    time_elapsed    | 842      |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=875.49 +/- 758.14
Episode length: 34.62 +/- 7.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 245000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-29 |
|    explained_variance   | 0.0283    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.88e+04  |
|    n_updates            | 2325      |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 1.12e+05  |
---------------------------------------
Eval num_timesteps=245500, episode_reward=789.20 +/- 581.68
Episode length: 35.90 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37       |
|    ep_rew_mean     | 987      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 240      |
|    time_elapsed    | 845      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=847.15 +/- 696.30
Episode length: 35.14 +/- 5.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 847       |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-30 |
|    explained_variance   | 0.0385    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.29e+04  |
|    n_updates            | 2335      |
|    policy_gradient_loss | 2.77e-09  |
|    value_loss           | 9.7e+04   |
---------------------------------------
Eval num_timesteps=246500, episode_reward=719.20 +/- 603.47
Episode length: 34.30 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 955      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 241      |
|    time_elapsed    | 849      |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=881.94 +/- 731.97
Episode length: 35.32 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 882       |
| time/                   |           |
|    total_timesteps      | 247000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-28 |
|    explained_variance   | 0.0673    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.48e+04  |
|    n_updates            | 2345      |
|    policy_gradient_loss | 1.92e-09  |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=247500, episode_reward=776.90 +/- 636.74
Episode length: 35.48 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 242      |
|    time_elapsed    | 852      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=873.38 +/- 682.40
Episode length: 36.28 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 873       |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.98e-31 |
|    explained_variance   | 0.0404    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.8e+04   |
|    n_updates            | 2355      |
|    policy_gradient_loss | 6.4e-10   |
|    value_loss           | 9.49e+04  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=701.40 +/- 617.83
Episode length: 33.96 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 738      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 243      |
|    time_elapsed    | 856      |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=780.88 +/- 657.11
Episode length: 34.86 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 781       |
| time/                   |           |
|    total_timesteps      | 249000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-41 |
|    explained_variance   | 0.0348    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.82e+04  |
|    n_updates            | 2365      |
|    policy_gradient_loss | 1.58e-09  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=249500, episode_reward=898.89 +/- 690.03
Episode length: 36.32 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 244      |
|    time_elapsed    | 859      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=822.23 +/- 665.25
Episode length: 35.86 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 822       |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.37e-30 |
|    explained_variance   | 0.0558    |
|    learning_rate        | 0.0005    |
|    loss                 | 8.56e+04  |
|    n_updates            | 2375      |
|    policy_gradient_loss | -1.56e-09 |
|    value_loss           | 1.58e+05  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=787.62 +/- 671.59
Episode length: 34.88 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 245      |
|    time_elapsed    | 863      |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=730.84 +/- 683.62
Episode length: 33.54 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 731       |
| time/                   |           |
|    total_timesteps      | 251000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.29e-16 |
|    explained_variance   | 0.0904    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.42e+04  |
|    n_updates            | 2385      |
|    policy_gradient_loss | 8.44e-10  |
|    value_loss           | 8.01e+04  |
---------------------------------------
Eval num_timesteps=251500, episode_reward=671.85 +/- 609.03
Episode length: 34.02 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 246      |
|    time_elapsed    | 866      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=942.13 +/- 736.67
Episode length: 36.04 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 942       |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.46e-18 |
|    explained_variance   | -0.00241  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.7e+04   |
|    n_updates            | 2395      |
|    policy_gradient_loss | 3.88e-09  |
|    value_loss           | 1.3e+05   |
---------------------------------------
Eval num_timesteps=252500, episode_reward=851.17 +/- 638.13
Episode length: 36.22 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 247      |
|    time_elapsed    | 870      |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=695.76 +/- 610.31
Episode length: 34.08 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 696       |
| time/                   |           |
|    total_timesteps      | 253000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-31 |
|    explained_variance   | 0.0236    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.58e+04  |
|    n_updates            | 2405      |
|    policy_gradient_loss | -1.9e-09  |
|    value_loss           | 9.24e+04  |
---------------------------------------
Eval num_timesteps=253500, episode_reward=980.10 +/- 695.19
Episode length: 37.02 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 248      |
|    time_elapsed    | 873      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=884.68 +/- 667.31
Episode length: 36.02 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 885       |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-29 |
|    explained_variance   | 0.0592    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.97e+04  |
|    n_updates            | 2415      |
|    policy_gradient_loss | 8.91e-10  |
|    value_loss           | 1.41e+05  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=767.98 +/- 664.04
Episode length: 34.42 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 249      |
|    time_elapsed    | 877      |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=753.00 +/- 708.74
Episode length: 33.80 +/- 8.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 753       |
| time/                   |           |
|    total_timesteps      | 255000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-18 |
|    explained_variance   | 0.0336    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.22e+04  |
|    n_updates            | 2425      |
|    policy_gradient_loss | -1.16e-10 |
|    value_loss           | 8.71e+04  |
---------------------------------------
Eval num_timesteps=255500, episode_reward=1060.99 +/- 729.01
Episode length: 37.30 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=1007.01 +/- 734.51
Episode length: 36.82 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 250      |
|    time_elapsed    | 881      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=875.20 +/- 684.08
Episode length: 35.56 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.48e-21 |
|    explained_variance   | 0.0138    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.84e+04  |
|    n_updates            | 2435      |
|    policy_gradient_loss | 2.39e-10  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=257000, episode_reward=776.54 +/- 642.92
Episode length: 34.90 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 251      |
|    time_elapsed    | 885      |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=1180.39 +/- 749.65
Episode length: 37.38 +/- 5.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.4      |
|    mean_reward          | 1.18e+03  |
| time/                   |           |
|    total_timesteps      | 257500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-31 |
|    explained_variance   | 0.0284    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.42e+04  |
|    n_updates            | 2445      |
|    policy_gradient_loss | 9.9e-11   |
|    value_loss           | 1.11e+05  |
---------------------------------------
New best mean reward!
Eval num_timesteps=258000, episode_reward=844.29 +/- 656.39
Episode length: 36.04 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 786      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 252      |
|    time_elapsed    | 888      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=771.84 +/- 674.60
Episode length: 34.90 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 772       |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-29 |
|    explained_variance   | 0.0898    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.71e+04  |
|    n_updates            | 2455      |
|    policy_gradient_loss | -3.99e-09 |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=743.84 +/- 667.40
Episode length: 33.94 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 253      |
|    time_elapsed    | 892      |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=897.15 +/- 692.54
Episode length: 36.18 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 897       |
| time/                   |           |
|    total_timesteps      | 259500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.77e-31 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.51e+04  |
|    n_updates            | 2465      |
|    policy_gradient_loss | -7.45e-10 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=260000, episode_reward=904.60 +/- 720.30
Episode length: 35.92 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 254      |
|    time_elapsed    | 895      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=887.69 +/- 721.85
Episode length: 35.36 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 888       |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.2e-30  |
|    explained_variance   | 0.0678    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.25e+04  |
|    n_updates            | 2475      |
|    policy_gradient_loss | -3.02e-09 |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=812.63 +/- 666.89
Episode length: 35.10 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 255      |
|    time_elapsed    | 899      |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=1100.09 +/- 713.67
Episode length: 37.84 +/- 5.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.8      |
|    mean_reward          | 1.1e+03   |
| time/                   |           |
|    total_timesteps      | 261500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.56e-20 |
|    explained_variance   | 0.0416    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.42e+04  |
|    n_updates            | 2485      |
|    policy_gradient_loss | -7.57e-10 |
|    value_loss           | 9.27e+04  |
---------------------------------------
Eval num_timesteps=262000, episode_reward=996.33 +/- 806.97
Episode length: 35.38 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 724      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 256      |
|    time_elapsed    | 902      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=766.03 +/- 657.45
Episode length: 34.26 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 766       |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.29e-13 |
|    explained_variance   | 0.0362    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.1e+04   |
|    n_updates            | 2495      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 6.81e+04  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=849.94 +/- 731.71
Episode length: 34.36 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 257      |
|    time_elapsed    | 906      |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=906.64 +/- 672.32
Episode length: 36.50 +/- 5.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 907       |
| time/                   |           |
|    total_timesteps      | 263500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.05e-14 |
|    explained_variance   | -0.0129   |
|    learning_rate        | 0.0005    |
|    loss                 | 7.98e+04  |
|    n_updates            | 2505      |
|    policy_gradient_loss | -3.38e-09 |
|    value_loss           | 1.44e+05  |
---------------------------------------
Eval num_timesteps=264000, episode_reward=817.54 +/- 669.88
Episode length: 35.24 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 258      |
|    time_elapsed    | 909      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=783.31 +/- 693.64
Episode length: 34.16 +/- 7.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 783       |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.02e-24 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.68e+04  |
|    n_updates            | 2515      |
|    policy_gradient_loss | -6.23e-10 |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=932.27 +/- 702.94
Episode length: 36.38 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 922      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 259      |
|    time_elapsed    | 913      |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=784.12 +/- 649.74
Episode length: 35.30 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 265500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.88e-34 |
|    explained_variance   | 0.0355    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.83e+04  |
|    n_updates            | 2525      |
|    policy_gradient_loss | 2.11e-09  |
|    value_loss           | 9.98e+04  |
---------------------------------------
Eval num_timesteps=266000, episode_reward=689.48 +/- 581.12
Episode length: 34.46 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 878      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 260      |
|    time_elapsed    | 916      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=923.71 +/- 704.20
Episode length: 36.70 +/- 7.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 924       |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.33e-32 |
|    explained_variance   | 0.0855    |
|    learning_rate        | 0.0005    |
|    loss                 | 9.17e+04  |
|    n_updates            | 2535      |
|    policy_gradient_loss | -1.57e-09 |
|    value_loss           | 1.72e+05  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=908.10 +/- 682.18
Episode length: 36.40 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 261      |
|    time_elapsed    | 920      |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=806.24 +/- 599.09
Episode length: 36.54 +/- 5.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 806       |
| time/                   |           |
|    total_timesteps      | 267500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.59e-21 |
|    explained_variance   | 0.0644    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.36e+04  |
|    n_updates            | 2545      |
|    policy_gradient_loss | 1.05e-09  |
|    value_loss           | 8.08e+04  |
---------------------------------------
Eval num_timesteps=268000, episode_reward=922.67 +/- 723.73
Episode length: 36.12 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 262      |
|    time_elapsed    | 923      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=755.02 +/- 686.85
Episode length: 33.32 +/- 7.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-23 |
|    explained_variance   | 0.0171    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.69e+04  |
|    n_updates            | 2555      |
|    policy_gradient_loss | 3.68e-09  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=751.22 +/- 590.44
Episode length: 35.84 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 263      |
|    time_elapsed    | 927      |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=1036.59 +/- 751.13
Episode length: 36.86 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 1.04e+03  |
| time/                   |           |
|    total_timesteps      | 269500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.26e-32 |
|    explained_variance   | 0.0305    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.84e+04  |
|    n_updates            | 2565      |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=270000, episode_reward=720.27 +/- 637.82
Episode length: 33.94 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 264      |
|    time_elapsed    | 930      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=886.04 +/- 693.83
Episode length: 35.52 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 886       |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-29 |
|    explained_variance   | 0.135     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.06e+04  |
|    n_updates            | 2575      |
|    policy_gradient_loss | -2.51e-09 |
|    value_loss           | 1.45e+05  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=750.06 +/- 611.21
Episode length: 35.54 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 265      |
|    time_elapsed    | 934      |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=838.27 +/- 697.21
Episode length: 35.00 +/- 7.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 271500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.89e-20 |
|    explained_variance   | 0.0405    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.92e+04  |
|    n_updates            | 2585      |
|    policy_gradient_loss | -2.76e-09 |
|    value_loss           | 9.44e+04  |
---------------------------------------
Eval num_timesteps=272000, episode_reward=1032.82 +/- 758.41
Episode length: 36.94 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 745      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 266      |
|    time_elapsed    | 938      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=649.59 +/- 563.78
Episode length: 34.08 +/- 7.44
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.1     |
|    mean_reward          | 650      |
| time/                   |          |
|    total_timesteps      | 272500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.3e-21 |
|    explained_variance   | 0.0339   |
|    learning_rate        | 0.0005   |
|    loss                 | 3.96e+04 |
|    n_updates            | 2595     |
|    policy_gradient_loss | 8.5e-10  |
|    value_loss           | 8.36e+04 |
--------------------------------------
Eval num_timesteps=273000, episode_reward=810.69 +/- 690.61
Episode length: 34.30 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 267      |
|    time_elapsed    | 941      |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=715.93 +/- 616.91
Episode length: 34.70 +/- 6.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 716       |
| time/                   |           |
|    total_timesteps      | 273500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.33e-20 |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.57e+04  |
|    n_updates            | 2605      |
|    policy_gradient_loss | 4.54e-10  |
|    value_loss           | 9.07e+04  |
---------------------------------------
Eval num_timesteps=274000, episode_reward=748.48 +/- 717.25
Episode length: 33.86 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 715      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 268      |
|    time_elapsed    | 944      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=983.51 +/- 705.54
Episode length: 36.92 +/- 5.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 984       |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.69e-22 |
|    explained_variance   | 0.0743    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.4e+04   |
|    n_updates            | 2615      |
|    policy_gradient_loss | 1.05e-10  |
|    value_loss           | 6.65e+04  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=859.15 +/- 732.57
Episode length: 34.86 +/- 8.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 722      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 269      |
|    time_elapsed    | 948      |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=795.97 +/- 648.67
Episode length: 34.62 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 796       |
| time/                   |           |
|    total_timesteps      | 275500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.47e-20 |
|    explained_variance   | 0.0393    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.73e+04  |
|    n_updates            | 2625      |
|    policy_gradient_loss | 3.96e-10  |
|    value_loss           | 9.85e+04  |
---------------------------------------
Eval num_timesteps=276000, episode_reward=922.32 +/- 733.19
Episode length: 35.74 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 270      |
|    time_elapsed    | 951      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=814.06 +/- 618.79
Episode length: 35.82 +/- 5.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.37e-21 |
|    explained_variance   | 0.038     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.13e+04  |
|    n_updates            | 2635      |
|    policy_gradient_loss | -1.16e-09 |
|    value_loss           | 8.85e+04  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=860.10 +/- 702.82
Episode length: 35.50 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=866.99 +/- 704.98
Episode length: 35.42 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 271      |
|    time_elapsed    | 956      |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=1001.33 +/- 770.55
Episode length: 36.22 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 278000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.81e-20 |
|    explained_variance   | 0.0696    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.67e+04  |
|    n_updates            | 2645      |
|    policy_gradient_loss | 8.61e-10  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=278500, episode_reward=770.63 +/- 734.69
Episode length: 33.40 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 272      |
|    time_elapsed    | 960      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=572.14 +/- 496.85
Episode length: 33.02 +/- 6.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33        |
|    mean_reward          | 572       |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.3e-22  |
|    explained_variance   | 0.055     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.03e+04  |
|    n_updates            | 2655      |
|    policy_gradient_loss | -2.97e-10 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=929.42 +/- 743.24
Episode length: 35.68 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 273      |
|    time_elapsed    | 963      |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=827.49 +/- 727.79
Episode length: 34.54 +/- 7.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 280000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-33 |
|    explained_variance   | 0.0408    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.39e+04  |
|    n_updates            | 2665      |
|    policy_gradient_loss | 1.87e-09  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=280500, episode_reward=774.17 +/- 661.80
Episode length: 34.84 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 904      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 274      |
|    time_elapsed    | 966      |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=873.73 +/- 697.96
Episode length: 36.08 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.54e-29 |
|    explained_variance   | 0.0887    |
|    learning_rate        | 0.0005    |
|    loss                 | 9.27e+04  |
|    n_updates            | 2675      |
|    policy_gradient_loss | -1.74e-09 |
|    value_loss           | 1.59e+05  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=769.49 +/- 621.83
Episode length: 35.64 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 970      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 275      |
|    time_elapsed    | 970      |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=816.45 +/- 703.89
Episode length: 34.80 +/- 6.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 282000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35e-29 |
|    explained_variance   | 0.0508    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.99e+04  |
|    n_updates            | 2685      |
|    policy_gradient_loss | -1.6e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=282500, episode_reward=756.71 +/- 673.87
Episode length: 34.50 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 985      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 276      |
|    time_elapsed    | 973      |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=924.21 +/- 718.34
Episode length: 35.86 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 924       |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.98e-27 |
|    explained_variance   | 0.104     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.19e+04  |
|    n_updates            | 2695      |
|    policy_gradient_loss | -1.75e-09 |
|    value_loss           | 1.24e+05  |
---------------------------------------
Eval num_timesteps=283500, episode_reward=760.92 +/- 615.25
Episode length: 35.44 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 913      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 277      |
|    time_elapsed    | 977      |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=654.11 +/- 593.64
Episode length: 33.62 +/- 7.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 654       |
| time/                   |           |
|    total_timesteps      | 284000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.54e-18 |
|    explained_variance   | 0.101     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.26e+04  |
|    n_updates            | 2705      |
|    policy_gradient_loss | -1.25e-09 |
|    value_loss           | 8.86e+04  |
---------------------------------------
Eval num_timesteps=284500, episode_reward=941.76 +/- 718.65
Episode length: 35.94 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 278      |
|    time_elapsed    | 980      |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=1026.87 +/- 696.08
Episode length: 37.32 +/- 5.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.3      |
|    mean_reward          | 1.03e+03  |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.83e-20 |
|    explained_variance   | 0.0646    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.98e+04  |
|    n_updates            | 2715      |
|    policy_gradient_loss | 3.67e-10  |
|    value_loss           | 8.88e+04  |
---------------------------------------
Eval num_timesteps=285500, episode_reward=802.32 +/- 682.05
Episode length: 34.58 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 279      |
|    time_elapsed    | 984      |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=921.17 +/- 695.85
Episode length: 36.58 +/- 5.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 921       |
| time/                   |           |
|    total_timesteps      | 286000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.03e-18 |
|    explained_variance   | 0.0397    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.92e+04  |
|    n_updates            | 2725      |
|    policy_gradient_loss | -1.16e-10 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=286500, episode_reward=764.18 +/- 652.72
Episode length: 34.16 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 280      |
|    time_elapsed    | 987      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=851.72 +/- 701.84
Episode length: 35.14 +/- 7.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.63e-21 |
|    explained_variance   | 0.0422    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.63e+04  |
|    n_updates            | 2735      |
|    policy_gradient_loss | 4.07e-10  |
|    value_loss           | 9.07e+04  |
---------------------------------------
Eval num_timesteps=287500, episode_reward=947.53 +/- 739.87
Episode length: 35.84 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 281      |
|    time_elapsed    | 991      |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=977.13 +/- 759.01
Episode length: 36.18 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 977       |
| time/                   |           |
|    total_timesteps      | 288000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.66e-19 |
|    explained_variance   | 0.0765    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.21e+04  |
|    n_updates            | 2745      |
|    policy_gradient_loss | -1.86e-09 |
|    value_loss           | 9.74e+04  |
---------------------------------------
Eval num_timesteps=288500, episode_reward=846.86 +/- 645.42
Episode length: 35.68 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 282      |
|    time_elapsed    | 994      |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=772.66 +/- 691.11
Episode length: 33.96 +/- 7.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.2e-20  |
|    explained_variance   | 0.0494    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.66e+04  |
|    n_updates            | 2755      |
|    policy_gradient_loss | -1.08e-09 |
|    value_loss           | 9.91e+04  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=774.81 +/- 656.79
Episode length: 34.78 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 927      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 283      |
|    time_elapsed    | 998      |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=876.46 +/- 764.84
Episode length: 34.44 +/- 7.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 876       |
| time/                   |           |
|    total_timesteps      | 290000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.05e-29 |
|    explained_variance   | 0.0571    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.43e+04  |
|    n_updates            | 2765      |
|    policy_gradient_loss | 1.39e-09  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=290500, episode_reward=742.52 +/- 646.37
Episode length: 34.42 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 994      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 284      |
|    time_elapsed    | 1001     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=862.41 +/- 662.22
Episode length: 35.96 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 862       |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.42e-25 |
|    explained_variance   | 0.0856    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.4e+04   |
|    n_updates            | 2775      |
|    policy_gradient_loss | 9.31e-10  |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=291500, episode_reward=867.19 +/- 697.32
Episode length: 35.58 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 972      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 285      |
|    time_elapsed    | 1005     |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=708.96 +/- 607.93
Episode length: 34.46 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 709       |
| time/                   |           |
|    total_timesteps      | 292000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.62e-17 |
|    explained_variance   | 0.0411    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.55e+04  |
|    n_updates            | 2785      |
|    policy_gradient_loss | -1.13e-09 |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=292500, episode_reward=654.09 +/- 558.81
Episode length: 34.32 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 286      |
|    time_elapsed    | 1008     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=807.95 +/- 612.47
Episode length: 35.60 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 808       |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.43e-21 |
|    explained_variance   | 0.0557    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.72e+04  |
|    n_updates            | 2795      |
|    policy_gradient_loss | -9.26e-10 |
|    value_loss           | 7.04e+04  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=738.40 +/- 605.94
Episode length: 35.04 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 287      |
|    time_elapsed    | 1012     |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=751.29 +/- 634.21
Episode length: 35.06 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 751       |
| time/                   |           |
|    total_timesteps      | 294000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.36e-20 |
|    explained_variance   | 0.0891    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.48e+04  |
|    n_updates            | 2805      |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 9.5e+04   |
---------------------------------------
Eval num_timesteps=294500, episode_reward=840.92 +/- 667.29
Episode length: 35.44 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 288      |
|    time_elapsed    | 1015     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=862.31 +/- 701.80
Episode length: 35.42 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 862       |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-21 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.33e+04  |
|    n_updates            | 2815      |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=295500, episode_reward=761.31 +/- 632.38
Episode length: 35.02 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 289      |
|    time_elapsed    | 1019     |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=920.88 +/- 706.62
Episode length: 36.62 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 921       |
| time/                   |           |
|    total_timesteps      | 296000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.77e-18 |
|    explained_variance   | 0.0868    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.14e+04  |
|    n_updates            | 2825      |
|    policy_gradient_loss | 9.08e-10  |
|    value_loss           | 9.59e+04  |
---------------------------------------
Eval num_timesteps=296500, episode_reward=881.36 +/- 680.40
Episode length: 35.96 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 290      |
|    time_elapsed    | 1022     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=823.03 +/- 652.55
Episode length: 35.46 +/- 5.82
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.5     |
|    mean_reward          | 823      |
| time/                   |          |
|    total_timesteps      | 297000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.6e-20 |
|    explained_variance   | 0.0942   |
|    learning_rate        | 0.0005   |
|    loss                 | 2.95e+04 |
|    n_updates            | 2835     |
|    policy_gradient_loss | 1.96e-09 |
|    value_loss           | 7.28e+04 |
--------------------------------------
Eval num_timesteps=297500, episode_reward=778.08 +/- 645.90
Episode length: 34.78 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 291      |
|    time_elapsed    | 1026     |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=858.49 +/- 651.49
Episode length: 36.30 +/- 5.76
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.3     |
|    mean_reward          | 858      |
| time/                   |          |
|    total_timesteps      | 298000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.9e-18 |
|    explained_variance   | 0.0892   |
|    learning_rate        | 0.0005   |
|    loss                 | 4.34e+04 |
|    n_updates            | 2845     |
|    policy_gradient_loss | 1.26e-09 |
|    value_loss           | 9.63e+04 |
--------------------------------------
Eval num_timesteps=298500, episode_reward=859.78 +/- 721.80
Episode length: 35.34 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=643.23 +/- 617.79
Episode length: 33.10 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 643      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 292      |
|    time_elapsed    | 1030     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=842.69 +/- 704.79
Episode length: 34.92 +/- 6.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 843       |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-20 |
|    explained_variance   | 0.0762    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.58e+04  |
|    n_updates            | 2855      |
|    policy_gradient_loss | -8.79e-10 |
|    value_loss           | 9.13e+04  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=686.67 +/- 654.07
Episode length: 33.52 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 293      |
|    time_elapsed    | 1034     |
|    total_timesteps | 300032   |
---------------------------------
Eval num_timesteps=300500, episode_reward=920.87 +/- 731.19
Episode length: 35.98 +/- 5.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 921       |
| time/                   |           |
|    total_timesteps      | 300500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.12e-17 |
|    explained_variance   | 0.096     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.23e+04  |
|    n_updates            | 2865      |
|    policy_gradient_loss | -1.16e-11 |
|    value_loss           | 9.59e+04  |
---------------------------------------
Eval num_timesteps=301000, episode_reward=763.65 +/- 660.47
Episode length: 34.52 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 704      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 294      |
|    time_elapsed    | 1037     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=788.73 +/- 671.16
Episode length: 34.72 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-20 |
|    explained_variance   | 0.0635    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.41e+04  |
|    n_updates            | 2875      |
|    policy_gradient_loss | 6.75e-10  |
|    value_loss           | 8.12e+04  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=781.60 +/- 681.86
Episode length: 34.98 +/- 7.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 683      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 295      |
|    time_elapsed    | 1041     |
|    total_timesteps | 302080   |
---------------------------------
Eval num_timesteps=302500, episode_reward=729.48 +/- 562.07
Episode length: 35.40 +/- 5.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 729       |
| time/                   |           |
|    total_timesteps      | 302500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.01e-17 |
|    explained_variance   | 0.103     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.56e+04  |
|    n_updates            | 2885      |
|    policy_gradient_loss | -1.61e-09 |
|    value_loss           | 9.14e+04  |
---------------------------------------
Eval num_timesteps=303000, episode_reward=654.64 +/- 546.18
Episode length: 34.34 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 296      |
|    time_elapsed    | 1044     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=771.08 +/- 636.54
Episode length: 34.84 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 771       |
| time/                   |           |
|    total_timesteps      | 303500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-19 |
|    explained_variance   | 0.0313    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.12e+04  |
|    n_updates            | 2895      |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 8.38e+04  |
---------------------------------------
Eval num_timesteps=304000, episode_reward=811.01 +/- 652.51
Episode length: 35.26 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 745      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 297      |
|    time_elapsed    | 1048     |
|    total_timesteps | 304128   |
---------------------------------
Eval num_timesteps=304500, episode_reward=828.13 +/- 648.08
Episode length: 36.00 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 304500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.68e-20 |
|    explained_variance   | 0.0948    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.83e+04  |
|    n_updates            | 2905      |
|    policy_gradient_loss | 4.07e-11  |
|    value_loss           | 9.02e+04  |
---------------------------------------
Eval num_timesteps=305000, episode_reward=842.78 +/- 721.88
Episode length: 34.66 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 298      |
|    time_elapsed    | 1051     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=892.98 +/- 731.71
Episode length: 35.88 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 893       |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.21e-21 |
|    explained_variance   | 0.128     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.76e+04  |
|    n_updates            | 2915      |
|    policy_gradient_loss | 2.29e-09  |
|    value_loss           | 8.22e+04  |
---------------------------------------
Eval num_timesteps=306000, episode_reward=678.10 +/- 635.75
Episode length: 33.56 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 844      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 299      |
|    time_elapsed    | 1055     |
|    total_timesteps | 306176   |
---------------------------------
Eval num_timesteps=306500, episode_reward=850.41 +/- 740.85
Episode length: 34.66 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 850       |
| time/                   |           |
|    total_timesteps      | 306500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.79e-20 |
|    explained_variance   | 0.1       |
|    learning_rate        | 0.0005    |
|    loss                 | 7e+04     |
|    n_updates            | 2925      |
|    policy_gradient_loss | -2.57e-09 |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=307000, episode_reward=796.84 +/- 632.08
Episode length: 35.58 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 300      |
|    time_elapsed    | 1058     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=783.78 +/- 604.14
Episode length: 35.80 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-19 |
|    explained_variance   | 0.000853  |
|    learning_rate        | 0.0005    |
|    loss                 | 5.43e+04  |
|    n_updates            | 2935      |
|    policy_gradient_loss | -2.88e-09 |
|    value_loss           | 1.12e+05  |
---------------------------------------
Eval num_timesteps=308000, episode_reward=701.95 +/- 602.98
Episode length: 34.74 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 301      |
|    time_elapsed    | 1062     |
|    total_timesteps | 308224   |
---------------------------------
Eval num_timesteps=308500, episode_reward=1087.31 +/- 756.43
Episode length: 36.80 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 1.09e+03  |
| time/                   |           |
|    total_timesteps      | 308500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.16e-20 |
|    explained_variance   | 0.0807    |
|    learning_rate        | 0.0005    |
|    loss                 | 6e+04     |
|    n_updates            | 2945      |
|    policy_gradient_loss | 6.17e-10  |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=309000, episode_reward=868.68 +/- 653.30
Episode length: 35.92 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 302      |
|    time_elapsed    | 1065     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=888.78 +/- 698.76
Episode length: 36.28 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 889       |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-22 |
|    explained_variance   | 0.0195    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.5e+04   |
|    n_updates            | 2955      |
|    policy_gradient_loss | -4.83e-10 |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=827.23 +/- 703.09
Episode length: 35.04 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 303      |
|    time_elapsed    | 1069     |
|    total_timesteps | 310272   |
---------------------------------
Eval num_timesteps=310500, episode_reward=968.40 +/- 708.20
Episode length: 36.78 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 968       |
| time/                   |           |
|    total_timesteps      | 310500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.08e-33 |
|    explained_variance   | 0.0374    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.29e+04  |
|    n_updates            | 2965      |
|    policy_gradient_loss | -6.98e-11 |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=311000, episode_reward=902.21 +/- 680.76
Episode length: 36.18 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 886      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 304      |
|    time_elapsed    | 1072     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=881.06 +/- 654.22
Episode length: 36.46 +/- 5.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 311500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.61e-29 |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.19e+04  |
|    n_updates            | 2975      |
|    policy_gradient_loss | -4.54e-10 |
|    value_loss           | 1.5e+05   |
---------------------------------------
Eval num_timesteps=312000, episode_reward=871.34 +/- 704.86
Episode length: 35.64 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 305      |
|    time_elapsed    | 1076     |
|    total_timesteps | 312320   |
---------------------------------
Eval num_timesteps=312500, episode_reward=829.25 +/- 685.22
Episode length: 35.26 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 312500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.42e-32 |
|    explained_variance   | 0.0531    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.37e+04  |
|    n_updates            | 2985      |
|    policy_gradient_loss | -4.07e-10 |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=313000, episode_reward=672.04 +/- 529.14
Episode length: 35.04 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 306      |
|    time_elapsed    | 1079     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=980.66 +/- 752.47
Episode length: 35.90 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 981       |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.61e-21 |
|    explained_variance   | 0.0928    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.45e+04  |
|    n_updates            | 2995      |
|    policy_gradient_loss | 1.25e-09  |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=1007.19 +/- 754.05
Episode length: 36.56 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 307      |
|    time_elapsed    | 1083     |
|    total_timesteps | 314368   |
---------------------------------
Eval num_timesteps=314500, episode_reward=765.89 +/- 635.23
Episode length: 34.62 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 766       |
| time/                   |           |
|    total_timesteps      | 314500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.41e-11 |
|    explained_variance   | 0.101     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.22e+04  |
|    n_updates            | 3005      |
|    policy_gradient_loss | 2e-09     |
|    value_loss           | 7.37e+04  |
---------------------------------------
Eval num_timesteps=315000, episode_reward=623.15 +/- 506.96
Episode length: 34.24 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 308      |
|    time_elapsed    | 1086     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=935.44 +/- 721.67
Episode length: 35.62 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 935       |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-13 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.31e+04  |
|    n_updates            | 3015      |
|    policy_gradient_loss | -2.6e-09  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=316000, episode_reward=900.95 +/- 698.92
Episode length: 35.86 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 309      |
|    time_elapsed    | 1090     |
|    total_timesteps | 316416   |
---------------------------------
Eval num_timesteps=316500, episode_reward=738.25 +/- 624.96
Episode length: 34.52 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 738       |
| time/                   |           |
|    total_timesteps      | 316500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.12e-12 |
|    explained_variance   | 0.0806    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.88e+04  |
|    n_updates            | 3025      |
|    policy_gradient_loss | -1.7e-09  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=317000, episode_reward=593.56 +/- 517.24
Episode length: 33.52 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 594      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 310      |
|    time_elapsed    | 1093     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=758.33 +/- 668.62
Episode length: 34.22 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.77e-14 |
|    explained_variance   | 0.041     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.8e+04   |
|    n_updates            | 3035      |
|    policy_gradient_loss | 1.07e-09  |
|    value_loss           | 9.4e+04   |
---------------------------------------
Eval num_timesteps=318000, episode_reward=733.94 +/- 572.59
Episode length: 35.32 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 311      |
|    time_elapsed    | 1097     |
|    total_timesteps | 318464   |
---------------------------------
Eval num_timesteps=318500, episode_reward=728.48 +/- 571.77
Episode length: 35.34 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 728       |
| time/                   |           |
|    total_timesteps      | 318500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.76e-24 |
|    explained_variance   | 0.0307    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.02e+04  |
|    n_updates            | 3045      |
|    policy_gradient_loss | 1.34e-09  |
|    value_loss           | 9.69e+04  |
---------------------------------------
Eval num_timesteps=319000, episode_reward=769.94 +/- 659.57
Episode length: 34.98 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 312      |
|    time_elapsed    | 1100     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=784.73 +/- 629.45
Episode length: 35.14 +/- 5.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.33e-16 |
|    explained_variance   | 0.0785    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.71e+04  |
|    n_updates            | 3055      |
|    policy_gradient_loss | 1.66e-09  |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=815.25 +/- 710.74
Episode length: 34.38 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=802.45 +/- 714.81
Episode length: 34.72 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 313      |
|    time_elapsed    | 1105     |
|    total_timesteps | 320512   |
---------------------------------
Eval num_timesteps=321000, episode_reward=715.20 +/- 612.92
Episode length: 34.32 +/- 6.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 715       |
| time/                   |           |
|    total_timesteps      | 321000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.82e-10 |
|    explained_variance   | 0.0816    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.42e+04  |
|    n_updates            | 3065      |
|    policy_gradient_loss | -1.15e-09 |
|    value_loss           | 8.09e+04  |
---------------------------------------
Eval num_timesteps=321500, episode_reward=815.95 +/- 659.62
Episode length: 35.72 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 314      |
|    time_elapsed    | 1108     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=793.81 +/- 629.05
Episode length: 35.72 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 794       |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.08e-12 |
|    explained_variance   | 0.0379    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.44e+04  |
|    n_updates            | 3075      |
|    policy_gradient_loss | 1.85e-09  |
|    value_loss           | 9.42e+04  |
---------------------------------------
Eval num_timesteps=322500, episode_reward=812.20 +/- 669.67
Episode length: 35.06 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 315      |
|    time_elapsed    | 1112     |
|    total_timesteps | 322560   |
---------------------------------
Eval num_timesteps=323000, episode_reward=919.19 +/- 700.99
Episode length: 36.38 +/- 5.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 919       |
| time/                   |           |
|    total_timesteps      | 323000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-22 |
|    explained_variance   | -0.00441  |
|    learning_rate        | 0.0005    |
|    loss                 | 7.46e+04  |
|    n_updates            | 3085      |
|    policy_gradient_loss | -6.75e-10 |
|    value_loss           | 1.38e+05  |
---------------------------------------
Eval num_timesteps=323500, episode_reward=715.20 +/- 582.59
Episode length: 34.18 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 916      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 316      |
|    time_elapsed    | 1115     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=775.00 +/- 633.15
Episode length: 35.32 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 775       |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.91e-18 |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.59e+04  |
|    n_updates            | 3095      |
|    policy_gradient_loss | 3.55e-10  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=855.17 +/- 719.36
Episode length: 35.30 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 988      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 317      |
|    time_elapsed    | 1119     |
|    total_timesteps | 324608   |
---------------------------------
Eval num_timesteps=325000, episode_reward=837.03 +/- 727.21
Episode length: 34.82 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 837       |
| time/                   |           |
|    total_timesteps      | 325000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-20 |
|    explained_variance   | 0.0229    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.03e+04  |
|    n_updates            | 3105      |
|    policy_gradient_loss | 2.05e-09  |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=325500, episode_reward=922.94 +/- 707.70
Episode length: 36.06 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 991      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 318      |
|    time_elapsed    | 1122     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=999.20 +/- 787.57
Episode length: 35.46 +/- 7.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 999       |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.09e-18 |
|    explained_variance   | 0.0656    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.37e+04  |
|    n_updates            | 3115      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=853.05 +/- 670.22
Episode length: 35.42 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 959      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 319      |
|    time_elapsed    | 1126     |
|    total_timesteps | 326656   |
---------------------------------
Eval num_timesteps=327000, episode_reward=701.41 +/- 585.01
Episode length: 35.32 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 327000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.29e-22 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.25e+04  |
|    n_updates            | 3125      |
|    policy_gradient_loss | -2.07e-09 |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=327500, episode_reward=722.23 +/- 671.23
Episode length: 33.46 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 974      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 320      |
|    time_elapsed    | 1129     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=932.67 +/- 756.14
Episode length: 35.82 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 933       |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.5e-26  |
|    explained_variance   | 0.0424    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.1e+04   |
|    n_updates            | 3135      |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 1.28e+05  |
---------------------------------------
Eval num_timesteps=328500, episode_reward=916.56 +/- 684.36
Episode length: 36.68 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 960      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 321      |
|    time_elapsed    | 1133     |
|    total_timesteps | 328704   |
---------------------------------
Eval num_timesteps=329000, episode_reward=814.45 +/- 634.63
Episode length: 35.74 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 329000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.22e-22 |
|    explained_variance   | 0.0151    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.16e+04  |
|    n_updates            | 3145      |
|    policy_gradient_loss | -2.8e-09  |
|    value_loss           | 1.13e+05  |
---------------------------------------
Eval num_timesteps=329500, episode_reward=974.67 +/- 711.44
Episode length: 36.10 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 322      |
|    time_elapsed    | 1136     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=709.84 +/- 574.19
Episode length: 35.28 +/- 5.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 710       |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.89e-15 |
|    explained_variance   | 0.085     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.77e+04  |
|    n_updates            | 3155      |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 8.34e+04  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=948.97 +/- 710.95
Episode length: 36.50 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 323      |
|    time_elapsed    | 1140     |
|    total_timesteps | 330752   |
---------------------------------
Eval num_timesteps=331000, episode_reward=801.29 +/- 596.93
Episode length: 36.24 +/- 5.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 331000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.71e-17 |
|    explained_variance   | 0.034     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.73e+04  |
|    n_updates            | 3165      |
|    policy_gradient_loss | -2.49e-09 |
|    value_loss           | 8.48e+04  |
---------------------------------------
Eval num_timesteps=331500, episode_reward=740.87 +/- 654.46
Episode length: 34.56 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 708      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 324      |
|    time_elapsed    | 1144     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=961.04 +/- 763.42
Episode length: 35.68 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 961       |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-25 |
|    explained_variance   | 0.0504    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.18e+04  |
|    n_updates            | 3175      |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 9.35e+04  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=878.68 +/- 671.84
Episode length: 36.46 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 325      |
|    time_elapsed    | 1147     |
|    total_timesteps | 332800   |
---------------------------------
Eval num_timesteps=333000, episode_reward=616.55 +/- 559.95
Episode length: 33.70 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 617       |
| time/                   |           |
|    total_timesteps      | 333000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.98e-21 |
|    explained_variance   | 0.0937    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.21e+04  |
|    n_updates            | 3185      |
|    policy_gradient_loss | -2.79e-10 |
|    value_loss           | 9.86e+04  |
---------------------------------------
Eval num_timesteps=333500, episode_reward=696.96 +/- 539.41
Episode length: 35.62 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 326      |
|    time_elapsed    | 1150     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=762.26 +/- 627.70
Episode length: 34.82 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 762       |
| time/                   |           |
|    total_timesteps      | 334000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.33e-23 |
|    explained_variance   | 0.0649    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.72e+04  |
|    n_updates            | 3195      |
|    policy_gradient_loss | 2.44e-10  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=334500, episode_reward=759.45 +/- 582.89
Episode length: 35.58 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 327      |
|    time_elapsed    | 1154     |
|    total_timesteps | 334848   |
---------------------------------
Eval num_timesteps=335000, episode_reward=881.55 +/- 687.85
Episode length: 35.90 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 882       |
| time/                   |           |
|    total_timesteps      | 335000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-20 |
|    explained_variance   | 0.0387    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.37e+04  |
|    n_updates            | 3205      |
|    policy_gradient_loss | -2.56e-10 |
|    value_loss           | 1.13e+05  |
---------------------------------------
Eval num_timesteps=335500, episode_reward=731.74 +/- 602.87
Episode length: 35.00 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 915      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 328      |
|    time_elapsed    | 1157     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=911.12 +/- 711.23
Episode length: 36.10 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 911       |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.48e-23 |
|    explained_variance   | 0.0341    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.51e+04  |
|    n_updates            | 3215      |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=830.36 +/- 676.43
Episode length: 35.56 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 329      |
|    time_elapsed    | 1161     |
|    total_timesteps | 336896   |
---------------------------------
Eval num_timesteps=337000, episode_reward=859.79 +/- 676.53
Episode length: 36.14 +/- 5.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 860       |
| time/                   |           |
|    total_timesteps      | 337000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.32e-20 |
|    explained_variance   | 0.111     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.68e+04  |
|    n_updates            | 3225      |
|    policy_gradient_loss | 2.13e-09  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=337500, episode_reward=769.59 +/- 648.05
Episode length: 35.30 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 330      |
|    time_elapsed    | 1164     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=701.24 +/- 567.65
Episode length: 35.34 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-23 |
|    explained_variance   | 0.0831    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.69e+04  |
|    n_updates            | 3235      |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 8.33e+04  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=932.88 +/- 745.72
Episode length: 35.10 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 331      |
|    time_elapsed    | 1168     |
|    total_timesteps | 338944   |
---------------------------------
Eval num_timesteps=339000, episode_reward=875.30 +/- 747.54
Episode length: 34.96 +/- 8.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 339000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.75e-20 |
|    explained_variance   | 0.0799    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.04e+04  |
|    n_updates            | 3245      |
|    policy_gradient_loss | -2.11e-09 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=339500, episode_reward=777.08 +/- 621.42
Episode length: 35.22 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 332      |
|    time_elapsed    | 1171     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=719.48 +/- 597.71
Episode length: 34.48 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 719       |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9e-22    |
|    explained_variance   | 0.0916    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.36e+04  |
|    n_updates            | 3255      |
|    policy_gradient_loss | -1.96e-09 |
|    value_loss           | 8.52e+04  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=736.58 +/- 649.00
Episode length: 34.58 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 921      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 333      |
|    time_elapsed    | 1175     |
|    total_timesteps | 340992   |
---------------------------------
Eval num_timesteps=341000, episode_reward=971.81 +/- 710.34
Episode length: 36.96 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 972       |
| time/                   |           |
|    total_timesteps      | 341000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.16e-20 |
|    explained_variance   | 0.0683    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.44e+04  |
|    n_updates            | 3265      |
|    policy_gradient_loss | -1.64e-09 |
|    value_loss           | 1.18e+05  |
---------------------------------------
Eval num_timesteps=341500, episode_reward=1022.45 +/- 770.39
Episode length: 35.88 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=1046.27 +/- 748.33
Episode length: 36.90 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 334      |
|    time_elapsed    | 1180     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=896.92 +/- 690.88
Episode length: 35.80 +/- 6.25
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.8     |
|    mean_reward          | 897      |
| time/                   |          |
|    total_timesteps      | 342500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1e-12   |
|    explained_variance   | 0.0901   |
|    learning_rate        | 0.0005   |
|    loss                 | 3.76e+04 |
|    n_updates            | 3275     |
|    policy_gradient_loss | 9.37e-10 |
|    value_loss           | 7.66e+04 |
--------------------------------------
Eval num_timesteps=343000, episode_reward=985.64 +/- 726.76
Episode length: 36.22 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 335      |
|    time_elapsed    | 1183     |
|    total_timesteps | 343040   |
---------------------------------
Eval num_timesteps=343500, episode_reward=681.39 +/- 619.43
Episode length: 33.64 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 681       |
| time/                   |           |
|    total_timesteps      | 343500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.48e-13 |
|    explained_variance   | 0.0429    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.07e+04  |
|    n_updates            | 3285      |
|    policy_gradient_loss | -7.68e-10 |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=344000, episode_reward=740.00 +/- 665.81
Episode length: 34.34 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 336      |
|    time_elapsed    | 1186     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=768.85 +/- 578.11
Episode length: 35.66 +/- 5.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 769       |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.01e-23 |
|    explained_variance   | 0.126     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.02e+04  |
|    n_updates            | 3295      |
|    policy_gradient_loss | 1.05e-10  |
|    value_loss           | 7.86e+04  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=822.05 +/- 666.08
Episode length: 34.96 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 684      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 337      |
|    time_elapsed    | 1190     |
|    total_timesteps | 345088   |
---------------------------------
Eval num_timesteps=345500, episode_reward=676.25 +/- 589.27
Episode length: 34.36 +/- 7.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 676       |
| time/                   |           |
|    total_timesteps      | 345500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.34e-19 |
|    explained_variance   | 0.14      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.46e+04  |
|    n_updates            | 3305      |
|    policy_gradient_loss | -3.05e-09 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=346000, episode_reward=881.13 +/- 715.20
Episode length: 35.52 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 724      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 338      |
|    time_elapsed    | 1193     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=781.04 +/- 636.07
Episode length: 35.24 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 781       |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.51e-18 |
|    explained_variance   | 0.112     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.3e+04   |
|    n_updates            | 3315      |
|    policy_gradient_loss | 4.42e-10  |
|    value_loss           | 8.21e+04  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=872.06 +/- 659.74
Episode length: 36.20 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 732      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 339      |
|    time_elapsed    | 1197     |
|    total_timesteps | 347136   |
---------------------------------
Eval num_timesteps=347500, episode_reward=840.61 +/- 694.10
Episode length: 35.34 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 347500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-16 |
|    explained_variance   | 0.0876    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.69e+04  |
|    n_updates            | 3325      |
|    policy_gradient_loss | 2.25e-09  |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=348000, episode_reward=885.00 +/- 715.54
Episode length: 35.28 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 735      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 340      |
|    time_elapsed    | 1201     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=858.57 +/- 669.10
Episode length: 35.66 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 859       |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.76e-19 |
|    explained_variance   | 0.0922    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.92e+04  |
|    n_updates            | 3335      |
|    policy_gradient_loss | -1.15e-09 |
|    value_loss           | 8.23e+04  |
---------------------------------------
Eval num_timesteps=349000, episode_reward=980.12 +/- 680.73
Episode length: 37.52 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 816      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 341      |
|    time_elapsed    | 1204     |
|    total_timesteps | 349184   |
---------------------------------
Eval num_timesteps=349500, episode_reward=890.91 +/- 667.97
Episode length: 36.44 +/- 5.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 891       |
| time/                   |           |
|    total_timesteps      | 349500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.85e-17 |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.34e+04  |
|    n_updates            | 3345      |
|    policy_gradient_loss | -1.66e-09 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=350000, episode_reward=783.45 +/- 615.57
Episode length: 35.08 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 342      |
|    time_elapsed    | 1208     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=777.38 +/- 641.27
Episode length: 35.18 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 777       |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.01e-19 |
|    explained_variance   | 0.0436    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.25e+04  |
|    n_updates            | 3355      |
|    policy_gradient_loss | 7.33e-10  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=881.48 +/- 720.61
Episode length: 35.54 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 896      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 343      |
|    time_elapsed    | 1212     |
|    total_timesteps | 351232   |
---------------------------------
Eval num_timesteps=351500, episode_reward=902.92 +/- 725.35
Episode length: 35.50 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 903       |
| time/                   |           |
|    total_timesteps      | 351500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.75e-17 |
|    explained_variance   | 0.118     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.55e+04  |
|    n_updates            | 3365      |
|    policy_gradient_loss | -3.9e-10  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=352000, episode_reward=852.88 +/- 694.61
Episode length: 35.32 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 344      |
|    time_elapsed    | 1215     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=1017.33 +/- 767.28
Episode length: 36.30 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.67e-18 |
|    explained_variance   | 0.0781    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.75e+04  |
|    n_updates            | 3375      |
|    policy_gradient_loss | 6.87e-10  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=353000, episode_reward=705.72 +/- 537.13
Episode length: 35.84 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 345      |
|    time_elapsed    | 1219     |
|    total_timesteps | 353280   |
---------------------------------
Eval num_timesteps=353500, episode_reward=923.21 +/- 732.27
Episode length: 36.08 +/- 6.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 923       |
| time/                   |           |
|    total_timesteps      | 353500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-14 |
|    explained_variance   | 0.163     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.23e+04  |
|    n_updates            | 3385      |
|    policy_gradient_loss | -2.63e-09 |
|    value_loss           | 9.83e+04  |
---------------------------------------
Eval num_timesteps=354000, episode_reward=1007.38 +/- 728.87
Episode length: 36.62 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 346      |
|    time_elapsed    | 1222     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=936.00 +/- 698.66
Episode length: 36.64 +/- 5.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 936       |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.22e-16 |
|    explained_variance   | 0.0753    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.07e+04  |
|    n_updates            | 3395      |
|    policy_gradient_loss | 7.8e-10   |
|    value_loss           | 9.18e+04  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=815.02 +/- 606.59
Episode length: 36.20 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 347      |
|    time_elapsed    | 1226     |
|    total_timesteps | 355328   |
---------------------------------
Eval num_timesteps=355500, episode_reward=987.52 +/- 761.91
Episode length: 35.88 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 988       |
| time/                   |           |
|    total_timesteps      | 355500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.67e-13 |
|    explained_variance   | 0.0955    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.17e+04  |
|    n_updates            | 3405      |
|    policy_gradient_loss | 8.03e-10  |
|    value_loss           | 9.17e+04  |
---------------------------------------
Eval num_timesteps=356000, episode_reward=741.88 +/- 681.23
Episode length: 34.08 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 348      |
|    time_elapsed    | 1229     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=783.77 +/- 645.06
Episode length: 35.12 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.42e-15 |
|    explained_variance   | 0.0154    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.74e+04  |
|    n_updates            | 3415      |
|    policy_gradient_loss | 1.19e-09  |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=885.86 +/- 648.87
Episode length: 36.02 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 349      |
|    time_elapsed    | 1233     |
|    total_timesteps | 357376   |
---------------------------------
Eval num_timesteps=357500, episode_reward=780.11 +/- 647.72
Episode length: 35.06 +/- 7.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 357500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-13 |
|    explained_variance   | 0.0793    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.98e+04  |
|    n_updates            | 3425      |
|    policy_gradient_loss | 7.22e-10  |
|    value_loss           | 8.98e+04  |
---------------------------------------
Eval num_timesteps=358000, episode_reward=878.47 +/- 688.61
Episode length: 36.06 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 743      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 350      |
|    time_elapsed    | 1236     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=910.43 +/- 684.98
Episode length: 35.90 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 910       |
| time/                   |           |
|    total_timesteps      | 358500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.3e-13  |
|    explained_variance   | 0.0821    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.98e+04  |
|    n_updates            | 3435      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 6.85e+04  |
---------------------------------------
Eval num_timesteps=359000, episode_reward=856.69 +/- 676.79
Episode length: 35.62 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 351      |
|    time_elapsed    | 1240     |
|    total_timesteps | 359424   |
---------------------------------
Eval num_timesteps=359500, episode_reward=888.84 +/- 652.28
Episode length: 36.24 +/- 5.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 889       |
| time/                   |           |
|    total_timesteps      | 359500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.85e-19 |
|    explained_variance   | 0.0704    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.46e+04  |
|    n_updates            | 3445      |
|    policy_gradient_loss | 5.82e-11  |
|    value_loss           | 9.37e+04  |
---------------------------------------
Eval num_timesteps=360000, episode_reward=720.83 +/- 579.18
Episode length: 35.12 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 692      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 352      |
|    time_elapsed    | 1243     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=875.19 +/- 708.89
Episode length: 35.06 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.08e-19 |
|    explained_variance   | 0.17      |
|    learning_rate        | 0.0005    |
|    loss                 | 7.36e+04  |
|    n_updates            | 3455      |
|    policy_gradient_loss | 8.03e-10  |
|    value_loss           | 1.3e+05   |
---------------------------------------
Eval num_timesteps=361000, episode_reward=888.05 +/- 695.93
Episode length: 36.18 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 727      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 353      |
|    time_elapsed    | 1247     |
|    total_timesteps | 361472   |
---------------------------------
Eval num_timesteps=361500, episode_reward=877.79 +/- 686.72
Episode length: 35.74 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 361500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.31e-22 |
|    explained_variance   | 0.0414    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.06e+04  |
|    n_updates            | 3465      |
|    policy_gradient_loss | -4.19e-10 |
|    value_loss           | 1.12e+05  |
---------------------------------------
Eval num_timesteps=362000, episode_reward=877.49 +/- 705.64
Episode length: 36.02 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 354      |
|    time_elapsed    | 1250     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=734.09 +/- 676.34
Episode length: 33.80 +/- 7.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.64e-20 |
|    explained_variance   | 0.0818    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.07e+04  |
|    n_updates            | 3475      |
|    policy_gradient_loss | 2.56e-09  |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=363000, episode_reward=892.31 +/- 686.43
Episode length: 36.26 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=953.71 +/- 743.95
Episode length: 35.78 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 355      |
|    time_elapsed    | 1255     |
|    total_timesteps | 363520   |
---------------------------------
Eval num_timesteps=364000, episode_reward=712.93 +/- 659.70
Episode length: 33.22 +/- 7.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.2      |
|    mean_reward          | 713       |
| time/                   |           |
|    total_timesteps      | 364000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.7e-13  |
|    explained_variance   | 0.108     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.32e+04  |
|    n_updates            | 3485      |
|    policy_gradient_loss | -9.49e-10 |
|    value_loss           | 8.8e+04   |
---------------------------------------
Eval num_timesteps=364500, episode_reward=754.83 +/- 666.95
Episode length: 34.20 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 356      |
|    time_elapsed    | 1258     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=752.29 +/- 631.86
Episode length: 34.98 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 752       |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.76e-14 |
|    explained_variance   | 0.0716    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.24e+04  |
|    n_updates            | 3495      |
|    policy_gradient_loss | 1.14e-09  |
|    value_loss           | 8.76e+04  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=1005.02 +/- 737.83
Episode length: 35.94 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 357      |
|    time_elapsed    | 1262     |
|    total_timesteps | 365568   |
---------------------------------
Eval num_timesteps=366000, episode_reward=864.52 +/- 676.54
Episode length: 36.02 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 865       |
| time/                   |           |
|    total_timesteps      | 366000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.05e-13 |
|    explained_variance   | 0.103     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.61e+04  |
|    n_updates            | 3505      |
|    policy_gradient_loss | 2.63e-09  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=366500, episode_reward=656.31 +/- 563.84
Episode length: 34.42 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 358      |
|    time_elapsed    | 1265     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=785.25 +/- 687.43
Episode length: 34.42 +/- 7.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.78e-14 |
|    explained_variance   | 0.0663    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.07e+04  |
|    n_updates            | 3515      |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 9.59e+04  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=729.64 +/- 641.61
Episode length: 34.26 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 359      |
|    time_elapsed    | 1269     |
|    total_timesteps | 367616   |
---------------------------------
Eval num_timesteps=368000, episode_reward=828.16 +/- 670.72
Episode length: 35.42 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 368000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.67e-21 |
|    explained_variance   | 0.0722    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.39e+04  |
|    n_updates            | 3525      |
|    policy_gradient_loss | 1.4e-10   |
|    value_loss           | 9.21e+04  |
---------------------------------------
Eval num_timesteps=368500, episode_reward=847.83 +/- 678.69
Episode length: 35.58 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 360      |
|    time_elapsed    | 1272     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=951.91 +/- 704.55
Episode length: 36.38 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 952       |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.23e-17 |
|    explained_variance   | 0.19      |
|    learning_rate        | 0.0005    |
|    loss                 | 6.86e+04  |
|    n_updates            | 3535      |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=843.20 +/- 650.16
Episode length: 36.24 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 361      |
|    time_elapsed    | 1276     |
|    total_timesteps | 369664   |
---------------------------------
Eval num_timesteps=370000, episode_reward=973.99 +/- 719.33
Episode length: 36.62 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 974       |
| time/                   |           |
|    total_timesteps      | 370000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.99e-11 |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.87e+04  |
|    n_updates            | 3545      |
|    policy_gradient_loss | -5.94e-10 |
|    value_loss           | 8.51e+04  |
---------------------------------------
Eval num_timesteps=370500, episode_reward=788.77 +/- 628.82
Episode length: 35.86 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 362      |
|    time_elapsed    | 1279     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=745.93 +/- 630.45
Episode length: 34.48 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 746       |
| time/                   |           |
|    total_timesteps      | 371000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.42e-13 |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.18e+04  |
|    n_updates            | 3555      |
|    policy_gradient_loss | 7.57e-10  |
|    value_loss           | 7.99e+04  |
---------------------------------------
Eval num_timesteps=371500, episode_reward=933.57 +/- 751.27
Episode length: 35.48 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 363      |
|    time_elapsed    | 1283     |
|    total_timesteps | 371712   |
---------------------------------
Eval num_timesteps=372000, episode_reward=804.37 +/- 643.36
Episode length: 35.16 +/- 6.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 804       |
| time/                   |           |
|    total_timesteps      | 372000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-12 |
|    explained_variance   | 0.111     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.37e+04  |
|    n_updates            | 3565      |
|    policy_gradient_loss | -1.16e-10 |
|    value_loss           | 9.48e+04  |
---------------------------------------
Eval num_timesteps=372500, episode_reward=891.19 +/- 682.03
Episode length: 35.92 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 364      |
|    time_elapsed    | 1286     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=933.70 +/- 757.99
Episode length: 35.68 +/- 7.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 934       |
| time/                   |           |
|    total_timesteps      | 373000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-13 |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.27e+04  |
|    n_updates            | 3575      |
|    policy_gradient_loss | -2.28e-09 |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=373500, episode_reward=706.95 +/- 599.10
Episode length: 33.96 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 365      |
|    time_elapsed    | 1290     |
|    total_timesteps | 373760   |
---------------------------------
Eval num_timesteps=374000, episode_reward=939.46 +/- 709.94
Episode length: 36.56 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 939       |
| time/                   |           |
|    total_timesteps      | 374000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.78e-11 |
|    explained_variance   | 0.176     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.71e+04  |
|    n_updates            | 3585      |
|    policy_gradient_loss | -1.59e-09 |
|    value_loss           | 8.1e+04   |
---------------------------------------
Eval num_timesteps=374500, episode_reward=808.80 +/- 665.59
Episode length: 34.76 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 366      |
|    time_elapsed    | 1293     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=777.82 +/- 642.54
Episode length: 34.44 +/- 5.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 778       |
| time/                   |           |
|    total_timesteps      | 375000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.99e-09 |
|    explained_variance   | 0.00135   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.8e+04   |
|    n_updates            | 3595      |
|    policy_gradient_loss | 2.26e-09  |
|    value_loss           | 9.42e+04  |
---------------------------------------
Eval num_timesteps=375500, episode_reward=917.14 +/- 682.59
Episode length: 36.64 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 665      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 367      |
|    time_elapsed    | 1297     |
|    total_timesteps | 375808   |
---------------------------------
Eval num_timesteps=376000, episode_reward=804.84 +/- 685.38
Episode length: 35.28 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 376000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.66e-10 |
|    explained_variance   | 0.119     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.68e+04  |
|    n_updates            | 3605      |
|    policy_gradient_loss | 2.91e-09  |
|    value_loss           | 6.9e+04   |
---------------------------------------
Eval num_timesteps=376500, episode_reward=795.47 +/- 646.15
Episode length: 35.08 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 678      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 368      |
|    time_elapsed    | 1300     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=837.44 +/- 663.29
Episode length: 35.30 +/- 6.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 837       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-16 |
|    explained_variance   | 0.0933    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.91e+04  |
|    n_updates            | 3615      |
|    policy_gradient_loss | -6.64e-10 |
|    value_loss           | 9.8e+04   |
---------------------------------------
Eval num_timesteps=377500, episode_reward=798.80 +/- 675.79
Episode length: 35.42 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 369      |
|    time_elapsed    | 1304     |
|    total_timesteps | 377856   |
---------------------------------
Eval num_timesteps=378000, episode_reward=919.81 +/- 754.12
Episode length: 35.18 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 920       |
| time/                   |           |
|    total_timesteps      | 378000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.58e-13 |
|    explained_variance   | 0.161     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.11e+04  |
|    n_updates            | 3625      |
|    policy_gradient_loss | 3.61e-10  |
|    value_loss           | 9.35e+04  |
---------------------------------------
Eval num_timesteps=378500, episode_reward=912.10 +/- 714.38
Episode length: 35.92 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 370      |
|    time_elapsed    | 1307     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=956.17 +/- 743.44
Episode length: 35.78 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 956       |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.2e-14  |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.36e+04  |
|    n_updates            | 3635      |
|    policy_gradient_loss | -2.82e-09 |
|    value_loss           | 9.39e+04  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=888.31 +/- 652.42
Episode length: 36.90 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 819      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 371      |
|    time_elapsed    | 1311     |
|    total_timesteps | 379904   |
---------------------------------
Eval num_timesteps=380000, episode_reward=773.05 +/- 658.23
Episode length: 34.32 +/- 6.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 380000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.46e-13 |
|    explained_variance   | 0.0469    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.45e+04  |
|    n_updates            | 3645      |
|    policy_gradient_loss | 9.78e-10  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=380500, episode_reward=819.01 +/- 642.64
Episode length: 35.76 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 372      |
|    time_elapsed    | 1314     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=750.90 +/- 576.59
Episode length: 36.06 +/- 5.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 751       |
| time/                   |           |
|    total_timesteps      | 381000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.79e-14 |
|    explained_variance   | 0.0787    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.87e+04  |
|    n_updates            | 3655      |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 9.43e+04  |
---------------------------------------
Eval num_timesteps=381500, episode_reward=822.84 +/- 710.89
Episode length: 34.72 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 373      |
|    time_elapsed    | 1318     |
|    total_timesteps | 381952   |
---------------------------------
Eval num_timesteps=382000, episode_reward=845.69 +/- 648.76
Episode length: 35.24 +/- 5.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 846       |
| time/                   |           |
|    total_timesteps      | 382000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.35e-13 |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.68e+04  |
|    n_updates            | 3665      |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 8.43e+04  |
---------------------------------------
Eval num_timesteps=382500, episode_reward=823.51 +/- 704.56
Episode length: 35.38 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 374      |
|    time_elapsed    | 1321     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=775.19 +/- 651.58
Episode length: 35.56 +/- 5.63
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.6     |
|    mean_reward          | 775      |
| time/                   |          |
|    total_timesteps      | 383000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.6e-14 |
|    explained_variance   | 0.0545   |
|    learning_rate        | 0.0005   |
|    loss                 | 4.62e+04 |
|    n_updates            | 3675     |
|    policy_gradient_loss | 2.56e-10 |
|    value_loss           | 9.83e+04 |
--------------------------------------
Eval num_timesteps=383500, episode_reward=737.65 +/- 619.01
Episode length: 33.92 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=823.00 +/- 650.43
Episode length: 35.58 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 375      |
|    time_elapsed    | 1326     |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=694.65 +/- 637.34
Episode length: 33.76 +/- 7.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 695       |
| time/                   |           |
|    total_timesteps      | 384500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.66e-14 |
|    explained_variance   | 0.0906    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.13e+04  |
|    n_updates            | 3685      |
|    policy_gradient_loss | -2.33e-11 |
|    value_loss           | 9.9e+04   |
---------------------------------------
Eval num_timesteps=385000, episode_reward=899.20 +/- 690.01
Episode length: 36.42 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 376      |
|    time_elapsed    | 1329     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=760.58 +/- 672.78
Episode length: 33.94 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 385500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-17 |
|    explained_variance   | 0.122     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.08e+04  |
|    n_updates            | 3695      |
|    policy_gradient_loss | 1.27e-09  |
|    value_loss           | 8.59e+04  |
---------------------------------------
Eval num_timesteps=386000, episode_reward=681.56 +/- 616.06
Episode length: 34.28 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 377      |
|    time_elapsed    | 1333     |
|    total_timesteps | 386048   |
---------------------------------
Eval num_timesteps=386500, episode_reward=786.10 +/- 619.09
Episode length: 35.50 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 386500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-14 |
|    explained_variance   | 0.0866    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.83e+04  |
|    n_updates            | 3705      |
|    policy_gradient_loss | 1.98e-10  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=387000, episode_reward=865.01 +/- 657.06
Episode length: 35.94 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 378      |
|    time_elapsed    | 1336     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=811.74 +/- 663.37
Episode length: 35.34 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-16 |
|    explained_variance   | 0.0675    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.18e+04  |
|    n_updates            | 3715      |
|    policy_gradient_loss | 3.55e-10  |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=388000, episode_reward=903.78 +/- 693.96
Episode length: 36.16 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 379      |
|    time_elapsed    | 1340     |
|    total_timesteps | 388096   |
---------------------------------
Eval num_timesteps=388500, episode_reward=1001.87 +/- 741.86
Episode length: 36.66 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 388500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.1e-23  |
|    explained_variance   | 0.0656    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.71e+04  |
|    n_updates            | 3725      |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=389000, episode_reward=778.74 +/- 630.50
Episode length: 34.94 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 938      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 380      |
|    time_elapsed    | 1343     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=805.59 +/- 685.17
Episode length: 34.64 +/- 7.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 806       |
| time/                   |           |
|    total_timesteps      | 389500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-20 |
|    explained_variance   | 0.0682    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.38e+04  |
|    n_updates            | 3735      |
|    policy_gradient_loss | 2.58e-09  |
|    value_loss           | 1.28e+05  |
---------------------------------------
Eval num_timesteps=390000, episode_reward=719.91 +/- 638.65
Episode length: 33.64 +/- 7.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 942      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 381      |
|    time_elapsed    | 1347     |
|    total_timesteps | 390144   |
---------------------------------
Eval num_timesteps=390500, episode_reward=570.09 +/- 505.37
Episode length: 33.20 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.2      |
|    mean_reward          | 570       |
| time/                   |           |
|    total_timesteps      | 390500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.58e-14 |
|    explained_variance   | 0.162     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.47e+04  |
|    n_updates            | 3745      |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 9.49e+04  |
---------------------------------------
Eval num_timesteps=391000, episode_reward=900.90 +/- 671.90
Episode length: 36.32 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 382      |
|    time_elapsed    | 1350     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=629.66 +/- 560.50
Episode length: 33.40 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 630       |
| time/                   |           |
|    total_timesteps      | 391500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.66e-16 |
|    explained_variance   | 0.00243   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.98e+04  |
|    n_updates            | 3755      |
|    policy_gradient_loss | 3.34e-09  |
|    value_loss           | 9.64e+04  |
---------------------------------------
Eval num_timesteps=392000, episode_reward=630.80 +/- 563.04
Episode length: 33.90 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 383      |
|    time_elapsed    | 1353     |
|    total_timesteps | 392192   |
---------------------------------
Eval num_timesteps=392500, episode_reward=758.28 +/- 594.31
Episode length: 35.72 +/- 5.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 392500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.58e-14 |
|    explained_variance   | 0.117     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.21e+04  |
|    n_updates            | 3765      |
|    policy_gradient_loss | -1.37e-09 |
|    value_loss           | 9.59e+04  |
---------------------------------------
Eval num_timesteps=393000, episode_reward=793.84 +/- 648.66
Episode length: 35.44 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 384      |
|    time_elapsed    | 1357     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=769.02 +/- 637.75
Episode length: 34.86 +/- 7.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 769       |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-15 |
|    explained_variance   | 0.0634    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.53e+04  |
|    n_updates            | 3775      |
|    policy_gradient_loss | 1.98e-10  |
|    value_loss           | 8.87e+04  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=781.13 +/- 668.66
Episode length: 34.54 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 385      |
|    time_elapsed    | 1360     |
|    total_timesteps | 394240   |
---------------------------------
Eval num_timesteps=394500, episode_reward=747.15 +/- 622.83
Episode length: 34.60 +/- 7.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 747       |
| time/                   |           |
|    total_timesteps      | 394500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.07e-14 |
|    explained_variance   | 0.138     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.07e+04  |
|    n_updates            | 3785      |
|    policy_gradient_loss | 1.84e-09  |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=395000, episode_reward=903.84 +/- 714.48
Episode length: 35.40 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 386      |
|    time_elapsed    | 1364     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=723.68 +/- 661.21
Episode length: 34.12 +/- 7.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 395500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.35e-16 |
|    explained_variance   | 0.0965    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.22e+04  |
|    n_updates            | 3795      |
|    policy_gradient_loss | -3.96e-10 |
|    value_loss           | 7.77e+04  |
---------------------------------------
Eval num_timesteps=396000, episode_reward=801.99 +/- 643.38
Episode length: 34.78 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 387      |
|    time_elapsed    | 1367     |
|    total_timesteps | 396288   |
---------------------------------
Eval num_timesteps=396500, episode_reward=663.50 +/- 611.36
Episode length: 33.22 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.2      |
|    mean_reward          | 664       |
| time/                   |           |
|    total_timesteps      | 396500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.57e-12 |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.85e+04  |
|    n_updates            | 3805      |
|    policy_gradient_loss | 9.37e-10  |
|    value_loss           | 9.49e+04  |
---------------------------------------
Eval num_timesteps=397000, episode_reward=807.14 +/- 648.22
Episode length: 35.50 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 388      |
|    time_elapsed    | 1371     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=771.88 +/- 679.00
Episode length: 34.42 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 772       |
| time/                   |           |
|    total_timesteps      | 397500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.33e-13 |
|    explained_variance   | 0.0441    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.5e+04   |
|    n_updates            | 3815      |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 8.74e+04  |
---------------------------------------
Eval num_timesteps=398000, episode_reward=951.44 +/- 714.45
Episode length: 36.78 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 389      |
|    time_elapsed    | 1374     |
|    total_timesteps | 398336   |
---------------------------------
Eval num_timesteps=398500, episode_reward=830.72 +/- 693.90
Episode length: 35.48 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 398500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.28e-21 |
|    explained_variance   | 0.112     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.35e+04  |
|    n_updates            | 3825      |
|    policy_gradient_loss | 7.39e-10  |
|    value_loss           | 8.9e+04   |
---------------------------------------
Eval num_timesteps=399000, episode_reward=788.46 +/- 657.76
Episode length: 35.32 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 390      |
|    time_elapsed    | 1378     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=818.83 +/- 648.61
Episode length: 36.20 +/- 6.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 819       |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-18 |
|    explained_variance   | 0.157     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.64e+04  |
|    n_updates            | 3835      |
|    policy_gradient_loss | -1.46e-10 |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=884.97 +/- 651.60
Episode length: 36.52 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 391      |
|    time_elapsed    | 1381     |
|    total_timesteps | 400384   |
---------------------------------
Eval num_timesteps=400500, episode_reward=821.87 +/- 674.90
Episode length: 35.18 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 822       |
| time/                   |           |
|    total_timesteps      | 400500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.56e-11 |
|    explained_variance   | 0.111     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.61e+04  |
|    n_updates            | 3845      |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=401000, episode_reward=796.94 +/- 644.44
Episode length: 35.60 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 392      |
|    time_elapsed    | 1385     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=899.36 +/- 686.72
Episode length: 35.90 +/- 5.70
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.9     |
|    mean_reward          | 899      |
| time/                   |          |
|    total_timesteps      | 401500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.1e-12 |
|    explained_variance   | 0.0841   |
|    learning_rate        | 0.0005   |
|    loss                 | 3.16e+04 |
|    n_updates            | 3855     |
|    policy_gradient_loss | 1.4e-10  |
|    value_loss           | 7.41e+04 |
--------------------------------------
Eval num_timesteps=402000, episode_reward=837.12 +/- 698.71
Episode length: 35.26 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 393      |
|    time_elapsed    | 1388     |
|    total_timesteps | 402432   |
---------------------------------
Eval num_timesteps=402500, episode_reward=800.04 +/- 632.32
Episode length: 35.16 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 800       |
| time/                   |           |
|    total_timesteps      | 402500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.58e-10 |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.15e+04  |
|    n_updates            | 3865      |
|    policy_gradient_loss | -1.73e-09 |
|    value_loss           | 8.37e+04  |
---------------------------------------
Eval num_timesteps=403000, episode_reward=968.10 +/- 730.30
Episode length: 36.26 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 683      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 394      |
|    time_elapsed    | 1391     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=815.07 +/- 714.75
Episode length: 34.76 +/- 7.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 815       |
| time/                   |           |
|    total_timesteps      | 403500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-12 |
|    explained_variance   | 0.0898    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.18e+04  |
|    n_updates            | 3875      |
|    policy_gradient_loss | -4.39e-09 |
|    value_loss           | 7.8e+04   |
---------------------------------------
Eval num_timesteps=404000, episode_reward=802.63 +/- 653.33
Episode length: 35.32 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 732      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 395      |
|    time_elapsed    | 1395     |
|    total_timesteps | 404480   |
---------------------------------
Eval num_timesteps=404500, episode_reward=770.54 +/- 611.95
Episode length: 35.72 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 771       |
| time/                   |           |
|    total_timesteps      | 404500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.8e-12  |
|    explained_variance   | 0.127     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.37e+04  |
|    n_updates            | 3885      |
|    policy_gradient_loss | -8.15e-11 |
|    value_loss           | 8.65e+04  |
---------------------------------------
Eval num_timesteps=405000, episode_reward=799.43 +/- 695.22
Episode length: 34.40 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=827.61 +/- 659.37
Episode length: 35.16 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 819      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 396      |
|    time_elapsed    | 1400     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=803.57 +/- 664.69
Episode length: 34.68 +/- 6.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 804       |
| time/                   |           |
|    total_timesteps      | 406000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.05e-14 |
|    explained_variance   | 0.085     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.64e+04  |
|    n_updates            | 3895      |
|    policy_gradient_loss | 1.27e-09  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=406500, episode_reward=781.05 +/- 664.67
Episode length: 34.76 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 397      |
|    time_elapsed    | 1403     |
|    total_timesteps | 406528   |
---------------------------------
Eval num_timesteps=407000, episode_reward=1002.00 +/- 722.04
Episode length: 36.52 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 407000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.18e-10 |
|    explained_variance   | 0.188     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.32e+04  |
|    n_updates            | 3905      |
|    policy_gradient_loss | -3.26e-10 |
|    value_loss           | 7.46e+04  |
---------------------------------------
Eval num_timesteps=407500, episode_reward=800.96 +/- 728.65
Episode length: 34.22 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 398      |
|    time_elapsed    | 1406     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=746.76 +/- 635.20
Episode length: 35.10 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 747       |
| time/                   |           |
|    total_timesteps      | 408000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.82e-14 |
|    explained_variance   | 0.101     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.77e+04  |
|    n_updates            | 3915      |
|    policy_gradient_loss | 1.02e-09  |
|    value_loss           | 8.53e+04  |
---------------------------------------
Eval num_timesteps=408500, episode_reward=859.09 +/- 670.79
Episode length: 35.54 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 399      |
|    time_elapsed    | 1410     |
|    total_timesteps | 408576   |
---------------------------------
Eval num_timesteps=409000, episode_reward=713.00 +/- 602.22
Episode length: 34.76 +/- 5.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 713       |
| time/                   |           |
|    total_timesteps      | 409000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.09e-18 |
|    explained_variance   | 0.0531    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.97e+04  |
|    n_updates            | 3925      |
|    policy_gradient_loss | 2.33e-11  |
|    value_loss           | 9.31e+04  |
---------------------------------------
Eval num_timesteps=409500, episode_reward=884.92 +/- 687.52
Episode length: 36.32 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 400      |
|    time_elapsed    | 1413     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=799.00 +/- 674.12
Episode length: 34.76 +/- 7.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.2e-16  |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.25e+04  |
|    n_updates            | 3935      |
|    policy_gradient_loss | -2.18e-09 |
|    value_loss           | 9.91e+04  |
---------------------------------------
Eval num_timesteps=410500, episode_reward=821.58 +/- 664.61
Episode length: 35.62 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 401      |
|    time_elapsed    | 1417     |
|    total_timesteps | 410624   |
---------------------------------
Eval num_timesteps=411000, episode_reward=906.67 +/- 722.49
Episode length: 35.60 +/- 7.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 907       |
| time/                   |           |
|    total_timesteps      | 411000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.52e-18 |
|    explained_variance   | 0.125     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.81e+04  |
|    n_updates            | 3945      |
|    policy_gradient_loss | 1.75e-10  |
|    value_loss           | 8.18e+04  |
---------------------------------------
Eval num_timesteps=411500, episode_reward=850.92 +/- 709.80
Episode length: 35.02 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 402      |
|    time_elapsed    | 1420     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=668.86 +/- 600.68
Episode length: 33.88 +/- 6.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 669       |
| time/                   |           |
|    total_timesteps      | 412000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.21e-16 |
|    explained_variance   | 0.175     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.35e+04  |
|    n_updates            | 3955      |
|    policy_gradient_loss | 1.03e-09  |
|    value_loss           | 9.79e+04  |
---------------------------------------
Eval num_timesteps=412500, episode_reward=882.70 +/- 734.16
Episode length: 35.20 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 403      |
|    time_elapsed    | 1424     |
|    total_timesteps | 412672   |
---------------------------------
Eval num_timesteps=413000, episode_reward=964.41 +/- 735.53
Episode length: 35.84 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 964       |
| time/                   |           |
|    total_timesteps      | 413000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.05e-16 |
|    explained_variance   | 0.0928    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.03e+04  |
|    n_updates            | 3965      |
|    policy_gradient_loss | 3.49e-10  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=413500, episode_reward=805.08 +/- 681.26
Episode length: 34.50 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 404      |
|    time_elapsed    | 1427     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=804.61 +/- 656.53
Episode length: 35.24 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 414000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.26e-15 |
|    explained_variance   | 0.148     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.71e+04  |
|    n_updates            | 3975      |
|    policy_gradient_loss | 1.86e-09  |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=414500, episode_reward=826.39 +/- 696.47
Episode length: 35.06 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 405      |
|    time_elapsed    | 1431     |
|    total_timesteps | 414720   |
---------------------------------
Eval num_timesteps=415000, episode_reward=826.73 +/- 664.26
Episode length: 35.34 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 415000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.82e-08 |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.8e+04   |
|    n_updates            | 3985      |
|    policy_gradient_loss | -2.05e-09 |
|    value_loss           | 8.21e+04  |
---------------------------------------
Eval num_timesteps=415500, episode_reward=910.09 +/- 682.51
Episode length: 35.92 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 406      |
|    time_elapsed    | 1434     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=737.70 +/- 588.92
Episode length: 35.60 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 738       |
| time/                   |           |
|    total_timesteps      | 416000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.06e-12 |
|    explained_variance   | 0.0347    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.81e+04  |
|    n_updates            | 3995      |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=416500, episode_reward=972.79 +/- 749.67
Episode length: 35.14 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 407      |
|    time_elapsed    | 1438     |
|    total_timesteps | 416768   |
---------------------------------
Eval num_timesteps=417000, episode_reward=833.12 +/- 669.39
Episode length: 35.42 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 833       |
| time/                   |           |
|    total_timesteps      | 417000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.32e-09 |
|    explained_variance   | 0.0725    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.44e+04  |
|    n_updates            | 4005      |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 8.48e+04  |
---------------------------------------
Eval num_timesteps=417500, episode_reward=733.40 +/- 593.08
Episode length: 35.72 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 819      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 408      |
|    time_elapsed    | 1441     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=920.79 +/- 684.10
Episode length: 36.90 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 921       |
| time/                   |           |
|    total_timesteps      | 418000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.61e-12 |
|    explained_variance   | 0.107     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.55e+04  |
|    n_updates            | 4015      |
|    policy_gradient_loss | 6.75e-10  |
|    value_loss           | 9.05e+04  |
---------------------------------------
Eval num_timesteps=418500, episode_reward=787.37 +/- 608.83
Episode length: 35.78 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 409      |
|    time_elapsed    | 1445     |
|    total_timesteps | 418816   |
---------------------------------
Eval num_timesteps=419000, episode_reward=890.57 +/- 688.81
Episode length: 35.86 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 891       |
| time/                   |           |
|    total_timesteps      | 419000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.95e-16 |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.87e+04  |
|    n_updates            | 4025      |
|    policy_gradient_loss | -9.37e-10 |
|    value_loss           | 8.78e+04  |
---------------------------------------
Eval num_timesteps=419500, episode_reward=923.23 +/- 703.66
Episode length: 36.18 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 410      |
|    time_elapsed    | 1448     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=752.88 +/- 600.85
Episode length: 35.40 +/- 5.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 753       |
| time/                   |           |
|    total_timesteps      | 420000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.96e-17 |
|    explained_variance   | 0.0945    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.37e+04  |
|    n_updates            | 4035      |
|    policy_gradient_loss | 2.11e-09  |
|    value_loss           | 1.18e+05  |
---------------------------------------
Eval num_timesteps=420500, episode_reward=911.27 +/- 742.50
Episode length: 35.84 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 411      |
|    time_elapsed    | 1452     |
|    total_timesteps | 420864   |
---------------------------------
Eval num_timesteps=421000, episode_reward=918.16 +/- 691.71
Episode length: 36.36 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 918       |
| time/                   |           |
|    total_timesteps      | 421000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.67e-13 |
|    explained_variance   | 0.109     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.6e+04   |
|    n_updates            | 4045      |
|    policy_gradient_loss | 3.14e-10  |
|    value_loss           | 8.8e+04   |
---------------------------------------
Eval num_timesteps=421500, episode_reward=896.06 +/- 714.08
Episode length: 35.68 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 412      |
|    time_elapsed    | 1455     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=701.10 +/- 599.29
Episode length: 34.44 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 422000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.54e-14 |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.63e+04  |
|    n_updates            | 4055      |
|    policy_gradient_loss | 1.46e-09  |
|    value_loss           | 8.29e+04  |
---------------------------------------
Eval num_timesteps=422500, episode_reward=822.48 +/- 649.49
Episode length: 35.68 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 413      |
|    time_elapsed    | 1459     |
|    total_timesteps | 422912   |
---------------------------------
Eval num_timesteps=423000, episode_reward=916.90 +/- 743.25
Episode length: 35.14 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 917       |
| time/                   |           |
|    total_timesteps      | 423000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-18 |
|    explained_variance   | 0.0614    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.1e+04   |
|    n_updates            | 4065      |
|    policy_gradient_loss | -8.38e-10 |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=423500, episode_reward=907.26 +/- 643.90
Episode length: 37.06 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 414      |
|    time_elapsed    | 1462     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=841.96 +/- 682.20
Episode length: 36.26 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 842       |
| time/                   |           |
|    total_timesteps      | 424000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-15 |
|    explained_variance   | 0.142     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.08e+04  |
|    n_updates            | 4075      |
|    policy_gradient_loss | -2.84e-09 |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=424500, episode_reward=800.40 +/- 633.71
Episode length: 35.18 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 415      |
|    time_elapsed    | 1466     |
|    total_timesteps | 424960   |
---------------------------------
Eval num_timesteps=425000, episode_reward=792.82 +/- 725.83
Episode length: 33.56 +/- 7.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 793       |
| time/                   |           |
|    total_timesteps      | 425000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.55e-17 |
|    explained_variance   | 0.0895    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.68e+04  |
|    n_updates            | 4085      |
|    policy_gradient_loss | -6.4e-11  |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=425500, episode_reward=783.64 +/- 712.30
Episode length: 33.66 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 416      |
|    time_elapsed    | 1469     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=722.15 +/- 615.23
Episode length: 34.34 +/- 5.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 722       |
| time/                   |           |
|    total_timesteps      | 426000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.87e-17 |
|    explained_variance   | 0.191     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.87e+04  |
|    n_updates            | 4095      |
|    policy_gradient_loss | 1.44e-09  |
|    value_loss           | 1.18e+05  |
---------------------------------------
Eval num_timesteps=426500, episode_reward=785.50 +/- 720.20
Episode length: 33.84 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=814.67 +/- 663.11
Episode length: 35.16 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 858      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 417      |
|    time_elapsed    | 1474     |
|    total_timesteps | 427008   |
---------------------------------
Eval num_timesteps=427500, episode_reward=878.32 +/- 699.52
Episode length: 35.54 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 427500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.38e-18 |
|    explained_variance   | 0.114     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.26e+04  |
|    n_updates            | 4105      |
|    policy_gradient_loss | -1.96e-09 |
|    value_loss           | 9.46e+04  |
---------------------------------------
Eval num_timesteps=428000, episode_reward=790.46 +/- 625.90
Episode length: 34.80 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 418      |
|    time_elapsed    | 1477     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=927.31 +/- 721.86
Episode length: 36.04 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 428500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.51e-15 |
|    explained_variance   | 0.142     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.73e+04  |
|    n_updates            | 4115      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 1.12e+05  |
---------------------------------------
Eval num_timesteps=429000, episode_reward=721.68 +/- 636.39
Episode length: 34.70 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 419      |
|    time_elapsed    | 1481     |
|    total_timesteps | 429056   |
---------------------------------
Eval num_timesteps=429500, episode_reward=657.15 +/- 615.05
Episode length: 33.18 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.2      |
|    mean_reward          | 657       |
| time/                   |           |
|    total_timesteps      | 429500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.16e-18 |
|    explained_variance   | 0.0581    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.9e+04   |
|    n_updates            | 4125      |
|    policy_gradient_loss | 1.37e-09  |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=430000, episode_reward=850.34 +/- 659.41
Episode length: 35.92 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 420      |
|    time_elapsed    | 1484     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=845.04 +/- 655.78
Episode length: 35.68 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 845       |
| time/                   |           |
|    total_timesteps      | 430500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-14 |
|    explained_variance   | 0.187     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.98e+04  |
|    n_updates            | 4135      |
|    policy_gradient_loss | -5.65e-10 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=431000, episode_reward=957.75 +/- 702.49
Episode length: 36.70 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 421      |
|    time_elapsed    | 1488     |
|    total_timesteps | 431104   |
---------------------------------
Eval num_timesteps=431500, episode_reward=825.64 +/- 707.72
Episode length: 35.02 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 431500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.51e-10 |
|    explained_variance   | 0.138     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.91e+04  |
|    n_updates            | 4145      |
|    policy_gradient_loss | -6.98e-10 |
|    value_loss           | 8.17e+04  |
---------------------------------------
Eval num_timesteps=432000, episode_reward=793.27 +/- 674.41
Episode length: 34.44 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 422      |
|    time_elapsed    | 1491     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=1027.23 +/- 742.24
Episode length: 36.26 +/- 6.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 1.03e+03  |
| time/                   |           |
|    total_timesteps      | 432500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.29e-11 |
|    explained_variance   | 0.00328   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.19e+04  |
|    n_updates            | 4155      |
|    policy_gradient_loss | -2.5e-09  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=433000, episode_reward=761.14 +/- 617.30
Episode length: 35.36 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 423      |
|    time_elapsed    | 1495     |
|    total_timesteps | 433152   |
---------------------------------
Eval num_timesteps=433500, episode_reward=926.65 +/- 737.71
Episode length: 35.46 +/- 7.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 433500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.56e-16 |
|    explained_variance   | 0.0596    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.17e+04  |
|    n_updates            | 4165      |
|    policy_gradient_loss | -9.78e-10 |
|    value_loss           | 9.04e+04  |
---------------------------------------
Eval num_timesteps=434000, episode_reward=742.88 +/- 657.08
Episode length: 34.38 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 424      |
|    time_elapsed    | 1498     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=750.01 +/- 623.29
Episode length: 34.74 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 750       |
| time/                   |           |
|    total_timesteps      | 434500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35e-22 |
|    explained_variance   | 0.0453    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.7e+04   |
|    n_updates            | 4175      |
|    policy_gradient_loss | 1.75e-10  |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=435000, episode_reward=770.06 +/- 629.34
Episode length: 35.26 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 425      |
|    time_elapsed    | 1502     |
|    total_timesteps | 435200   |
---------------------------------
Eval num_timesteps=435500, episode_reward=813.71 +/- 692.18
Episode length: 34.84 +/- 7.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 435500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.32e-23 |
|    explained_variance   | 0.138     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.6e+04   |
|    n_updates            | 4185      |
|    policy_gradient_loss | 2.13e-09  |
|    value_loss           | 1.48e+05  |
---------------------------------------
Eval num_timesteps=436000, episode_reward=762.25 +/- 615.97
Episode length: 35.54 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 426      |
|    time_elapsed    | 1505     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=753.72 +/- 698.06
Episode length: 33.58 +/- 7.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 754       |
| time/                   |           |
|    total_timesteps      | 436500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.91e-13 |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.06e+04  |
|    n_updates            | 4195      |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 8.8e+04   |
---------------------------------------
Eval num_timesteps=437000, episode_reward=962.40 +/- 683.44
Episode length: 37.52 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 427      |
|    time_elapsed    | 1509     |
|    total_timesteps | 437248   |
---------------------------------
Eval num_timesteps=437500, episode_reward=1152.26 +/- 741.77
Episode length: 37.64 +/- 5.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.6      |
|    mean_reward          | 1.15e+03  |
| time/                   |           |
|    total_timesteps      | 437500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.63e-10 |
|    explained_variance   | 0.106     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.61e+04  |
|    n_updates            | 4205      |
|    policy_gradient_loss | -2.03e-09 |
|    value_loss           | 8.12e+04  |
---------------------------------------
Eval num_timesteps=438000, episode_reward=749.26 +/- 708.13
Episode length: 33.46 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 428      |
|    time_elapsed    | 1512     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=839.05 +/- 667.36
Episode length: 35.54 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 438500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.36e-11 |
|    explained_variance   | 0.146     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.57e+04  |
|    n_updates            | 4215      |
|    policy_gradient_loss | -1.16e-11 |
|    value_loss           | 8.11e+04  |
---------------------------------------
Eval num_timesteps=439000, episode_reward=792.95 +/- 689.57
Episode length: 34.70 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 429      |
|    time_elapsed    | 1516     |
|    total_timesteps | 439296   |
---------------------------------
Eval num_timesteps=439500, episode_reward=908.46 +/- 727.94
Episode length: 35.32 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 908       |
| time/                   |           |
|    total_timesteps      | 439500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.45e-16 |
|    explained_variance   | 0.101     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.43e+04  |
|    n_updates            | 4225      |
|    policy_gradient_loss | 6.4e-11   |
|    value_loss           | 9.17e+04  |
---------------------------------------
Eval num_timesteps=440000, episode_reward=870.45 +/- 647.74
Episode length: 36.42 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 430      |
|    time_elapsed    | 1519     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=833.96 +/- 694.95
Episode length: 35.16 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 440500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.35e-15 |
|    explained_variance   | 0.146     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.91e+04  |
|    n_updates            | 4235      |
|    policy_gradient_loss | 2.39e-10  |
|    value_loss           | 1.23e+05  |
---------------------------------------
Eval num_timesteps=441000, episode_reward=873.24 +/- 682.52
Episode length: 35.64 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 431      |
|    time_elapsed    | 1523     |
|    total_timesteps | 441344   |
---------------------------------
Eval num_timesteps=441500, episode_reward=804.73 +/- 737.63
Episode length: 34.12 +/- 8.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 441500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.99e-09 |
|    explained_variance   | 0.0983    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.28e+04  |
|    n_updates            | 4245      |
|    policy_gradient_loss | -5.36e-10 |
|    value_loss           | 9.71e+04  |
---------------------------------------
Eval num_timesteps=442000, episode_reward=922.87 +/- 675.01
Episode length: 37.08 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 432      |
|    time_elapsed    | 1526     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=881.15 +/- 704.08
Episode length: 35.56 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 442500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-10 |
|    explained_variance   | 0.0627    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.2e+04   |
|    n_updates            | 4255      |
|    policy_gradient_loss | -3.61e-10 |
|    value_loss           | 9.52e+04  |
---------------------------------------
Eval num_timesteps=443000, episode_reward=850.39 +/- 675.90
Episode length: 35.64 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 727      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 433      |
|    time_elapsed    | 1530     |
|    total_timesteps | 443392   |
---------------------------------
Eval num_timesteps=443500, episode_reward=939.74 +/- 728.59
Episode length: 35.66 +/- 6.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 940       |
| time/                   |           |
|    total_timesteps      | 443500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-14 |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.53e+04  |
|    n_updates            | 4265      |
|    policy_gradient_loss | 8.61e-10  |
|    value_loss           | 8.02e+04  |
---------------------------------------
Eval num_timesteps=444000, episode_reward=942.65 +/- 683.72
Episode length: 36.80 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 434      |
|    time_elapsed    | 1533     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=1010.86 +/- 725.22
Episode length: 36.60 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 444500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.61e-13 |
|    explained_variance   | 0.0962    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.95e+04  |
|    n_updates            | 4275      |
|    policy_gradient_loss | -6e-10    |
|    value_loss           | 1.13e+05  |
---------------------------------------
Eval num_timesteps=445000, episode_reward=788.24 +/- 682.03
Episode length: 34.88 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 435      |
|    time_elapsed    | 1537     |
|    total_timesteps | 445440   |
---------------------------------
Eval num_timesteps=445500, episode_reward=917.04 +/- 756.43
Episode length: 35.04 +/- 7.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 917       |
| time/                   |           |
|    total_timesteps      | 445500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.42e-14 |
|    explained_variance   | 0.0825    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.82e+04  |
|    n_updates            | 4285      |
|    policy_gradient_loss | -2.36e-09 |
|    value_loss           | 8.54e+04  |
---------------------------------------
Eval num_timesteps=446000, episode_reward=855.15 +/- 667.09
Episode length: 36.46 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 436      |
|    time_elapsed    | 1540     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=763.36 +/- 619.23
Episode length: 35.76 +/- 5.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 763       |
| time/                   |           |
|    total_timesteps      | 446500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-11 |
|    explained_variance   | 0.0902    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.68e+04  |
|    n_updates            | 4295      |
|    policy_gradient_loss | 4.27e-09  |
|    value_loss           | 9.07e+04  |
---------------------------------------
Eval num_timesteps=447000, episode_reward=893.21 +/- 724.93
Episode length: 35.32 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 734      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 437      |
|    time_elapsed    | 1544     |
|    total_timesteps | 447488   |
---------------------------------
Eval num_timesteps=447500, episode_reward=896.44 +/- 698.12
Episode length: 36.20 +/- 6.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 896       |
| time/                   |           |
|    total_timesteps      | 447500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-13 |
|    explained_variance   | 0.175     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.75e+04  |
|    n_updates            | 4305      |
|    policy_gradient_loss | -2.33e-11 |
|    value_loss           | 7.37e+04  |
---------------------------------------
Eval num_timesteps=448000, episode_reward=702.07 +/- 661.81
Episode length: 33.74 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=740.63 +/- 584.12
Episode length: 35.62 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 713      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 438      |
|    time_elapsed    | 1548     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=727.43 +/- 612.68
Episode length: 34.26 +/- 7.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 727       |
| time/                   |           |
|    total_timesteps      | 449000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-09 |
|    explained_variance   | 0.0912    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.78e+04  |
|    n_updates            | 4315      |
|    policy_gradient_loss | 8.03e-10  |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=449500, episode_reward=782.20 +/- 649.00
Episode length: 34.52 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 439      |
|    time_elapsed    | 1552     |
|    total_timesteps | 449536   |
---------------------------------
Eval num_timesteps=450000, episode_reward=908.65 +/- 690.17
Episode length: 36.02 +/- 5.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 909       |
| time/                   |           |
|    total_timesteps      | 450000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.79e-10 |
|    explained_variance   | 0.0959    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.46e+04  |
|    n_updates            | 4325      |
|    policy_gradient_loss | 7.8e-10   |
|    value_loss           | 9.75e+04  |
---------------------------------------
Eval num_timesteps=450500, episode_reward=834.26 +/- 673.53
Episode length: 34.76 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 878      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 440      |
|    time_elapsed    | 1555     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=989.35 +/- 677.99
Episode length: 36.94 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 989       |
| time/                   |           |
|    total_timesteps      | 451000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-15 |
|    explained_variance   | 0.0433    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.41e+04  |
|    n_updates            | 4335      |
|    policy_gradient_loss | -8.21e-10 |
|    value_loss           | 9.77e+04  |
---------------------------------------
Eval num_timesteps=451500, episode_reward=780.36 +/- 689.17
Episode length: 34.56 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 992      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 441      |
|    time_elapsed    | 1559     |
|    total_timesteps | 451584   |
---------------------------------
Eval num_timesteps=452000, episode_reward=873.98 +/- 731.09
Episode length: 35.40 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 452000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-15 |
|    explained_variance   | 0.139     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.47e+04  |
|    n_updates            | 4345      |
|    policy_gradient_loss | -1.57e-09 |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=452500, episode_reward=776.14 +/- 665.02
Episode length: 34.44 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 974      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 442      |
|    time_elapsed    | 1562     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=979.55 +/- 711.33
Episode length: 36.78 +/- 5.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 980       |
| time/                   |           |
|    total_timesteps      | 453000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.62e-17 |
|    explained_variance   | 0.0598    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.91e+04  |
|    n_updates            | 4355      |
|    policy_gradient_loss | 1.62e-09  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=453500, episode_reward=920.80 +/- 763.76
Episode length: 34.34 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 905      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 443      |
|    time_elapsed    | 1566     |
|    total_timesteps | 453632   |
---------------------------------
Eval num_timesteps=454000, episode_reward=955.92 +/- 740.68
Episode length: 35.92 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 956       |
| time/                   |           |
|    total_timesteps      | 454000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.76e-12 |
|    explained_variance   | 0.165     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.88e+04  |
|    n_updates            | 4365      |
|    policy_gradient_loss | 1.51e-09  |
|    value_loss           | 1.47e+05  |
---------------------------------------
Eval num_timesteps=454500, episode_reward=852.14 +/- 659.75
Episode length: 35.92 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 444      |
|    time_elapsed    | 1569     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=1016.13 +/- 761.40
Episode length: 36.22 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 455000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-10 |
|    explained_variance   | 0.0968    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.32e+04  |
|    n_updates            | 4375      |
|    policy_gradient_loss | -1.06e-09 |
|    value_loss           | 9.55e+04  |
---------------------------------------
Eval num_timesteps=455500, episode_reward=1005.68 +/- 755.26
Episode length: 35.88 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 445      |
|    time_elapsed    | 1573     |
|    total_timesteps | 455680   |
---------------------------------
Eval num_timesteps=456000, episode_reward=837.09 +/- 696.86
Episode length: 35.12 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 837       |
| time/                   |           |
|    total_timesteps      | 456000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.29e-06 |
|    explained_variance   | 0.169     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.44e+04  |
|    n_updates            | 4385      |
|    policy_gradient_loss | 3.64e-08  |
|    value_loss           | 7.46e+04  |
---------------------------------------
Eval num_timesteps=456500, episode_reward=938.96 +/- 695.86
Episode length: 36.56 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 732      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 446      |
|    time_elapsed    | 1576     |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=457000, episode_reward=674.92 +/- 641.40
Episode length: 35.62 +/- 7.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.6       |
|    mean_reward          | 675        |
| time/                   |            |
|    total_timesteps      | 457000     |
| train/                  |            |
|    approx_kl            | 0.02162181 |
|    clip_fraction        | 0.0406     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.039     |
|    explained_variance   | 0.0987     |
|    learning_rate        | 0.0005     |
|    loss                 | 3.28e+04   |
|    n_updates            | 4388       |
|    policy_gradient_loss | -0.00201   |
|    value_loss           | 6.88e+04   |
----------------------------------------
Eval num_timesteps=457500, episode_reward=488.10 +/- 546.70
Episode length: 33.00 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 632      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 447      |
|    time_elapsed    | 1580     |
|    total_timesteps | 457728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=458000, episode_reward=690.34 +/- 627.51
Episode length: 33.84 +/- 6.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | 690         |
| time/                   |             |
|    total_timesteps      | 458000      |
| train/                  |             |
|    approx_kl            | 0.044490125 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0851     |
|    explained_variance   | 0.119       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.46e+04    |
|    n_updates            | 4389        |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 6.46e+04    |
-----------------------------------------
Eval num_timesteps=458500, episode_reward=751.34 +/- 677.25
Episode length: 34.68 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 598      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 448      |
|    time_elapsed    | 1583     |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=459000, episode_reward=818.01 +/- 673.77
Episode length: 34.96 +/- 7.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 818         |
| time/                   |             |
|    total_timesteps      | 459000      |
| train/                  |             |
|    approx_kl            | 0.021044562 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0207     |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.33e+04    |
|    n_updates            | 4390        |
|    policy_gradient_loss | 0.00602     |
|    value_loss           | 8.66e+04    |
-----------------------------------------
Eval num_timesteps=459500, episode_reward=851.41 +/- 706.32
Episode length: 34.92 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 673      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 449      |
|    time_elapsed    | 1586     |
|    total_timesteps | 459776   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.39
Eval num_timesteps=460000, episode_reward=83.67 +/- 246.14
Episode length: 35.30 +/- 15.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.3       |
|    mean_reward          | 83.7       |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.39329636 |
|    clip_fraction        | 0.0556     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0353    |
|    explained_variance   | 0.0785     |
|    learning_rate        | 0.0005     |
|    loss                 | 5.53e+04   |
|    n_updates            | 4392       |
|    policy_gradient_loss | 0.022      |
|    value_loss           | 1.12e+05   |
----------------------------------------
Eval num_timesteps=460500, episode_reward=40.54 +/- 356.96
Episode length: 34.12 +/- 13.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 40.5     |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 505      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 450      |
|    time_elapsed    | 1589     |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 4.01
Eval num_timesteps=461000, episode_reward=-524.22 +/- 72.89
Episode length: 46.30 +/- 13.28
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 46.3     |
|    mean_reward          | -524     |
| time/                   |          |
|    total_timesteps      | 461000   |
| train/                  |          |
|    approx_kl            | 2.005449 |
|    clip_fraction        | 0.25     |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -0.053   |
|    explained_variance   | 0.284    |
|    learning_rate        | 0.0005   |
|    loss                 | 6.34e+04 |
|    n_updates            | 4393     |
|    policy_gradient_loss | 0.331    |
|    value_loss           | 1.47e+05 |
--------------------------------------
Eval num_timesteps=461500, episode_reward=-530.00 +/- 76.95
Episode length: 47.72 +/- 15.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 203      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 451      |
|    time_elapsed    | 1594     |
|    total_timesteps | 461824   |
---------------------------------
Eval num_timesteps=462000, episode_reward=-531.11 +/- 83.89
Episode length: 53.70 +/- 15.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.7         |
|    mean_reward          | -531         |
| time/                   |              |
|    total_timesteps      | 462000       |
| train/                  |              |
|    approx_kl            | 5.378388e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.41e-05    |
|    explained_variance   | -0.156       |
|    learning_rate        | 0.0005       |
|    loss                 | 9.15e+04     |
|    n_updates            | 4403         |
|    policy_gradient_loss | -5.87e-05    |
|    value_loss           | 1.69e+05     |
------------------------------------------
Eval num_timesteps=462500, episode_reward=-522.86 +/- 76.33
Episode length: 50.72 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.1     |
|    ep_rew_mean     | 31.8     |
| time/              |          |
|    fps             | 289      |
|    iterations      | 452      |
|    time_elapsed    | 1598     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=-528.98 +/- 64.61
Episode length: 52.66 +/- 21.02
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 52.7           |
|    mean_reward          | -529           |
| time/                   |                |
|    total_timesteps      | 463000         |
| train/                  |                |
|    approx_kl            | -2.3283064e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -2.5e-05       |
|    explained_variance   | -0.647         |
|    learning_rate        | 0.0005         |
|    loss                 | 3.67e+04       |
|    n_updates            | 4413           |
|    policy_gradient_loss | 2.02e-06       |
|    value_loss           | 6.8e+04        |
--------------------------------------------
Eval num_timesteps=463500, episode_reward=-541.29 +/- 63.30
Episode length: 56.80 +/- 19.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.1     |
|    ep_rew_mean     | -218     |
| time/              |          |
|    fps             | 289      |
|    iterations      | 453      |
|    time_elapsed    | 1603     |
|    total_timesteps | 463872   |
---------------------------------
Eval num_timesteps=464000, episode_reward=-523.81 +/- 100.09
Episode length: 47.78 +/- 15.97
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 47.8           |
|    mean_reward          | -524           |
| time/                   |                |
|    total_timesteps      | 464000         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -0.00045       |
|    explained_variance   | -0.994         |
|    learning_rate        | 0.0005         |
|    loss                 | 2.21e+04       |
|    n_updates            | 4423           |
|    policy_gradient_loss | 3.08e-05       |
|    value_loss           | 4.24e+04       |
--------------------------------------------
Eval num_timesteps=464500, episode_reward=-526.60 +/- 91.14
Episode length: 49.00 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -421     |
| time/              |          |
|    fps             | 289      |
|    iterations      | 454      |
|    time_elapsed    | 1607     |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=465000, episode_reward=-476.98 +/- 95.39
Episode length: 37.84 +/- 7.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 37.8       |
|    mean_reward          | -477       |
| time/                   |            |
|    total_timesteps      | 465000     |
| train/                  |            |
|    approx_kl            | 0.03551376 |
|    clip_fraction        | 0.0206     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.052     |
|    explained_variance   | -0.429     |
|    learning_rate        | 0.0005     |
|    loss                 | 1.51e+04   |
|    n_updates            | 4426       |
|    policy_gradient_loss | -0.000429  |
|    value_loss           | 3.03e+04   |
----------------------------------------
Eval num_timesteps=465500, episode_reward=-491.80 +/- 86.59
Episode length: 41.00 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | -492     |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 289      |
|    iterations      | 455      |
|    time_elapsed    | 1611     |
|    total_timesteps | 465920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=466000, episode_reward=16.19 +/- 380.04
Episode length: 35.06 +/- 13.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 466000      |
| train/                  |             |
|    approx_kl            | 0.077197805 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.694      |
|    explained_variance   | -0.00868    |
|    learning_rate        | 0.0005      |
|    loss                 | 1.26e+04    |
|    n_updates            | 4427        |
|    policy_gradient_loss | 0.0359      |
|    value_loss           | 2.41e+04    |
-----------------------------------------
Eval num_timesteps=466500, episode_reward=57.33 +/- 392.44
Episode length: 50.32 +/- 54.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | 57.3     |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.9     |
|    ep_rew_mean     | -430     |
| time/              |          |
|    fps             | 289      |
|    iterations      | 456      |
|    time_elapsed    | 1615     |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.25
Eval num_timesteps=467000, episode_reward=726.80 +/- 742.82
Episode length: 35.88 +/- 6.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 727         |
| time/                   |             |
|    total_timesteps      | 467000      |
| train/                  |             |
|    approx_kl            | 0.123288624 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.0105      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.26e+04    |
|    n_updates            | 4428        |
|    policy_gradient_loss | 0.0115      |
|    value_loss           | 2.58e+04    |
-----------------------------------------
Eval num_timesteps=467500, episode_reward=508.27 +/- 575.14
Episode length: 34.22 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 508      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.4     |
|    ep_rew_mean     | -230     |
| time/              |          |
|    fps             | 289      |
|    iterations      | 457      |
|    time_elapsed    | 1618     |
|    total_timesteps | 467968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=468000, episode_reward=247.60 +/- 459.23
Episode length: 33.36 +/- 8.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.4        |
|    mean_reward          | 248         |
| time/                   |             |
|    total_timesteps      | 468000      |
| train/                  |             |
|    approx_kl            | 0.051569536 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.0268      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.2e+04     |
|    n_updates            | 4429        |
|    policy_gradient_loss | 0.0411      |
|    value_loss           | 2.2e+04     |
-----------------------------------------
Eval num_timesteps=468500, episode_reward=388.83 +/- 666.74
Episode length: 35.22 +/- 11.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | -84.2    |
| time/              |          |
|    fps             | 289      |
|    iterations      | 458      |
|    time_elapsed    | 1621     |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.24
Eval num_timesteps=469000, episode_reward=-135.88 +/- 134.41
Episode length: 40.88 +/- 18.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40.9       |
|    mean_reward          | -136       |
| time/                   |            |
|    total_timesteps      | 469000     |
| train/                  |            |
|    approx_kl            | 0.11797291 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.508     |
|    explained_variance   | 0.0559     |
|    learning_rate        | 0.0005     |
|    loss                 | 1e+04      |
|    n_updates            | 4430       |
|    policy_gradient_loss | 0.126      |
|    value_loss           | 2.03e+04   |
----------------------------------------
Eval num_timesteps=469500, episode_reward=-126.19 +/- 112.74
Episode length: 39.54 +/- 14.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.5     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-143.50 +/- 107.31
Episode length: 39.52 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.5     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.1     |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 288      |
|    iterations      | 459      |
|    time_elapsed    | 1627     |
|    total_timesteps | 470016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=470500, episode_reward=-523.92 +/- 70.29
Episode length: 51.54 +/- 20.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.5        |
|    mean_reward          | -524        |
| time/                   |             |
|    total_timesteps      | 470500      |
| train/                  |             |
|    approx_kl            | 0.051868618 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.235      |
|    explained_variance   | 0.00824     |
|    learning_rate        | 0.0005      |
|    loss                 | 1.55e+04    |
|    n_updates            | 4431        |
|    policy_gradient_loss | 0.0506      |
|    value_loss           | 3.19e+04    |
-----------------------------------------
Eval num_timesteps=471000, episode_reward=-528.48 +/- 65.26
Episode length: 50.08 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 288      |
|    iterations      | 460      |
|    time_elapsed    | 1631     |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=471500, episode_reward=-503.67 +/- 83.44
Episode length: 48.64 +/- 18.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.6        |
|    mean_reward          | -504        |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.044743918 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.053      |
|    explained_variance   | -0.0298     |
|    learning_rate        | 0.0005      |
|    loss                 | 1.15e+04    |
|    n_updates            | 4432        |
|    policy_gradient_loss | 0.00897     |
|    value_loss           | 2.28e+04    |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=-537.29 +/- 67.09
Episode length: 52.54 +/- 17.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.7     |
|    ep_rew_mean     | -340     |
| time/              |          |
|    fps             | 288      |
|    iterations      | 461      |
|    time_elapsed    | 1635     |
|    total_timesteps | 472064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.24
Eval num_timesteps=472500, episode_reward=-544.19 +/- 58.52
Episode length: 52.32 +/- 16.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.3       |
|    mean_reward          | -544       |
| time/                   |            |
|    total_timesteps      | 472500     |
| train/                  |            |
|    approx_kl            | 0.04749009 |
|    clip_fraction        | 0.00156    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.00152   |
|    explained_variance   | -0.0806    |
|    learning_rate        | 0.0005     |
|    loss                 | 8.11e+03   |
|    n_updates            | 4433       |
|    policy_gradient_loss | 0.00108    |
|    value_loss           | 1.87e+04   |
----------------------------------------
Eval num_timesteps=473000, episode_reward=-525.10 +/- 87.98
Episode length: 51.30 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.6     |
|    ep_rew_mean     | -450     |
| time/              |          |
|    fps             | 288      |
|    iterations      | 462      |
|    time_elapsed    | 1640     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=-512.61 +/- 96.63
Episode length: 50.56 +/- 15.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -513      |
| time/                   |           |
|    total_timesteps      | 473500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.89e-12 |
|    explained_variance   | -0.283    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.96e+03  |
|    n_updates            | 4443      |
|    policy_gradient_loss | 1.79e-09  |
|    value_loss           | 1.48e+04  |
---------------------------------------
Eval num_timesteps=474000, episode_reward=-539.42 +/- 74.93
Episode length: 51.10 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -508     |
| time/              |          |
|    fps             | 288      |
|    iterations      | 463      |
|    time_elapsed    | 1644     |
|    total_timesteps | 474112   |
---------------------------------
Eval num_timesteps=474500, episode_reward=-531.94 +/- 76.43
Episode length: 47.86 +/- 15.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.9      |
|    mean_reward          | -532      |
| time/                   |           |
|    total_timesteps      | 474500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.76e-29 |
|    explained_variance   | -0.349    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.41e+05  |
|    n_updates            | 4453      |
|    policy_gradient_loss | -3.49e-09 |
|    value_loss           | 1.47e+06  |
---------------------------------------
Eval num_timesteps=475000, episode_reward=-528.22 +/- 85.19
Episode length: 52.06 +/- 17.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 288      |
|    iterations      | 464      |
|    time_elapsed    | 1649     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=-535.04 +/- 73.48
Episode length: 47.96 +/- 14.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48        |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 475500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.71e-18 |
|    explained_variance   | -0.405    |
|    learning_rate        | 0.0005    |
|    loss                 | 1.75e+05  |
|    n_updates            | 4463      |
|    policy_gradient_loss | 9.9e-10   |
|    value_loss           | 3.82e+05  |
---------------------------------------
Eval num_timesteps=476000, episode_reward=-527.51 +/- 83.20
Episode length: 50.24 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 287      |
|    iterations      | 465      |
|    time_elapsed    | 1653     |
|    total_timesteps | 476160   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-525.91 +/- 87.93
Episode length: 47.72 +/- 16.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.7      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 476500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-11 |
|    explained_variance   | -0.436    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.14e+04  |
|    n_updates            | 4473      |
|    policy_gradient_loss | -2.21e-09 |
|    value_loss           | 9.2e+04   |
---------------------------------------
Eval num_timesteps=477000, episode_reward=-525.13 +/- 99.29
Episode length: 49.80 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 287      |
|    iterations      | 466      |
|    time_elapsed    | 1658     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=-544.89 +/- 69.14
Episode length: 54.36 +/- 17.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 54.4          |
|    mean_reward          | -545          |
| time/                   |               |
|    total_timesteps      | 477500        |
| train/                  |               |
|    approx_kl            | 2.0954758e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -9.21e-07     |
|    explained_variance   | -0.317        |
|    learning_rate        | 0.0005        |
|    loss                 | 1.07e+04      |
|    n_updates            | 4483          |
|    policy_gradient_loss | -1.64e-08     |
|    value_loss           | 2.23e+04      |
-------------------------------------------
Eval num_timesteps=478000, episode_reward=-541.13 +/- 61.76
Episode length: 51.78 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 287      |
|    iterations      | 467      |
|    time_elapsed    | 1662     |
|    total_timesteps | 478208   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-538.27 +/- 73.27
Episode length: 51.22 +/- 16.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.2      |
|    mean_reward          | -538      |
| time/                   |           |
|    total_timesteps      | 478500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.24e-08 |
|    explained_variance   | -0.137    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.35e+03  |
|    n_updates            | 4493      |
|    policy_gradient_loss | 2.41e-07  |
|    value_loss           | 1.33e+04  |
---------------------------------------
Eval num_timesteps=479000, episode_reward=-525.92 +/- 72.06
Episode length: 48.58 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 287      |
|    iterations      | 468      |
|    time_elapsed    | 1667     |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 6.16
Eval num_timesteps=479500, episode_reward=-527.58 +/- 74.50
Episode length: 52.86 +/- 20.53
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 52.9     |
|    mean_reward          | -528     |
| time/                   |          |
|    total_timesteps      | 479500   |
| train/                  |          |
|    approx_kl            | 1.232234 |
|    clip_fraction        | 0.0192   |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -0.00295 |
|    explained_variance   | -0.216   |
|    learning_rate        | 0.0005   |
|    loss                 | 1.16e+04 |
|    n_updates            | 4500     |
|    policy_gradient_loss | 0.00584  |
|    value_loss           | 2.13e+04 |
--------------------------------------
Eval num_timesteps=480000, episode_reward=-528.79 +/- 68.03
Episode length: 53.02 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 287      |
|    iterations      | 469      |
|    time_elapsed    | 1672     |
|    total_timesteps | 480256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=480500, episode_reward=-541.39 +/- 58.39
Episode length: 54.56 +/- 21.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 54.6       |
|    mean_reward          | -541       |
| time/                   |            |
|    total_timesteps      | 480500     |
| train/                  |            |
|    approx_kl            | 0.03458581 |
|    clip_fraction        | 0.00391    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0193    |
|    explained_variance   | -0.0217    |
|    learning_rate        | 0.0005     |
|    loss                 | 6.42e+03   |
|    n_updates            | 4501       |
|    policy_gradient_loss | -0.00287   |
|    value_loss           | 1.25e+04   |
----------------------------------------
Eval num_timesteps=481000, episode_reward=-505.93 +/- 93.18
Episode length: 51.56 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 287      |
|    iterations      | 470      |
|    time_elapsed    | 1676     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=-541.35 +/- 59.27
Episode length: 52.38 +/- 13.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.4      |
|    mean_reward          | -541      |
| time/                   |           |
|    total_timesteps      | 481500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.42e-08 |
|    explained_variance   | -0.0288   |
|    learning_rate        | 0.0005    |
|    loss                 | 4.89e+03  |
|    n_updates            | 4511      |
|    policy_gradient_loss | -1.47e-09 |
|    value_loss           | 9.76e+03  |
---------------------------------------
Eval num_timesteps=482000, episode_reward=-523.35 +/- 66.57
Episode length: 54.56 +/- 21.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 471      |
|    time_elapsed    | 1681     |
|    total_timesteps | 482304   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-522.14 +/- 61.59
Episode length: 51.10 +/- 21.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 51.1     |
|    mean_reward          | -522     |
| time/                   |          |
|    total_timesteps      | 482500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0235  |
|    learning_rate        | 0.0005   |
|    loss                 | 2.5e+04  |
|    n_updates            | 4521     |
|    policy_gradient_loss | 3.38e-10 |
|    value_loss           | 4.2e+04  |
--------------------------------------
Eval num_timesteps=483000, episode_reward=-529.95 +/- 51.56
Episode length: 51.64 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 472      |
|    time_elapsed    | 1685     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=-498.14 +/- 86.13
Episode length: 45.28 +/- 15.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.3      |
|    mean_reward          | -498      |
| time/                   |           |
|    total_timesteps      | 483500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.14e-29 |
|    explained_variance   | -0.0232   |
|    learning_rate        | 0.0005    |
|    loss                 | 6.96e+03  |
|    n_updates            | 4531      |
|    policy_gradient_loss | 8.96e-10  |
|    value_loss           | 1.51e+04  |
---------------------------------------
Eval num_timesteps=484000, episode_reward=-513.19 +/- 75.31
Episode length: 49.42 +/- 17.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 473      |
|    time_elapsed    | 1690     |
|    total_timesteps | 484352   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-505.89 +/- 84.89
Episode length: 48.82 +/- 18.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -506      |
| time/                   |           |
|    total_timesteps      | 484500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.92e-31 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.27e+03  |
|    n_updates            | 4541      |
|    policy_gradient_loss | -3.32e-10 |
|    value_loss           | 1.2e+04   |
---------------------------------------
Eval num_timesteps=485000, episode_reward=-512.54 +/- 59.51
Episode length: 49.96 +/- 14.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 474      |
|    time_elapsed    | 1694     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=-514.99 +/- 75.48
Episode length: 50.16 +/- 16.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.2      |
|    mean_reward          | -515      |
| time/                   |           |
|    total_timesteps      | 485500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0194    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.93e+03  |
|    n_updates            | 4551      |
|    policy_gradient_loss | -7.86e-10 |
|    value_loss           | 1.36e+04  |
---------------------------------------
Eval num_timesteps=486000, episode_reward=-517.81 +/- 84.28
Episode length: 44.64 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 475      |
|    time_elapsed    | 1698     |
|    total_timesteps | 486400   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-531.79 +/- 67.36
Episode length: 53.66 +/- 18.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 53.7     |
|    mean_reward          | -532     |
| time/                   |          |
|    total_timesteps      | 486500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0497   |
|    learning_rate        | 0.0005   |
|    loss                 | 2.38e+04 |
|    n_updates            | 4561     |
|    policy_gradient_loss | 2.42e-09 |
|    value_loss           | 4.92e+04 |
--------------------------------------
Eval num_timesteps=487000, episode_reward=-528.79 +/- 53.86
Episode length: 49.72 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 476      |
|    time_elapsed    | 1703     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=-509.47 +/- 69.70
Episode length: 46.12 +/- 15.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.1      |
|    mean_reward          | -509      |
| time/                   |           |
|    total_timesteps      | 487500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.14e-27 |
|    explained_variance   | 0.0418    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.93e+03  |
|    n_updates            | 4571      |
|    policy_gradient_loss | -1.93e-09 |
|    value_loss           | 1.41e+04  |
---------------------------------------
Eval num_timesteps=488000, episode_reward=-510.73 +/- 65.50
Episode length: 45.20 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -508     |
| time/              |          |
|    fps             | 286      |
|    iterations      | 477      |
|    time_elapsed    | 1707     |
|    total_timesteps | 488448   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-501.79 +/- 70.48
Episode length: 49.96 +/- 15.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -502      |
| time/                   |           |
|    total_timesteps      | 488500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.35e-28 |
|    explained_variance   | 0.0752    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.47e+03  |
|    n_updates            | 4581      |
|    policy_gradient_loss | 1.35e-09  |
|    value_loss           | 1.06e+04  |
---------------------------------------
Eval num_timesteps=489000, episode_reward=-521.59 +/- 67.95
Episode length: 47.94 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 478      |
|    time_elapsed    | 1712     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=-522.79 +/- 67.66
Episode length: 48.56 +/- 15.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.6      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 489500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.69e-27 |
|    explained_variance   | 0.0826    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.03e+03  |
|    n_updates            | 4591      |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 1.28e+04  |
---------------------------------------
Eval num_timesteps=490000, episode_reward=-529.95 +/- 70.48
Episode length: 53.50 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -506     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 479      |
|    time_elapsed    | 1716     |
|    total_timesteps | 490496   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-517.37 +/- 80.71
Episode length: 52.82 +/- 19.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 490500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-28 |
|    explained_variance   | 0.133     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.54e+03  |
|    n_updates            | 4601      |
|    policy_gradient_loss | 1.87e-09  |
|    value_loss           | 9.76e+03  |
---------------------------------------
Eval num_timesteps=491000, episode_reward=-537.74 +/- 69.45
Episode length: 52.08 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-516.76 +/- 69.11
Episode length: 50.16 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 480      |
|    time_elapsed    | 1722     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=-534.18 +/- 57.99
Episode length: 51.28 +/- 15.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.3      |
|    mean_reward          | -534      |
| time/                   |           |
|    total_timesteps      | 492000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-26 |
|    explained_variance   | 0.136     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.16e+03  |
|    n_updates            | 4611      |
|    policy_gradient_loss | -1.63e-10 |
|    value_loss           | 1.3e+04   |
---------------------------------------
Eval num_timesteps=492500, episode_reward=-518.55 +/- 88.17
Episode length: 53.16 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 481      |
|    time_elapsed    | 1727     |
|    total_timesteps | 492544   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-502.29 +/- 88.90
Episode length: 56.62 +/- 24.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 56.6      |
|    mean_reward          | -502      |
| time/                   |           |
|    total_timesteps      | 493000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.6e-28  |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.54e+03  |
|    n_updates            | 4621      |
|    policy_gradient_loss | -4.21e-09 |
|    value_loss           | 1.26e+04  |
---------------------------------------
Eval num_timesteps=493500, episode_reward=-523.99 +/- 76.60
Episode length: 54.22 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 482      |
|    time_elapsed    | 1732     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=-533.59 +/- 57.81
Episode length: 53.64 +/- 21.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.6      |
|    mean_reward          | -534      |
| time/                   |           |
|    total_timesteps      | 494000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.55e-26 |
|    explained_variance   | 0.165     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.58e+03  |
|    n_updates            | 4631      |
|    policy_gradient_loss | -2.74e-09 |
|    value_loss           | 1.42e+04  |
---------------------------------------
Eval num_timesteps=494500, episode_reward=-522.19 +/- 67.41
Episode length: 49.22 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 483      |
|    time_elapsed    | 1736     |
|    total_timesteps | 494592   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-522.78 +/- 64.11
Episode length: 49.66 +/- 13.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 495000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-27 |
|    explained_variance   | 0.15      |
|    learning_rate        | 0.0005    |
|    loss                 | 5.87e+03  |
|    n_updates            | 4641      |
|    policy_gradient_loss | -2.9e-09  |
|    value_loss           | 1.24e+04  |
---------------------------------------
Eval num_timesteps=495500, episode_reward=-508.99 +/- 80.33
Episode length: 47.06 +/- 16.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.8     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 484      |
|    time_elapsed    | 1741     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=-517.34 +/- 68.63
Episode length: 48.28 +/- 14.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 496000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.01e-25 |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.3e+03   |
|    n_updates            | 4651      |
|    policy_gradient_loss | 9.2e-10   |
|    value_loss           | 1.38e+04  |
---------------------------------------
Eval num_timesteps=496500, episode_reward=-514.91 +/- 72.36
Episode length: 46.78 +/- 14.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 485      |
|    time_elapsed    | 1745     |
|    total_timesteps | 496640   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-534.13 +/- 62.57
Episode length: 46.84 +/- 16.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.8      |
|    mean_reward          | -534      |
| time/                   |           |
|    total_timesteps      | 497000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.5e-27  |
|    explained_variance   | 0.192     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.58e+03  |
|    n_updates            | 4661      |
|    policy_gradient_loss | -2.04e-10 |
|    value_loss           | 9.9e+03   |
---------------------------------------
Eval num_timesteps=497500, episode_reward=-514.98 +/- 72.32
Episode length: 48.40 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 486      |
|    time_elapsed    | 1749     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=-534.77 +/- 67.08
Episode length: 53.74 +/- 17.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.7      |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 498000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-24 |
|    explained_variance   | 0.218     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.46e+03  |
|    n_updates            | 4671      |
|    policy_gradient_loss | -4.89e-10 |
|    value_loss           | 1.2e+04   |
---------------------------------------
Eval num_timesteps=498500, episode_reward=-536.54 +/- 60.70
Episode length: 48.28 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 487      |
|    time_elapsed    | 1754     |
|    total_timesteps | 498688   |
---------------------------------
Eval num_timesteps=499000, episode_reward=-506.59 +/- 87.87
Episode length: 49.22 +/- 17.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.2      |
|    mean_reward          | -507      |
| time/                   |           |
|    total_timesteps      | 499000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.14e-26 |
|    explained_variance   | 0.275     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.14e+03  |
|    n_updates            | 4681      |
|    policy_gradient_loss | 2.44e-09  |
|    value_loss           | 1.01e+04  |
---------------------------------------
Eval num_timesteps=499500, episode_reward=-536.54 +/- 59.85
Episode length: 54.12 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 488      |
|    time_elapsed    | 1758     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-528.18 +/- 51.56
Episode length: 53.56 +/- 18.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.6      |
|    mean_reward          | -528      |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.03e-24 |
|    explained_variance   | 0.355     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.28e+03  |
|    n_updates            | 4691      |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 1.23e+04  |
---------------------------------------
Eval num_timesteps=500500, episode_reward=-541.99 +/- 51.97
Episode length: 49.38 +/- 14.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -542     |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 489      |
|    time_elapsed    | 1763     |
|    total_timesteps | 500736   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-528.75 +/- 67.51
Episode length: 56.12 +/- 23.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 56.1      |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 501000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.62e-26 |
|    explained_variance   | 0.338     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.42e+03  |
|    n_updates            | 4701      |
|    policy_gradient_loss | 1.13e-09  |
|    value_loss           | 7.11e+03  |
---------------------------------------
Eval num_timesteps=501500, episode_reward=-513.69 +/- 68.43
Episode length: 49.62 +/- 12.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 490      |
|    time_elapsed    | 1768     |
|    total_timesteps | 501760   |
---------------------------------
Eval num_timesteps=502000, episode_reward=-525.74 +/- 59.23
Episode length: 52.98 +/- 19.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53        |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 502000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.37e-23 |
|    explained_variance   | 0.331     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.14e+03  |
|    n_updates            | 4711      |
|    policy_gradient_loss | -2.29e-09 |
|    value_loss           | 9.62e+03  |
---------------------------------------
Eval num_timesteps=502500, episode_reward=-513.19 +/- 65.33
Episode length: 51.38 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 491      |
|    time_elapsed    | 1772     |
|    total_timesteps | 502784   |
---------------------------------
Eval num_timesteps=503000, episode_reward=-520.97 +/- 79.49
Episode length: 49.46 +/- 17.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -521      |
| time/                   |           |
|    total_timesteps      | 503000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.66e-25 |
|    explained_variance   | 0.344     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.3e+03   |
|    n_updates            | 4721      |
|    policy_gradient_loss | 3.03e-10  |
|    value_loss           | 8.2e+03   |
---------------------------------------
Eval num_timesteps=503500, episode_reward=-516.75 +/- 69.11
Episode length: 52.24 +/- 17.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 492      |
|    time_elapsed    | 1777     |
|    total_timesteps | 503808   |
---------------------------------
Eval num_timesteps=504000, episode_reward=-514.90 +/- 75.42
Episode length: 48.06 +/- 18.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.1      |
|    mean_reward          | -515      |
| time/                   |           |
|    total_timesteps      | 504000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.95e-22 |
|    explained_variance   | 0.406     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.27e+03  |
|    n_updates            | 4731      |
|    policy_gradient_loss | -8.27e-10 |
|    value_loss           | 1.1e+04   |
---------------------------------------
Eval num_timesteps=504500, episode_reward=-493.39 +/- 86.56
Episode length: 48.10 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -493     |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 493      |
|    time_elapsed    | 1781     |
|    total_timesteps | 504832   |
---------------------------------
Eval num_timesteps=505000, episode_reward=-507.19 +/- 68.67
Episode length: 52.70 +/- 20.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.7      |
|    mean_reward          | -507      |
| time/                   |           |
|    total_timesteps      | 505000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.62e-24 |
|    explained_variance   | 0.322     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.71e+03  |
|    n_updates            | 4741      |
|    policy_gradient_loss | -2.64e-09 |
|    value_loss           | 5.99e+03  |
---------------------------------------
Eval num_timesteps=505500, episode_reward=-512.56 +/- 79.04
Episode length: 50.10 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 494      |
|    time_elapsed    | 1786     |
|    total_timesteps | 505856   |
---------------------------------
Eval num_timesteps=506000, episode_reward=-536.55 +/- 59.94
Episode length: 49.28 +/- 15.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.3      |
|    mean_reward          | -537      |
| time/                   |           |
|    total_timesteps      | 506000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.65e-20 |
|    explained_variance   | 0.446     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.26e+03  |
|    n_updates            | 4751      |
|    policy_gradient_loss | -1.48e-09 |
|    value_loss           | 9.86e+03  |
---------------------------------------
Eval num_timesteps=506500, episode_reward=-521.55 +/- 67.70
Episode length: 49.60 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 495      |
|    time_elapsed    | 1790     |
|    total_timesteps | 506880   |
---------------------------------
Eval num_timesteps=507000, episode_reward=-534.78 +/- 65.44
Episode length: 50.26 +/- 18.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 507000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.45e-21 |
|    explained_variance   | 0.477     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.7e+03   |
|    n_updates            | 4761      |
|    policy_gradient_loss | -4.03e-09 |
|    value_loss           | 7.08e+03  |
---------------------------------------
Eval num_timesteps=507500, episode_reward=-526.35 +/- 55.44
Episode length: 48.44 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 496      |
|    time_elapsed    | 1794     |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=508000, episode_reward=-544.32 +/- 69.69
Episode length: 54.54 +/- 21.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.5      |
|    mean_reward          | -544      |
| time/                   |           |
|    total_timesteps      | 508000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.16e-21 |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.82e+03  |
|    n_updates            | 4771      |
|    policy_gradient_loss | 3.61e-09  |
|    value_loss           | 9.35e+03  |
---------------------------------------
Eval num_timesteps=508500, episode_reward=-521.54 +/- 57.66
Episode length: 48.82 +/- 14.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 497      |
|    time_elapsed    | 1799     |
|    total_timesteps | 508928   |
---------------------------------
Eval num_timesteps=509000, episode_reward=-526.99 +/- 56.05
Episode length: 52.90 +/- 15.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.9      |
|    mean_reward          | -527      |
| time/                   |           |
|    total_timesteps      | 509000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.35e-23 |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.66e+03  |
|    n_updates            | 4781      |
|    policy_gradient_loss | -1.29e-09 |
|    value_loss           | 6.59e+03  |
---------------------------------------
Eval num_timesteps=509500, episode_reward=-511.38 +/- 68.86
Episode length: 50.64 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 498      |
|    time_elapsed    | 1804     |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-517.39 +/- 77.97
Episode length: 49.86 +/- 20.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.9      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 510000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-18 |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.34e+03  |
|    n_updates            | 4791      |
|    policy_gradient_loss | 2.61e-09  |
|    value_loss           | 9.71e+03  |
---------------------------------------
Eval num_timesteps=510500, episode_reward=-526.30 +/- 87.69
Episode length: 48.68 +/- 19.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 499      |
|    time_elapsed    | 1808     |
|    total_timesteps | 510976   |
---------------------------------
Eval num_timesteps=511000, episode_reward=-525.77 +/- 69.90
Episode length: 54.92 +/- 20.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.9      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 511000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.92e-21 |
|    explained_variance   | 0.461     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.7e+03   |
|    n_updates            | 4801      |
|    policy_gradient_loss | -2.37e-09 |
|    value_loss           | 5.22e+03  |
---------------------------------------
Eval num_timesteps=511500, episode_reward=-527.58 +/- 66.01
Episode length: 49.96 +/- 16.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=-516.75 +/- 76.29
Episode length: 49.00 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 500      |
|    time_elapsed    | 1814     |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=512500, episode_reward=-526.37 +/- 69.12
Episode length: 46.34 +/- 13.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.3      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 512500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.59e-19 |
|    explained_variance   | 0.616     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.47e+03  |
|    n_updates            | 4811      |
|    policy_gradient_loss | -8.38e-10 |
|    value_loss           | 1.02e+04  |
---------------------------------------
Eval num_timesteps=513000, episode_reward=-513.13 +/- 85.66
Episode length: 50.20 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 501      |
|    time_elapsed    | 1819     |
|    total_timesteps | 513024   |
---------------------------------
Eval num_timesteps=513500, episode_reward=-529.99 +/- 62.36
Episode length: 53.30 +/- 16.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.3      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 513500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-20 |
|    explained_variance   | 0.546     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.53e+03  |
|    n_updates            | 4821      |
|    policy_gradient_loss | -3.19e-09 |
|    value_loss           | 7.09e+03  |
---------------------------------------
Eval num_timesteps=514000, episode_reward=-527.27 +/- 84.96
Episode length: 49.96 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 502      |
|    time_elapsed    | 1823     |
|    total_timesteps | 514048   |
---------------------------------
Eval num_timesteps=514500, episode_reward=-502.37 +/- 78.65
Episode length: 50.84 +/- 17.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.8      |
|    mean_reward          | -502      |
| time/                   |           |
|    total_timesteps      | 514500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-18 |
|    explained_variance   | 0.573     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.48e+03  |
|    n_updates            | 4831      |
|    policy_gradient_loss | 5.98e-09  |
|    value_loss           | 9.2e+03   |
---------------------------------------
Eval num_timesteps=515000, episode_reward=-517.38 +/- 78.90
Episode length: 47.34 +/- 15.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 503      |
|    time_elapsed    | 1828     |
|    total_timesteps | 515072   |
---------------------------------
Eval num_timesteps=515500, episode_reward=-525.19 +/- 87.11
Episode length: 53.38 +/- 19.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.4      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 515500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-19 |
|    explained_variance   | 0.515     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.5e+03   |
|    n_updates            | 4841      |
|    policy_gradient_loss | -3.2e-09  |
|    value_loss           | 7.35e+03  |
---------------------------------------
Eval num_timesteps=516000, episode_reward=-517.94 +/- 67.86
Episode length: 52.36 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 504      |
|    time_elapsed    | 1833     |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=516500, episode_reward=-517.97 +/- 89.85
Episode length: 51.26 +/- 19.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.3      |
|    mean_reward          | -518      |
| time/                   |           |
|    total_timesteps      | 516500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.37e-18 |
|    explained_variance   | 0.551     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.19e+03  |
|    n_updates            | 4851      |
|    policy_gradient_loss | -1.85e-09 |
|    value_loss           | 9.43e+03  |
---------------------------------------
Eval num_timesteps=517000, episode_reward=-526.38 +/- 56.75
Episode length: 55.18 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 505      |
|    time_elapsed    | 1838     |
|    total_timesteps | 517120   |
---------------------------------
Eval num_timesteps=517500, episode_reward=-520.35 +/- 67.74
Episode length: 49.18 +/- 16.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.2      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 517500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.08e-19 |
|    explained_variance   | 0.573     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.34e+03  |
|    n_updates            | 4861      |
|    policy_gradient_loss | -1.54e-09 |
|    value_loss           | 6.41e+03  |
---------------------------------------
Eval num_timesteps=518000, episode_reward=-520.87 +/- 79.80
Episode length: 53.02 +/- 19.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 506      |
|    time_elapsed    | 1842     |
|    total_timesteps | 518144   |
---------------------------------
Eval num_timesteps=518500, episode_reward=-526.35 +/- 64.16
Episode length: 54.18 +/- 16.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.2      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 518500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.36e-16 |
|    explained_variance   | 0.622     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.29e+03  |
|    n_updates            | 4871      |
|    policy_gradient_loss | 1.97e-09  |
|    value_loss           | 8.29e+03  |
---------------------------------------
Eval num_timesteps=519000, episode_reward=-482.95 +/- 101.60
Episode length: 46.28 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -483     |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 507      |
|    time_elapsed    | 1847     |
|    total_timesteps | 519168   |
---------------------------------
Eval num_timesteps=519500, episode_reward=-516.75 +/- 63.41
Episode length: 52.20 +/- 15.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 519500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.46e-18 |
|    explained_variance   | 0.592     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.56e+03  |
|    n_updates            | 4881      |
|    policy_gradient_loss | -2.78e-09 |
|    value_loss           | 6.6e+03   |
---------------------------------------
Eval num_timesteps=520000, episode_reward=-514.30 +/- 75.29
Episode length: 50.28 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 508      |
|    time_elapsed    | 1851     |
|    total_timesteps | 520192   |
---------------------------------
Eval num_timesteps=520500, episode_reward=-529.93 +/- 68.15
Episode length: 48.52 +/- 16.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 520500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.59e-16 |
|    explained_variance   | 0.712     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.91e+03  |
|    n_updates            | 4891      |
|    policy_gradient_loss | -1.68e-09 |
|    value_loss           | 7.46e+03  |
---------------------------------------
Eval num_timesteps=521000, episode_reward=-530.59 +/- 59.30
Episode length: 48.84 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 509      |
|    time_elapsed    | 1856     |
|    total_timesteps | 521216   |
---------------------------------
Eval num_timesteps=521500, episode_reward=-514.97 +/- 70.03
Episode length: 47.44 +/- 17.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.4      |
|    mean_reward          | -515      |
| time/                   |           |
|    total_timesteps      | 521500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.59e-17 |
|    explained_variance   | 0.642     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.76e+03  |
|    n_updates            | 4901      |
|    policy_gradient_loss | 1.8e-10   |
|    value_loss           | 5.13e+03  |
---------------------------------------
Eval num_timesteps=522000, episode_reward=-516.18 +/- 66.45
Episode length: 51.40 +/- 19.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 510      |
|    time_elapsed    | 1860     |
|    total_timesteps | 522240   |
---------------------------------
Eval num_timesteps=522500, episode_reward=-535.98 +/- 60.89
Episode length: 48.52 +/- 13.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -536      |
| time/                   |           |
|    total_timesteps      | 522500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-14 |
|    explained_variance   | 0.68      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.8e+03   |
|    n_updates            | 4911      |
|    policy_gradient_loss | 5.22e-09  |
|    value_loss           | 7.61e+03  |
---------------------------------------
Eval num_timesteps=523000, episode_reward=-505.36 +/- 89.91
Episode length: 47.22 +/- 14.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 511      |
|    time_elapsed    | 1865     |
|    total_timesteps | 523264   |
---------------------------------
Eval num_timesteps=523500, episode_reward=-520.35 +/- 77.25
Episode length: 51.40 +/- 16.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 523500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.71e-16 |
|    explained_variance   | 0.574     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.33e+03  |
|    n_updates            | 4921      |
|    policy_gradient_loss | 1.51e-09  |
|    value_loss           | 4.45e+03  |
---------------------------------------
Eval num_timesteps=524000, episode_reward=-504.79 +/- 79.14
Episode length: 49.50 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 512      |
|    time_elapsed    | 1870     |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=524500, episode_reward=-494.48 +/- 84.77
Episode length: 50.36 +/- 17.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -494      |
| time/                   |           |
|    total_timesteps      | 524500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-16 |
|    explained_variance   | 0.609     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.28e+03  |
|    n_updates            | 4931      |
|    policy_gradient_loss | -7.51e-10 |
|    value_loss           | 8.23e+03  |
---------------------------------------
Eval num_timesteps=525000, episode_reward=-522.79 +/- 54.38
Episode length: 49.74 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 513      |
|    time_elapsed    | 1874     |
|    total_timesteps | 525312   |
---------------------------------
Eval num_timesteps=525500, episode_reward=-522.11 +/- 66.10
Episode length: 49.50 +/- 16.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 525500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.33e-18 |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.72e+03  |
|    n_updates            | 4941      |
|    policy_gradient_loss | -4.81e-09 |
|    value_loss           | 4.96e+03  |
---------------------------------------
Eval num_timesteps=526000, episode_reward=-518.52 +/- 75.73
Episode length: 47.66 +/- 16.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53       |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 514      |
|    time_elapsed    | 1879     |
|    total_timesteps | 526336   |
---------------------------------
Eval num_timesteps=526500, episode_reward=-525.18 +/- 62.86
Episode length: 50.38 +/- 16.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 526500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.46e-14 |
|    explained_variance   | 0.621     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.45e+03  |
|    n_updates            | 4951      |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 6.46e+03  |
---------------------------------------
Eval num_timesteps=527000, episode_reward=-511.95 +/- 69.26
Episode length: 50.10 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 515      |
|    time_elapsed    | 1883     |
|    total_timesteps | 527360   |
---------------------------------
Eval num_timesteps=527500, episode_reward=-520.35 +/- 82.53
Episode length: 57.42 +/- 19.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 57.4      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 527500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.7e-15  |
|    explained_variance   | 0.629     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.3e+03   |
|    n_updates            | 4961      |
|    policy_gradient_loss | -2.85e-10 |
|    value_loss           | 4.61e+03  |
---------------------------------------
Eval num_timesteps=528000, episode_reward=-529.39 +/- 68.23
Episode length: 49.26 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 516      |
|    time_elapsed    | 1888     |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=528500, episode_reward=-543.18 +/- 47.07
Episode length: 50.38 +/- 16.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -543      |
| time/                   |           |
|    total_timesteps      | 528500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.36e-14 |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.52e+03  |
|    n_updates            | 4971      |
|    policy_gradient_loss | 2.39e-10  |
|    value_loss           | 6.86e+03  |
---------------------------------------
Eval num_timesteps=529000, episode_reward=-518.58 +/- 59.73
Episode length: 49.30 +/- 15.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 517      |
|    time_elapsed    | 1892     |
|    total_timesteps | 529408   |
---------------------------------
Eval num_timesteps=529500, episode_reward=-545.56 +/- 51.39
Episode length: 48.90 +/- 13.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.9      |
|    mean_reward          | -546      |
| time/                   |           |
|    total_timesteps      | 529500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.77e-16 |
|    explained_variance   | 0.622     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.7e+03   |
|    n_updates            | 4981      |
|    policy_gradient_loss | 2.04e-10  |
|    value_loss           | 5.44e+03  |
---------------------------------------
Eval num_timesteps=530000, episode_reward=-514.97 +/- 65.79
Episode length: 49.74 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 518      |
|    time_elapsed    | 1897     |
|    total_timesteps | 530432   |
---------------------------------
Eval num_timesteps=530500, episode_reward=-504.63 +/- 78.05
Episode length: 50.34 +/- 19.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -505      |
| time/                   |           |
|    total_timesteps      | 530500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-12 |
|    explained_variance   | 0.686     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.58e+03  |
|    n_updates            | 4991      |
|    policy_gradient_loss | 1.71e-09  |
|    value_loss           | 6.49e+03  |
---------------------------------------
Eval num_timesteps=531000, episode_reward=-525.75 +/- 62.20
Episode length: 52.24 +/- 18.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 519      |
|    time_elapsed    | 1901     |
|    total_timesteps | 531456   |
---------------------------------
Eval num_timesteps=531500, episode_reward=-533.57 +/- 65.43
Episode length: 49.06 +/- 17.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -534      |
| time/                   |           |
|    total_timesteps      | 531500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.47e-14 |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.49e+03  |
|    n_updates            | 5001      |
|    policy_gradient_loss | 1.06e-09  |
|    value_loss           | 4.28e+03  |
---------------------------------------
Eval num_timesteps=532000, episode_reward=-520.37 +/- 87.44
Episode length: 50.94 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 520      |
|    time_elapsed    | 1906     |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=532500, episode_reward=-515.59 +/- 78.56
Episode length: 49.90 +/- 16.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.9      |
|    mean_reward          | -516      |
| time/                   |           |
|    total_timesteps      | 532500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.57e-13 |
|    explained_variance   | 0.625     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.28e+03  |
|    n_updates            | 5011      |
|    policy_gradient_loss | -2.26e-09 |
|    value_loss           | 8.64e+03  |
---------------------------------------
Eval num_timesteps=533000, episode_reward=-512.58 +/- 71.08
Episode length: 49.14 +/- 18.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=-521.51 +/- 75.04
Episode length: 47.40 +/- 14.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 521      |
|    time_elapsed    | 1912     |
|    total_timesteps | 533504   |
---------------------------------
Eval num_timesteps=534000, episode_reward=-537.71 +/- 56.70
Episode length: 57.28 +/- 19.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 57.3      |
|    mean_reward          | -538      |
| time/                   |           |
|    total_timesteps      | 534000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.55e-16 |
|    explained_variance   | 0.667     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.71e+03  |
|    n_updates            | 5021      |
|    policy_gradient_loss | 5.03e-09  |
|    value_loss           | 6.06e+03  |
---------------------------------------
Eval num_timesteps=534500, episode_reward=-511.30 +/- 77.77
Episode length: 45.50 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 522      |
|    time_elapsed    | 1916     |
|    total_timesteps | 534528   |
---------------------------------
Eval num_timesteps=535000, episode_reward=-535.38 +/- 60.15
Episode length: 50.14 +/- 16.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 535000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-10 |
|    explained_variance   | 0.737     |
|    learning_rate        | 0.0005    |
|    loss                 | 3e+03     |
|    n_updates            | 5031      |
|    policy_gradient_loss | 1e-09     |
|    value_loss           | 5.57e+03  |
---------------------------------------
Eval num_timesteps=535500, episode_reward=-508.99 +/- 81.89
Episode length: 49.86 +/- 18.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 523      |
|    time_elapsed    | 1921     |
|    total_timesteps | 535552   |
---------------------------------
Eval num_timesteps=536000, episode_reward=-520.38 +/- 64.70
Episode length: 47.18 +/- 18.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.2      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 536000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.49e-12 |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.25e+03  |
|    n_updates            | 5041      |
|    policy_gradient_loss | -1.92e-10 |
|    value_loss           | 4.66e+03  |
---------------------------------------
Eval num_timesteps=536500, episode_reward=-519.76 +/- 60.44
Episode length: 49.00 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 524      |
|    time_elapsed    | 1925     |
|    total_timesteps | 536576   |
---------------------------------
Eval num_timesteps=537000, episode_reward=-527.58 +/- 76.38
Episode length: 46.76 +/- 15.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.8      |
|    mean_reward          | -528      |
| time/                   |           |
|    total_timesteps      | 537000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.57e-12 |
|    explained_variance   | 0.691     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.63e+03  |
|    n_updates            | 5051      |
|    policy_gradient_loss | -1.36e-09 |
|    value_loss           | 6.6e+03   |
---------------------------------------
Eval num_timesteps=537500, episode_reward=-523.95 +/- 69.23
Episode length: 53.44 +/- 20.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 525      |
|    time_elapsed    | 1929     |
|    total_timesteps | 537600   |
---------------------------------
Eval num_timesteps=538000, episode_reward=-516.78 +/- 71.18
Episode length: 47.24 +/- 16.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.2      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 538000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.68e-13 |
|    explained_variance   | 0.717     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.77e+03  |
|    n_updates            | 5061      |
|    policy_gradient_loss | -4.97e-09 |
|    value_loss           | 4.5e+03   |
---------------------------------------
Eval num_timesteps=538500, episode_reward=-524.59 +/- 65.70
Episode length: 51.86 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 526      |
|    time_elapsed    | 1934     |
|    total_timesteps | 538624   |
---------------------------------
Eval num_timesteps=539000, episode_reward=-510.69 +/- 76.77
Episode length: 47.52 +/- 17.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.5      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 539000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.44e-09 |
|    explained_variance   | 0.755     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.65e+03  |
|    n_updates            | 5071      |
|    policy_gradient_loss | -9.31e-10 |
|    value_loss           | 5.7e+03   |
---------------------------------------
Eval num_timesteps=539500, episode_reward=-528.02 +/- 70.69
Episode length: 48.08 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 527      |
|    time_elapsed    | 1938     |
|    total_timesteps | 539648   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-522.19 +/- 68.73
Episode length: 53.22 +/- 18.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 540000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.71e-11 |
|    explained_variance   | 0.637     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.79e+03  |
|    n_updates            | 5081      |
|    policy_gradient_loss | 4.39e-09  |
|    value_loss           | 5.47e+03  |
---------------------------------------
Eval num_timesteps=540500, episode_reward=-534.15 +/- 76.63
Episode length: 50.74 +/- 14.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 528      |
|    time_elapsed    | 1943     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=541000, episode_reward=-540.76 +/- 59.88
Episode length: 50.90 +/- 16.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -541      |
| time/                   |           |
|    total_timesteps      | 541000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.32e-10 |
|    explained_variance   | 0.722     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.58e+03  |
|    n_updates            | 5091      |
|    policy_gradient_loss | -1.94e-09 |
|    value_loss           | 5.13e+03  |
---------------------------------------
Eval num_timesteps=541500, episode_reward=-528.74 +/- 75.36
Episode length: 52.34 +/- 20.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 529      |
|    time_elapsed    | 1947     |
|    total_timesteps | 541696   |
---------------------------------
Eval num_timesteps=542000, episode_reward=-506.56 +/- 77.91
Episode length: 45.64 +/- 16.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.6      |
|    mean_reward          | -507      |
| time/                   |           |
|    total_timesteps      | 542000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-11 |
|    explained_variance   | 0.664     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.98e+03  |
|    n_updates            | 5101      |
|    policy_gradient_loss | -1.77e-09 |
|    value_loss           | 5.34e+03  |
---------------------------------------
Eval num_timesteps=542500, episode_reward=-516.64 +/- 77.27
Episode length: 50.64 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 530      |
|    time_elapsed    | 1952     |
|    total_timesteps | 542720   |
---------------------------------
Eval num_timesteps=543000, episode_reward=-513.67 +/- 80.40
Episode length: 47.26 +/- 15.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.3      |
|    mean_reward          | -514      |
| time/                   |           |
|    total_timesteps      | 543000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.83e-08 |
|    explained_variance   | 0.813     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.13e+03  |
|    n_updates            | 5111      |
|    policy_gradient_loss | 1.83e-09  |
|    value_loss           | 3.83e+03  |
---------------------------------------
Eval num_timesteps=543500, episode_reward=-510.19 +/- 69.72
Episode length: 46.60 +/- 14.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 531      |
|    time_elapsed    | 1956     |
|    total_timesteps | 543744   |
---------------------------------
Eval num_timesteps=544000, episode_reward=-530.45 +/- 68.59
Episode length: 51.36 +/- 19.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 544000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-09 |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.59e+03  |
|    n_updates            | 5121      |
|    policy_gradient_loss | 6.44e-09  |
|    value_loss           | 4.08e+03  |
---------------------------------------
Eval num_timesteps=544500, episode_reward=-501.79 +/- 71.50
Episode length: 49.44 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -536     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 532      |
|    time_elapsed    | 1960     |
|    total_timesteps | 544768   |
---------------------------------
Eval num_timesteps=545000, episode_reward=-537.19 +/- 68.92
Episode length: 50.00 +/- 15.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -537      |
| time/                   |           |
|    total_timesteps      | 545000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.44e-10 |
|    explained_variance   | 0.785     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.45e+03  |
|    n_updates            | 5131      |
|    policy_gradient_loss | -1.44e-09 |
|    value_loss           | 6.08e+03  |
---------------------------------------
Eval num_timesteps=545500, episode_reward=-506.55 +/- 82.12
Episode length: 49.68 +/- 17.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -540     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 533      |
|    time_elapsed    | 1965     |
|    total_timesteps | 545792   |
---------------------------------
Eval num_timesteps=546000, episode_reward=-521.54 +/- 67.41
Episode length: 51.68 +/- 18.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 546000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.49e-11 |
|    explained_variance   | 0.707     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.16e+03  |
|    n_updates            | 5141      |
|    policy_gradient_loss | -2.04e-09 |
|    value_loss           | 4.57e+03  |
---------------------------------------
Eval num_timesteps=546500, episode_reward=-508.31 +/- 67.45
Episode length: 47.92 +/- 14.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -538     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 534      |
|    time_elapsed    | 1969     |
|    total_timesteps | 546816   |
---------------------------------
Eval num_timesteps=547000, episode_reward=-503.58 +/- 69.94
Episode length: 48.82 +/- 17.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -504      |
| time/                   |           |
|    total_timesteps      | 547000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.96e-10 |
|    explained_variance   | 0.695     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.89e+03  |
|    n_updates            | 5151      |
|    policy_gradient_loss | -1.57e-10 |
|    value_loss           | 6.47e+03  |
---------------------------------------
Eval num_timesteps=547500, episode_reward=-522.78 +/- 72.29
Episode length: 48.90 +/- 14.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 535      |
|    time_elapsed    | 1974     |
|    total_timesteps | 547840   |
---------------------------------
Eval num_timesteps=548000, episode_reward=-525.18 +/- 69.93
Episode length: 49.58 +/- 16.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 548000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.57e-12 |
|    explained_variance   | 0.722     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.47e+03  |
|    n_updates            | 5161      |
|    policy_gradient_loss | 1.27e-09  |
|    value_loss           | 3.4e+03   |
---------------------------------------
Eval num_timesteps=548500, episode_reward=-537.79 +/- 53.80
Episode length: 50.38 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 536      |
|    time_elapsed    | 1978     |
|    total_timesteps | 548864   |
---------------------------------
Eval num_timesteps=549000, episode_reward=-502.25 +/- 73.96
Episode length: 49.38 +/- 18.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.4      |
|    mean_reward          | -502      |
| time/                   |           |
|    total_timesteps      | 549000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.03e-09 |
|    explained_variance   | 0.716     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.84e+03  |
|    n_updates            | 5171      |
|    policy_gradient_loss | 6.36e-09  |
|    value_loss           | 5.25e+03  |
---------------------------------------
Eval num_timesteps=549500, episode_reward=-516.19 +/- 78.60
Episode length: 55.34 +/- 16.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 537      |
|    time_elapsed    | 1983     |
|    total_timesteps | 549888   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-522.66 +/- 78.95
Episode length: 47.08 +/- 16.26
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47.1     |
|    mean_reward          | -523     |
| time/                   |          |
|    total_timesteps      | 550000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.2e-10 |
|    explained_variance   | 0.753    |
|    learning_rate        | 0.0005   |
|    loss                 | 2.92e+03 |
|    n_updates            | 5181     |
|    policy_gradient_loss | 1.32e-09 |
|    value_loss           | 5.01e+03 |
--------------------------------------
Eval num_timesteps=550500, episode_reward=-516.19 +/- 68.04
Episode length: 47.70 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 538      |
|    time_elapsed    | 1987     |
|    total_timesteps | 550912   |
---------------------------------
Eval num_timesteps=551000, episode_reward=-517.39 +/- 73.22
Episode length: 53.78 +/- 19.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.8      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 551000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.54e-09 |
|    explained_variance   | 0.773     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.79e+03  |
|    n_updates            | 5191      |
|    policy_gradient_loss | 2.17e-09  |
|    value_loss           | 4.62e+03  |
---------------------------------------
Eval num_timesteps=551500, episode_reward=-522.16 +/- 78.57
Episode length: 50.48 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 539      |
|    time_elapsed    | 1992     |
|    total_timesteps | 551936   |
---------------------------------
Eval num_timesteps=552000, episode_reward=-515.53 +/- 67.18
Episode length: 50.46 +/- 15.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -516      |
| time/                   |           |
|    total_timesteps      | 552000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.23e-11 |
|    explained_variance   | 0.72      |
|    learning_rate        | 0.0005    |
|    loss                 | 2.26e+03  |
|    n_updates            | 5201      |
|    policy_gradient_loss | -2.26e-09 |
|    value_loss           | 4.05e+03  |
---------------------------------------
Eval num_timesteps=552500, episode_reward=-531.19 +/- 57.98
Episode length: 50.42 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 540      |
|    time_elapsed    | 1996     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=553000, episode_reward=-520.98 +/- 80.33
Episode length: 49.40 +/- 19.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 49.4          |
|    mean_reward          | -521          |
| time/                   |               |
|    total_timesteps      | 553000        |
| train/                  |               |
|    approx_kl            | -8.731149e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -3.52e-06     |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.0005        |
|    loss                 | 1.79e+03      |
|    n_updates            | 5211          |
|    policy_gradient_loss | 1.03e-08      |
|    value_loss           | 3.97e+03      |
-------------------------------------------
Eval num_timesteps=553500, episode_reward=-529.38 +/- 61.86
Episode length: 49.10 +/- 12.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 541      |
|    time_elapsed    | 2001     |
|    total_timesteps | 553984   |
---------------------------------
Eval num_timesteps=554000, episode_reward=-521.57 +/- 64.42
Episode length: 49.64 +/- 21.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 554000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.66e-07 |
|    explained_variance   | 0.778     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.18e+03  |
|    n_updates            | 5221      |
|    policy_gradient_loss | 6.3e-08   |
|    value_loss           | 3.44e+03  |
---------------------------------------
Eval num_timesteps=554500, episode_reward=-523.32 +/- 68.42
Episode length: 49.54 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=-516.76 +/- 70.71
Episode length: 50.12 +/- 19.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 542      |
|    time_elapsed    | 2007     |
|    total_timesteps | 555008   |
---------------------------------
Eval num_timesteps=555500, episode_reward=-504.19 +/- 73.10
Episode length: 47.50 +/- 17.27
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 47.5           |
|    mean_reward          | -504           |
| time/                   |                |
|    total_timesteps      | 555500         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -1.12e-07      |
|    explained_variance   | 0.817          |
|    learning_rate        | 0.0005         |
|    loss                 | 1.94e+03       |
|    n_updates            | 5231           |
|    policy_gradient_loss | -7.16e-10      |
|    value_loss           | 4.31e+03       |
--------------------------------------------
Eval num_timesteps=556000, episode_reward=-514.38 +/- 50.93
Episode length: 49.38 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 543      |
|    time_elapsed    | 2011     |
|    total_timesteps | 556032   |
---------------------------------
Eval num_timesteps=556500, episode_reward=-518.55 +/- 81.84
Episode length: 55.26 +/- 17.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 55.3      |
|    mean_reward          | -519      |
| time/                   |           |
|    total_timesteps      | 556500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.5e-09  |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.67e+03  |
|    n_updates            | 5241      |
|    policy_gradient_loss | -5.72e-09 |
|    value_loss           | 3.6e+03   |
---------------------------------------
Eval num_timesteps=557000, episode_reward=-520.39 +/- 67.15
Episode length: 49.02 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 544      |
|    time_elapsed    | 2016     |
|    total_timesteps | 557056   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=557500, episode_reward=-519.79 +/- 70.53
Episode length: 48.72 +/- 13.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -520        |
| time/                   |             |
|    total_timesteps      | 557500      |
| train/                  |             |
|    approx_kl            | 0.033519063 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0183     |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.64e+03    |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.000947   |
|    value_loss           | 3.81e+03    |
-----------------------------------------
Eval num_timesteps=558000, episode_reward=-525.76 +/- 76.50
Episode length: 51.04 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 545      |
|    time_elapsed    | 2020     |
|    total_timesteps | 558080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=558500, episode_reward=-522.18 +/- 73.78
Episode length: 49.72 +/- 16.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.7       |
|    mean_reward          | -522       |
| time/                   |            |
|    total_timesteps      | 558500     |
| train/                  |            |
|    approx_kl            | 0.03611193 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0204    |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.75e+03   |
|    n_updates            | 5251       |
|    policy_gradient_loss | 0.00342    |
|    value_loss           | 3.92e+03   |
----------------------------------------
Eval num_timesteps=559000, episode_reward=-529.39 +/- 59.49
Episode length: 49.64 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 546      |
|    time_elapsed    | 2025     |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=559500, episode_reward=-526.95 +/- 65.78
Episode length: 51.14 +/- 21.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -527        |
| time/                   |             |
|    total_timesteps      | 559500      |
| train/                  |             |
|    approx_kl            | 0.029155651 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00439    |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21e+03    |
|    n_updates            | 5252        |
|    policy_gradient_loss | 0.000524    |
|    value_loss           | 2.46e+03    |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=-522.10 +/- 67.76
Episode length: 51.82 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 547      |
|    time_elapsed    | 2029     |
|    total_timesteps | 560128   |
---------------------------------
Eval num_timesteps=560500, episode_reward=-526.81 +/- 73.27
Episode length: 55.24 +/- 17.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.2         |
|    mean_reward          | -527         |
| time/                   |              |
|    total_timesteps      | 560500       |
| train/                  |              |
|    approx_kl            | 4.656613e-10 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0028      |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.81e+03     |
|    n_updates            | 5262         |
|    policy_gradient_loss | -4.73e-05    |
|    value_loss           | 4.04e+03     |
------------------------------------------
Eval num_timesteps=561000, episode_reward=-520.98 +/- 68.48
Episode length: 51.44 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 548      |
|    time_elapsed    | 2034     |
|    total_timesteps | 561152   |
---------------------------------
Eval num_timesteps=561500, episode_reward=-511.93 +/- 81.18
Episode length: 50.74 +/- 19.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -512      |
| time/                   |           |
|    total_timesteps      | 561500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.96e-07 |
|    explained_variance   | 0.672     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.87e+03  |
|    n_updates            | 5272      |
|    policy_gradient_loss | -2.03e-08 |
|    value_loss           | 4.34e+03  |
---------------------------------------
Eval num_timesteps=562000, episode_reward=-510.19 +/- 66.00
Episode length: 49.92 +/- 16.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 549      |
|    time_elapsed    | 2038     |
|    total_timesteps | 562176   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.12
Eval num_timesteps=562500, episode_reward=-495.43 +/- 71.40
Episode length: 50.40 +/- 17.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.4       |
|    mean_reward          | -495       |
| time/                   |            |
|    total_timesteps      | 562500     |
| train/                  |            |
|    approx_kl            | 0.04084016 |
|    clip_fraction        | 0.0067     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.00473   |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.09e+03   |
|    n_updates            | 5276       |
|    policy_gradient_loss | -0.000335  |
|    value_loss           | 4.28e+03   |
----------------------------------------
Eval num_timesteps=563000, episode_reward=-519.73 +/- 57.02
Episode length: 48.54 +/- 13.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 550      |
|    time_elapsed    | 2042     |
|    total_timesteps | 563200   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=563500, episode_reward=-503.71 +/- 70.26
Episode length: 50.00 +/- 17.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50           |
|    mean_reward          | -504         |
| time/                   |              |
|    total_timesteps      | 563500       |
| train/                  |              |
|    approx_kl            | 0.0145312715 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0123      |
|    explained_variance   | 0.767        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.35e+03     |
|    n_updates            | 5278         |
|    policy_gradient_loss | 0.0016       |
|    value_loss           | 2.96e+03     |
------------------------------------------
Eval num_timesteps=564000, episode_reward=-512.42 +/- 71.02
Episode length: 50.92 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 551      |
|    time_elapsed    | 2047     |
|    total_timesteps | 564224   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=564500, episode_reward=-526.35 +/- 60.73
Episode length: 52.18 +/- 18.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.2       |
|    mean_reward          | -526       |
| time/                   |            |
|    total_timesteps      | 564500     |
| train/                  |            |
|    approx_kl            | 0.03920773 |
|    clip_fraction        | 0.0136     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.00586   |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.68e+03   |
|    n_updates            | 5282       |
|    policy_gradient_loss | -0.000568  |
|    value_loss           | 3.29e+03   |
----------------------------------------
Eval num_timesteps=565000, episode_reward=-541.98 +/- 40.25
Episode length: 53.22 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -542     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 552      |
|    time_elapsed    | 2051     |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=565500, episode_reward=-528.19 +/- 59.94
Episode length: 53.46 +/- 17.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 53.5          |
|    mean_reward          | -528          |
| time/                   |               |
|    total_timesteps      | 565500        |
| train/                  |               |
|    approx_kl            | 1.9649742e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -2.01e-05     |
|    explained_variance   | 0.826         |
|    learning_rate        | 0.0005        |
|    loss                 | 1.41e+03      |
|    n_updates            | 5292          |
|    policy_gradient_loss | 0.000209      |
|    value_loss           | 2.73e+03      |
-------------------------------------------
Eval num_timesteps=566000, episode_reward=-518.59 +/- 62.67
Episode length: 49.28 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 553      |
|    time_elapsed    | 2056     |
|    total_timesteps | 566272   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.06
Eval num_timesteps=566500, episode_reward=-507.26 +/- 78.97
Episode length: 50.80 +/- 18.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -507        |
| time/                   |             |
|    total_timesteps      | 566500      |
| train/                  |             |
|    approx_kl            | 0.024513364 |
|    clip_fraction        | 0.00318     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00362    |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0005      |
|    loss                 | 2.2e+03     |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.000642   |
|    value_loss           | 4.42e+03    |
-----------------------------------------
Eval num_timesteps=567000, episode_reward=-502.26 +/- 72.21
Episode length: 46.20 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 554      |
|    time_elapsed    | 2060     |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.15
Eval num_timesteps=567500, episode_reward=-519.58 +/- 69.24
Episode length: 53.02 +/- 22.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53         |
|    mean_reward          | -520       |
| time/                   |            |
|    total_timesteps      | 567500     |
| train/                  |            |
|    approx_kl            | 0.07669377 |
|    clip_fraction        | 0.0508     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0347    |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.07e+03   |
|    n_updates            | 5302       |
|    policy_gradient_loss | 0.0102     |
|    value_loss           | 3.67e+03   |
----------------------------------------
Eval num_timesteps=568000, episode_reward=-522.98 +/- 80.15
Episode length: 52.24 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 555      |
|    time_elapsed    | 2065     |
|    total_timesteps | 568320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.17
Eval num_timesteps=568500, episode_reward=-496.67 +/- 87.91
Episode length: 46.20 +/- 14.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.2       |
|    mean_reward          | -497       |
| time/                   |            |
|    total_timesteps      | 568500     |
| train/                  |            |
|    approx_kl            | 0.02982415 |
|    clip_fraction        | 0.0117     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0144    |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.0005     |
|    loss                 | 991        |
|    n_updates            | 5303       |
|    policy_gradient_loss | 0.0114     |
|    value_loss           | 2.82e+03   |
----------------------------------------
Eval num_timesteps=569000, episode_reward=-522.69 +/- 70.11
Episode length: 53.02 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 556      |
|    time_elapsed    | 2070     |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=569500, episode_reward=-496.38 +/- 89.90
Episode length: 46.06 +/- 17.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.1       |
|    mean_reward          | -496       |
| time/                   |            |
|    total_timesteps      | 569500     |
| train/                  |            |
|    approx_kl            | 0.06741416 |
|    clip_fraction        | 0.063      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.08      |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.16e+03   |
|    n_updates            | 5306       |
|    policy_gradient_loss | 9.56e-05   |
|    value_loss           | 2.44e+03   |
----------------------------------------
Eval num_timesteps=570000, episode_reward=-508.95 +/- 80.03
Episode length: 52.78 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 557      |
|    time_elapsed    | 2074     |
|    total_timesteps | 570368   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=570500, episode_reward=-531.73 +/- 61.44
Episode length: 50.62 +/- 15.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.6       |
|    mean_reward          | -532       |
| time/                   |            |
|    total_timesteps      | 570500     |
| train/                  |            |
|    approx_kl            | 0.09290396 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.164     |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.66e+03   |
|    n_updates            | 5308       |
|    policy_gradient_loss | 0.00789    |
|    value_loss           | 7.49e+03   |
----------------------------------------
Eval num_timesteps=571000, episode_reward=-481.36 +/- 77.50
Episode length: 47.94 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -481     |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 558      |
|    time_elapsed    | 2078     |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=571500, episode_reward=-532.95 +/- 62.23
Episode length: 48.52 +/- 14.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.5       |
|    mean_reward          | -533       |
| time/                   |            |
|    total_timesteps      | 571500     |
| train/                  |            |
|    approx_kl            | 0.02938115 |
|    clip_fraction        | 0.0938     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.28e+03   |
|    n_updates            | 5309       |
|    policy_gradient_loss | 0.00472    |
|    value_loss           | 5.1e+03    |
----------------------------------------
Eval num_timesteps=572000, episode_reward=-514.39 +/- 80.05
Episode length: 50.08 +/- 20.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.5     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 559      |
|    time_elapsed    | 2083     |
|    total_timesteps | 572416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=572500, episode_reward=-528.78 +/- 66.16
Episode length: 52.24 +/- 17.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.2        |
|    mean_reward          | -529        |
| time/                   |             |
|    total_timesteps      | 572500      |
| train/                  |             |
|    approx_kl            | 0.020751141 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0758     |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.77e+03    |
|    n_updates            | 5310        |
|    policy_gradient_loss | 0.0022      |
|    value_loss           | 3.92e+03    |
-----------------------------------------
Eval num_timesteps=573000, episode_reward=-509.52 +/- 81.08
Episode length: 50.38 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.3     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 560      |
|    time_elapsed    | 2087     |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=573500, episode_reward=-515.93 +/- 60.64
Episode length: 48.70 +/- 15.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.7       |
|    mean_reward          | -516       |
| time/                   |            |
|    total_timesteps      | 573500     |
| train/                  |            |
|    approx_kl            | 0.03086582 |
|    clip_fraction        | 0.0456     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0681    |
|    explained_variance   | 0.831      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.38e+03   |
|    n_updates            | 5311       |
|    policy_gradient_loss | 0.00522    |
|    value_loss           | 2.7e+03    |
----------------------------------------
Eval num_timesteps=574000, episode_reward=-529.72 +/- 60.08
Episode length: 51.60 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 561      |
|    time_elapsed    | 2091     |
|    total_timesteps | 574464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.11
Eval num_timesteps=574500, episode_reward=-511.52 +/- 72.24
Episode length: 51.98 +/- 15.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | -512        |
| time/                   |             |
|    total_timesteps      | 574500      |
| train/                  |             |
|    approx_kl            | 0.033328936 |
|    clip_fraction        | 0.0358      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.041      |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.35e+03    |
|    n_updates            | 5313        |
|    policy_gradient_loss | 0.00107     |
|    value_loss           | 2.72e+03    |
-----------------------------------------
Eval num_timesteps=575000, episode_reward=-501.06 +/- 72.77
Episode length: 52.96 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 562      |
|    time_elapsed    | 2096     |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.10
Eval num_timesteps=575500, episode_reward=-509.22 +/- 59.60
Episode length: 42.58 +/- 13.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.6      |
|    mean_reward          | -509      |
| time/                   |           |
|    total_timesteps      | 575500    |
| train/                  |           |
|    approx_kl            | 0.0400774 |
|    clip_fraction        | 0.0316    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.0454   |
|    explained_variance   | 0.812     |
|    learning_rate        | 0.0005    |
|    loss                 | 1.36e+03  |
|    n_updates            | 5316      |
|    policy_gradient_loss | 0.00226   |
|    value_loss           | 2.97e+03  |
---------------------------------------
Eval num_timesteps=576000, episode_reward=-510.23 +/- 67.90
Episode length: 49.46 +/- 16.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=-502.06 +/- 73.51
Episode length: 43.62 +/- 15.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 563      |
|    time_elapsed    | 2101     |
|    total_timesteps | 576512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=577000, episode_reward=-511.21 +/- 78.76
Episode length: 54.32 +/- 18.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.3        |
|    mean_reward          | -511        |
| time/                   |             |
|    total_timesteps      | 577000      |
| train/                  |             |
|    approx_kl            | 0.031482358 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0311     |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.69e+03    |
|    n_updates            | 5318        |
|    policy_gradient_loss | 0.00479     |
|    value_loss           | 3.59e+03    |
-----------------------------------------
Eval num_timesteps=577500, episode_reward=-527.85 +/- 54.57
Episode length: 51.80 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 564      |
|    time_elapsed    | 2106     |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=578000, episode_reward=-505.03 +/- 82.06
Episode length: 46.48 +/- 13.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.5        |
|    mean_reward          | -505        |
| time/                   |             |
|    total_timesteps      | 578000      |
| train/                  |             |
|    approx_kl            | 0.023739021 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0323     |
|    explained_variance   | 0.844       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.27e+03    |
|    n_updates            | 5319        |
|    policy_gradient_loss | 0.00115     |
|    value_loss           | 2.41e+03    |
-----------------------------------------
Eval num_timesteps=578500, episode_reward=-505.76 +/- 88.27
Episode length: 50.22 +/- 15.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 565      |
|    time_elapsed    | 2110     |
|    total_timesteps | 578560   |
---------------------------------
Eval num_timesteps=579000, episode_reward=-529.95 +/- 66.95
Episode length: 50.70 +/- 14.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.7         |
|    mean_reward          | -530         |
| time/                   |              |
|    total_timesteps      | 579000       |
| train/                  |              |
|    approx_kl            | 0.0021717048 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0323      |
|    explained_variance   | 0.8          |
|    learning_rate        | 0.0005       |
|    loss                 | 1.78e+03     |
|    n_updates            | 5329         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 2.88e+03     |
------------------------------------------
Eval num_timesteps=579500, episode_reward=-502.32 +/- 82.15
Episode length: 48.04 +/- 14.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 566      |
|    time_elapsed    | 2115     |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=580000, episode_reward=-531.75 +/- 61.15
Episode length: 55.14 +/- 18.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.1        |
|    mean_reward          | -532        |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.020868678 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0284     |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.11e+03    |
|    n_updates            | 5330        |
|    policy_gradient_loss | 0.00267     |
|    value_loss           | 6.61e+03    |
-----------------------------------------
Eval num_timesteps=580500, episode_reward=-503.54 +/- 79.79
Episode length: 49.52 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 567      |
|    time_elapsed    | 2119     |
|    total_timesteps | 580608   |
---------------------------------
Eval num_timesteps=581000, episode_reward=-514.83 +/- 83.63
Episode length: 48.40 +/- 14.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.4        |
|    mean_reward          | -515        |
| time/                   |             |
|    total_timesteps      | 581000      |
| train/                  |             |
|    approx_kl            | 0.009009633 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0148     |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.25e+03    |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.000109   |
|    value_loss           | 2.9e+03     |
-----------------------------------------
Eval num_timesteps=581500, episode_reward=-508.49 +/- 82.83
Episode length: 47.42 +/- 18.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 568      |
|    time_elapsed    | 2123     |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=582000, episode_reward=-511.77 +/- 92.51
Episode length: 47.60 +/- 15.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.6         |
|    mean_reward          | -512         |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0026327018 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0476      |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.39e+03     |
|    n_updates            | 5350         |
|    policy_gradient_loss | 0.00394      |
|    value_loss           | 2.34e+03     |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-509.53 +/- 76.32
Episode length: 54.28 +/- 16.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 569      |
|    time_elapsed    | 2128     |
|    total_timesteps | 582656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=583000, episode_reward=-544.91 +/- 53.13
Episode length: 50.86 +/- 17.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | -545        |
| time/                   |             |
|    total_timesteps      | 583000      |
| train/                  |             |
|    approx_kl            | 0.023183893 |
|    clip_fraction        | 0.00586     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0122     |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.68e+03    |
|    n_updates            | 5352        |
|    policy_gradient_loss | 0.0018      |
|    value_loss           | 3.84e+03    |
-----------------------------------------
Eval num_timesteps=583500, episode_reward=-537.16 +/- 56.90
Episode length: 53.50 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.4     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 570      |
|    time_elapsed    | 2132     |
|    total_timesteps | 583680   |
---------------------------------
Eval num_timesteps=584000, episode_reward=-527.59 +/- 59.71
Episode length: 46.92 +/- 15.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.9         |
|    mean_reward          | -528         |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 2.573477e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00035     |
|    explained_variance   | 0.78         |
|    learning_rate        | 0.0005       |
|    loss                 | 1.01e+03     |
|    n_updates            | 5362         |
|    policy_gradient_loss | 0.000105     |
|    value_loss           | 2.48e+03     |
------------------------------------------
Eval num_timesteps=584500, episode_reward=-528.18 +/- 59.94
Episode length: 51.54 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.3     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 571      |
|    time_elapsed    | 2137     |
|    total_timesteps | 584704   |
---------------------------------
Eval num_timesteps=585000, episode_reward=-508.03 +/- 63.83
Episode length: 52.38 +/- 19.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.4        |
|    mean_reward          | -508        |
| time/                   |             |
|    total_timesteps      | 585000      |
| train/                  |             |
|    approx_kl            | 0.018088823 |
|    clip_fraction        | 0.0714      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.056      |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.18e+03    |
|    n_updates            | 5372        |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 2.97e+03    |
-----------------------------------------
Eval num_timesteps=585500, episode_reward=-513.05 +/- 62.68
Episode length: 52.76 +/- 18.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.5     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 572      |
|    time_elapsed    | 2141     |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=586000, episode_reward=-532.98 +/- 64.42
Episode length: 47.48 +/- 16.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | -533        |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.019234754 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0282     |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0005      |
|    loss                 | 862         |
|    n_updates            | 5373        |
|    policy_gradient_loss | 0.00172     |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=586500, episode_reward=-537.77 +/- 54.78
Episode length: 52.10 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.3     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 573      |
|    time_elapsed    | 2146     |
|    total_timesteps | 586752   |
---------------------------------
Eval num_timesteps=587000, episode_reward=-532.06 +/- 59.69
Episode length: 54.52 +/- 18.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.5        |
|    mean_reward          | -532        |
| time/                   |             |
|    total_timesteps      | 587000      |
| train/                  |             |
|    approx_kl            | 0.002041854 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0573     |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.11e+03    |
|    n_updates            | 5383        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 2.32e+03    |
-----------------------------------------
Eval num_timesteps=587500, episode_reward=-508.15 +/- 63.40
Episode length: 51.60 +/- 18.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.2     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 574      |
|    time_elapsed    | 2150     |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.08
Eval num_timesteps=588000, episode_reward=-515.85 +/- 62.56
Episode length: 47.16 +/- 16.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -516        |
| time/                   |             |
|    total_timesteps      | 588000      |
| train/                  |             |
|    approx_kl            | 0.029005775 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0236     |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.42e+03    |
|    n_updates            | 5389        |
|    policy_gradient_loss | 1.48e-05    |
|    value_loss           | 3.26e+03    |
-----------------------------------------
Eval num_timesteps=588500, episode_reward=-488.51 +/- 75.86
Episode length: 52.42 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -489     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 575      |
|    time_elapsed    | 2155     |
|    total_timesteps | 588800   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.09
Eval num_timesteps=589000, episode_reward=-507.94 +/- 64.73
Episode length: 52.18 +/- 18.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.2       |
|    mean_reward          | -508       |
| time/                   |            |
|    total_timesteps      | 589000     |
| train/                  |            |
|    approx_kl            | 0.03697543 |
|    clip_fraction        | 0.0396     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0606    |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.6e+03    |
|    n_updates            | 5399       |
|    policy_gradient_loss | 0.000258   |
|    value_loss           | 3.51e+03   |
----------------------------------------
Eval num_timesteps=589500, episode_reward=-498.75 +/- 55.28
Episode length: 50.80 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 576      |
|    time_elapsed    | 2159     |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=590000, episode_reward=-494.09 +/- 73.24
Episode length: 49.86 +/- 16.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -494        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.024910005 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0348     |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.16e+03    |
|    n_updates            | 5400        |
|    policy_gradient_loss | 0.00353     |
|    value_loss           | 2.37e+03    |
-----------------------------------------
Eval num_timesteps=590500, episode_reward=-515.43 +/- 58.37
Episode length: 50.00 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -508     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 577      |
|    time_elapsed    | 2164     |
|    total_timesteps | 590848   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.06
Eval num_timesteps=591000, episode_reward=-515.27 +/- 86.41
Episode length: 49.46 +/- 16.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -515        |
| time/                   |             |
|    total_timesteps      | 591000      |
| train/                  |             |
|    approx_kl            | 0.062289037 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0106     |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.07e+03    |
|    n_updates            | 5406        |
|    policy_gradient_loss | 4.03e-05    |
|    value_loss           | 2.65e+03    |
-----------------------------------------
Eval num_timesteps=591500, episode_reward=-511.09 +/- 67.51
Episode length: 47.84 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 578      |
|    time_elapsed    | 2168     |
|    total_timesteps | 591872   |
---------------------------------
Eval num_timesteps=592000, episode_reward=-512.57 +/- 77.19
Episode length: 48.42 +/- 14.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.4         |
|    mean_reward          | -513         |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0030298037 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0155      |
|    explained_variance   | 0.808        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.36e+03     |
|    n_updates            | 5416         |
|    policy_gradient_loss | 0.00132      |
|    value_loss           | 2.9e+03      |
------------------------------------------
Eval num_timesteps=592500, episode_reward=-509.86 +/- 63.54
Episode length: 47.30 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -508     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 579      |
|    time_elapsed    | 2172     |
|    total_timesteps | 592896   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=593000, episode_reward=-532.80 +/- 73.89
Episode length: 48.76 +/- 17.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.8        |
|    mean_reward          | -533        |
| time/                   |             |
|    total_timesteps      | 593000      |
| train/                  |             |
|    approx_kl            | 0.011839336 |
|    clip_fraction        | 0.00857     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00495    |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.44e+03    |
|    n_updates            | 5425        |
|    policy_gradient_loss | -7.33e-05   |
|    value_loss           | 4.84e+03    |
-----------------------------------------
Eval num_timesteps=593500, episode_reward=-519.78 +/- 72.06
Episode length: 50.50 +/- 19.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 580      |
|    time_elapsed    | 2177     |
|    total_timesteps | 593920   |
---------------------------------
Eval num_timesteps=594000, episode_reward=-507.19 +/- 71.96
Episode length: 50.72 +/- 15.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.7       |
|    mean_reward          | -507       |
| time/                   |            |
|    total_timesteps      | 594000     |
| train/                  |            |
|    approx_kl            | 0.01239271 |
|    clip_fraction        | 0.0085     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.00576   |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.03e+03   |
|    n_updates            | 5435       |
|    policy_gradient_loss | -0.000545  |
|    value_loss           | 2.38e+03   |
----------------------------------------
Eval num_timesteps=594500, episode_reward=-500.90 +/- 69.93
Episode length: 49.52 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -505     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 581      |
|    time_elapsed    | 2181     |
|    total_timesteps | 594944   |
---------------------------------
Eval num_timesteps=595000, episode_reward=-523.24 +/- 54.64
Episode length: 47.38 +/- 16.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.4         |
|    mean_reward          | -523         |
| time/                   |              |
|    total_timesteps      | 595000       |
| train/                  |              |
|    approx_kl            | 0.0027482186 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0235      |
|    explained_variance   | 0.801        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.95e+03     |
|    n_updates            | 5445         |
|    policy_gradient_loss | -0.000537    |
|    value_loss           | 3.28e+03     |
------------------------------------------
Eval num_timesteps=595500, episode_reward=-487.26 +/- 74.65
Episode length: 47.64 +/- 12.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -487     |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -499     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 582      |
|    time_elapsed    | 2186     |
|    total_timesteps | 595968   |
---------------------------------
Eval num_timesteps=596000, episode_reward=-505.13 +/- 73.59
Episode length: 46.56 +/- 16.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 46.6         |
|    mean_reward          | -505         |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0025444631 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.733        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.68e+03     |
|    n_updates            | 5455         |
|    policy_gradient_loss | 6.55e-05     |
|    value_loss           | 3.47e+03     |
------------------------------------------
Eval num_timesteps=596500, episode_reward=-481.44 +/- 74.10
Episode length: 45.78 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -481     |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -499     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 583      |
|    time_elapsed    | 2190     |
|    total_timesteps | 596992   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.09
Eval num_timesteps=597000, episode_reward=-507.54 +/- 73.14
Episode length: 47.56 +/- 17.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.6        |
|    mean_reward          | -508        |
| time/                   |             |
|    total_timesteps      | 597000      |
| train/                  |             |
|    approx_kl            | 0.013484132 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.021      |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.8e+03     |
|    n_updates            | 5462        |
|    policy_gradient_loss | -0.000616   |
|    value_loss           | 3.38e+03    |
-----------------------------------------
Eval num_timesteps=597500, episode_reward=-499.80 +/- 70.48
Episode length: 46.68 +/- 15.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-508.78 +/- 68.49
Episode length: 45.96 +/- 14.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -494     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 584      |
|    time_elapsed    | 2196     |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.06
Eval num_timesteps=598500, episode_reward=-498.02 +/- 85.66
Episode length: 49.28 +/- 17.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -498        |
| time/                   |             |
|    total_timesteps      | 598500      |
| train/                  |             |
|    approx_kl            | 0.017645933 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0379     |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.22e+03    |
|    n_updates            | 5468        |
|    policy_gradient_loss | 0.000589    |
|    value_loss           | 2.17e+03    |
-----------------------------------------
Eval num_timesteps=599000, episode_reward=-482.46 +/- 69.88
Episode length: 47.10 +/- 15.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -482     |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -496     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 585      |
|    time_elapsed    | 2200     |
|    total_timesteps | 599040   |
---------------------------------
Eval num_timesteps=599500, episode_reward=-505.92 +/- 68.71
Episode length: 45.78 +/- 14.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.8         |
|    mean_reward          | -506         |
| time/                   |              |
|    total_timesteps      | 599500       |
| train/                  |              |
|    approx_kl            | 0.0043230425 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0261      |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.07e+03     |
|    n_updates            | 5478         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 2.37e+03     |
------------------------------------------
Eval num_timesteps=600000, episode_reward=-505.79 +/- 59.73
Episode length: 48.00 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.6     |
|    ep_rew_mean     | -490     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 586      |
|    time_elapsed    | 2204     |
|    total_timesteps | 600064   |
---------------------------------
Eval num_timesteps=600500, episode_reward=-494.04 +/- 66.88
Episode length: 47.76 +/- 17.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.8        |
|    mean_reward          | -494        |
| time/                   |             |
|    total_timesteps      | 600500      |
| train/                  |             |
|    approx_kl            | 0.009611125 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0513     |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.24e+03    |
|    n_updates            | 5488        |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 2.53e+03    |
-----------------------------------------
Eval num_timesteps=601000, episode_reward=-500.85 +/- 58.46
Episode length: 50.36 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 601000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 587      |
|    time_elapsed    | 2208     |
|    total_timesteps | 601088   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.07
Eval num_timesteps=601500, episode_reward=-509.98 +/- 61.89
Episode length: 44.52 +/- 13.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.5        |
|    mean_reward          | -510        |
| time/                   |             |
|    total_timesteps      | 601500      |
| train/                  |             |
|    approx_kl            | 0.042030532 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0997     |
|    explained_variance   | 0.861       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.3e+03     |
|    n_updates            | 5495        |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 2.24e+03    |
-----------------------------------------
Eval num_timesteps=602000, episode_reward=-508.00 +/- 76.25
Episode length: 49.06 +/- 19.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -483     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 588      |
|    time_elapsed    | 2213     |
|    total_timesteps | 602112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.09
Eval num_timesteps=602500, episode_reward=-502.02 +/- 66.62
Episode length: 51.92 +/- 17.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.9       |
|    mean_reward          | -502       |
| time/                   |            |
|    total_timesteps      | 602500     |
| train/                  |            |
|    approx_kl            | 0.13841294 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0351    |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.88e+03   |
|    n_updates            | 5496       |
|    policy_gradient_loss | 0.116      |
|    value_loss           | 4e+03      |
----------------------------------------
Eval num_timesteps=603000, episode_reward=-493.17 +/- 65.65
Episode length: 49.30 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -493     |
| time/              |          |
|    total_timesteps | 603000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -489     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 589      |
|    time_elapsed    | 2217     |
|    total_timesteps | 603136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=603500, episode_reward=-504.31 +/- 65.81
Episode length: 50.78 +/- 16.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -504        |
| time/                   |             |
|    total_timesteps      | 603500      |
| train/                  |             |
|    approx_kl            | 0.042706784 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0489     |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.44e+03    |
|    n_updates            | 5498        |
|    policy_gradient_loss | 0.00268     |
|    value_loss           | 2.86e+03    |
-----------------------------------------
Eval num_timesteps=604000, episode_reward=-504.32 +/- 73.24
Episode length: 47.72 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 590      |
|    time_elapsed    | 2221     |
|    total_timesteps | 604160   |
---------------------------------
Eval num_timesteps=604500, episode_reward=-510.80 +/- 54.65
Episode length: 52.58 +/- 16.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.6         |
|    mean_reward          | -511         |
| time/                   |              |
|    total_timesteps      | 604500       |
| train/                  |              |
|    approx_kl            | 0.0107867755 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0234      |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.0005       |
|    loss                 | 958          |
|    n_updates            | 5508         |
|    policy_gradient_loss | -9.45e-05    |
|    value_loss           | 2.34e+03     |
------------------------------------------
Eval num_timesteps=605000, episode_reward=-510.26 +/- 64.33
Episode length: 51.12 +/- 17.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -490     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 591      |
|    time_elapsed    | 2226     |
|    total_timesteps | 605184   |
---------------------------------
Eval num_timesteps=605500, episode_reward=-508.18 +/- 51.69
Episode length: 46.98 +/- 14.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47          |
|    mean_reward          | -508        |
| time/                   |             |
|    total_timesteps      | 605500      |
| train/                  |             |
|    approx_kl            | 0.010645214 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0248     |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.37e+03    |
|    n_updates            | 5518        |
|    policy_gradient_loss | -0.00333    |
|    value_loss           | 4.14e+03    |
-----------------------------------------
Eval num_timesteps=606000, episode_reward=-507.90 +/- 61.19
Episode length: 49.04 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -493     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 592      |
|    time_elapsed    | 2230     |
|    total_timesteps | 606208   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=606500, episode_reward=-490.97 +/- 105.58
Episode length: 52.34 +/- 20.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.3       |
|    mean_reward          | -491       |
| time/                   |            |
|    total_timesteps      | 606500     |
| train/                  |            |
|    approx_kl            | 0.06020578 |
|    clip_fraction        | 0.0234     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0413    |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.95e+03   |
|    n_updates            | 5521       |
|    policy_gradient_loss | 0.00389    |
|    value_loss           | 3.79e+03   |
----------------------------------------
Eval num_timesteps=607000, episode_reward=-499.86 +/- 60.22
Episode length: 52.46 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 607000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -497     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 593      |
|    time_elapsed    | 2235     |
|    total_timesteps | 607232   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=607500, episode_reward=-517.80 +/- 52.93
Episode length: 51.04 +/- 18.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -518        |
| time/                   |             |
|    total_timesteps      | 607500      |
| train/                  |             |
|    approx_kl            | 0.012369781 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.033      |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.15e+03    |
|    n_updates            | 5526        |
|    policy_gradient_loss | 0.000115    |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=608000, episode_reward=-498.96 +/- 71.43
Episode length: 52.80 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -495     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 594      |
|    time_elapsed    | 2239     |
|    total_timesteps | 608256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=608500, episode_reward=-492.88 +/- 76.20
Episode length: 47.22 +/- 16.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | -493        |
| time/                   |             |
|    total_timesteps      | 608500      |
| train/                  |             |
|    approx_kl            | 0.018063854 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0955     |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.24e+03    |
|    n_updates            | 5528        |
|    policy_gradient_loss | 0.00722     |
|    value_loss           | 2.37e+03    |
-----------------------------------------
Eval num_timesteps=609000, episode_reward=-511.17 +/- 68.59
Episode length: 50.36 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 609000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -492     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 595      |
|    time_elapsed    | 2243     |
|    total_timesteps | 609280   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=609500, episode_reward=-522.13 +/- 71.32
Episode length: 49.48 +/- 16.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.5       |
|    mean_reward          | -522       |
| time/                   |            |
|    total_timesteps      | 609500     |
| train/                  |            |
|    approx_kl            | 0.03184025 |
|    clip_fraction        | 0.0456     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0606    |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0005     |
|    loss                 | 1.45e+03   |
|    n_updates            | 5530       |
|    policy_gradient_loss | -0.000813  |
|    value_loss           | 2.86e+03   |
----------------------------------------
Eval num_timesteps=610000, episode_reward=-504.53 +/- 61.53
Episode length: 48.96 +/- 14.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -502     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 596      |
|    time_elapsed    | 2248     |
|    total_timesteps | 610304   |
---------------------------------
Eval num_timesteps=610500, episode_reward=-506.61 +/- 62.78
Episode length: 48.94 +/- 16.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.9         |
|    mean_reward          | -507         |
| time/                   |              |
|    total_timesteps      | 610500       |
| train/                  |              |
|    approx_kl            | 0.0029523745 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0404      |
|    explained_variance   | 0.817        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.13e+03     |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 3.42e+03     |
------------------------------------------
Eval num_timesteps=611000, episode_reward=-516.22 +/- 70.66
Episode length: 50.54 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 611000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -497     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 597      |
|    time_elapsed    | 2252     |
|    total_timesteps | 611328   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.07
Eval num_timesteps=611500, episode_reward=-498.15 +/- 77.25
Episode length: 49.00 +/- 17.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | -498        |
| time/                   |             |
|    total_timesteps      | 611500      |
| train/                  |             |
|    approx_kl            | 0.030887824 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0516     |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.33e+03    |
|    n_updates            | 5549        |
|    policy_gradient_loss | 1.11e-05    |
|    value_loss           | 2.55e+03    |
-----------------------------------------
Eval num_timesteps=612000, episode_reward=-498.52 +/- 70.32
Episode length: 46.78 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | -492     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 598      |
|    time_elapsed    | 2256     |
|    total_timesteps | 612352   |
---------------------------------
Eval num_timesteps=612500, episode_reward=-513.85 +/- 64.56
Episode length: 53.06 +/- 18.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.1       |
|    mean_reward          | -514       |
| time/                   |            |
|    total_timesteps      | 612500     |
| train/                  |            |
|    approx_kl            | 0.00395744 |
|    clip_fraction        | 0.0104     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0124    |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.38e+03   |
|    n_updates            | 5559       |
|    policy_gradient_loss | -0.001     |
|    value_loss           | 3.23e+03   |
----------------------------------------
Eval num_timesteps=613000, episode_reward=-500.23 +/- 70.08
Episode length: 46.44 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 613000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -496     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 599      |
|    time_elapsed    | 2261     |
|    total_timesteps | 613376   |
---------------------------------
Eval num_timesteps=613500, episode_reward=-516.41 +/- 65.34
Episode length: 47.52 +/- 14.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.5         |
|    mean_reward          | -516         |
| time/                   |              |
|    total_timesteps      | 613500       |
| train/                  |              |
|    approx_kl            | 0.0100459345 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0538      |
|    explained_variance   | 0.781        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.12e+03     |
|    n_updates            | 5569         |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 2.72e+03     |
------------------------------------------
Eval num_timesteps=614000, episode_reward=-526.96 +/- 65.25
Episode length: 48.70 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -502     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 600      |
|    time_elapsed    | 2265     |
|    total_timesteps | 614400   |
---------------------------------
Eval num_timesteps=614500, episode_reward=-498.28 +/- 58.21
Episode length: 50.48 +/- 17.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.5        |
|    mean_reward          | -498        |
| time/                   |             |
|    total_timesteps      | 614500      |
| train/                  |             |
|    approx_kl            | 0.021239536 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0834     |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.99e+03    |
|    n_updates            | 5579        |
|    policy_gradient_loss | -0.00726    |
|    value_loss           | 3.33e+03    |
-----------------------------------------
Eval num_timesteps=615000, episode_reward=-481.75 +/- 82.99
Episode length: 48.54 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -482     |
| time/              |          |
|    total_timesteps | 615000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 601      |
|    time_elapsed    | 2269     |
|    total_timesteps | 615424   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.08
Eval num_timesteps=615500, episode_reward=-465.64 +/- 87.44
Episode length: 48.32 +/- 15.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.3        |
|    mean_reward          | -466        |
| time/                   |             |
|    total_timesteps      | 615500      |
| train/                  |             |
|    approx_kl            | 0.035952847 |
|    clip_fraction        | 0.0757      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.192      |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.65e+03    |
|    n_updates            | 5589        |
|    policy_gradient_loss | -0.00529    |
|    value_loss           | 2.88e+03    |
-----------------------------------------
Eval num_timesteps=616000, episode_reward=-452.05 +/- 98.11
Episode length: 47.24 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -452     |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 602      |
|    time_elapsed    | 2274     |
|    total_timesteps | 616448   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=616500, episode_reward=-476.22 +/- 80.22
Episode length: 47.38 +/- 15.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.4        |
|    mean_reward          | -476        |
| time/                   |             |
|    total_timesteps      | 616500      |
| train/                  |             |
|    approx_kl            | 0.061239425 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.851       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.66e+03    |
|    n_updates            | 5592        |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 2.92e+03    |
-----------------------------------------
Eval num_timesteps=617000, episode_reward=-485.66 +/- 82.87
Episode length: 48.30 +/- 16.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 617000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.4     |
|    ep_rew_mean     | -495     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 603      |
|    time_elapsed    | 2278     |
|    total_timesteps | 617472   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=617500, episode_reward=-477.90 +/- 79.35
Episode length: 49.60 +/- 15.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | -478        |
| time/                   |             |
|    total_timesteps      | 617500      |
| train/                  |             |
|    approx_kl            | 0.025827209 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.227      |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.96e+03    |
|    n_updates            | 5594        |
|    policy_gradient_loss | 0.00601     |
|    value_loss           | 3.45e+03    |
-----------------------------------------
Eval num_timesteps=618000, episode_reward=-464.17 +/- 87.97
Episode length: 46.10 +/- 16.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -464     |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -488     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 604      |
|    time_elapsed    | 2282     |
|    total_timesteps | 618496   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=618500, episode_reward=-445.68 +/- 105.65
Episode length: 57.08 +/- 21.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.1        |
|    mean_reward          | -446        |
| time/                   |             |
|    total_timesteps      | 618500      |
| train/                  |             |
|    approx_kl            | 0.035318453 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.52e+03    |
|    n_updates            | 5596        |
|    policy_gradient_loss | 0.00221     |
|    value_loss           | 2.89e+03    |
-----------------------------------------
Eval num_timesteps=619000, episode_reward=-436.74 +/- 74.39
Episode length: 49.04 +/- 18.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 619000   |
---------------------------------
Eval num_timesteps=619500, episode_reward=-435.59 +/- 89.13
Episode length: 51.10 +/- 19.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 619500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -480     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 605      |
|    time_elapsed    | 2288     |
|    total_timesteps | 619520   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.21
Eval num_timesteps=620000, episode_reward=-422.29 +/- 95.22
Episode length: 51.06 +/- 13.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.1       |
|    mean_reward          | -422       |
| time/                   |            |
|    total_timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.07074937 |
|    clip_fraction        | 0.0703     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.183     |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.27e+03   |
|    n_updates            | 5601       |
|    policy_gradient_loss | -0.004     |
|    value_loss           | 3.09e+03   |
----------------------------------------
Eval num_timesteps=620500, episode_reward=-432.22 +/- 80.02
Episode length: 46.34 +/- 12.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 620500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -470     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 606      |
|    time_elapsed    | 2293     |
|    total_timesteps | 620544   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.08
Eval num_timesteps=621000, episode_reward=-438.88 +/- 66.55
Episode length: 45.18 +/- 12.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.2       |
|    mean_reward          | -439       |
| time/                   |            |
|    total_timesteps      | 621000     |
| train/                  |            |
|    approx_kl            | 0.03185623 |
|    clip_fraction        | 0.0841     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.221     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.48e+03   |
|    n_updates            | 5605       |
|    policy_gradient_loss | -7.85e-05  |
|    value_loss           | 2.99e+03   |
----------------------------------------
Eval num_timesteps=621500, episode_reward=-424.26 +/- 87.44
Episode length: 50.48 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 621500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -468     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 607      |
|    time_elapsed    | 2297     |
|    total_timesteps | 621568   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.06
Eval num_timesteps=622000, episode_reward=-424.33 +/- 72.63
Episode length: 46.06 +/- 11.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.1        |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 622000      |
| train/                  |             |
|    approx_kl            | 0.056955755 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.36e+03    |
|    n_updates            | 5609        |
|    policy_gradient_loss | 0.00527     |
|    value_loss           | 2.5e+03     |
-----------------------------------------
Eval num_timesteps=622500, episode_reward=-410.34 +/- 73.67
Episode length: 44.06 +/- 13.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 622500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -459     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 608      |
|    time_elapsed    | 2301     |
|    total_timesteps | 622592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=623000, episode_reward=-380.73 +/- 75.04
Episode length: 44.90 +/- 15.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44.9       |
|    mean_reward          | -381       |
| time/                   |            |
|    total_timesteps      | 623000     |
| train/                  |            |
|    approx_kl            | 0.03975179 |
|    clip_fraction        | 0.0703     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.202     |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.53e+03   |
|    n_updates            | 5610       |
|    policy_gradient_loss | 0.032      |
|    value_loss           | 3.71e+03   |
----------------------------------------
Eval num_timesteps=623500, episode_reward=-408.80 +/- 85.65
Episode length: 41.60 +/- 12.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 623500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -451     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 609      |
|    time_elapsed    | 2305     |
|    total_timesteps | 623616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=624000, episode_reward=-352.65 +/- 117.37
Episode length: 43.56 +/- 13.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.6        |
|    mean_reward          | -353        |
| time/                   |             |
|    total_timesteps      | 624000      |
| train/                  |             |
|    approx_kl            | 0.034045126 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.49e+03    |
|    n_updates            | 5611        |
|    policy_gradient_loss | 0.00843     |
|    value_loss           | 3.4e+03     |
-----------------------------------------
Eval num_timesteps=624500, episode_reward=-333.59 +/- 85.27
Episode length: 38.80 +/- 9.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.8     |
|    mean_reward     | -334     |
| time/              |          |
|    total_timesteps | 624500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.7     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 610      |
|    time_elapsed    | 2308     |
|    total_timesteps | 624640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=625000, episode_reward=-315.00 +/- 125.67
Episode length: 39.86 +/- 9.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.9        |
|    mean_reward          | -315        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.031967822 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.249      |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.08e+03    |
|    n_updates            | 5612        |
|    policy_gradient_loss | 0.0239      |
|    value_loss           | 4.72e+03    |
-----------------------------------------
Eval num_timesteps=625500, episode_reward=-349.59 +/- 98.33
Episode length: 38.72 +/- 10.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.7     |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 625500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | -381     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 611      |
|    time_elapsed    | 2312     |
|    total_timesteps | 625664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=626000, episode_reward=-324.70 +/- 154.85
Episode length: 41.96 +/- 10.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42         |
|    mean_reward          | -325       |
| time/                   |            |
|    total_timesteps      | 626000     |
| train/                  |            |
|    approx_kl            | 0.02640952 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.258     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0005     |
|    loss                 | 3e+03      |
|    n_updates            | 5613       |
|    policy_gradient_loss | 0.0115     |
|    value_loss           | 6.01e+03   |
----------------------------------------
Eval num_timesteps=626500, episode_reward=-327.41 +/- 112.87
Episode length: 39.98 +/- 10.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40       |
|    mean_reward     | -327     |
| time/              |          |
|    total_timesteps | 626500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.3     |
|    ep_rew_mean     | -348     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 612      |
|    time_elapsed    | 2316     |
|    total_timesteps | 626688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=627000, episode_reward=-379.16 +/- 138.52
Episode length: 39.44 +/- 10.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.4        |
|    mean_reward          | -379        |
| time/                   |             |
|    total_timesteps      | 627000      |
| train/                  |             |
|    approx_kl            | 0.024338825 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.273      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.45e+03    |
|    n_updates            | 5614        |
|    policy_gradient_loss | 0.0122      |
|    value_loss           | 4.6e+03     |
-----------------------------------------
Eval num_timesteps=627500, episode_reward=-318.73 +/- 118.42
Episode length: 39.94 +/- 12.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.9     |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 627500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.1     |
|    ep_rew_mean     | -324     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 613      |
|    time_elapsed    | 2319     |
|    total_timesteps | 627712   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=628000, episode_reward=-358.43 +/- 119.41
Episode length: 37.14 +/- 9.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 37.1       |
|    mean_reward          | -358       |
| time/                   |            |
|    total_timesteps      | 628000     |
| train/                  |            |
|    approx_kl            | 0.06484906 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.315     |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.38e+03   |
|    n_updates            | 5616       |
|    policy_gradient_loss | 0.0113     |
|    value_loss           | 5.11e+03   |
----------------------------------------
Eval num_timesteps=628500, episode_reward=-361.71 +/- 121.14
Episode length: 38.54 +/- 9.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.5     |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 628500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.3     |
|    ep_rew_mean     | -331     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 614      |
|    time_elapsed    | 2323     |
|    total_timesteps | 628736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=629000, episode_reward=-365.07 +/- 125.38
Episode length: 38.64 +/- 14.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.6        |
|    mean_reward          | -365        |
| time/                   |             |
|    total_timesteps      | 629000      |
| train/                  |             |
|    approx_kl            | 0.030342596 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0005      |
|    loss                 | 3.59e+03    |
|    n_updates            | 5618        |
|    policy_gradient_loss | 0.00708     |
|    value_loss           | 7.29e+03    |
-----------------------------------------
Eval num_timesteps=629500, episode_reward=-352.34 +/- 143.90
Episode length: 37.82 +/- 10.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | -352     |
| time/              |          |
|    total_timesteps | 629500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | -328     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 615      |
|    time_elapsed    | 2326     |
|    total_timesteps | 629760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=630000, episode_reward=-281.21 +/- 126.74
Episode length: 39.84 +/- 9.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.8        |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 630000      |
| train/                  |             |
|    approx_kl            | 0.019753668 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.262      |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.39e+03    |
|    n_updates            | 5619        |
|    policy_gradient_loss | 0.00866     |
|    value_loss           | 4.6e+03     |
-----------------------------------------
Eval num_timesteps=630500, episode_reward=-334.50 +/- 125.49
Episode length: 39.64 +/- 12.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 630500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.5     |
|    ep_rew_mean     | -325     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 616      |
|    time_elapsed    | 2330     |
|    total_timesteps | 630784   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=631000, episode_reward=-299.38 +/- 117.39
Episode length: 38.64 +/- 9.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 38.6       |
|    mean_reward          | -299       |
| time/                   |            |
|    total_timesteps      | 631000     |
| train/                  |            |
|    approx_kl            | 0.07708222 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.267     |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.97e+03   |
|    n_updates            | 5621       |
|    policy_gradient_loss | 0.00171    |
|    value_loss           | 7.1e+03    |
----------------------------------------
Eval num_timesteps=631500, episode_reward=-312.93 +/- 108.12
Episode length: 39.42 +/- 11.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.4     |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 631500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.5     |
|    ep_rew_mean     | -325     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 617      |
|    time_elapsed    | 2334     |
|    total_timesteps | 631808   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=632000, episode_reward=-309.39 +/- 100.97
Episode length: 40.84 +/- 10.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.8        |
|    mean_reward          | -309        |
| time/                   |             |
|    total_timesteps      | 632000      |
| train/                  |             |
|    approx_kl            | 0.038154375 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.309      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.43e+03    |
|    n_updates            | 5624        |
|    policy_gradient_loss | 0.00137     |
|    value_loss           | 4.69e+03    |
-----------------------------------------
Eval num_timesteps=632500, episode_reward=-287.88 +/- 135.23
Episode length: 39.80 +/- 12.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.8     |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 632500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | -300     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 618      |
|    time_elapsed    | 2337     |
|    total_timesteps | 632832   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.06
Eval num_timesteps=633000, episode_reward=-321.89 +/- 106.07
Episode length: 42.90 +/- 12.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.9        |
|    mean_reward          | -322        |
| time/                   |             |
|    total_timesteps      | 633000      |
| train/                  |             |
|    approx_kl            | 0.030562961 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.343      |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.05e+03    |
|    n_updates            | 5629        |
|    policy_gradient_loss | 0.00198     |
|    value_loss           | 5.82e+03    |
-----------------------------------------
Eval num_timesteps=633500, episode_reward=-309.86 +/- 127.11
Episode length: 41.24 +/- 11.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 633500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | -294     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 619      |
|    time_elapsed    | 2341     |
|    total_timesteps | 633856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=634000, episode_reward=-294.74 +/- 136.42
Episode length: 45.12 +/- 14.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.1        |
|    mean_reward          | -295        |
| time/                   |             |
|    total_timesteps      | 634000      |
| train/                  |             |
|    approx_kl            | 0.021299586 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.02e+03    |
|    n_updates            | 5630        |
|    policy_gradient_loss | 0.00655     |
|    value_loss           | 4.66e+03    |
-----------------------------------------
Eval num_timesteps=634500, episode_reward=-312.29 +/- 116.67
Episode length: 45.78 +/- 12.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.8     |
|    ep_rew_mean     | -300     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 620      |
|    time_elapsed    | 2345     |
|    total_timesteps | 634880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=635000, episode_reward=-320.92 +/- 164.55
Episode length: 47.82 +/- 15.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.8       |
|    mean_reward          | -321       |
| time/                   |            |
|    total_timesteps      | 635000     |
| train/                  |            |
|    approx_kl            | 0.01776577 |
|    clip_fraction        | 0.0737     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.305     |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.98e+03   |
|    n_updates            | 5631       |
|    policy_gradient_loss | 0.0119     |
|    value_loss           | 6.09e+03   |
----------------------------------------
Eval num_timesteps=635500, episode_reward=-317.12 +/- 146.85
Episode length: 44.70 +/- 11.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 635500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 621      |
|    time_elapsed    | 2349     |
|    total_timesteps | 635904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=636000, episode_reward=-306.72 +/- 159.41
Episode length: 48.10 +/- 18.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -307        |
| time/                   |             |
|    total_timesteps      | 636000      |
| train/                  |             |
|    approx_kl            | 0.013718429 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.29e+03    |
|    n_updates            | 5632        |
|    policy_gradient_loss | 0.00605     |
|    value_loss           | 4.37e+03    |
-----------------------------------------
Eval num_timesteps=636500, episode_reward=-322.41 +/- 157.25
Episode length: 45.42 +/- 13.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -322     |
| time/              |          |
|    total_timesteps | 636500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | -335     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 622      |
|    time_elapsed    | 2353     |
|    total_timesteps | 636928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=637000, episode_reward=-340.59 +/- 145.58
Episode length: 43.68 +/- 16.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.7        |
|    mean_reward          | -341        |
| time/                   |             |
|    total_timesteps      | 637000      |
| train/                  |             |
|    approx_kl            | 0.026576754 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.21e+03    |
|    n_updates            | 5634        |
|    policy_gradient_loss | 0.00427     |
|    value_loss           | 4.52e+03    |
-----------------------------------------
Eval num_timesteps=637500, episode_reward=-303.05 +/- 156.87
Episode length: 45.28 +/- 16.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 637500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.6     |
|    ep_rew_mean     | -348     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 623      |
|    time_elapsed    | 2357     |
|    total_timesteps | 637952   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=638000, episode_reward=-276.87 +/- 200.07
Episode length: 48.62 +/- 14.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.6       |
|    mean_reward          | -277       |
| time/                   |            |
|    total_timesteps      | 638000     |
| train/                  |            |
|    approx_kl            | 0.05595127 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.205     |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.91e+03   |
|    n_updates            | 5636       |
|    policy_gradient_loss | 0.0101     |
|    value_loss           | 3.78e+03   |
----------------------------------------
Eval num_timesteps=638500, episode_reward=-339.16 +/- 123.45
Episode length: 42.66 +/- 12.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 638500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.9     |
|    ep_rew_mean     | -335     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 624      |
|    time_elapsed    | 2361     |
|    total_timesteps | 638976   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.28
Eval num_timesteps=639000, episode_reward=-217.80 +/- 222.60
Episode length: 42.32 +/- 13.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.3      |
|    mean_reward          | -218      |
| time/                   |           |
|    total_timesteps      | 639000    |
| train/                  |           |
|    approx_kl            | 0.2758019 |
|    clip_fraction        | 0.0694    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.218    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.23e+03  |
|    n_updates            | 5638      |
|    policy_gradient_loss | 0.00172   |
|    value_loss           | 6.59e+03  |
---------------------------------------
Eval num_timesteps=639500, episode_reward=-184.66 +/- 252.38
Episode length: 47.56 +/- 13.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 639500   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-154.07 +/- 401.19
Episode length: 48.98 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46       |
|    ep_rew_mean     | -322     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 625      |
|    time_elapsed    | 2367     |
|    total_timesteps | 640000   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.13
Eval num_timesteps=640500, episode_reward=-258.58 +/- 172.90
Episode length: 42.80 +/- 13.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.8       |
|    mean_reward          | -259       |
| time/                   |            |
|    total_timesteps      | 640500     |
| train/                  |            |
|    approx_kl            | 0.06877752 |
|    clip_fraction        | 0.0898     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.44e+03   |
|    n_updates            | 5642       |
|    policy_gradient_loss | -0.00707   |
|    value_loss           | 6.8e+03    |
----------------------------------------
Eval num_timesteps=641000, episode_reward=-260.10 +/- 165.85
Episode length: 42.38 +/- 14.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.2     |
|    ep_rew_mean     | -301     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 626      |
|    time_elapsed    | 2371     |
|    total_timesteps | 641024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=641500, episode_reward=-244.15 +/- 129.34
Episode length: 41.24 +/- 12.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.2        |
|    mean_reward          | -244        |
| time/                   |             |
|    total_timesteps      | 641500      |
| train/                  |             |
|    approx_kl            | 0.036731638 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0005      |
|    loss                 | 3e+03       |
|    n_updates            | 5643        |
|    policy_gradient_loss | 0.0261      |
|    value_loss           | 5.66e+03    |
-----------------------------------------
Eval num_timesteps=642000, episode_reward=-250.85 +/- 164.66
Episode length: 39.94 +/- 12.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.9     |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | -277     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 627      |
|    time_elapsed    | 2374     |
|    total_timesteps | 642048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=642500, episode_reward=-227.09 +/- 146.63
Episode length: 40.84 +/- 11.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.8        |
|    mean_reward          | -227        |
| time/                   |             |
|    total_timesteps      | 642500      |
| train/                  |             |
|    approx_kl            | 0.031166356 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.18e+03    |
|    n_updates            | 5644        |
|    policy_gradient_loss | 0.00132     |
|    value_loss           | 5.6e+03     |
-----------------------------------------
Eval num_timesteps=643000, episode_reward=-228.23 +/- 136.61
Episode length: 38.80 +/- 11.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.8     |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | -257     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 628      |
|    time_elapsed    | 2378     |
|    total_timesteps | 643072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.21
Eval num_timesteps=643500, episode_reward=-235.18 +/- 136.94
Episode length: 35.62 +/- 9.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | -235        |
| time/                   |             |
|    total_timesteps      | 643500      |
| train/                  |             |
|    approx_kl            | 0.081313565 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.487      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.58e+03    |
|    n_updates            | 5645        |
|    policy_gradient_loss | 0.00236     |
|    value_loss           | 1.09e+04    |
-----------------------------------------
Eval num_timesteps=644000, episode_reward=-246.98 +/- 111.64
Episode length: 36.22 +/- 8.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 644000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.1     |
|    ep_rew_mean     | -245     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 629      |
|    time_elapsed    | 2381     |
|    total_timesteps | 644096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=644500, episode_reward=-245.91 +/- 161.33
Episode length: 34.94 +/- 10.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.9       |
|    mean_reward          | -246       |
| time/                   |            |
|    total_timesteps      | 644500     |
| train/                  |            |
|    approx_kl            | 0.03492604 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.567     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0005     |
|    loss                 | 1.77e+03   |
|    n_updates            | 5646       |
|    policy_gradient_loss | 0.00904    |
|    value_loss           | 4.42e+03   |
----------------------------------------
Eval num_timesteps=645000, episode_reward=-236.98 +/- 144.18
Episode length: 35.42 +/- 10.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.9     |
|    ep_rew_mean     | -243     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 630      |
|    time_elapsed    | 2385     |
|    total_timesteps | 645120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=645500, episode_reward=-289.34 +/- 132.04
Episode length: 32.54 +/- 8.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.5        |
|    mean_reward          | -289        |
| time/                   |             |
|    total_timesteps      | 645500      |
| train/                  |             |
|    approx_kl            | 0.042252887 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.67e+03    |
|    n_updates            | 5647        |
|    policy_gradient_loss | 0.0259      |
|    value_loss           | 6.73e+03    |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=-286.04 +/- 139.39
Episode length: 37.16 +/- 9.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 646000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.2     |
|    ep_rew_mean     | -271     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 631      |
|    time_elapsed    | 2388     |
|    total_timesteps | 646144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=646500, episode_reward=-302.18 +/- 128.00
Episode length: 32.28 +/- 7.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.3        |
|    mean_reward          | -302        |
| time/                   |             |
|    total_timesteps      | 646500      |
| train/                  |             |
|    approx_kl            | 0.027457908 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.5e+03     |
|    n_updates            | 5648        |
|    policy_gradient_loss | 0.0244      |
|    value_loss           | 5.36e+03    |
-----------------------------------------
Eval num_timesteps=647000, episode_reward=-292.28 +/- 142.17
Episode length: 36.00 +/- 11.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -303     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 632      |
|    time_elapsed    | 2391     |
|    total_timesteps | 647168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=647500, episode_reward=-437.45 +/- 141.04
Episode length: 38.86 +/- 13.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.9        |
|    mean_reward          | -437        |
| time/                   |             |
|    total_timesteps      | 647500      |
| train/                  |             |
|    approx_kl            | 0.030488623 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.551      |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.0005      |
|    loss                 | 3.92e+03    |
|    n_updates            | 5649        |
|    policy_gradient_loss | 0.0343      |
|    value_loss           | 7.06e+03    |
-----------------------------------------
Eval num_timesteps=648000, episode_reward=-434.44 +/- 141.73
Episode length: 37.92 +/- 10.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.9     |
|    mean_reward     | -434     |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | -338     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 633      |
|    time_elapsed    | 2395     |
|    total_timesteps | 648192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=648500, episode_reward=-344.06 +/- 144.91
Episode length: 33.10 +/- 7.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.1        |
|    mean_reward          | -344        |
| time/                   |             |
|    total_timesteps      | 648500      |
| train/                  |             |
|    approx_kl            | 0.036492515 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.0005      |
|    loss                 | 4.5e+03     |
|    n_updates            | 5650        |
|    policy_gradient_loss | 0.022       |
|    value_loss           | 8.75e+03    |
-----------------------------------------
Eval num_timesteps=649000, episode_reward=-329.25 +/- 139.33
Episode length: 36.12 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | -342     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 634      |
|    time_elapsed    | 2398     |
|    total_timesteps | 649216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=649500, episode_reward=-294.10 +/- 143.66
Episode length: 33.24 +/- 9.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.2        |
|    mean_reward          | -294        |
| time/                   |             |
|    total_timesteps      | 649500      |
| train/                  |             |
|    approx_kl            | 0.046289746 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.05e+03    |
|    n_updates            | 5651        |
|    policy_gradient_loss | 0.0232      |
|    value_loss           | 7.57e+03    |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=-329.52 +/- 126.69
Episode length: 32.06 +/- 11.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.1     |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.1     |
|    ep_rew_mean     | -337     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 635      |
|    time_elapsed    | 2401     |
|    total_timesteps | 650240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=650500, episode_reward=-329.42 +/- 142.90
Episode length: 33.10 +/- 9.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.1        |
|    mean_reward          | -329        |
| time/                   |             |
|    total_timesteps      | 650500      |
| train/                  |             |
|    approx_kl            | 0.024878118 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.246      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.44e+03    |
|    n_updates            | 5652        |
|    policy_gradient_loss | 0.0204      |
|    value_loss           | 2.73e+03    |
-----------------------------------------
Eval num_timesteps=651000, episode_reward=-331.41 +/- 138.07
Episode length: 35.64 +/- 26.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | -331     |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.9     |
|    ep_rew_mean     | -342     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 636      |
|    time_elapsed    | 2404     |
|    total_timesteps | 651264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=651500, episode_reward=-279.46 +/- 108.86
Episode length: 31.74 +/- 7.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 31.7       |
|    mean_reward          | -279       |
| time/                   |            |
|    total_timesteps      | 651500     |
| train/                  |            |
|    approx_kl            | 0.03468075 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.24e+03   |
|    n_updates            | 5653       |
|    policy_gradient_loss | 0.0101     |
|    value_loss           | 6.36e+03   |
----------------------------------------
Eval num_timesteps=652000, episode_reward=-254.58 +/- 152.13
Episode length: 33.40 +/- 9.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 652000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.2     |
|    ep_rew_mean     | -310     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 637      |
|    time_elapsed    | 2407     |
|    total_timesteps | 652288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.31
Eval num_timesteps=652500, episode_reward=-223.52 +/- 122.95
Episode length: 33.82 +/- 9.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | -224        |
| time/                   |             |
|    total_timesteps      | 652500      |
| train/                  |             |
|    approx_kl            | 0.116105944 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.69e+03    |
|    n_updates            | 5654        |
|    policy_gradient_loss | 0.0167      |
|    value_loss           | 6.83e+03    |
-----------------------------------------
Eval num_timesteps=653000, episode_reward=-193.92 +/- 148.82
Episode length: 36.54 +/- 12.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.2     |
|    ep_rew_mean     | -281     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 638      |
|    time_elapsed    | 2411     |
|    total_timesteps | 653312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=653500, episode_reward=-234.50 +/- 163.77
Episode length: 35.94 +/- 10.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | -234        |
| time/                   |             |
|    total_timesteps      | 653500      |
| train/                  |             |
|    approx_kl            | 0.034839664 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.267      |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.93e+03    |
|    n_updates            | 5655        |
|    policy_gradient_loss | 0.0213      |
|    value_loss           | 7.58e+03    |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=-240.64 +/- 141.07
Episode length: 34.04 +/- 10.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37       |
|    ep_rew_mean     | -260     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 639      |
|    time_elapsed    | 2414     |
|    total_timesteps | 654336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=654500, episode_reward=-186.75 +/- 189.00
Episode length: 41.02 +/- 13.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41          |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 654500      |
| train/                  |             |
|    approx_kl            | 0.032548662 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.51e+03    |
|    n_updates            | 5656        |
|    policy_gradient_loss | 0.0206      |
|    value_loss           | 6.76e+03    |
-----------------------------------------
Eval num_timesteps=655000, episode_reward=-240.03 +/- 137.39
Episode length: 37.62 +/- 10.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | -240     |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.4     |
|    ep_rew_mean     | -229     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 640      |
|    time_elapsed    | 2417     |
|    total_timesteps | 655360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=655500, episode_reward=-161.98 +/- 228.98
Episode length: 41.78 +/- 12.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 655500      |
| train/                  |             |
|    approx_kl            | 0.029766282 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.89e+03    |
|    n_updates            | 5657        |
|    policy_gradient_loss | 0.0261      |
|    value_loss           | 6.31e+03    |
-----------------------------------------
Eval num_timesteps=656000, episode_reward=-190.72 +/- 169.23
Episode length: 42.00 +/- 11.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 656000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.3     |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 641      |
|    time_elapsed    | 2421     |
|    total_timesteps | 656384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=656500, episode_reward=-186.89 +/- 170.54
Episode length: 37.60 +/- 11.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.6        |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 656500      |
| train/                  |             |
|    approx_kl            | 0.029825935 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.69e+03    |
|    n_updates            | 5658        |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 9.28e+03    |
-----------------------------------------
Eval num_timesteps=657000, episode_reward=-158.76 +/- 170.92
Episode length: 38.54 +/- 10.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.5     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.5     |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 642      |
|    time_elapsed    | 2425     |
|    total_timesteps | 657408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=657500, episode_reward=-161.06 +/- 161.50
Episode length: 36.26 +/- 10.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.3        |
|    mean_reward          | -161        |
| time/                   |             |
|    total_timesteps      | 657500      |
| train/                  |             |
|    approx_kl            | 0.061708733 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.335      |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.6e+03     |
|    n_updates            | 5659        |
|    policy_gradient_loss | 0.0274      |
|    value_loss           | 9.58e+03    |
-----------------------------------------
Eval num_timesteps=658000, episode_reward=-164.51 +/- 182.58
Episode length: 34.36 +/- 11.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 658000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.6     |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 643      |
|    time_elapsed    | 2428     |
|    total_timesteps | 658432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=658500, episode_reward=-155.56 +/- 173.72
Episode length: 37.28 +/- 11.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 37.3       |
|    mean_reward          | -156       |
| time/                   |            |
|    total_timesteps      | 658500     |
| train/                  |            |
|    approx_kl            | 0.02831606 |
|    clip_fraction        | 0.188      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.36      |
|    explained_variance   | 0.286      |
|    learning_rate        | 0.0005     |
|    loss                 | 4e+03      |
|    n_updates            | 5660       |
|    policy_gradient_loss | 0.0252     |
|    value_loss           | 7e+03      |
----------------------------------------
Eval num_timesteps=659000, episode_reward=-204.62 +/- 149.00
Episode length: 35.48 +/- 9.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.5     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 644      |
|    time_elapsed    | 2432     |
|    total_timesteps | 659456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=659500, episode_reward=-170.49 +/- 212.70
Episode length: 39.02 +/- 11.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | -170        |
| time/                   |             |
|    total_timesteps      | 659500      |
| train/                  |             |
|    approx_kl            | 0.033814553 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.342      |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.96e+03    |
|    n_updates            | 5661        |
|    policy_gradient_loss | 0.0195      |
|    value_loss           | 7.08e+03    |
-----------------------------------------
Eval num_timesteps=660000, episode_reward=-109.55 +/- 221.63
Episode length: 41.96 +/- 13.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 645      |
|    time_elapsed    | 2435     |
|    total_timesteps | 660480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=660500, episode_reward=-210.57 +/- 164.40
Episode length: 38.66 +/- 10.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.7        |
|    mean_reward          | -211        |
| time/                   |             |
|    total_timesteps      | 660500      |
| train/                  |             |
|    approx_kl            | 0.022646258 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.1e+03     |
|    n_updates            | 5662        |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 8.42e+03    |
-----------------------------------------
Eval num_timesteps=661000, episode_reward=-201.08 +/- 170.06
Episode length: 38.88 +/- 11.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.9     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=661500, episode_reward=-186.10 +/- 187.70
Episode length: 39.34 +/- 11.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.3     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 661500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | -224     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 646      |
|    time_elapsed    | 2440     |
|    total_timesteps | 661504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=662000, episode_reward=-209.35 +/- 170.25
Episode length: 39.52 +/- 11.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.5        |
|    mean_reward          | -209        |
| time/                   |             |
|    total_timesteps      | 662000      |
| train/                  |             |
|    approx_kl            | 0.048547547 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.269      |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.93e+03    |
|    n_updates            | 5664        |
|    policy_gradient_loss | 0.019       |
|    value_loss           | 6.1e+03     |
-----------------------------------------
Eval num_timesteps=662500, episode_reward=-190.62 +/- 177.72
Episode length: 39.88 +/- 10.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.9     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 662500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | -224     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 647      |
|    time_elapsed    | 2444     |
|    total_timesteps | 662528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=663000, episode_reward=-206.80 +/- 177.55
Episode length: 37.44 +/- 14.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.4        |
|    mean_reward          | -207        |
| time/                   |             |
|    total_timesteps      | 663000      |
| train/                  |             |
|    approx_kl            | 0.044039357 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.496      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.9e+03     |
|    n_updates            | 5665        |
|    policy_gradient_loss | 0.0411      |
|    value_loss           | 6.01e+03    |
-----------------------------------------
Eval num_timesteps=663500, episode_reward=-279.09 +/- 111.28
Episode length: 33.98 +/- 9.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 663500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.9     |
|    ep_rew_mean     | -241     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 648      |
|    time_elapsed    | 2447     |
|    total_timesteps | 663552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=664000, episode_reward=-277.71 +/- 137.38
Episode length: 34.08 +/- 7.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.1        |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 664000      |
| train/                  |             |
|    approx_kl            | 0.045483723 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.11e+03    |
|    n_updates            | 5666        |
|    policy_gradient_loss | 0.0129      |
|    value_loss           | 5.96e+03    |
-----------------------------------------
Eval num_timesteps=664500, episode_reward=-316.15 +/- 137.51
Episode length: 33.56 +/- 13.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 664500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | -241     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 649      |
|    time_elapsed    | 2450     |
|    total_timesteps | 664576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=665000, episode_reward=-364.70 +/- 121.54
Episode length: 31.60 +/- 8.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 31.6       |
|    mean_reward          | -365       |
| time/                   |            |
|    total_timesteps      | 665000     |
| train/                  |            |
|    approx_kl            | 0.03861778 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.448     |
|    explained_variance   | 0.476      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.57e+03   |
|    n_updates            | 5667       |
|    policy_gradient_loss | 0.0238     |
|    value_loss           | 5.71e+03   |
----------------------------------------
Eval num_timesteps=665500, episode_reward=-359.35 +/- 132.66
Episode length: 32.60 +/- 10.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 665500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 650      |
|    time_elapsed    | 2453     |
|    total_timesteps | 665600   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.16
Eval num_timesteps=666000, episode_reward=-276.57 +/- 145.42
Episode length: 32.82 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.8      |
|    mean_reward          | -277      |
| time/                   |           |
|    total_timesteps      | 666000    |
| train/                  |           |
|    approx_kl            | 0.1586964 |
|    clip_fraction        | 0.193     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.383    |
|    explained_variance   | 0.322     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.93e+03  |
|    n_updates            | 5669      |
|    policy_gradient_loss | 0.0199    |
|    value_loss           | 8.37e+03  |
---------------------------------------
Eval num_timesteps=666500, episode_reward=-266.41 +/- 139.63
Episode length: 37.16 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 666500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | -303     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 651      |
|    time_elapsed    | 2457     |
|    total_timesteps | 666624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=667000, episode_reward=-268.89 +/- 127.02
Episode length: 36.84 +/- 8.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.8        |
|    mean_reward          | -269        |
| time/                   |             |
|    total_timesteps      | 667000      |
| train/                  |             |
|    approx_kl            | 0.050665423 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.313      |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.74e+03    |
|    n_updates            | 5670        |
|    policy_gradient_loss | 0.0221      |
|    value_loss           | 6.2e+03     |
-----------------------------------------
Eval num_timesteps=667500, episode_reward=-231.33 +/- 198.55
Episode length: 35.90 +/- 12.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 667500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.8     |
|    ep_rew_mean     | -298     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 652      |
|    time_elapsed    | 2460     |
|    total_timesteps | 667648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.14
Eval num_timesteps=668000, episode_reward=-293.96 +/- 128.81
Episode length: 36.50 +/- 11.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.5        |
|    mean_reward          | -294        |
| time/                   |             |
|    total_timesteps      | 668000      |
| train/                  |             |
|    approx_kl            | 0.048258256 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.04e+03    |
|    n_updates            | 5672        |
|    policy_gradient_loss | 0.0184      |
|    value_loss           | 6.33e+03    |
-----------------------------------------
Eval num_timesteps=668500, episode_reward=-260.24 +/- 109.62
Episode length: 35.12 +/- 8.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 668500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | -292     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 653      |
|    time_elapsed    | 2464     |
|    total_timesteps | 668672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=669000, episode_reward=-330.60 +/- 124.93
Episode length: 35.44 +/- 10.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | -331        |
| time/                   |             |
|    total_timesteps      | 669000      |
| train/                  |             |
|    approx_kl            | 0.035755884 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.318      |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.47e+03    |
|    n_updates            | 5674        |
|    policy_gradient_loss | 0.0205      |
|    value_loss           | 7.46e+03    |
-----------------------------------------
Eval num_timesteps=669500, episode_reward=-275.10 +/- 108.44
Episode length: 30.68 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.7     |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 669500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.6     |
|    ep_rew_mean     | -261     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 654      |
|    time_elapsed    | 2467     |
|    total_timesteps | 669696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.08
Eval num_timesteps=670000, episode_reward=-230.48 +/- 148.16
Episode length: 36.08 +/- 8.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | -230      |
| time/                   |           |
|    total_timesteps      | 670000    |
| train/                  |           |
|    approx_kl            | 0.0753483 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.354    |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.55e+03  |
|    n_updates            | 5676      |
|    policy_gradient_loss | 0.0071    |
|    value_loss           | 4.99e+03  |
---------------------------------------
Eval num_timesteps=670500, episode_reward=-231.84 +/- 127.72
Episode length: 34.86 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 670500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | -251     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 655      |
|    time_elapsed    | 2470     |
|    total_timesteps | 670720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=671000, episode_reward=-230.29 +/- 148.00
Episode length: 33.58 +/- 9.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | -230        |
| time/                   |             |
|    total_timesteps      | 671000      |
| train/                  |             |
|    approx_kl            | 0.016685465 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.506       |
|    learning_rate        | 0.0005      |
|    loss                 | 3e+03       |
|    n_updates            | 5677        |
|    policy_gradient_loss | 0.0165      |
|    value_loss           | 6.19e+03    |
-----------------------------------------
Eval num_timesteps=671500, episode_reward=-218.81 +/- 170.28
Episode length: 34.72 +/- 10.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 671500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | -249     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 656      |
|    time_elapsed    | 2474     |
|    total_timesteps | 671744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=672000, episode_reward=-349.15 +/- 145.82
Episode length: 35.22 +/- 15.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.2       |
|    mean_reward          | -349       |
| time/                   |            |
|    total_timesteps      | 672000     |
| train/                  |            |
|    approx_kl            | 0.05189419 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.422     |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.74e+03   |
|    n_updates            | 5678       |
|    policy_gradient_loss | 0.0148     |
|    value_loss           | 5.28e+03   |
----------------------------------------
Eval num_timesteps=672500, episode_reward=-371.98 +/- 153.25
Episode length: 36.38 +/- 12.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | -372     |
| time/              |          |
|    total_timesteps | 672500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | -271     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 657      |
|    time_elapsed    | 2477     |
|    total_timesteps | 672768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=673000, episode_reward=-280.51 +/- 172.74
Episode length: 35.88 +/- 11.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 673000      |
| train/                  |             |
|    approx_kl            | 0.035554666 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.336      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.35e+03    |
|    n_updates            | 5679        |
|    policy_gradient_loss | 0.0165      |
|    value_loss           | 1.01e+04    |
-----------------------------------------
Eval num_timesteps=673500, episode_reward=-232.53 +/- 140.57
Episode length: 35.56 +/- 10.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 673500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | -283     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 658      |
|    time_elapsed    | 2480     |
|    total_timesteps | 673792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=674000, episode_reward=-247.40 +/- 163.97
Episode length: 35.28 +/- 15.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.3       |
|    mean_reward          | -247       |
| time/                   |            |
|    total_timesteps      | 674000     |
| train/                  |            |
|    approx_kl            | 0.03232241 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.318     |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.98e+03   |
|    n_updates            | 5680       |
|    policy_gradient_loss | 0.0183     |
|    value_loss           | 7.24e+03   |
----------------------------------------
Eval num_timesteps=674500, episode_reward=-191.33 +/- 207.28
Episode length: 34.02 +/- 11.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 674500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 659      |
|    time_elapsed    | 2483     |
|    total_timesteps | 674816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=675000, episode_reward=-179.01 +/- 136.26
Episode length: 33.74 +/- 8.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.7       |
|    mean_reward          | -179       |
| time/                   |            |
|    total_timesteps      | 675000     |
| train/                  |            |
|    approx_kl            | 0.02902143 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.278     |
|    explained_variance   | 0.512      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.58e+03   |
|    n_updates            | 5681       |
|    policy_gradient_loss | 0.00399    |
|    value_loss           | 8.53e+03   |
----------------------------------------
Eval num_timesteps=675500, episode_reward=-145.88 +/- 164.17
Episode length: 35.00 +/- 12.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 675500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | -217     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 660      |
|    time_elapsed    | 2487     |
|    total_timesteps | 675840   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=676000, episode_reward=-191.20 +/- 186.00
Episode length: 39.40 +/- 30.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.4        |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.040268745 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.296      |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.0005      |
|    loss                 | 6.5e+03     |
|    n_updates            | 5683        |
|    policy_gradient_loss | 0.00981     |
|    value_loss           | 1.12e+04    |
-----------------------------------------
Eval num_timesteps=676500, episode_reward=-210.58 +/- 133.47
Episode length: 31.12 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.1     |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 676500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 661      |
|    time_elapsed    | 2490     |
|    total_timesteps | 676864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.23
Eval num_timesteps=677000, episode_reward=-349.27 +/- 156.04
Episode length: 34.64 +/- 13.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.6       |
|    mean_reward          | -349       |
| time/                   |            |
|    total_timesteps      | 677000     |
| train/                  |            |
|    approx_kl            | 0.09240746 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.284     |
|    explained_variance   | 0.31       |
|    learning_rate        | 0.0005     |
|    loss                 | 3.67e+03   |
|    n_updates            | 5684       |
|    policy_gradient_loss | 0.025      |
|    value_loss           | 6.63e+03   |
----------------------------------------
Eval num_timesteps=677500, episode_reward=-346.05 +/- 169.26
Episode length: 34.46 +/- 11.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | -346     |
| time/              |          |
|    total_timesteps | 677500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | -220     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 662      |
|    time_elapsed    | 2493     |
|    total_timesteps | 677888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=678000, episode_reward=-365.87 +/- 141.12
Episode length: 32.46 +/- 11.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 32.5       |
|    mean_reward          | -366       |
| time/                   |            |
|    total_timesteps      | 678000     |
| train/                  |            |
|    approx_kl            | 0.02537927 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.217     |
|    explained_variance   | 0.448      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.81e+03   |
|    n_updates            | 5685       |
|    policy_gradient_loss | 0.0286     |
|    value_loss           | 9.23e+03   |
----------------------------------------
Eval num_timesteps=678500, episode_reward=-358.53 +/- 178.10
Episode length: 36.44 +/- 13.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 678500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.8     |
|    ep_rew_mean     | -279     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 663      |
|    time_elapsed    | 2496     |
|    total_timesteps | 678912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.19
Eval num_timesteps=679000, episode_reward=-270.85 +/- 196.87
Episode length: 37.30 +/- 12.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.3        |
|    mean_reward          | -271        |
| time/                   |             |
|    total_timesteps      | 679000      |
| train/                  |             |
|    approx_kl            | 0.044484653 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.96e+03    |
|    n_updates            | 5686        |
|    policy_gradient_loss | 0.0185      |
|    value_loss           | 9.47e+03    |
-----------------------------------------
Eval num_timesteps=679500, episode_reward=-286.16 +/- 130.88
Episode length: 30.14 +/- 8.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.1     |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 679500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.2     |
|    ep_rew_mean     | -328     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 664      |
|    time_elapsed    | 2500     |
|    total_timesteps | 679936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=680000, episode_reward=-316.90 +/- 143.80
Episode length: 33.80 +/- 9.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | -317        |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.032308765 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.33e+03    |
|    n_updates            | 5687        |
|    policy_gradient_loss | 0.0336      |
|    value_loss           | 5.33e+03    |
-----------------------------------------
Eval num_timesteps=680500, episode_reward=-300.21 +/- 154.25
Episode length: 31.54 +/- 8.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.5     |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 680500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38       |
|    ep_rew_mean     | -357     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 665      |
|    time_elapsed    | 2503     |
|    total_timesteps | 680960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=681000, episode_reward=-247.86 +/- 154.46
Episode length: 41.56 +/- 69.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | -248        |
| time/                   |             |
|    total_timesteps      | 681000      |
| train/                  |             |
|    approx_kl            | 0.049590174 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.227      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0005      |
|    loss                 | 2.61e+03    |
|    n_updates            | 5688        |
|    policy_gradient_loss | 0.025       |
|    value_loss           | 5.54e+03    |
-----------------------------------------
Eval num_timesteps=681500, episode_reward=-253.80 +/- 151.35
Episode length: 33.08 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 681500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | -299     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 666      |
|    time_elapsed    | 2506     |
|    total_timesteps | 681984   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=682000, episode_reward=-162.56 +/- 159.43
Episode length: 35.42 +/- 13.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.4       |
|    mean_reward          | -163       |
| time/                   |            |
|    total_timesteps      | 682000     |
| train/                  |            |
|    approx_kl            | 0.07370407 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.206     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.52e+03   |
|    n_updates            | 5690       |
|    policy_gradient_loss | 0.0236     |
|    value_loss           | 9.18e+03   |
----------------------------------------
Eval num_timesteps=682500, episode_reward=-125.77 +/- 171.80
Episode length: 34.72 +/- 10.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 682500   |
---------------------------------
Eval num_timesteps=683000, episode_reward=-159.86 +/- 144.51
Episode length: 32.92 +/- 10.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | -267     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 667      |
|    time_elapsed    | 2511     |
|    total_timesteps | 683008   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.09
Eval num_timesteps=683500, episode_reward=-179.25 +/- 202.52
Episode length: 33.66 +/- 10.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.7       |
|    mean_reward          | -179       |
| time/                   |            |
|    total_timesteps      | 683500     |
| train/                  |            |
|    approx_kl            | 0.03931453 |
|    clip_fraction        | 0.0768     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.492      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.04e+03   |
|    n_updates            | 5693       |
|    policy_gradient_loss | 0.00626    |
|    value_loss           | 6.3e+03    |
----------------------------------------
Eval num_timesteps=684000, episode_reward=-199.34 +/- 134.01
Episode length: 33.50 +/- 8.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.3     |
|    ep_rew_mean     | -228     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 668      |
|    time_elapsed    | 2514     |
|    total_timesteps | 684032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=684500, episode_reward=-217.86 +/- 144.73
Episode length: 33.66 +/- 14.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.7        |
|    mean_reward          | -218        |
| time/                   |             |
|    total_timesteps      | 684500      |
| train/                  |             |
|    approx_kl            | 0.039789543 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.202      |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.5e+03     |
|    n_updates            | 5694        |
|    policy_gradient_loss | 0.0223      |
|    value_loss           | 5.22e+03    |
-----------------------------------------
Eval num_timesteps=685000, episode_reward=-212.40 +/- 174.30
Episode length: 33.52 +/- 9.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.7     |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 669      |
|    time_elapsed    | 2517     |
|    total_timesteps | 685056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=685500, episode_reward=-294.79 +/- 169.44
Episode length: 33.44 +/- 12.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.4        |
|    mean_reward          | -295        |
| time/                   |             |
|    total_timesteps      | 685500      |
| train/                  |             |
|    approx_kl            | 0.068155974 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0005      |
|    loss                 | 2.84e+03    |
|    n_updates            | 5696        |
|    policy_gradient_loss | 0.0176      |
|    value_loss           | 5.99e+03    |
-----------------------------------------
Eval num_timesteps=686000, episode_reward=-266.00 +/- 174.84
Episode length: 34.12 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.9     |
|    ep_rew_mean     | -239     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 670      |
|    time_elapsed    | 2520     |
|    total_timesteps | 686080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=686500, episode_reward=-293.23 +/- 197.99
Episode length: 34.68 +/- 13.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | -293        |
| time/                   |             |
|    total_timesteps      | 686500      |
| train/                  |             |
|    approx_kl            | 0.024761265 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.09e+03    |
|    n_updates            | 5697        |
|    policy_gradient_loss | 0.0158      |
|    value_loss           | 1.01e+04    |
-----------------------------------------
Eval num_timesteps=687000, episode_reward=-325.62 +/- 172.48
Episode length: 35.86 +/- 12.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | -326     |
| time/              |          |
|    total_timesteps | 687000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | -269     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 671      |
|    time_elapsed    | 2524     |
|    total_timesteps | 687104   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.09
Eval num_timesteps=687500, episode_reward=-301.12 +/- 172.12
Episode length: 34.68 +/- 12.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | -301        |
| time/                   |             |
|    total_timesteps      | 687500      |
| train/                  |             |
|    approx_kl            | 0.041183874 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.67e+03    |
|    n_updates            | 5699        |
|    policy_gradient_loss | 0.0119      |
|    value_loss           | 9.59e+03    |
-----------------------------------------
Eval num_timesteps=688000, episode_reward=-335.36 +/- 138.18
Episode length: 32.70 +/- 8.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | -315     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 672      |
|    time_elapsed    | 2527     |
|    total_timesteps | 688128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=688500, episode_reward=-431.84 +/- 148.25
Episode length: 42.84 +/- 23.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.8       |
|    mean_reward          | -432       |
| time/                   |            |
|    total_timesteps      | 688500     |
| train/                  |            |
|    approx_kl            | 0.04540057 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.197     |
|    explained_variance   | 0.511      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.59e+03   |
|    n_updates            | 5700       |
|    policy_gradient_loss | 0.0604     |
|    value_loss           | 6.93e+03   |
----------------------------------------
Eval num_timesteps=689000, episode_reward=-440.58 +/- 146.29
Episode length: 41.84 +/- 13.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 689000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | -363     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 673      |
|    time_elapsed    | 2530     |
|    total_timesteps | 689152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=689500, episode_reward=-333.60 +/- 178.40
Episode length: 34.96 +/- 12.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | -334        |
| time/                   |             |
|    total_timesteps      | 689500      |
| train/                  |             |
|    approx_kl            | 0.028544247 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.07e+03    |
|    n_updates            | 5701        |
|    policy_gradient_loss | 0.00842     |
|    value_loss           | 8.01e+03    |
-----------------------------------------
Eval num_timesteps=690000, episode_reward=-350.40 +/- 158.59
Episode length: 35.28 +/- 12.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | -397     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 674      |
|    time_elapsed    | 2534     |
|    total_timesteps | 690176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=690500, episode_reward=-281.11 +/- 136.31
Episode length: 32.46 +/- 7.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.5        |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 690500      |
| train/                  |             |
|    approx_kl            | 0.033953883 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.91e+03    |
|    n_updates            | 5702        |
|    policy_gradient_loss | 0.0202      |
|    value_loss           | 7.15e+03    |
-----------------------------------------
Eval num_timesteps=691000, episode_reward=-310.61 +/- 149.37
Episode length: 33.78 +/- 11.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 691000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | -373     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 675      |
|    time_elapsed    | 2537     |
|    total_timesteps | 691200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.17
Eval num_timesteps=691500, episode_reward=-154.03 +/- 137.97
Episode length: 31.94 +/- 10.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 31.9      |
|    mean_reward          | -154      |
| time/                   |           |
|    total_timesteps      | 691500    |
| train/                  |           |
|    approx_kl            | 0.0723102 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.152    |
|    explained_variance   | 0.445     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.18e+03  |
|    n_updates            | 5703      |
|    policy_gradient_loss | 0.0581    |
|    value_loss           | 9.05e+03  |
---------------------------------------
Eval num_timesteps=692000, episode_reward=-179.96 +/- 149.48
Episode length: 34.24 +/- 12.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | -317     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 676      |
|    time_elapsed    | 2540     |
|    total_timesteps | 692224   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.15
Eval num_timesteps=692500, episode_reward=-205.72 +/- 150.22
Episode length: 32.76 +/- 8.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.8        |
|    mean_reward          | -206        |
| time/                   |             |
|    total_timesteps      | 692500      |
| train/                  |             |
|    approx_kl            | 0.049616378 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0005      |
|    loss                 | 6.75e+03    |
|    n_updates            | 5706        |
|    policy_gradient_loss | 0.0067      |
|    value_loss           | 1.29e+04    |
-----------------------------------------
Eval num_timesteps=693000, episode_reward=-226.74 +/- 121.21
Episode length: 31.50 +/- 10.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.5     |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 693000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39       |
|    ep_rew_mean     | -272     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 677      |
|    time_elapsed    | 2543     |
|    total_timesteps | 693248   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.08
Eval num_timesteps=693500, episode_reward=-93.80 +/- 183.42
Episode length: 35.68 +/- 10.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.7        |
|    mean_reward          | -93.8       |
| time/                   |             |
|    total_timesteps      | 693500      |
| train/                  |             |
|    approx_kl            | 0.027950063 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0873     |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.0005      |
|    loss                 | 2.72e+03    |
|    n_updates            | 5713        |
|    policy_gradient_loss | -0.000135   |
|    value_loss           | 5.62e+03    |
-----------------------------------------
Eval num_timesteps=694000, episode_reward=-120.65 +/- 157.57
Episode length: 34.82 +/- 13.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.3     |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 678      |
|    time_elapsed    | 2547     |
|    total_timesteps | 694272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=694500, episode_reward=-154.34 +/- 165.09
Episode length: 30.94 +/- 9.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 30.9      |
|    mean_reward          | -154      |
| time/                   |           |
|    total_timesteps      | 694500    |
| train/                  |           |
|    approx_kl            | 0.1193368 |
|    clip_fraction        | 0.066     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.154    |
|    explained_variance   | 0.309     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.57e+03  |
|    n_updates            | 5715      |
|    policy_gradient_loss | 0.0102    |
|    value_loss           | 9.16e+03  |
---------------------------------------
Eval num_timesteps=695000, episode_reward=-84.92 +/- 183.69
Episode length: 34.38 +/- 12.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | -84.9    |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.3     |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 679      |
|    time_elapsed    | 2550     |
|    total_timesteps | 695296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=695500, episode_reward=-143.22 +/- 156.13
Episode length: 34.46 +/- 14.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.5        |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 695500      |
| train/                  |             |
|    approx_kl            | 0.017914085 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.22e+03    |
|    n_updates            | 5716        |
|    policy_gradient_loss | 0.0152      |
|    value_loss           | 7.61e+03    |
-----------------------------------------
Eval num_timesteps=696000, episode_reward=-60.65 +/- 171.59
Episode length: 37.50 +/- 11.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | -60.7    |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.6     |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 680      |
|    time_elapsed    | 2553     |
|    total_timesteps | 696320   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.07
Eval num_timesteps=696500, episode_reward=-74.95 +/- 142.28
Episode length: 33.66 +/- 9.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.7        |
|    mean_reward          | -74.9       |
| time/                   |             |
|    total_timesteps      | 696500      |
| train/                  |             |
|    approx_kl            | 0.052051898 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.8e+03     |
|    n_updates            | 5720        |
|    policy_gradient_loss | 0.0132      |
|    value_loss           | 8.79e+03    |
-----------------------------------------
Eval num_timesteps=697000, episode_reward=-79.03 +/- 170.74
Episode length: 34.70 +/- 9.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | -79      |
| time/              |          |
|    total_timesteps | 697000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 681      |
|    time_elapsed    | 2556     |
|    total_timesteps | 697344   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=697500, episode_reward=-63.83 +/- 186.65
Episode length: 36.30 +/- 11.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.3        |
|    mean_reward          | -63.8       |
| time/                   |             |
|    total_timesteps      | 697500      |
| train/                  |             |
|    approx_kl            | 0.038645346 |
|    clip_fraction        | 0.054       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.83e+03    |
|    n_updates            | 5722        |
|    policy_gradient_loss | 0.00371     |
|    value_loss           | 9.59e+03    |
-----------------------------------------
Eval num_timesteps=698000, episode_reward=-67.72 +/- 213.11
Episode length: 35.88 +/- 12.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | -67.7    |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 682      |
|    time_elapsed    | 2560     |
|    total_timesteps | 698368   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.09
Eval num_timesteps=698500, episode_reward=-102.24 +/- 161.43
Episode length: 34.04 +/- 10.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34         |
|    mean_reward          | -102       |
| time/                   |            |
|    total_timesteps      | 698500     |
| train/                  |            |
|    approx_kl            | 0.08603139 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.283     |
|    explained_variance   | 0.415      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.39e+03   |
|    n_updates            | 5727       |
|    policy_gradient_loss | 0.013      |
|    value_loss           | 8.38e+03   |
----------------------------------------
Eval num_timesteps=699000, episode_reward=-100.33 +/- 211.74
Episode length: 35.40 +/- 11.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 699000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | -107     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 683      |
|    time_elapsed    | 2563     |
|    total_timesteps | 699392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=699500, episode_reward=-169.86 +/- 142.28
Episode length: 34.40 +/- 9.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | -170        |
| time/                   |             |
|    total_timesteps      | 699500      |
| train/                  |             |
|    approx_kl            | 0.031983096 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.0005      |
|    loss                 | 3.15e+03    |
|    n_updates            | 5728        |
|    policy_gradient_loss | 0.0247      |
|    value_loss           | 6.87e+03    |
-----------------------------------------
Eval num_timesteps=700000, episode_reward=-168.81 +/- 152.24
Episode length: 36.06 +/- 12.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 684      |
|    time_elapsed    | 2567     |
|    total_timesteps | 700416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=700500, episode_reward=-174.13 +/- 144.89
Episode length: 33.82 +/- 8.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.8     |
|    mean_reward          | -174     |
| time/                   |          |
|    total_timesteps      | 700500   |
| train/                  |          |
|    approx_kl            | 0.039338 |
|    clip_fraction        | 0.146    |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -0.4     |
|    explained_variance   | 0.446    |
|    learning_rate        | 0.0005   |
|    loss                 | 4.33e+03 |
|    n_updates            | 5729     |
|    policy_gradient_loss | 0.0223   |
|    value_loss           | 8.52e+03 |
--------------------------------------
Eval num_timesteps=701000, episode_reward=-157.05 +/- 158.20
Episode length: 34.18 +/- 9.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 701000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 685      |
|    time_elapsed    | 2570     |
|    total_timesteps | 701440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=701500, episode_reward=-160.20 +/- 156.74
Episode length: 34.78 +/- 12.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | -160        |
| time/                   |             |
|    total_timesteps      | 701500      |
| train/                  |             |
|    approx_kl            | 0.062349528 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.87e+03    |
|    n_updates            | 5731        |
|    policy_gradient_loss | 0.0285      |
|    value_loss           | 7.49e+03    |
-----------------------------------------
Eval num_timesteps=702000, episode_reward=-158.73 +/- 180.58
Episode length: 34.04 +/- 11.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 686      |
|    time_elapsed    | 2573     |
|    total_timesteps | 702464   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.09
Eval num_timesteps=702500, episode_reward=-136.16 +/- 174.69
Episode length: 34.58 +/- 11.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | -136        |
| time/                   |             |
|    total_timesteps      | 702500      |
| train/                  |             |
|    approx_kl            | 0.046843186 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0005      |
|    loss                 | 4.5e+03     |
|    n_updates            | 5735        |
|    policy_gradient_loss | 0.0198      |
|    value_loss           | 7.97e+03    |
-----------------------------------------
Eval num_timesteps=703000, episode_reward=-137.23 +/- 199.65
Episode length: 34.10 +/- 12.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 703000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 687      |
|    time_elapsed    | 2576     |
|    total_timesteps | 703488   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=703500, episode_reward=-91.69 +/- 141.63
Episode length: 36.62 +/- 9.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.6       |
|    mean_reward          | -91.7      |
| time/                   |            |
|    total_timesteps      | 703500     |
| train/                  |            |
|    approx_kl            | 0.12463748 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.239      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.64e+03   |
|    n_updates            | 5737       |
|    policy_gradient_loss | 0.0011     |
|    value_loss           | 8.38e+03   |
----------------------------------------
Eval num_timesteps=704000, episode_reward=-128.88 +/- 121.69
Episode length: 34.26 +/- 8.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=704500, episode_reward=-92.32 +/- 175.60
Episode length: 36.84 +/- 11.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | -92.3    |
| time/              |          |
|    total_timesteps | 704500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 688      |
|    time_elapsed    | 2581     |
|    total_timesteps | 704512   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.11
Eval num_timesteps=705000, episode_reward=-71.94 +/- 156.61
Episode length: 35.64 +/- 10.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.6       |
|    mean_reward          | -71.9      |
| time/                   |            |
|    total_timesteps      | 705000     |
| train/                  |            |
|    approx_kl            | 0.10708695 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.214     |
|    explained_variance   | 0.413      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.07e+03   |
|    n_updates            | 5740       |
|    policy_gradient_loss | 0.00984    |
|    value_loss           | 6.82e+03   |
----------------------------------------
Eval num_timesteps=705500, episode_reward=-94.01 +/- 146.41
Episode length: 35.26 +/- 9.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | -94      |
| time/              |          |
|    total_timesteps | 705500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 689      |
|    time_elapsed    | 2584     |
|    total_timesteps | 705536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=706000, episode_reward=-160.87 +/- 166.99
Episode length: 37.44 +/- 9.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.4        |
|    mean_reward          | -161        |
| time/                   |             |
|    total_timesteps      | 706000      |
| train/                  |             |
|    approx_kl            | 0.054925844 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.64e+03    |
|    n_updates            | 5741        |
|    policy_gradient_loss | 0.025       |
|    value_loss           | 9.42e+03    |
-----------------------------------------
Eval num_timesteps=706500, episode_reward=-114.30 +/- 181.08
Episode length: 36.68 +/- 11.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 706500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 690      |
|    time_elapsed    | 2588     |
|    total_timesteps | 706560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=707000, episode_reward=-145.12 +/- 228.55
Episode length: 41.12 +/- 12.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.1       |
|    mean_reward          | -145       |
| time/                   |            |
|    total_timesteps      | 707000     |
| train/                  |            |
|    approx_kl            | 0.05376487 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.142     |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.72e+03   |
|    n_updates            | 5742       |
|    policy_gradient_loss | 0.016      |
|    value_loss           | 1.03e+04   |
----------------------------------------
Eval num_timesteps=707500, episode_reward=-165.74 +/- 185.90
Episode length: 41.68 +/- 10.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 707500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 691      |
|    time_elapsed    | 2591     |
|    total_timesteps | 707584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=708000, episode_reward=-116.64 +/- 181.30
Episode length: 37.98 +/- 10.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38          |
|    mean_reward          | -117        |
| time/                   |             |
|    total_timesteps      | 708000      |
| train/                  |             |
|    approx_kl            | 0.021877943 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.209      |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.61e+03    |
|    n_updates            | 5743        |
|    policy_gradient_loss | 0.0148      |
|    value_loss           | 9.73e+03    |
-----------------------------------------
Eval num_timesteps=708500, episode_reward=-109.54 +/- 198.51
Episode length: 39.82 +/- 10.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.8     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 708500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38       |
|    ep_rew_mean     | -106     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 692      |
|    time_elapsed    | 2595     |
|    total_timesteps | 708608   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=709000, episode_reward=-114.08 +/- 176.36
Episode length: 34.58 +/- 10.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | -114      |
| time/                   |           |
|    total_timesteps      | 709000    |
| train/                  |           |
|    approx_kl            | 0.1193514 |
|    clip_fraction        | 0.187     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.265    |
|    explained_variance   | 0.461     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.66e+03  |
|    n_updates            | 5745      |
|    policy_gradient_loss | 0.0142    |
|    value_loss           | 8.13e+03  |
---------------------------------------
Eval num_timesteps=709500, episode_reward=-108.35 +/- 191.56
Episode length: 34.70 +/- 9.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 709500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.3     |
|    ep_rew_mean     | -108     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 693      |
|    time_elapsed    | 2598     |
|    total_timesteps | 709632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=710000, episode_reward=-70.78 +/- 239.43
Episode length: 39.64 +/- 11.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.6        |
|    mean_reward          | -70.8       |
| time/                   |             |
|    total_timesteps      | 710000      |
| train/                  |             |
|    approx_kl            | 0.057601534 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.272      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.71e+03    |
|    n_updates            | 5747        |
|    policy_gradient_loss | 0.0128      |
|    value_loss           | 8.35e+03    |
-----------------------------------------
Eval num_timesteps=710500, episode_reward=-97.53 +/- 165.97
Episode length: 36.90 +/- 10.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | -97.5    |
| time/              |          |
|    total_timesteps | 710500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.5     |
|    ep_rew_mean     | -93      |
| time/              |          |
|    fps             | 273      |
|    iterations      | 694      |
|    time_elapsed    | 2602     |
|    total_timesteps | 710656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=711000, episode_reward=-88.74 +/- 365.96
Episode length: 38.64 +/- 12.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.6        |
|    mean_reward          | -88.7       |
| time/                   |             |
|    total_timesteps      | 711000      |
| train/                  |             |
|    approx_kl            | 0.027813759 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.0005      |
|    loss                 | 8.19e+03    |
|    n_updates            | 5748        |
|    policy_gradient_loss | 0.0144      |
|    value_loss           | 1.17e+04    |
-----------------------------------------
Eval num_timesteps=711500, episode_reward=-80.82 +/- 215.80
Episode length: 43.98 +/- 13.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44       |
|    mean_reward     | -80.8    |
| time/              |          |
|    total_timesteps | 711500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.4     |
|    ep_rew_mean     | -81.3    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 695      |
|    time_elapsed    | 2605     |
|    total_timesteps | 711680   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.07
Eval num_timesteps=712000, episode_reward=-123.85 +/- 165.09
Episode length: 36.64 +/- 9.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.6        |
|    mean_reward          | -124        |
| time/                   |             |
|    total_timesteps      | 712000      |
| train/                  |             |
|    approx_kl            | 0.039410517 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.36       |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0005      |
|    loss                 | 9.94e+03    |
|    n_updates            | 5751        |
|    policy_gradient_loss | 0.00672     |
|    value_loss           | 1.53e+04    |
-----------------------------------------
Eval num_timesteps=712500, episode_reward=-110.61 +/- 186.71
Episode length: 37.26 +/- 9.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 712500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37       |
|    ep_rew_mean     | -89.7    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 696      |
|    time_elapsed    | 2609     |
|    total_timesteps | 712704   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=713000, episode_reward=-78.10 +/- 224.89
Episode length: 41.02 +/- 10.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41          |
|    mean_reward          | -78.1       |
| time/                   |             |
|    total_timesteps      | 713000      |
| train/                  |             |
|    approx_kl            | 0.028452499 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.336      |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.11e+03    |
|    n_updates            | 5753        |
|    policy_gradient_loss | 0.0132      |
|    value_loss           | 9.22e+03    |
-----------------------------------------
Eval num_timesteps=713500, episode_reward=-153.20 +/- 261.76
Episode length: 40.56 +/- 10.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 713500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37       |
|    ep_rew_mean     | -110     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 697      |
|    time_elapsed    | 2613     |
|    total_timesteps | 713728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=714000, episode_reward=56.81 +/- 554.94
Episode length: 45.90 +/- 13.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.9        |
|    mean_reward          | 56.8        |
| time/                   |             |
|    total_timesteps      | 714000      |
| train/                  |             |
|    approx_kl            | 0.024574555 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.21e+03    |
|    n_updates            | 5754        |
|    policy_gradient_loss | 0.00732     |
|    value_loss           | 6.65e+03    |
-----------------------------------------
Eval num_timesteps=714500, episode_reward=66.59 +/- 487.79
Episode length: 44.70 +/- 13.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | 66.6     |
| time/              |          |
|    total_timesteps | 714500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.8     |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 698      |
|    time_elapsed    | 2617     |
|    total_timesteps | 714752   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.07
Eval num_timesteps=715000, episode_reward=75.61 +/- 496.76
Episode length: 50.00 +/- 16.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50          |
|    mean_reward          | 75.6        |
| time/                   |             |
|    total_timesteps      | 715000      |
| train/                  |             |
|    approx_kl            | 0.061740834 |
|    clip_fraction        | 0.0648      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.93e+03    |
|    n_updates            | 5756        |
|    policy_gradient_loss | 0.0127      |
|    value_loss           | 1.52e+04    |
-----------------------------------------
Eval num_timesteps=715500, episode_reward=3.19 +/- 501.82
Episode length: 44.52 +/- 13.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | 3.19     |
| time/              |          |
|    total_timesteps | 715500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.7     |
|    ep_rew_mean     | -109     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 699      |
|    time_elapsed    | 2621     |
|    total_timesteps | 715776   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.17
Eval num_timesteps=716000, episode_reward=-28.36 +/- 282.19
Episode length: 41.48 +/- 13.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.5        |
|    mean_reward          | -28.4       |
| time/                   |             |
|    total_timesteps      | 716000      |
| train/                  |             |
|    approx_kl            | 0.099878535 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.39e+03    |
|    n_updates            | 5759        |
|    policy_gradient_loss | 0.00853     |
|    value_loss           | 1.28e+04    |
-----------------------------------------
Eval num_timesteps=716500, episode_reward=-86.87 +/- 340.68
Episode length: 38.28 +/- 11.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.3     |
|    mean_reward     | -86.9    |
| time/              |          |
|    total_timesteps | 716500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.8     |
|    ep_rew_mean     | -100     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 700      |
|    time_elapsed    | 2624     |
|    total_timesteps | 716800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=717000, episode_reward=-72.23 +/- 221.38
Episode length: 37.50 +/- 12.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.5        |
|    mean_reward          | -72.2       |
| time/                   |             |
|    total_timesteps      | 717000      |
| train/                  |             |
|    approx_kl            | 0.041803118 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.238      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.47e+03    |
|    n_updates            | 5760        |
|    policy_gradient_loss | 0.0176      |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=717500, episode_reward=-106.68 +/- 181.87
Episode length: 35.14 +/- 9.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 717500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.5     |
|    ep_rew_mean     | -104     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 701      |
|    time_elapsed    | 2628     |
|    total_timesteps | 717824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=718000, episode_reward=-73.00 +/- 179.22
Episode length: 36.70 +/- 10.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.7        |
|    mean_reward          | -73         |
| time/                   |             |
|    total_timesteps      | 718000      |
| train/                  |             |
|    approx_kl            | 0.035220362 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.219      |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.11e+03    |
|    n_updates            | 5761        |
|    policy_gradient_loss | 0.04        |
|    value_loss           | 7.83e+03    |
-----------------------------------------
Eval num_timesteps=718500, episode_reward=-67.89 +/- 206.65
Episode length: 34.50 +/- 11.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | -67.9    |
| time/              |          |
|    total_timesteps | 718500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -115     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 702      |
|    time_elapsed    | 2631     |
|    total_timesteps | 718848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=719000, episode_reward=-65.38 +/- 167.31
Episode length: 38.42 +/- 9.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.4        |
|    mean_reward          | -65.4       |
| time/                   |             |
|    total_timesteps      | 719000      |
| train/                  |             |
|    approx_kl            | 0.025440484 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.03e+03    |
|    n_updates            | 5762        |
|    policy_gradient_loss | 0.00196     |
|    value_loss           | 6.57e+03    |
-----------------------------------------
Eval num_timesteps=719500, episode_reward=-72.89 +/- 175.32
Episode length: 37.24 +/- 10.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | -72.9    |
| time/              |          |
|    total_timesteps | 719500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | -101     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 703      |
|    time_elapsed    | 2635     |
|    total_timesteps | 719872   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=720000, episode_reward=-122.23 +/- 142.23
Episode length: 31.90 +/- 8.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.9        |
|    mean_reward          | -122        |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.049433522 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.18e+03    |
|    n_updates            | 5764        |
|    policy_gradient_loss | 0.0111      |
|    value_loss           | 6.64e+03    |
-----------------------------------------
Eval num_timesteps=720500, episode_reward=-61.07 +/- 150.07
Episode length: 35.96 +/- 9.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | -61.1    |
| time/              |          |
|    total_timesteps | 720500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | -79.2    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 704      |
|    time_elapsed    | 2638     |
|    total_timesteps | 720896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=721000, episode_reward=-109.32 +/- 167.64
Episode length: 34.20 +/- 11.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.2       |
|    mean_reward          | -109       |
| time/                   |            |
|    total_timesteps      | 721000     |
| train/                  |            |
|    approx_kl            | 0.03797653 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.179     |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.99e+03   |
|    n_updates            | 5765       |
|    policy_gradient_loss | 0.0234     |
|    value_loss           | 8.19e+03   |
----------------------------------------
Eval num_timesteps=721500, episode_reward=-147.82 +/- 147.50
Episode length: 34.26 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 721500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | -93.8    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 705      |
|    time_elapsed    | 2641     |
|    total_timesteps | 721920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=722000, episode_reward=-191.12 +/- 162.33
Episode length: 32.58 +/- 9.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.6        |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 722000      |
| train/                  |             |
|    approx_kl            | 0.038889054 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.289       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.89e+03    |
|    n_updates            | 5766        |
|    policy_gradient_loss | 0.0317      |
|    value_loss           | 7.9e+03     |
-----------------------------------------
Eval num_timesteps=722500, episode_reward=-191.28 +/- 186.21
Episode length: 34.24 +/- 9.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 722500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 706      |
|    time_elapsed    | 2644     |
|    total_timesteps | 722944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=723000, episode_reward=-398.96 +/- 167.73
Episode length: 42.36 +/- 16.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.4        |
|    mean_reward          | -399        |
| time/                   |             |
|    total_timesteps      | 723000      |
| train/                  |             |
|    approx_kl            | 0.030653214 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.27e+03    |
|    n_updates            | 5767        |
|    policy_gradient_loss | 0.0384      |
|    value_loss           | 8.85e+03    |
-----------------------------------------
Eval num_timesteps=723500, episode_reward=-388.05 +/- 169.94
Episode length: 40.84 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 723500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 707      |
|    time_elapsed    | 2648     |
|    total_timesteps | 723968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=724000, episode_reward=-130.33 +/- 119.27
Episode length: 34.26 +/- 7.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.3       |
|    mean_reward          | -130       |
| time/                   |            |
|    total_timesteps      | 724000     |
| train/                  |            |
|    approx_kl            | 0.03067632 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.188     |
|    explained_variance   | 0.311      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.36e+03   |
|    n_updates            | 5768       |
|    policy_gradient_loss | 0.0151     |
|    value_loss           | 1.48e+04   |
----------------------------------------
Eval num_timesteps=724500, episode_reward=-170.76 +/- 131.65
Episode length: 30.82 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 30.8     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 724500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | -212     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 708      |
|    time_elapsed    | 2651     |
|    total_timesteps | 724992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=725000, episode_reward=-277.06 +/- 152.43
Episode length: 31.04 +/- 7.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 31         |
|    mean_reward          | -277       |
| time/                   |            |
|    total_timesteps      | 725000     |
| train/                  |            |
|    approx_kl            | 0.02128173 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.147     |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.0005     |
|    loss                 | 4.17e+03   |
|    n_updates            | 5769       |
|    policy_gradient_loss | 0.0127     |
|    value_loss           | 8.19e+03   |
----------------------------------------
Eval num_timesteps=725500, episode_reward=-265.77 +/- 149.82
Episode length: 33.92 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 725500   |
---------------------------------
Eval num_timesteps=726000, episode_reward=-238.96 +/- 169.08
Episode length: 31.66 +/- 7.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.7     |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | -239     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 709      |
|    time_elapsed    | 2655     |
|    total_timesteps | 726016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=726500, episode_reward=-480.06 +/- 111.70
Episode length: 40.84 +/- 9.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.8        |
|    mean_reward          | -480        |
| time/                   |             |
|    total_timesteps      | 726500      |
| train/                  |             |
|    approx_kl            | 0.027692514 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0005      |
|    loss                 | 6.4e+03     |
|    n_updates            | 5770        |
|    policy_gradient_loss | 0.0352      |
|    value_loss           | 1.16e+04    |
-----------------------------------------
Eval num_timesteps=727000, episode_reward=-474.84 +/- 140.10
Episode length: 45.46 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | -475     |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | -254     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 710      |
|    time_elapsed    | 2659     |
|    total_timesteps | 727040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=727500, episode_reward=-448.53 +/- 143.09
Episode length: 43.18 +/- 14.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 43.2       |
|    mean_reward          | -449       |
| time/                   |            |
|    total_timesteps      | 727500     |
| train/                  |            |
|    approx_kl            | 0.03329581 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.0005     |
|    loss                 | 7.38e+03   |
|    n_updates            | 5771       |
|    policy_gradient_loss | 0.0189     |
|    value_loss           | 1.43e+04   |
----------------------------------------
Eval num_timesteps=728000, episode_reward=-492.67 +/- 144.34
Episode length: 43.82 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.8     |
|    mean_reward     | -493     |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.2     |
|    ep_rew_mean     | -312     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 711      |
|    time_elapsed    | 2663     |
|    total_timesteps | 728064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=728500, episode_reward=-443.87 +/- 149.79
Episode length: 42.34 +/- 13.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | -444        |
| time/                   |             |
|    total_timesteps      | 728500      |
| train/                  |             |
|    approx_kl            | 0.025662476 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.192      |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.0005      |
|    loss                 | 7.46e+03    |
|    n_updates            | 5772        |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 1.47e+04    |
-----------------------------------------
Eval num_timesteps=729000, episode_reward=-392.11 +/- 183.85
Episode length: 39.56 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.5     |
|    ep_rew_mean     | -355     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 712      |
|    time_elapsed    | 2667     |
|    total_timesteps | 729088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=729500, episode_reward=-185.46 +/- 163.08
Episode length: 33.50 +/- 9.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.5       |
|    mean_reward          | -185       |
| time/                   |            |
|    total_timesteps      | 729500     |
| train/                  |            |
|    approx_kl            | 0.03658063 |
|    clip_fraction        | 0.099      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.17      |
|    explained_variance   | 0.424      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.36e+03   |
|    n_updates            | 5773       |
|    policy_gradient_loss | 0.038      |
|    value_loss           | 1.34e+04   |
----------------------------------------
Eval num_timesteps=730000, episode_reward=-188.74 +/- 151.59
Episode length: 32.90 +/- 11.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 730000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.2     |
|    ep_rew_mean     | -325     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 713      |
|    time_elapsed    | 2670     |
|    total_timesteps | 730112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=730500, episode_reward=-84.05 +/- 160.67
Episode length: 35.24 +/- 10.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.2       |
|    mean_reward          | -84        |
| time/                   |            |
|    total_timesteps      | 730500     |
| train/                  |            |
|    approx_kl            | 0.03251032 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.0005     |
|    loss                 | 3.28e+03   |
|    n_updates            | 5774       |
|    policy_gradient_loss | 0.0329     |
|    value_loss           | 8.03e+03   |
----------------------------------------
Eval num_timesteps=731000, episode_reward=-87.19 +/- 166.79
Episode length: 34.08 +/- 9.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | -87.2    |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.9     |
|    ep_rew_mean     | -231     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 714      |
|    time_elapsed    | 2673     |
|    total_timesteps | 731136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.13
Eval num_timesteps=731500, episode_reward=-56.18 +/- 153.18
Episode length: 34.44 +/- 8.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | -56.2       |
| time/                   |             |
|    total_timesteps      | 731500      |
| train/                  |             |
|    approx_kl            | 0.040981077 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.91e+03    |
|    n_updates            | 5775        |
|    policy_gradient_loss | 0.00796     |
|    value_loss           | 9.74e+03    |
-----------------------------------------
Eval num_timesteps=732000, episode_reward=-66.42 +/- 142.10
Episode length: 34.00 +/- 10.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | -66.4    |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 715      |
|    time_elapsed    | 2676     |
|    total_timesteps | 732160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=732500, episode_reward=-30.87 +/- 125.81
Episode length: 34.38 +/- 7.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | -30.9       |
| time/                   |             |
|    total_timesteps      | 732500      |
| train/                  |             |
|    approx_kl            | 0.023748487 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.228       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.74e+03    |
|    n_updates            | 5776        |
|    policy_gradient_loss | 0.0178      |
|    value_loss           | 1e+04       |
-----------------------------------------
Eval num_timesteps=733000, episode_reward=-74.98 +/- 200.91
Episode length: 33.36 +/- 9.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | -75      |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | -61.9    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 716      |
|    time_elapsed    | 2680     |
|    total_timesteps | 733184   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.08
Eval num_timesteps=733500, episode_reward=-34.99 +/- 176.36
Episode length: 35.88 +/- 8.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | -35         |
| time/                   |             |
|    total_timesteps      | 733500      |
| train/                  |             |
|    approx_kl            | 0.056966763 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.209       |
|    learning_rate        | 0.0005      |
|    loss                 | 7.27e+03    |
|    n_updates            | 5779        |
|    policy_gradient_loss | 0.0164      |
|    value_loss           | 1.45e+04    |
-----------------------------------------
Eval num_timesteps=734000, episode_reward=-81.63 +/- 146.29
Episode length: 32.48 +/- 8.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.5     |
|    mean_reward     | -81.6    |
| time/              |          |
|    total_timesteps | 734000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | -45      |
| time/              |          |
|    fps             | 273      |
|    iterations      | 717      |
|    time_elapsed    | 2683     |
|    total_timesteps | 734208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=734500, episode_reward=34.91 +/- 333.69
Episode length: 37.98 +/- 12.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38          |
|    mean_reward          | 34.9        |
| time/                   |             |
|    total_timesteps      | 734500      |
| train/                  |             |
|    approx_kl            | 0.021889882 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.01e+03    |
|    n_updates            | 5780        |
|    policy_gradient_loss | 0.00934     |
|    value_loss           | 1.11e+04    |
-----------------------------------------
Eval num_timesteps=735000, episode_reward=-10.80 +/- 173.62
Episode length: 36.28 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | -10.8    |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | -22.6    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 718      |
|    time_elapsed    | 2686     |
|    total_timesteps | 735232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=735500, episode_reward=-79.08 +/- 165.16
Episode length: 32.28 +/- 8.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 32.3       |
|    mean_reward          | -79.1      |
| time/                   |            |
|    total_timesteps      | 735500     |
| train/                  |            |
|    approx_kl            | 0.03156363 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.185      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.57e+03   |
|    n_updates            | 5781       |
|    policy_gradient_loss | 0.0195     |
|    value_loss           | 1.28e+04   |
----------------------------------------
Eval num_timesteps=736000, episode_reward=-40.07 +/- 137.14
Episode length: 34.30 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | -40.1    |
| time/              |          |
|    total_timesteps | 736000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | -38.7    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 719      |
|    time_elapsed    | 2690     |
|    total_timesteps | 736256   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.12
Eval num_timesteps=736500, episode_reward=-34.85 +/- 204.83
Episode length: 34.70 +/- 10.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | -34.9       |
| time/                   |             |
|    total_timesteps      | 736500      |
| train/                  |             |
|    approx_kl            | 0.053442176 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.0005      |
|    loss                 | 4.21e+03    |
|    n_updates            | 5784        |
|    policy_gradient_loss | 0.00467     |
|    value_loss           | 9.02e+03    |
-----------------------------------------
Eval num_timesteps=737000, episode_reward=-18.09 +/- 170.98
Episode length: 35.54 +/- 9.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | -18.1    |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | -53.8    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 720      |
|    time_elapsed    | 2693     |
|    total_timesteps | 737280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=737500, episode_reward=4.62 +/- 179.46
Episode length: 36.60 +/- 9.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.6       |
|    mean_reward          | 4.62       |
| time/                   |            |
|    total_timesteps      | 737500     |
| train/                  |            |
|    approx_kl            | 0.03162188 |
|    clip_fraction        | 0.0641     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.071     |
|    explained_variance   | 0.169      |
|    learning_rate        | 0.0005     |
|    loss                 | 6.06e+03   |
|    n_updates            | 5785       |
|    policy_gradient_loss | 0.00799    |
|    value_loss           | 1.06e+04   |
----------------------------------------
Eval num_timesteps=738000, episode_reward=-3.49 +/- 342.55
Episode length: 33.78 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | -3.49    |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.4     |
|    ep_rew_mean     | -63.9    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 721      |
|    time_elapsed    | 2696     |
|    total_timesteps | 738304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=738500, episode_reward=-0.48 +/- 197.91
Episode length: 35.46 +/- 10.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | -0.475      |
| time/                   |             |
|    total_timesteps      | 738500      |
| train/                  |             |
|    approx_kl            | 0.022670023 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.0005      |
|    loss                 | 4.84e+03    |
|    n_updates            | 5786        |
|    policy_gradient_loss | 0.0093      |
|    value_loss           | 8.62e+03    |
-----------------------------------------
Eval num_timesteps=739000, episode_reward=-45.52 +/- 189.27
Episode length: 34.08 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | -45.5    |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | -37.9    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 722      |
|    time_elapsed    | 2700     |
|    total_timesteps | 739328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=739500, episode_reward=14.43 +/- 200.39
Episode length: 35.20 +/- 10.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.2       |
|    mean_reward          | 14.4       |
| time/                   |            |
|    total_timesteps      | 739500     |
| train/                  |            |
|    approx_kl            | 0.02764352 |
|    clip_fraction        | 0.0615     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.126     |
|    explained_variance   | 0.127      |
|    learning_rate        | 0.0005     |
|    loss                 | 9.68e+03   |
|    n_updates            | 5787       |
|    policy_gradient_loss | 0.00313    |
|    value_loss           | 2.62e+04   |
----------------------------------------
Eval num_timesteps=740000, episode_reward=8.14 +/- 182.33
Episode length: 35.88 +/- 8.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 8.14     |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | -2.92    |
| time/              |          |
|    fps             | 273      |
|    iterations      | 723      |
|    time_elapsed    | 2703     |
|    total_timesteps | 740352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=740500, episode_reward=-25.70 +/- 132.74
Episode length: 35.00 +/- 7.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | -25.7       |
| time/                   |             |
|    total_timesteps      | 740500      |
| train/                  |             |
|    approx_kl            | 0.026660984 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0005      |
|    loss                 | 6.13e+03    |
|    n_updates            | 5788        |
|    policy_gradient_loss | 0.0156      |
|    value_loss           | 1.21e+04    |
-----------------------------------------
Eval num_timesteps=741000, episode_reward=-62.54 +/- 162.29
Episode length: 33.24 +/- 8.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | -62.5    |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 724      |
|    time_elapsed    | 2706     |
|    total_timesteps | 741376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.33
Eval num_timesteps=741500, episode_reward=172.02 +/- 335.46
Episode length: 35.06 +/- 6.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.1       |
|    mean_reward          | 172        |
| time/                   |            |
|    total_timesteps      | 741500     |
| train/                  |            |
|    approx_kl            | 0.05895309 |
|    clip_fraction        | 0.0755     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.115      |
|    learning_rate        | 0.0005     |
|    loss                 | 5.63e+03   |
|    n_updates            | 5789       |
|    policy_gradient_loss | 0.0413     |
|    value_loss           | 9.86e+03   |
----------------------------------------
Eval num_timesteps=742000, episode_reward=231.85 +/- 386.00
Episode length: 35.92 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 742000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 68.2     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 725      |
|    time_elapsed    | 2710     |
|    total_timesteps | 742400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=742500, episode_reward=422.34 +/- 417.03
Episode length: 34.96 +/- 8.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 422         |
| time/                   |             |
|    total_timesteps      | 742500      |
| train/                  |             |
|    approx_kl            | 0.035121664 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.0325      |
|    learning_rate        | 0.0005      |
|    loss                 | 1.23e+04    |
|    n_updates            | 5790        |
|    policy_gradient_loss | 0.0033      |
|    value_loss           | 2.92e+04    |
-----------------------------------------
Eval num_timesteps=743000, episode_reward=411.62 +/- 529.66
Episode length: 33.84 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 273      |
|    iterations      | 726      |
|    time_elapsed    | 2713     |
|    total_timesteps | 743424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=743500, episode_reward=624.55 +/- 555.50
Episode length: 35.14 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 625       |
| time/                   |           |
|    total_timesteps      | 743500    |
| train/                  |           |
|    approx_kl            | 0.0454073 |
|    clip_fraction        | 0.0755    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.0694   |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.0005    |
|    loss                 | 1.57e+04  |
|    n_updates            | 5791      |
|    policy_gradient_loss | -0.00196  |
|    value_loss           | 2.77e+04  |
---------------------------------------
Eval num_timesteps=744000, episode_reward=1021.14 +/- 727.75
Episode length: 38.16 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.2     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 727      |
|    time_elapsed    | 2716     |
|    total_timesteps | 744448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=744500, episode_reward=833.01 +/- 649.80
Episode length: 36.72 +/- 6.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.7       |
|    mean_reward          | 833        |
| time/                   |            |
|    total_timesteps      | 744500     |
| train/                  |            |
|    approx_kl            | 0.03892418 |
|    clip_fraction        | 0.0312     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0227    |
|    explained_variance   | -0.000793  |
|    learning_rate        | 0.0005     |
|    loss                 | 7.06e+04   |
|    n_updates            | 5792       |
|    policy_gradient_loss | 0.00348    |
|    value_loss           | 1.29e+05   |
----------------------------------------
Eval num_timesteps=745000, episode_reward=790.39 +/- 649.68
Episode length: 36.30 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 569      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 728      |
|    time_elapsed    | 2720     |
|    total_timesteps | 745472   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.15
Eval num_timesteps=745500, episode_reward=671.75 +/- 560.10
Episode length: 36.20 +/- 7.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.2        |
|    mean_reward          | 672         |
| time/                   |             |
|    total_timesteps      | 745500      |
| train/                  |             |
|    approx_kl            | 0.061415102 |
|    clip_fraction        | 0.037       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0278     |
|    explained_variance   | 0.00666     |
|    learning_rate        | 0.0005      |
|    loss                 | 9.76e+04    |
|    n_updates            | 5796        |
|    policy_gradient_loss | 0.00602     |
|    value_loss           | 1.53e+05    |
-----------------------------------------
Eval num_timesteps=746000, episode_reward=687.06 +/- 600.18
Episode length: 35.14 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 746000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 732      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 729      |
|    time_elapsed    | 2723     |
|    total_timesteps | 746496   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.10
Eval num_timesteps=746500, episode_reward=764.14 +/- 667.86
Episode length: 35.54 +/- 7.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.5       |
|    mean_reward          | 764        |
| time/                   |            |
|    total_timesteps      | 746500     |
| train/                  |            |
|    approx_kl            | 0.03214576 |
|    clip_fraction        | 0.0299     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0424    |
|    explained_variance   | 0.00694    |
|    learning_rate        | 0.0005     |
|    loss                 | 6.24e+04   |
|    n_updates            | 5798       |
|    policy_gradient_loss | 0.00376    |
|    value_loss           | 1.32e+05   |
----------------------------------------
Eval num_timesteps=747000, episode_reward=837.72 +/- 688.60
Episode length: 36.42 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=747500, episode_reward=795.28 +/- 635.45
Episode length: 36.56 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 747500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 730      |
|    time_elapsed    | 2728     |
|    total_timesteps | 747520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=748000, episode_reward=842.52 +/- 734.60
Episode length: 35.04 +/- 7.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35         |
|    mean_reward          | 843        |
| time/                   |            |
|    total_timesteps      | 748000     |
| train/                  |            |
|    approx_kl            | 0.04165532 |
|    clip_fraction        | 0.0195     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.00818   |
|    explained_variance   | 0.00672    |
|    learning_rate        | 0.0005     |
|    loss                 | 5.78e+04   |
|    n_updates            | 5799       |
|    policy_gradient_loss | 0.00239    |
|    value_loss           | 1.18e+05   |
----------------------------------------
Eval num_timesteps=748500, episode_reward=796.58 +/- 688.00
Episode length: 34.66 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 748500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 731      |
|    time_elapsed    | 2731     |
|    total_timesteps | 748544   |
---------------------------------
Eval num_timesteps=749000, episode_reward=800.09 +/- 680.05
Episode length: 34.94 +/- 7.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 800           |
| time/                   |               |
|    total_timesteps      | 749000        |
| train/                  |               |
|    approx_kl            | 3.7037535e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -3.22e-05     |
|    explained_variance   | 0.00745       |
|    learning_rate        | 0.0005        |
|    loss                 | 6.67e+04      |
|    n_updates            | 5809          |
|    policy_gradient_loss | 3.61e-05      |
|    value_loss           | 1.55e+05      |
-------------------------------------------
Eval num_timesteps=749500, episode_reward=843.40 +/- 690.78
Episode length: 35.36 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 749500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 732      |
|    time_elapsed    | 2734     |
|    total_timesteps | 749568   |
---------------------------------
Eval num_timesteps=750000, episode_reward=868.23 +/- 682.25
Episode length: 35.74 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 868       |
| time/                   |           |
|    total_timesteps      | 750000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-13 |
|    explained_variance   | 0.00993   |
|    learning_rate        | 0.0005    |
|    loss                 | 5.01e+04  |
|    n_updates            | 5819      |
|    policy_gradient_loss | 1.16e-09  |
|    value_loss           | 9.87e+04  |
---------------------------------------
Eval num_timesteps=750500, episode_reward=839.60 +/- 712.66
Episode length: 35.24 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 750500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 733      |
|    time_elapsed    | 2738     |
|    total_timesteps | 750592   |
---------------------------------
Eval num_timesteps=751000, episode_reward=1077.01 +/- 737.57
Episode length: 36.76 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 1.08e+03  |
| time/                   |           |
|    total_timesteps      | 751000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.87e-17 |
|    explained_variance   | 0.0118    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.72e+04  |
|    n_updates            | 5829      |
|    policy_gradient_loss | 3.63e-09  |
|    value_loss           | 9.48e+04  |
---------------------------------------
Eval num_timesteps=751500, episode_reward=923.12 +/- 721.50
Episode length: 36.44 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 751500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 734      |
|    time_elapsed    | 2742     |
|    total_timesteps | 751616   |
---------------------------------
Eval num_timesteps=752000, episode_reward=816.75 +/- 669.15
Episode length: 35.32 +/- 5.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 752000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.52e-21 |
|    explained_variance   | 0.0157    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.49e+04  |
|    n_updates            | 5839      |
|    policy_gradient_loss | 1.93e-09  |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=752500, episode_reward=790.71 +/- 673.53
Episode length: 34.60 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 752500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 735      |
|    time_elapsed    | 2745     |
|    total_timesteps | 752640   |
---------------------------------
Eval num_timesteps=753000, episode_reward=743.89 +/- 629.33
Episode length: 35.16 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 744       |
| time/                   |           |
|    total_timesteps      | 753000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-26 |
|    explained_variance   | 0.0184    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.14e+04  |
|    n_updates            | 5849      |
|    policy_gradient_loss | 1.65e-09  |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=753500, episode_reward=919.13 +/- 724.20
Episode length: 35.58 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 753500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 909      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 736      |
|    time_elapsed    | 2748     |
|    total_timesteps | 753664   |
---------------------------------
Eval num_timesteps=754000, episode_reward=902.87 +/- 675.96
Episode length: 36.66 +/- 6.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.7     |
|    mean_reward          | 903      |
| time/                   |          |
|    total_timesteps      | 754000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.2e-25 |
|    explained_variance   | 0.0186   |
|    learning_rate        | 0.0005   |
|    loss                 | 5.45e+04 |
|    n_updates            | 5859     |
|    policy_gradient_loss | 1.01e-09 |
|    value_loss           | 1.07e+05 |
--------------------------------------
Eval num_timesteps=754500, episode_reward=740.15 +/- 608.39
Episode length: 34.72 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 754500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 737      |
|    time_elapsed    | 2752     |
|    total_timesteps | 754688   |
---------------------------------
Eval num_timesteps=755000, episode_reward=882.63 +/- 711.02
Episode length: 35.80 +/- 5.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 755000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.04e-19 |
|    explained_variance   | 0.0243    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.24e+04  |
|    n_updates            | 5869      |
|    policy_gradient_loss | -1.9e-09  |
|    value_loss           | 8.52e+04  |
---------------------------------------
Eval num_timesteps=755500, episode_reward=849.89 +/- 675.79
Episode length: 35.32 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 755500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 738      |
|    time_elapsed    | 2756     |
|    total_timesteps | 755712   |
---------------------------------
Eval num_timesteps=756000, episode_reward=860.65 +/- 738.34
Episode length: 34.66 +/- 7.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 756000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.35e-21 |
|    explained_variance   | 0.0195    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.98e+04  |
|    n_updates            | 5879      |
|    policy_gradient_loss | 8.5e-10   |
|    value_loss           | 9.12e+04  |
---------------------------------------
Eval num_timesteps=756500, episode_reward=802.99 +/- 673.43
Episode length: 34.76 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 756500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 718      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 739      |
|    time_elapsed    | 2759     |
|    total_timesteps | 756736   |
---------------------------------
Eval num_timesteps=757000, episode_reward=913.43 +/- 691.84
Episode length: 35.40 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 913       |
| time/                   |           |
|    total_timesteps      | 757000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35e-26 |
|    explained_variance   | 0.016     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.85e+04  |
|    n_updates            | 5889      |
|    policy_gradient_loss | -9.2e-10  |
|    value_loss           | 9.78e+04  |
---------------------------------------
Eval num_timesteps=757500, episode_reward=815.24 +/- 698.16
Episode length: 34.98 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 757500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 740      |
|    time_elapsed    | 2762     |
|    total_timesteps | 757760   |
---------------------------------
Eval num_timesteps=758000, episode_reward=715.47 +/- 605.60
Episode length: 34.08 +/- 6.23
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.1     |
|    mean_reward          | 715      |
| time/                   |          |
|    total_timesteps      | 758000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.1e-24 |
|    explained_variance   | 0.0255   |
|    learning_rate        | 0.0005   |
|    loss                 | 5.46e+04 |
|    n_updates            | 5899     |
|    policy_gradient_loss | 1.46e-10 |
|    value_loss           | 1.09e+05 |
--------------------------------------
Eval num_timesteps=758500, episode_reward=779.64 +/- 631.67
Episode length: 34.94 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 758500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 741      |
|    time_elapsed    | 2766     |
|    total_timesteps | 758784   |
---------------------------------
Eval num_timesteps=759000, episode_reward=784.20 +/- 662.93
Episode length: 34.44 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 759000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-26 |
|    explained_variance   | 0.0254    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.52e+04  |
|    n_updates            | 5909      |
|    policy_gradient_loss | 1.95e-09  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=759500, episode_reward=817.82 +/- 670.32
Episode length: 34.88 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 759500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 742      |
|    time_elapsed    | 2769     |
|    total_timesteps | 759808   |
---------------------------------
Eval num_timesteps=760000, episode_reward=914.12 +/- 703.78
Episode length: 35.98 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 914       |
| time/                   |           |
|    total_timesteps      | 760000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.95e-24 |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.38e+04  |
|    n_updates            | 5919      |
|    policy_gradient_loss | 3.73e-10  |
|    value_loss           | 9.84e+04  |
---------------------------------------
Eval num_timesteps=760500, episode_reward=855.25 +/- 690.18
Episode length: 35.32 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 760500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 743      |
|    time_elapsed    | 2773     |
|    total_timesteps | 760832   |
---------------------------------
Eval num_timesteps=761000, episode_reward=717.49 +/- 660.32
Episode length: 33.88 +/- 7.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 717       |
| time/                   |           |
|    total_timesteps      | 761000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.39e-18 |
|    explained_variance   | 0.0313    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.12e+04  |
|    n_updates            | 5929      |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 8.64e+04  |
---------------------------------------
Eval num_timesteps=761500, episode_reward=842.36 +/- 629.07
Episode length: 36.48 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 761500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 744      |
|    time_elapsed    | 2776     |
|    total_timesteps | 761856   |
---------------------------------
Eval num_timesteps=762000, episode_reward=743.28 +/- 601.22
Episode length: 34.82 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 762000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.25e-19 |
|    explained_variance   | 0.0237    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.42e+04  |
|    n_updates            | 5939      |
|    policy_gradient_loss | 1.76e-09  |
|    value_loss           | 9.22e+04  |
---------------------------------------
Eval num_timesteps=762500, episode_reward=754.35 +/- 634.68
Episode length: 34.72 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 762500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 745      |
|    time_elapsed    | 2780     |
|    total_timesteps | 762880   |
---------------------------------
Eval num_timesteps=763000, episode_reward=775.92 +/- 638.80
Episode length: 35.36 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 776       |
| time/                   |           |
|    total_timesteps      | 763000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.75e-26 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.94e+04  |
|    n_updates            | 5949      |
|    policy_gradient_loss | 7.57e-11  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=763500, episode_reward=751.85 +/- 646.93
Episode length: 34.96 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 763500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 746      |
|    time_elapsed    | 2783     |
|    total_timesteps | 763904   |
---------------------------------
Eval num_timesteps=764000, episode_reward=940.80 +/- 747.88
Episode length: 36.16 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 941       |
| time/                   |           |
|    total_timesteps      | 764000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.42e-24 |
|    explained_variance   | 0.0408    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.07e+04  |
|    n_updates            | 5959      |
|    policy_gradient_loss | -3.39e-09 |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=764500, episode_reward=913.82 +/- 740.95
Episode length: 36.12 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 764500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 946      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 747      |
|    time_elapsed    | 2787     |
|    total_timesteps | 764928   |
---------------------------------
Eval num_timesteps=765000, episode_reward=767.35 +/- 707.36
Episode length: 33.82 +/- 8.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 767       |
| time/                   |           |
|    total_timesteps      | 765000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.01e-26 |
|    explained_variance   | 0.0275    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.08e+04  |
|    n_updates            | 5969      |
|    policy_gradient_loss | 2.6e-09   |
|    value_loss           | 1.23e+05  |
---------------------------------------
Eval num_timesteps=765500, episode_reward=791.52 +/- 670.40
Episode length: 34.64 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 765500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 748      |
|    time_elapsed    | 2790     |
|    total_timesteps | 765952   |
---------------------------------
Eval num_timesteps=766000, episode_reward=994.86 +/- 767.90
Episode length: 36.16 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 995       |
| time/                   |           |
|    total_timesteps      | 766000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.43e-24 |
|    explained_variance   | 0.0485    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.52e+04  |
|    n_updates            | 5979      |
|    policy_gradient_loss | 2.43e-09  |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=766500, episode_reward=845.67 +/- 726.59
Episode length: 34.60 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 766500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 917      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 749      |
|    time_elapsed    | 2794     |
|    total_timesteps | 766976   |
---------------------------------
Eval num_timesteps=767000, episode_reward=906.21 +/- 704.47
Episode length: 35.94 +/- 5.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 767000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.51e-25 |
|    explained_variance   | 0.0281    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.95e+04  |
|    n_updates            | 5989      |
|    policy_gradient_loss | 2.88e-09  |
|    value_loss           | 9.84e+04  |
---------------------------------------
Eval num_timesteps=767500, episode_reward=734.46 +/- 634.27
Episode length: 35.18 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 767500   |
---------------------------------
Eval num_timesteps=768000, episode_reward=753.88 +/- 623.70
Episode length: 34.58 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 750      |
|    time_elapsed    | 2798     |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=768500, episode_reward=805.05 +/- 629.84
Episode length: 35.80 +/- 5.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 768500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55e-23 |
|    explained_variance   | 0.0384    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.56e+04  |
|    n_updates            | 5999      |
|    policy_gradient_loss | -8.32e-10 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=769000, episode_reward=801.36 +/- 656.44
Episode length: 35.02 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 769000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 751      |
|    time_elapsed    | 2802     |
|    total_timesteps | 769024   |
---------------------------------
Eval num_timesteps=769500, episode_reward=784.89 +/- 660.04
Episode length: 34.48 +/- 6.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 769500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.16e-25 |
|    explained_variance   | 0.0282    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.95e+04  |
|    n_updates            | 6009      |
|    policy_gradient_loss | -7.92e-10 |
|    value_loss           | 9.46e+04  |
---------------------------------------
Eval num_timesteps=770000, episode_reward=681.55 +/- 633.77
Episode length: 33.18 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 752      |
|    time_elapsed    | 2805     |
|    total_timesteps | 770048   |
---------------------------------
Eval num_timesteps=770500, episode_reward=817.34 +/- 698.59
Episode length: 34.70 +/- 7.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 770500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-22 |
|    explained_variance   | 0.033     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.42e+04  |
|    n_updates            | 6019      |
|    policy_gradient_loss | -2.17e-09 |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=771000, episode_reward=840.76 +/- 731.61
Episode length: 34.46 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 771000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 854      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 753      |
|    time_elapsed    | 2809     |
|    total_timesteps | 771072   |
---------------------------------
Eval num_timesteps=771500, episode_reward=719.26 +/- 616.38
Episode length: 34.44 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 719       |
| time/                   |           |
|    total_timesteps      | 771500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-24 |
|    explained_variance   | 0.0274    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.72e+04  |
|    n_updates            | 6029      |
|    policy_gradient_loss | 7.51e-10  |
|    value_loss           | 9.34e+04  |
---------------------------------------
Eval num_timesteps=772000, episode_reward=841.43 +/- 692.54
Episode length: 35.30 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 754      |
|    time_elapsed    | 2812     |
|    total_timesteps | 772096   |
---------------------------------
Eval num_timesteps=772500, episode_reward=1010.27 +/- 740.52
Episode length: 36.70 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 772500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.59e-21 |
|    explained_variance   | 0.0276    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.71e+04  |
|    n_updates            | 6039      |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=773000, episode_reward=843.64 +/- 708.41
Episode length: 35.30 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 773000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 755      |
|    time_elapsed    | 2816     |
|    total_timesteps | 773120   |
---------------------------------
Eval num_timesteps=773500, episode_reward=694.62 +/- 602.61
Episode length: 34.36 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 695       |
| time/                   |           |
|    total_timesteps      | 773500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.62e-23 |
|    explained_variance   | 0.0474    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.26e+04  |
|    n_updates            | 6049      |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 9.46e+04  |
---------------------------------------
Eval num_timesteps=774000, episode_reward=887.82 +/- 706.35
Episode length: 35.68 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 756      |
|    time_elapsed    | 2819     |
|    total_timesteps | 774144   |
---------------------------------
Eval num_timesteps=774500, episode_reward=786.91 +/- 717.88
Episode length: 34.00 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 774500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.83e-23 |
|    explained_variance   | 0.057     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.64e+04  |
|    n_updates            | 6059      |
|    policy_gradient_loss | 1.83e-09  |
|    value_loss           | 9.56e+04  |
---------------------------------------
Eval num_timesteps=775000, episode_reward=848.72 +/- 689.96
Episode length: 35.46 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 775000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 757      |
|    time_elapsed    | 2823     |
|    total_timesteps | 775168   |
---------------------------------
Eval num_timesteps=775500, episode_reward=852.24 +/- 727.81
Episode length: 34.64 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 775500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.32e-25 |
|    explained_variance   | 0.0657    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.77e+04  |
|    n_updates            | 6069      |
|    policy_gradient_loss | -4.83e-10 |
|    value_loss           | 9.58e+04  |
---------------------------------------
Eval num_timesteps=776000, episode_reward=937.35 +/- 670.97
Episode length: 37.24 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 827      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 758      |
|    time_elapsed    | 2826     |
|    total_timesteps | 776192   |
---------------------------------
Eval num_timesteps=776500, episode_reward=763.09 +/- 608.69
Episode length: 34.96 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 763       |
| time/                   |           |
|    total_timesteps      | 776500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.83e-21 |
|    explained_variance   | 0.0973    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.1e+04   |
|    n_updates            | 6079      |
|    policy_gradient_loss | 2.85e-09  |
|    value_loss           | 9.84e+04  |
---------------------------------------
Eval num_timesteps=777000, episode_reward=737.07 +/- 595.97
Episode length: 35.28 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 777000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 916      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 759      |
|    time_elapsed    | 2830     |
|    total_timesteps | 777216   |
---------------------------------
Eval num_timesteps=777500, episode_reward=684.69 +/- 587.14
Episode length: 34.38 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 685       |
| time/                   |           |
|    total_timesteps      | 777500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.28e-23 |
|    explained_variance   | 0.02      |
|    learning_rate        | 0.0005    |
|    loss                 | 6.39e+04  |
|    n_updates            | 6089      |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=778000, episode_reward=896.61 +/- 653.23
Episode length: 36.68 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 905      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 760      |
|    time_elapsed    | 2833     |
|    total_timesteps | 778240   |
---------------------------------
Eval num_timesteps=778500, episode_reward=671.36 +/- 586.67
Episode length: 34.00 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 671       |
| time/                   |           |
|    total_timesteps      | 778500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.23e-21 |
|    explained_variance   | 0.0594    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.74e+04  |
|    n_updates            | 6099      |
|    policy_gradient_loss | -2.82e-09 |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=779000, episode_reward=853.21 +/- 682.78
Episode length: 35.44 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 779000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37       |
|    ep_rew_mean     | 950      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 761      |
|    time_elapsed    | 2837     |
|    total_timesteps | 779264   |
---------------------------------
Eval num_timesteps=779500, episode_reward=757.50 +/- 626.27
Episode length: 35.30 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 779500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.47e-23 |
|    explained_variance   | 0.0707    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.77e+04  |
|    n_updates            | 6109      |
|    policy_gradient_loss | 1.67e-09  |
|    value_loss           | 9.17e+04  |
---------------------------------------
Eval num_timesteps=780000, episode_reward=1007.47 +/- 746.56
Episode length: 36.62 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 762      |
|    time_elapsed    | 2840     |
|    total_timesteps | 780288   |
---------------------------------
Eval num_timesteps=780500, episode_reward=742.60 +/- 674.59
Episode length: 33.94 +/- 7.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 780500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.65e-20 |
|    explained_variance   | 0.114     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.37e+04  |
|    n_updates            | 6119      |
|    policy_gradient_loss | 2.74e-10  |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=781000, episode_reward=847.59 +/- 650.07
Episode length: 35.92 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 781000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 763      |
|    time_elapsed    | 2843     |
|    total_timesteps | 781312   |
---------------------------------
Eval num_timesteps=781500, episode_reward=1014.94 +/- 796.94
Episode length: 35.92 +/- 7.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 781500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.15e-22 |
|    explained_variance   | 0.0536    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.91e+04  |
|    n_updates            | 6129      |
|    policy_gradient_loss | 2.07e-09  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=782000, episode_reward=861.82 +/- 693.19
Episode length: 35.28 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 764      |
|    time_elapsed    | 2847     |
|    total_timesteps | 782336   |
---------------------------------
Eval num_timesteps=782500, episode_reward=969.85 +/- 744.11
Episode length: 35.82 +/- 7.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 970       |
| time/                   |           |
|    total_timesteps      | 782500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.21e-19 |
|    explained_variance   | 0.0533    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.02e+04  |
|    n_updates            | 6139      |
|    policy_gradient_loss | 4.19e-10  |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=783000, episode_reward=771.68 +/- 632.36
Episode length: 35.28 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 783000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 919      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 765      |
|    time_elapsed    | 2850     |
|    total_timesteps | 783360   |
---------------------------------
Eval num_timesteps=783500, episode_reward=849.30 +/- 689.95
Episode length: 35.50 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 849       |
| time/                   |           |
|    total_timesteps      | 783500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.95e-21 |
|    explained_variance   | 0.0524    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.08e+04  |
|    n_updates            | 6149      |
|    policy_gradient_loss | -1.12e-09 |
|    value_loss           | 9.85e+04  |
---------------------------------------
Eval num_timesteps=784000, episode_reward=788.87 +/- 612.44
Episode length: 35.98 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 953      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 766      |
|    time_elapsed    | 2854     |
|    total_timesteps | 784384   |
---------------------------------
Eval num_timesteps=784500, episode_reward=659.16 +/- 643.24
Episode length: 32.90 +/- 6.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.9      |
|    mean_reward          | 659       |
| time/                   |           |
|    total_timesteps      | 784500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.68e-31 |
|    explained_variance   | 0.0632    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.5e+04   |
|    n_updates            | 6159      |
|    policy_gradient_loss | -2.81e-09 |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=785000, episode_reward=748.07 +/- 682.39
Episode length: 34.06 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 767      |
|    time_elapsed    | 2857     |
|    total_timesteps | 785408   |
---------------------------------
Eval num_timesteps=785500, episode_reward=822.59 +/- 658.64
Episode length: 35.82 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 823       |
| time/                   |           |
|    total_timesteps      | 785500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-29 |
|    explained_variance   | 0.133     |
|    learning_rate        | 0.0005    |
|    loss                 | 9.45e+04  |
|    n_updates            | 6169      |
|    policy_gradient_loss | -7.86e-09 |
|    value_loss           | 2.11e+05  |
---------------------------------------
Eval num_timesteps=786000, episode_reward=743.70 +/- 628.97
Episode length: 34.76 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 768      |
|    time_elapsed    | 2861     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=786500, episode_reward=788.95 +/- 650.22
Episode length: 35.22 +/- 5.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 786500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6e-32    |
|    explained_variance   | 0.0411    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.24e+04  |
|    n_updates            | 6179      |
|    policy_gradient_loss | -2.27e-09 |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=787000, episode_reward=1028.38 +/- 750.72
Episode length: 36.82 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 787000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 769      |
|    time_elapsed    | 2864     |
|    total_timesteps | 787456   |
---------------------------------
Eval num_timesteps=787500, episode_reward=699.30 +/- 601.81
Episode length: 34.90 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 699       |
| time/                   |           |
|    total_timesteps      | 787500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.45e-29 |
|    explained_variance   | 0.0784    |
|    learning_rate        | 0.0005    |
|    loss                 | 8.64e+04  |
|    n_updates            | 6189      |
|    policy_gradient_loss | 9.55e-10  |
|    value_loss           | 1.95e+05  |
---------------------------------------
Eval num_timesteps=788000, episode_reward=800.86 +/- 693.59
Episode length: 34.60 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 770      |
|    time_elapsed    | 2868     |
|    total_timesteps | 788480   |
---------------------------------
Eval num_timesteps=788500, episode_reward=930.85 +/- 770.00
Episode length: 34.54 +/- 7.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 931       |
| time/                   |           |
|    total_timesteps      | 788500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.07e-19 |
|    explained_variance   | 0.114     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.21e+04  |
|    n_updates            | 6199      |
|    policy_gradient_loss | 9.43e-10  |
|    value_loss           | 9.87e+04  |
---------------------------------------
Eval num_timesteps=789000, episode_reward=708.84 +/- 601.23
Episode length: 34.46 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 789000   |
---------------------------------
Eval num_timesteps=789500, episode_reward=876.70 +/- 719.29
Episode length: 34.72 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 789500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 771      |
|    time_elapsed    | 2873     |
|    total_timesteps | 789504   |
---------------------------------
Eval num_timesteps=790000, episode_reward=872.32 +/- 668.17
Episode length: 36.22 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 872       |
| time/                   |           |
|    total_timesteps      | 790000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.72e-20 |
|    explained_variance   | 0.0583    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.7e+04   |
|    n_updates            | 6209      |
|    policy_gradient_loss | -2.21e-10 |
|    value_loss           | 8.4e+04   |
---------------------------------------
Eval num_timesteps=790500, episode_reward=830.91 +/- 653.25
Episode length: 35.64 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 790500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 772      |
|    time_elapsed    | 2876     |
|    total_timesteps | 790528   |
---------------------------------
Eval num_timesteps=791000, episode_reward=744.83 +/- 591.53
Episode length: 35.48 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 745       |
| time/                   |           |
|    total_timesteps      | 791000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.18e-18 |
|    explained_variance   | 0.0851    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.83e+04  |
|    n_updates            | 6219      |
|    policy_gradient_loss | -1.76e-09 |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=791500, episode_reward=724.49 +/- 650.62
Episode length: 34.28 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 791500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 773      |
|    time_elapsed    | 2880     |
|    total_timesteps | 791552   |
---------------------------------
Eval num_timesteps=792000, episode_reward=787.81 +/- 649.22
Episode length: 35.20 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 788       |
| time/                   |           |
|    total_timesteps      | 792000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-20 |
|    explained_variance   | 0.0637    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.98e+04  |
|    n_updates            | 6229      |
|    policy_gradient_loss | 5.01e-10  |
|    value_loss           | 8.95e+04  |
---------------------------------------
Eval num_timesteps=792500, episode_reward=937.96 +/- 740.45
Episode length: 35.84 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 792500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 774      |
|    time_elapsed    | 2883     |
|    total_timesteps | 792576   |
---------------------------------
Eval num_timesteps=793000, episode_reward=1030.01 +/- 742.33
Episode length: 36.74 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 1.03e+03  |
| time/                   |           |
|    total_timesteps      | 793000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.93e-28 |
|    explained_variance   | 0.0315    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.58e+04  |
|    n_updates            | 6239      |
|    policy_gradient_loss | -1.75e-11 |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=793500, episode_reward=870.97 +/- 716.67
Episode length: 34.78 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 793500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 775      |
|    time_elapsed    | 2887     |
|    total_timesteps | 793600   |
---------------------------------
Eval num_timesteps=794000, episode_reward=752.84 +/- 618.49
Episode length: 34.78 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 753       |
| time/                   |           |
|    total_timesteps      | 794000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.78e-28 |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.93e+04  |
|    n_updates            | 6249      |
|    policy_gradient_loss | -6.78e-09 |
|    value_loss           | 1.61e+05  |
---------------------------------------
Eval num_timesteps=794500, episode_reward=674.46 +/- 563.36
Episode length: 34.46 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 794500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 776      |
|    time_elapsed    | 2890     |
|    total_timesteps | 794624   |
---------------------------------
Eval num_timesteps=795000, episode_reward=689.25 +/- 572.88
Episode length: 34.98 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 689       |
| time/                   |           |
|    total_timesteps      | 795000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.58e-18 |
|    explained_variance   | 0.0823    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.2e+04   |
|    n_updates            | 6259      |
|    policy_gradient_loss | -1.13e-09 |
|    value_loss           | 1.13e+05  |
---------------------------------------
Eval num_timesteps=795500, episode_reward=725.28 +/- 575.88
Episode length: 35.54 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 795500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 777      |
|    time_elapsed    | 2893     |
|    total_timesteps | 795648   |
---------------------------------
Eval num_timesteps=796000, episode_reward=711.30 +/- 612.16
Episode length: 34.48 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 711       |
| time/                   |           |
|    total_timesteps      | 796000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.58e-19 |
|    explained_variance   | 0.0574    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.23e+04  |
|    n_updates            | 6269      |
|    policy_gradient_loss | 7.45e-10  |
|    value_loss           | 8.27e+04  |
---------------------------------------
Eval num_timesteps=796500, episode_reward=910.43 +/- 719.75
Episode length: 35.58 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 796500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 778      |
|    time_elapsed    | 2897     |
|    total_timesteps | 796672   |
---------------------------------
Eval num_timesteps=797000, episode_reward=860.13 +/- 642.72
Episode length: 36.20 +/- 5.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 860       |
| time/                   |           |
|    total_timesteps      | 797000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.91e-16 |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.0005    |
|    loss                 | 5.48e+04  |
|    n_updates            | 6279      |
|    policy_gradient_loss | 2.83e-09  |
|    value_loss           | 9.78e+04  |
---------------------------------------
Eval num_timesteps=797500, episode_reward=914.67 +/- 742.94
Episode length: 35.32 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 797500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 274      |
|    iterations      | 779      |
|    time_elapsed    | 2900     |
|    total_timesteps | 797696   |
---------------------------------
Eval num_timesteps=798000, episode_reward=671.56 +/- 585.61
Episode length: 33.88 +/- 6.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 672           |
| time/                   |               |
|    total_timesteps      | 798000        |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -1.55e-05     |
|    explained_variance   | 0.0919        |
|    learning_rate        | 0.0005        |
|    loss                 | 3.72e+04      |
|    n_updates            | 6289          |
|    policy_gradient_loss | 3.59e-07      |
|    value_loss           | 8.19e+04      |
-------------------------------------------
Eval num_timesteps=798500, episode_reward=867.88 +/- 739.51
Episode length: 34.50 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 798500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 780      |
|    time_elapsed    | 2904     |
|    total_timesteps | 798720   |
---------------------------------
Eval num_timesteps=799000, episode_reward=900.49 +/- 710.20
Episode length: 35.88 +/- 7.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 900       |
| time/                   |           |
|    total_timesteps      | 799000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.09e-08 |
|    explained_variance   | 0.0285    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.46e+04  |
|    n_updates            | 6299      |
|    policy_gradient_loss | 1.22e-08  |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=799500, episode_reward=815.98 +/- 686.70
Episode length: 34.64 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 799500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 781      |
|    time_elapsed    | 2907     |
|    total_timesteps | 799744   |
---------------------------------
Eval num_timesteps=800000, episode_reward=801.95 +/- 669.60
Episode length: 34.84 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 800000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.35e-21 |
|    explained_variance   | 0.129     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.85e+04  |
|    n_updates            | 6309      |
|    policy_gradient_loss | 1.95e-09  |
|    value_loss           | 9.04e+04  |
---------------------------------------
Eval num_timesteps=800500, episode_reward=828.46 +/- 668.47
Episode length: 35.26 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 800500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 782      |
|    time_elapsed    | 2911     |
|    total_timesteps | 800768   |
---------------------------------
Eval num_timesteps=801000, episode_reward=927.35 +/- 723.99
Episode length: 35.90 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 801000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-18 |
|    explained_variance   | 0.0979    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.13e+04  |
|    n_updates            | 6319      |
|    policy_gradient_loss | -5.24e-11 |
|    value_loss           | 1.12e+05  |
---------------------------------------
Eval num_timesteps=801500, episode_reward=683.94 +/- 580.20
Episode length: 34.30 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 801500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 783      |
|    time_elapsed    | 2914     |
|    total_timesteps | 801792   |
---------------------------------
Eval num_timesteps=802000, episode_reward=855.20 +/- 733.87
Episode length: 35.10 +/- 7.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 802000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.39e-20 |
|    explained_variance   | 0.0949    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.63e+04  |
|    n_updates            | 6329      |
|    policy_gradient_loss | 4.77e-10  |
|    value_loss           | 7.86e+04  |
---------------------------------------
Eval num_timesteps=802500, episode_reward=822.64 +/- 679.79
Episode length: 35.20 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 802500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 784      |
|    time_elapsed    | 2918     |
|    total_timesteps | 802816   |
---------------------------------
Eval num_timesteps=803000, episode_reward=799.35 +/- 549.49
Episode length: 36.90 +/- 4.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 803000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.75e-18 |
|    explained_variance   | 0.154     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.97e+04  |
|    n_updates            | 6339      |
|    policy_gradient_loss | -5.76e-10 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=803500, episode_reward=686.30 +/- 621.17
Episode length: 34.00 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 803500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 785      |
|    time_elapsed    | 2921     |
|    total_timesteps | 803840   |
---------------------------------
Eval num_timesteps=804000, episode_reward=788.02 +/- 622.64
Episode length: 35.48 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 788       |
| time/                   |           |
|    total_timesteps      | 804000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.04e-20 |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.32e+04  |
|    n_updates            | 6349      |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 8.88e+04  |
---------------------------------------
Eval num_timesteps=804500, episode_reward=789.31 +/- 657.93
Episode length: 35.28 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 804500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 751      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 786      |
|    time_elapsed    | 2925     |
|    total_timesteps | 804864   |
---------------------------------
Eval num_timesteps=805000, episode_reward=967.13 +/- 744.58
Episode length: 35.80 +/- 7.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 967       |
| time/                   |           |
|    total_timesteps      | 805000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.95e-15 |
|    explained_variance   | 0.136     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.36e+04  |
|    n_updates            | 6359      |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=805500, episode_reward=740.81 +/- 651.66
Episode length: 34.20 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 805500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 787      |
|    time_elapsed    | 2928     |
|    total_timesteps | 805888   |
---------------------------------
Eval num_timesteps=806000, episode_reward=951.33 +/- 713.00
Episode length: 35.74 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 951       |
| time/                   |           |
|    total_timesteps      | 806000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.63e-15 |
|    explained_variance   | 0.15      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.46e+04  |
|    n_updates            | 6369      |
|    policy_gradient_loss | 1.13e-09  |
|    value_loss           | 8.05e+04  |
---------------------------------------
Eval num_timesteps=806500, episode_reward=838.21 +/- 679.22
Episode length: 35.68 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 806500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 788      |
|    time_elapsed    | 2932     |
|    total_timesteps | 806912   |
---------------------------------
Eval num_timesteps=807000, episode_reward=880.59 +/- 663.39
Episode length: 36.00 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 807000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.88e-14 |
|    explained_variance   | 0.211     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.48e+04  |
|    n_updates            | 6379      |
|    policy_gradient_loss | 3.49e-10  |
|    value_loss           | 9.68e+04  |
---------------------------------------
Eval num_timesteps=807500, episode_reward=904.29 +/- 687.49
Episode length: 36.14 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 807500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 789      |
|    time_elapsed    | 2935     |
|    total_timesteps | 807936   |
---------------------------------
Eval num_timesteps=808000, episode_reward=843.28 +/- 664.49
Episode length: 36.38 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 843       |
| time/                   |           |
|    total_timesteps      | 808000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.6e-15  |
|    explained_variance   | -0.00523  |
|    learning_rate        | 0.0005    |
|    loss                 | 7.07e+04  |
|    n_updates            | 6389      |
|    policy_gradient_loss | -3.96e-09 |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=808500, episode_reward=759.75 +/- 582.66
Episode length: 35.42 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 808500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 790      |
|    time_elapsed    | 2939     |
|    total_timesteps | 808960   |
---------------------------------
Eval num_timesteps=809000, episode_reward=857.62 +/- 709.32
Episode length: 34.32 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 858       |
| time/                   |           |
|    total_timesteps      | 809000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.94e-14 |
|    explained_variance   | 0.195     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.47e+04  |
|    n_updates            | 6399      |
|    policy_gradient_loss | -1.59e-09 |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=809500, episode_reward=806.35 +/- 720.63
Episode length: 34.36 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 809500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 791      |
|    time_elapsed    | 2942     |
|    total_timesteps | 809984   |
---------------------------------
Eval num_timesteps=810000, episode_reward=786.73 +/- 727.17
Episode length: 34.10 +/- 7.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 810000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.58e-16 |
|    explained_variance   | 0.0851    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.33e+04  |
|    n_updates            | 6409      |
|    policy_gradient_loss | 2.39e-09  |
|    value_loss           | 7.02e+04  |
---------------------------------------
Eval num_timesteps=810500, episode_reward=932.60 +/- 717.68
Episode length: 36.06 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 810500   |
---------------------------------
Eval num_timesteps=811000, episode_reward=824.62 +/- 661.13
Episode length: 35.66 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 706      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 792      |
|    time_elapsed    | 2947     |
|    total_timesteps | 811008   |
---------------------------------
Eval num_timesteps=811500, episode_reward=778.59 +/- 642.37
Episode length: 34.96 +/- 5.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 779       |
| time/                   |           |
|    total_timesteps      | 811500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.48e-14 |
|    explained_variance   | 0.0878    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.42e+04  |
|    n_updates            | 6419      |
|    policy_gradient_loss | -1.26e-09 |
|    value_loss           | 9.61e+04  |
---------------------------------------
Eval num_timesteps=812000, episode_reward=672.61 +/- 589.69
Episode length: 34.26 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 812000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 667      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 793      |
|    time_elapsed    | 2950     |
|    total_timesteps | 812032   |
---------------------------------
Eval num_timesteps=812500, episode_reward=711.29 +/- 608.31
Episode length: 34.02 +/- 6.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 711       |
| time/                   |           |
|    total_timesteps      | 812500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.16e-15 |
|    explained_variance   | 0.0912    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.08e+04  |
|    n_updates            | 6429      |
|    policy_gradient_loss | 2.25e-09  |
|    value_loss           | 7.28e+04  |
---------------------------------------
Eval num_timesteps=813000, episode_reward=904.51 +/- 710.52
Episode length: 36.18 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 649      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 794      |
|    time_elapsed    | 2954     |
|    total_timesteps | 813056   |
---------------------------------
Eval num_timesteps=813500, episode_reward=727.31 +/- 624.20
Episode length: 34.60 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 727       |
| time/                   |           |
|    total_timesteps      | 813500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.34e-14 |
|    explained_variance   | 0.231     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.62e+04  |
|    n_updates            | 6439      |
|    policy_gradient_loss | -4.02e-09 |
|    value_loss           | 8.92e+04  |
---------------------------------------
Eval num_timesteps=814000, episode_reward=835.19 +/- 701.80
Episode length: 34.88 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 814000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 795      |
|    time_elapsed    | 2957     |
|    total_timesteps | 814080   |
---------------------------------
Eval num_timesteps=814500, episode_reward=847.52 +/- 712.06
Episode length: 34.94 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 848       |
| time/                   |           |
|    total_timesteps      | 814500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.41e-16 |
|    explained_variance   | 0.0802    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.74e+04  |
|    n_updates            | 6449      |
|    policy_gradient_loss | 6.1e-09   |
|    value_loss           | 1.18e+05  |
---------------------------------------
Eval num_timesteps=815000, episode_reward=768.79 +/- 623.89
Episode length: 35.44 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 796      |
|    time_elapsed    | 2961     |
|    total_timesteps | 815104   |
---------------------------------
Eval num_timesteps=815500, episode_reward=625.58 +/- 561.79
Episode length: 33.04 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33        |
|    mean_reward          | 626       |
| time/                   |           |
|    total_timesteps      | 815500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-26 |
|    explained_variance   | 0.118     |
|    learning_rate        | 0.0005    |
|    loss                 | 5e+04     |
|    n_updates            | 6459      |
|    policy_gradient_loss | -1.2e-09  |
|    value_loss           | 9.8e+04   |
---------------------------------------
Eval num_timesteps=816000, episode_reward=868.21 +/- 702.97
Episode length: 35.52 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 952      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 797      |
|    time_elapsed    | 2964     |
|    total_timesteps | 816128   |
---------------------------------
Eval num_timesteps=816500, episode_reward=811.14 +/- 686.60
Episode length: 34.86 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 811       |
| time/                   |           |
|    total_timesteps      | 816500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.31e-22 |
|    explained_variance   | 0.0939    |
|    learning_rate        | 0.0005    |
|    loss                 | 7.55e+04  |
|    n_updates            | 6469      |
|    policy_gradient_loss | 2.29e-09  |
|    value_loss           | 1.38e+05  |
---------------------------------------
Eval num_timesteps=817000, episode_reward=869.32 +/- 648.22
Episode length: 36.70 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.3     |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    fps             | 275      |
|    iterations      | 798      |
|    time_elapsed    | 2968     |
|    total_timesteps | 817152   |
---------------------------------
Eval num_timesteps=817500, episode_reward=1003.56 +/- 770.91
Episode length: 35.98 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 817500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.99e-24 |
|    explained_variance   | 0.153     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.12e+04  |
|    n_updates            | 6479      |
|    policy_gradient_loss | 1.7e-09   |
|    value_loss           | 9.32e+04  |
---------------------------------------
Eval num_timesteps=818000, episode_reward=870.05 +/- 629.66
Episode length: 36.66 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 818000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 943      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 799      |
|    time_elapsed    | 2971     |
|    total_timesteps | 818176   |
---------------------------------
Eval num_timesteps=818500, episode_reward=718.65 +/- 545.15
Episode length: 35.50 +/- 5.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 719       |
| time/                   |           |
|    total_timesteps      | 818500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-21 |
|    explained_variance   | 0.164     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.76e+04  |
|    n_updates            | 6489      |
|    policy_gradient_loss | 4.94e-09  |
|    value_loss           | 1.68e+05  |
---------------------------------------
Eval num_timesteps=819000, episode_reward=961.57 +/- 745.27
Episode length: 36.38 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 881      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 800      |
|    time_elapsed    | 2975     |
|    total_timesteps | 819200   |
---------------------------------
Eval num_timesteps=819500, episode_reward=775.63 +/- 626.54
Episode length: 34.98 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 776       |
| time/                   |           |
|    total_timesteps      | 819500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.23e-25 |
|    explained_variance   | 0.121     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.36e+04  |
|    n_updates            | 6499      |
|    policy_gradient_loss | -2.33e-11 |
|    value_loss           | 9.91e+04  |
---------------------------------------
Eval num_timesteps=820000, episode_reward=1027.77 +/- 709.07
Episode length: 36.82 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 820000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 801      |
|    time_elapsed    | 2978     |
|    total_timesteps | 820224   |
---------------------------------
Eval num_timesteps=820500, episode_reward=972.15 +/- 728.07
Episode length: 36.14 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 972       |
| time/                   |           |
|    total_timesteps      | 820500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-17 |
|    explained_variance   | 0.0686    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.58e+04  |
|    n_updates            | 6509      |
|    policy_gradient_loss | -2.61e-09 |
|    value_loss           | 1.37e+05  |
---------------------------------------
Eval num_timesteps=821000, episode_reward=913.72 +/- 731.12
Episode length: 35.44 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 802      |
|    time_elapsed    | 2982     |
|    total_timesteps | 821248   |
---------------------------------
Eval num_timesteps=821500, episode_reward=692.14 +/- 677.44
Episode length: 33.10 +/- 7.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.1      |
|    mean_reward          | 692       |
| time/                   |           |
|    total_timesteps      | 821500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-05 |
|    explained_variance   | 0.0219    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.02e+04  |
|    n_updates            | 6519      |
|    policy_gradient_loss | 1.32e-06  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=822000, episode_reward=674.27 +/- 566.37
Episode length: 34.24 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 803      |
|    time_elapsed    | 2985     |
|    total_timesteps | 822272   |
---------------------------------
Eval num_timesteps=822500, episode_reward=883.21 +/- 696.95
Episode length: 36.18 +/- 5.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 822500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.72e-17 |
|    explained_variance   | 0.0844    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.78e+04  |
|    n_updates            | 6529      |
|    policy_gradient_loss | 4.07e-11  |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=823000, episode_reward=850.19 +/- 672.30
Episode length: 35.86 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 716      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 804      |
|    time_elapsed    | 2989     |
|    total_timesteps | 823296   |
---------------------------------
Eval num_timesteps=823500, episode_reward=765.92 +/- 618.10
Episode length: 35.46 +/- 5.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 766       |
| time/                   |           |
|    total_timesteps      | 823500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.7e-15  |
|    explained_variance   | 0.194     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.48e+04  |
|    n_updates            | 6539      |
|    policy_gradient_loss | -2.35e-09 |
|    value_loss           | 9.69e+04  |
---------------------------------------
Eval num_timesteps=824000, episode_reward=884.31 +/- 743.88
Episode length: 34.60 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 824000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 805      |
|    time_elapsed    | 2992     |
|    total_timesteps | 824320   |
---------------------------------
Eval num_timesteps=824500, episode_reward=653.95 +/- 586.16
Episode length: 33.50 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 654       |
| time/                   |           |
|    total_timesteps      | 824500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.72e-16 |
|    explained_variance   | 0.0588    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.88e+04  |
|    n_updates            | 6549      |
|    policy_gradient_loss | 2.83e-09  |
|    value_loss           | 1.44e+05  |
---------------------------------------
Eval num_timesteps=825000, episode_reward=956.01 +/- 725.72
Episode length: 36.10 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 879      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 806      |
|    time_elapsed    | 2995     |
|    total_timesteps | 825344   |
---------------------------------
Eval num_timesteps=825500, episode_reward=809.76 +/- 659.06
Episode length: 35.04 +/- 5.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 810       |
| time/                   |           |
|    total_timesteps      | 825500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-28 |
|    explained_variance   | 0.136     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.35e+04  |
|    n_updates            | 6559      |
|    policy_gradient_loss | 1.97e-09  |
|    value_loss           | 8.68e+04  |
---------------------------------------
Eval num_timesteps=826000, episode_reward=811.04 +/- 695.19
Episode length: 34.66 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 826000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 907      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 807      |
|    time_elapsed    | 2999     |
|    total_timesteps | 826368   |
---------------------------------
Eval num_timesteps=826500, episode_reward=962.76 +/- 726.48
Episode length: 35.62 +/- 7.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.6     |
|    mean_reward          | 963      |
| time/                   |          |
|    total_timesteps      | 826500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9.1e-26 |
|    explained_variance   | 0.146    |
|    learning_rate        | 0.0005   |
|    loss                 | 7.36e+04 |
|    n_updates            | 6569     |
|    policy_gradient_loss | 3.38e-10 |
|    value_loss           | 1.39e+05 |
--------------------------------------
Eval num_timesteps=827000, episode_reward=756.75 +/- 719.08
Episode length: 33.26 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 994      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 808      |
|    time_elapsed    | 3002     |
|    total_timesteps | 827392   |
---------------------------------
Eval num_timesteps=827500, episode_reward=936.31 +/- 721.36
Episode length: 36.22 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 936       |
| time/                   |           |
|    total_timesteps      | 827500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.29e-28 |
|    explained_variance   | 0.138     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.36e+04  |
|    n_updates            | 6579      |
|    policy_gradient_loss | 1.26e-09  |
|    value_loss           | 9.72e+04  |
---------------------------------------
Eval num_timesteps=828000, episode_reward=742.48 +/- 660.88
Episode length: 34.18 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 946      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 809      |
|    time_elapsed    | 3006     |
|    total_timesteps | 828416   |
---------------------------------
Eval num_timesteps=828500, episode_reward=813.24 +/- 635.50
Episode length: 35.66 +/- 5.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 813       |
| time/                   |           |
|    total_timesteps      | 828500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.6e-24  |
|    explained_variance   | 0.102     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.19e+04  |
|    n_updates            | 6589      |
|    policy_gradient_loss | -2.21e-09 |
|    value_loss           | 1.41e+05  |
---------------------------------------
Eval num_timesteps=829000, episode_reward=766.98 +/- 678.21
Episode length: 34.62 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 915      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 810      |
|    time_elapsed    | 3009     |
|    total_timesteps | 829440   |
---------------------------------
Eval num_timesteps=829500, episode_reward=618.02 +/- 538.82
Episode length: 33.54 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 618       |
| time/                   |           |
|    total_timesteps      | 829500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55e-26 |
|    explained_variance   | 0.0515    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.8e+04   |
|    n_updates            | 6599      |
|    policy_gradient_loss | 4.66e-10  |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=830000, episode_reward=763.92 +/- 661.26
Episode length: 34.82 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 830000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 811      |
|    time_elapsed    | 3013     |
|    total_timesteps | 830464   |
---------------------------------
Eval num_timesteps=830500, episode_reward=774.18 +/- 659.95
Episode length: 34.74 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 774       |
| time/                   |           |
|    total_timesteps      | 830500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.56e-25 |
|    explained_variance   | 0.175     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.01e+04  |
|    n_updates            | 6609      |
|    policy_gradient_loss | -9.55e-10 |
|    value_loss           | 1.47e+05  |
---------------------------------------
Eval num_timesteps=831000, episode_reward=1044.45 +/- 734.80
Episode length: 37.22 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 812      |
|    time_elapsed    | 3016     |
|    total_timesteps | 831488   |
---------------------------------
Eval num_timesteps=831500, episode_reward=802.31 +/- 662.90
Episode length: 35.14 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 831500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-27 |
|    explained_variance   | 0.049     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.31e+04  |
|    n_updates            | 6619      |
|    policy_gradient_loss | -1.16e-11 |
|    value_loss           | 9.91e+04  |
---------------------------------------
Eval num_timesteps=832000, episode_reward=907.94 +/- 722.46
Episode length: 35.60 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=832500, episode_reward=940.78 +/- 758.75
Episode length: 35.56 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 832500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 965      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 813      |
|    time_elapsed    | 3021     |
|    total_timesteps | 832512   |
---------------------------------
Eval num_timesteps=833000, episode_reward=1027.61 +/- 721.68
Episode length: 37.32 +/- 5.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.3      |
|    mean_reward          | 1.03e+03  |
| time/                   |           |
|    total_timesteps      | 833000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.11e-22 |
|    explained_variance   | 0.181     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.87e+04  |
|    n_updates            | 6629      |
|    policy_gradient_loss | 1.25e-09  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=833500, episode_reward=717.39 +/- 617.87
Episode length: 34.52 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 833500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 275      |
|    iterations      | 814      |
|    time_elapsed    | 3025     |
|    total_timesteps | 833536   |
---------------------------------
Eval num_timesteps=834000, episode_reward=923.32 +/- 711.27
Episode length: 35.90 +/- 6.23
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.9     |
|    mean_reward          | 923      |
| time/                   |          |
|    total_timesteps      | 834000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.6e-26 |
|    explained_variance   | 0.143    |
|    learning_rate        | 0.0005   |
|    loss                 | 4.7e+04  |
|    n_updates            | 6639     |
|    policy_gradient_loss | 9.92e-10 |
|    value_loss           | 9.34e+04 |
--------------------------------------
Eval num_timesteps=834500, episode_reward=767.75 +/- 666.62
Episode length: 34.16 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 834500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 971      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 815      |
|    time_elapsed    | 3028     |
|    total_timesteps | 834560   |
---------------------------------
Eval num_timesteps=835000, episode_reward=767.09 +/- 661.29
Episode length: 34.66 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 767       |
| time/                   |           |
|    total_timesteps      | 835000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-15 |
|    explained_variance   | 0.168     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.48e+04  |
|    n_updates            | 6649      |
|    policy_gradient_loss | 1.28e-10  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=835500, episode_reward=736.47 +/- 692.38
Episode length: 33.68 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 835500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 942      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 816      |
|    time_elapsed    | 3032     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=836000, episode_reward=907.65 +/- 741.83
Episode length: 35.42 +/- 6.96
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.4     |
|    mean_reward          | 908      |
| time/                   |          |
|    total_timesteps      | 836000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4.6e-18 |
|    explained_variance   | 0.0764   |
|    learning_rate        | 0.0005   |
|    loss                 | 4.61e+04 |
|    n_updates            | 6659     |
|    policy_gradient_loss | 1.21e-09 |
|    value_loss           | 9.92e+04 |
--------------------------------------
Eval num_timesteps=836500, episode_reward=806.64 +/- 659.12
Episode length: 35.12 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 836500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 817      |
|    time_elapsed    | 3035     |
|    total_timesteps | 836608   |
---------------------------------
Eval num_timesteps=837000, episode_reward=817.11 +/- 758.12
Episode length: 33.90 +/- 7.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 837000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-15 |
|    explained_variance   | 0.236     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.55e+04  |
|    n_updates            | 6669      |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=837500, episode_reward=815.04 +/- 671.01
Episode length: 35.18 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 837500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 878      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 818      |
|    time_elapsed    | 3039     |
|    total_timesteps | 837632   |
---------------------------------
Eval num_timesteps=838000, episode_reward=708.69 +/- 635.10
Episode length: 34.46 +/- 7.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 709       |
| time/                   |           |
|    total_timesteps      | 838000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-17 |
|    explained_variance   | 0.128     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.74e+04  |
|    n_updates            | 6679      |
|    policy_gradient_loss | 2.81e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=838500, episode_reward=887.88 +/- 753.14
Episode length: 34.34 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 838500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 819      |
|    time_elapsed    | 3042     |
|    total_timesteps | 838656   |
---------------------------------
Eval num_timesteps=839000, episode_reward=757.63 +/- 607.80
Episode length: 35.14 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 839000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-15 |
|    explained_variance   | 0.181     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.99e+04  |
|    n_updates            | 6689      |
|    policy_gradient_loss | 1.16e-09  |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=839500, episode_reward=725.28 +/- 684.56
Episode length: 33.92 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 839500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 820      |
|    time_elapsed    | 3046     |
|    total_timesteps | 839680   |
---------------------------------
Eval num_timesteps=840000, episode_reward=872.63 +/- 706.44
Episode length: 35.58 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 873       |
| time/                   |           |
|    total_timesteps      | 840000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.78e-19 |
|    explained_variance   | 0.0428    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.91e+04  |
|    n_updates            | 6699      |
|    policy_gradient_loss | -2.3e-09  |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=840500, episode_reward=806.28 +/- 649.24
Episode length: 35.58 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 840500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 821      |
|    time_elapsed    | 3049     |
|    total_timesteps | 840704   |
---------------------------------
Eval num_timesteps=841000, episode_reward=968.86 +/- 765.15
Episode length: 35.50 +/- 7.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 969       |
| time/                   |           |
|    total_timesteps      | 841000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.32e-16 |
|    explained_variance   | 0.136     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.01e+04  |
|    n_updates            | 6709      |
|    policy_gradient_loss | -3.41e-09 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=841500, episode_reward=892.74 +/- 746.50
Episode length: 35.10 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 841500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 822      |
|    time_elapsed    | 3053     |
|    total_timesteps | 841728   |
---------------------------------
Eval num_timesteps=842000, episode_reward=752.81 +/- 623.72
Episode length: 34.74 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 753       |
| time/                   |           |
|    total_timesteps      | 842000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-17 |
|    explained_variance   | 0.112     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.79e+04  |
|    n_updates            | 6719      |
|    policy_gradient_loss | -3.32e-09 |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=842500, episode_reward=686.20 +/- 639.41
Episode length: 33.72 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 842500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 823      |
|    time_elapsed    | 3056     |
|    total_timesteps | 842752   |
---------------------------------
Eval num_timesteps=843000, episode_reward=884.32 +/- 656.64
Episode length: 36.28 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 884       |
| time/                   |           |
|    total_timesteps      | 843000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.13e-16 |
|    explained_variance   | 0.129     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.02e+04  |
|    n_updates            | 6729      |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=843500, episode_reward=931.19 +/- 724.94
Episode length: 36.16 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 843500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 890      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 824      |
|    time_elapsed    | 3059     |
|    total_timesteps | 843776   |
---------------------------------
Eval num_timesteps=844000, episode_reward=793.77 +/- 571.63
Episode length: 36.54 +/- 5.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 794       |
| time/                   |           |
|    total_timesteps      | 844000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-18 |
|    explained_variance   | 0.101     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.79e+04  |
|    n_updates            | 6739      |
|    policy_gradient_loss | -2.91e-11 |
|    value_loss           | 9.69e+04  |
---------------------------------------
Eval num_timesteps=844500, episode_reward=772.60 +/- 707.68
Episode length: 33.48 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 844500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 825      |
|    time_elapsed    | 3063     |
|    total_timesteps | 844800   |
---------------------------------
Eval num_timesteps=845000, episode_reward=905.80 +/- 702.70
Episode length: 35.92 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 845000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-15 |
|    explained_variance   | 0.234     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.56e+04  |
|    n_updates            | 6749      |
|    policy_gradient_loss | -2.14e-09 |
|    value_loss           | 9.99e+04  |
---------------------------------------
Eval num_timesteps=845500, episode_reward=789.79 +/- 694.75
Episode length: 34.76 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 845500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 826      |
|    time_elapsed    | 3066     |
|    total_timesteps | 845824   |
---------------------------------
Eval num_timesteps=846000, episode_reward=774.50 +/- 617.81
Episode length: 35.56 +/- 5.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 774       |
| time/                   |           |
|    total_timesteps      | 846000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.22e-17 |
|    explained_variance   | 0.00131   |
|    learning_rate        | 0.0005    |
|    loss                 | 3.94e+04  |
|    n_updates            | 6759      |
|    policy_gradient_loss | -8.15e-10 |
|    value_loss           | 9.53e+04  |
---------------------------------------
Eval num_timesteps=846500, episode_reward=979.53 +/- 717.11
Episode length: 36.56 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 846500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 827      |
|    time_elapsed    | 3070     |
|    total_timesteps | 846848   |
---------------------------------
Eval num_timesteps=847000, episode_reward=900.34 +/- 718.05
Episode length: 35.40 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 900       |
| time/                   |           |
|    total_timesteps      | 847000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.16e-23 |
|    explained_variance   | 0.113     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.44e+04  |
|    n_updates            | 6769      |
|    policy_gradient_loss | 9.08e-10  |
|    value_loss           | 9.99e+04  |
---------------------------------------
Eval num_timesteps=847500, episode_reward=865.03 +/- 720.06
Episode length: 35.20 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 847500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 828      |
|    time_elapsed    | 3073     |
|    total_timesteps | 847872   |
---------------------------------
Eval num_timesteps=848000, episode_reward=933.58 +/- 741.19
Episode length: 35.86 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 934       |
| time/                   |           |
|    total_timesteps      | 848000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.69e-21 |
|    explained_variance   | 0.163     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.76e+04  |
|    n_updates            | 6779      |
|    policy_gradient_loss | 1.62e-09  |
|    value_loss           | 1.19e+05  |
---------------------------------------
Eval num_timesteps=848500, episode_reward=853.09 +/- 668.21
Episode length: 36.40 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 848500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 858      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 829      |
|    time_elapsed    | 3077     |
|    total_timesteps | 848896   |
---------------------------------
Eval num_timesteps=849000, episode_reward=817.86 +/- 676.29
Episode length: 35.36 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 818       |
| time/                   |           |
|    total_timesteps      | 849000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.6e-12  |
|    explained_variance   | 0.181     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.32e+04  |
|    n_updates            | 6789      |
|    policy_gradient_loss | -6.46e-10 |
|    value_loss           | 8.17e+04  |
---------------------------------------
Eval num_timesteps=849500, episode_reward=834.57 +/- 652.52
Episode length: 35.60 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 849500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 815      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 830      |
|    time_elapsed    | 3081     |
|    total_timesteps | 849920   |
---------------------------------
Eval num_timesteps=850000, episode_reward=838.42 +/- 716.28
Episode length: 35.00 +/- 7.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35       |
|    mean_reward          | 838      |
| time/                   |          |
|    total_timesteps      | 850000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.1e-14 |
|    explained_variance   | 0.104    |
|    learning_rate        | 0.0005   |
|    loss                 | 3.47e+04 |
|    n_updates            | 6799     |
|    policy_gradient_loss | -4.2e-09 |
|    value_loss           | 7.58e+04 |
--------------------------------------
Eval num_timesteps=850500, episode_reward=902.18 +/- 696.17
Episode length: 35.92 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 850500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 708      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 831      |
|    time_elapsed    | 3084     |
|    total_timesteps | 850944   |
---------------------------------
Eval num_timesteps=851000, episode_reward=819.98 +/- 614.58
Episode length: 36.28 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 820       |
| time/                   |           |
|    total_timesteps      | 851000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55e-12 |
|    explained_variance   | 0.151     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.15e+04  |
|    n_updates            | 6809      |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 9.33e+04  |
---------------------------------------
Eval num_timesteps=851500, episode_reward=765.57 +/- 642.59
Episode length: 34.96 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 851500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 684      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 832      |
|    time_elapsed    | 3088     |
|    total_timesteps | 851968   |
---------------------------------
Eval num_timesteps=852000, episode_reward=844.27 +/- 744.72
Episode length: 34.30 +/- 7.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 844       |
| time/                   |           |
|    total_timesteps      | 852000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.27e-14 |
|    explained_variance   | 0.0765    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.73e+04  |
|    n_updates            | 6819      |
|    policy_gradient_loss | -1.98e-09 |
|    value_loss           | 8.01e+04  |
---------------------------------------
Eval num_timesteps=852500, episode_reward=788.16 +/- 672.93
Episode length: 34.38 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 852500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 723      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 833      |
|    time_elapsed    | 3091     |
|    total_timesteps | 852992   |
---------------------------------
Eval num_timesteps=853000, episode_reward=806.42 +/- 691.76
Episode length: 34.98 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 806       |
| time/                   |           |
|    total_timesteps      | 853000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.81e-13 |
|    explained_variance   | 0.188     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.72e+04  |
|    n_updates            | 6829      |
|    policy_gradient_loss | 7.1e-10   |
|    value_loss           | 9.36e+04  |
---------------------------------------
Eval num_timesteps=853500, episode_reward=789.79 +/- 673.78
Episode length: 34.50 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 853500   |
---------------------------------
Eval num_timesteps=854000, episode_reward=617.84 +/- 513.42
Episode length: 34.56 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 834      |
|    time_elapsed    | 3096     |
|    total_timesteps | 854016   |
---------------------------------
Eval num_timesteps=854500, episode_reward=845.10 +/- 681.11
Episode length: 35.12 +/- 6.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 845       |
| time/                   |           |
|    total_timesteps      | 854500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.82e-15 |
|    explained_variance   | 0.0979    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.63e+04  |
|    n_updates            | 6839      |
|    policy_gradient_loss | 2.95e-09  |
|    value_loss           | 9.44e+04  |
---------------------------------------
Eval num_timesteps=855000, episode_reward=904.79 +/- 675.12
Episode length: 36.28 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 855000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 835      |
|    time_elapsed    | 3099     |
|    total_timesteps | 855040   |
---------------------------------
Eval num_timesteps=855500, episode_reward=899.36 +/- 727.02
Episode length: 35.48 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 899       |
| time/                   |           |
|    total_timesteps      | 855500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-20 |
|    explained_variance   | 0.142     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.38e+04  |
|    n_updates            | 6849      |
|    policy_gradient_loss | -7.68e-10 |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=856000, episode_reward=933.37 +/- 742.89
Episode length: 35.58 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 836      |
|    time_elapsed    | 3103     |
|    total_timesteps | 856064   |
---------------------------------
Eval num_timesteps=856500, episode_reward=859.12 +/- 680.94
Episode length: 35.40 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 859       |
| time/                   |           |
|    total_timesteps      | 856500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.85e-21 |
|    explained_variance   | 0.247     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.13e+04  |
|    n_updates            | 6859      |
|    policy_gradient_loss | -2.27e-09 |
|    value_loss           | 1.68e+05  |
---------------------------------------
Eval num_timesteps=857000, episode_reward=851.28 +/- 678.52
Episode length: 36.34 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 857000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 855      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 837      |
|    time_elapsed    | 3106     |
|    total_timesteps | 857088   |
---------------------------------
Eval num_timesteps=857500, episode_reward=863.23 +/- 692.68
Episode length: 35.56 +/- 6.28
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.6     |
|    mean_reward          | 863      |
| time/                   |          |
|    total_timesteps      | 857500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.6e-13 |
|    explained_variance   | 0.0994   |
|    learning_rate        | 0.0005   |
|    loss                 | 6.65e+04 |
|    n_updates            | 6869     |
|    policy_gradient_loss | 3.17e-09 |
|    value_loss           | 1.16e+05 |
--------------------------------------
Eval num_timesteps=858000, episode_reward=934.71 +/- 747.86
Episode length: 35.40 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 838      |
|    time_elapsed    | 3110     |
|    total_timesteps | 858112   |
---------------------------------
Eval num_timesteps=858500, episode_reward=1004.37 +/- 739.21
Episode length: 36.80 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 858500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-09 |
|    explained_variance   | 0.0605    |
|    learning_rate        | 0.0005    |
|    loss                 | 2.88e+04  |
|    n_updates            | 6879      |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 7.19e+04  |
---------------------------------------
Eval num_timesteps=859000, episode_reward=832.63 +/- 679.23
Episode length: 35.56 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 859000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 839      |
|    time_elapsed    | 3113     |
|    total_timesteps | 859136   |
---------------------------------
Eval num_timesteps=859500, episode_reward=865.24 +/- 706.05
Episode length: 35.06 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 865       |
| time/                   |           |
|    total_timesteps      | 859500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.69e-11 |
|    explained_variance   | 0.0992    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.1e+04   |
|    n_updates            | 6889      |
|    policy_gradient_loss | 3.18e-09  |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=860000, episode_reward=776.15 +/- 618.58
Episode length: 35.42 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 840      |
|    time_elapsed    | 3117     |
|    total_timesteps | 860160   |
---------------------------------
Eval num_timesteps=860500, episode_reward=783.88 +/- 646.85
Episode length: 35.74 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 860500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-16 |
|    explained_variance   | 0.149     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.93e+04  |
|    n_updates            | 6899      |
|    policy_gradient_loss | -8.27e-10 |
|    value_loss           | 7.81e+04  |
---------------------------------------
Eval num_timesteps=861000, episode_reward=728.61 +/- 653.92
Episode length: 33.90 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 861000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 841      |
|    time_elapsed    | 3120     |
|    total_timesteps | 861184   |
---------------------------------
Eval num_timesteps=861500, episode_reward=781.86 +/- 657.92
Episode length: 34.38 +/- 6.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 861500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.82e-25 |
|    explained_variance   | 0.172     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.63e+04  |
|    n_updates            | 6909      |
|    policy_gradient_loss | -8.5e-10  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=862000, episode_reward=741.19 +/- 638.68
Episode length: 33.96 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 842      |
|    time_elapsed    | 3124     |
|    total_timesteps | 862208   |
---------------------------------
Eval num_timesteps=862500, episode_reward=679.62 +/- 574.73
Episode length: 34.46 +/- 5.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 680       |
| time/                   |           |
|    total_timesteps      | 862500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.16e-14 |
|    explained_variance   | 0.209     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.88e+04  |
|    n_updates            | 6919      |
|    policy_gradient_loss | -1.39e-09 |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=863000, episode_reward=857.36 +/- 637.05
Episode length: 36.16 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 863000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 843      |
|    time_elapsed    | 3127     |
|    total_timesteps | 863232   |
---------------------------------
Eval num_timesteps=863500, episode_reward=678.84 +/- 619.98
Episode length: 33.34 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 679       |
| time/                   |           |
|    total_timesteps      | 863500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-09 |
|    explained_variance   | 0.0382    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.26e+04  |
|    n_updates            | 6929      |
|    policy_gradient_loss | 1.34e-09  |
|    value_loss           | 8.45e+04  |
---------------------------------------
Eval num_timesteps=864000, episode_reward=902.95 +/- 758.38
Episode length: 34.88 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 735      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 844      |
|    time_elapsed    | 3131     |
|    total_timesteps | 864256   |
---------------------------------
Eval num_timesteps=864500, episode_reward=817.01 +/- 682.35
Episode length: 35.02 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 864500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-11 |
|    explained_variance   | 0.0914    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.16e+04  |
|    n_updates            | 6939      |
|    policy_gradient_loss | -5.53e-10 |
|    value_loss           | 8.66e+04  |
---------------------------------------
Eval num_timesteps=865000, episode_reward=858.99 +/- 738.34
Episode length: 34.58 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 865000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 678      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 845      |
|    time_elapsed    | 3134     |
|    total_timesteps | 865280   |
---------------------------------
Eval num_timesteps=865500, episode_reward=788.82 +/- 653.02
Episode length: 35.20 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 865500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.34e-17 |
|    explained_variance   | 0.101     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.73e+04  |
|    n_updates            | 6949      |
|    policy_gradient_loss | 4.02e-10  |
|    value_loss           | 9.34e+04  |
---------------------------------------
Eval num_timesteps=866000, episode_reward=815.75 +/- 678.28
Episode length: 35.06 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 846      |
|    time_elapsed    | 3138     |
|    total_timesteps | 866304   |
---------------------------------
Eval num_timesteps=866500, episode_reward=825.92 +/- 685.60
Episode length: 34.84 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 866500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.33e-15 |
|    explained_variance   | 0.107     |
|    learning_rate        | 0.0005    |
|    loss                 | 5e+04     |
|    n_updates            | 6959      |
|    policy_gradient_loss | -6.69e-10 |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=867000, episode_reward=822.43 +/- 668.46
Episode length: 35.40 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 867000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 847      |
|    time_elapsed    | 3141     |
|    total_timesteps | 867328   |
---------------------------------
Eval num_timesteps=867500, episode_reward=911.10 +/- 762.61
Episode length: 34.82 +/- 7.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 911       |
| time/                   |           |
|    total_timesteps      | 867500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.48e-10 |
|    explained_variance   | 0.0544    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.1e+04   |
|    n_updates            | 6969      |
|    policy_gradient_loss | 5.41e-10  |
|    value_loss           | 9.1e+04   |
---------------------------------------
Eval num_timesteps=868000, episode_reward=766.46 +/- 666.12
Episode length: 34.68 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 848      |
|    time_elapsed    | 3144     |
|    total_timesteps | 868352   |
---------------------------------
Eval num_timesteps=868500, episode_reward=952.22 +/- 684.58
Episode length: 37.04 +/- 5.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 952       |
| time/                   |           |
|    total_timesteps      | 868500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-11 |
|    explained_variance   | 0.0878    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.3e+04   |
|    n_updates            | 6979      |
|    policy_gradient_loss | 1.51e-09  |
|    value_loss           | 8.58e+04  |
---------------------------------------
Eval num_timesteps=869000, episode_reward=892.86 +/- 731.61
Episode length: 35.64 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 869000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 849      |
|    time_elapsed    | 3148     |
|    total_timesteps | 869376   |
---------------------------------
Eval num_timesteps=869500, episode_reward=917.35 +/- 676.64
Episode length: 36.46 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 917       |
| time/                   |           |
|    total_timesteps      | 869500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.26e-22 |
|    explained_variance   | 0.111     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.1e+04   |
|    n_updates            | 6989      |
|    policy_gradient_loss | -4.31e-10 |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=870000, episode_reward=901.17 +/- 693.41
Episode length: 35.96 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 850      |
|    time_elapsed    | 3152     |
|    total_timesteps | 870400   |
---------------------------------
Eval num_timesteps=870500, episode_reward=864.21 +/- 739.58
Episode length: 33.96 +/- 7.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 864       |
| time/                   |           |
|    total_timesteps      | 870500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.84e-16 |
|    explained_variance   | 0.155     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.58e+04  |
|    n_updates            | 6999      |
|    policy_gradient_loss | 1.12e-09  |
|    value_loss           | 1.36e+05  |
---------------------------------------
Eval num_timesteps=871000, episode_reward=865.70 +/- 688.42
Episode length: 35.72 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 871000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 851      |
|    time_elapsed    | 3155     |
|    total_timesteps | 871424   |
---------------------------------
Eval num_timesteps=871500, episode_reward=696.14 +/- 611.63
Episode length: 33.92 +/- 6.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.9     |
|    mean_reward          | 696      |
| time/                   |          |
|    total_timesteps      | 871500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6e-19   |
|    explained_variance   | 0.145    |
|    learning_rate        | 0.0005   |
|    loss                 | 5.8e+04  |
|    n_updates            | 7009     |
|    policy_gradient_loss | 7.8e-10  |
|    value_loss           | 1.04e+05 |
--------------------------------------
Eval num_timesteps=872000, episode_reward=919.17 +/- 704.38
Episode length: 36.42 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 852      |
|    time_elapsed    | 3159     |
|    total_timesteps | 872448   |
---------------------------------
Eval num_timesteps=872500, episode_reward=870.93 +/- 677.99
Episode length: 36.10 +/- 5.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 872500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.15e-15 |
|    explained_variance   | 0.113     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.09e+04  |
|    n_updates            | 7019      |
|    policy_gradient_loss | 2.05e-09  |
|    value_loss           | 1.17e+05  |
---------------------------------------
Eval num_timesteps=873000, episode_reward=932.47 +/- 718.59
Episode length: 36.08 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 873000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 853      |
|    time_elapsed    | 3162     |
|    total_timesteps | 873472   |
---------------------------------
Eval num_timesteps=873500, episode_reward=681.80 +/- 554.50
Episode length: 34.66 +/- 5.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 682       |
| time/                   |           |
|    total_timesteps      | 873500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.88e-19 |
|    explained_variance   | 0.0848    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.85e+04  |
|    n_updates            | 7029      |
|    policy_gradient_loss | -7.74e-10 |
|    value_loss           | 9.8e+04   |
---------------------------------------
Eval num_timesteps=874000, episode_reward=682.54 +/- 580.24
Episode length: 34.56 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 903      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 854      |
|    time_elapsed    | 3166     |
|    total_timesteps | 874496   |
---------------------------------
Eval num_timesteps=874500, episode_reward=785.96 +/- 684.44
Episode length: 34.52 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 874500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.25e-15 |
|    explained_variance   | 0.121     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.48e+04  |
|    n_updates            | 7039      |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 1.12e+05  |
---------------------------------------
Eval num_timesteps=875000, episode_reward=887.63 +/- 752.97
Episode length: 35.30 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
Eval num_timesteps=875500, episode_reward=826.55 +/- 724.81
Episode length: 34.16 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 875500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 932      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 855      |
|    time_elapsed    | 3170     |
|    total_timesteps | 875520   |
---------------------------------
Eval num_timesteps=876000, episode_reward=898.74 +/- 676.00
Episode length: 36.20 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 899       |
| time/                   |           |
|    total_timesteps      | 876000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.61e-17 |
|    explained_variance   | 0.0973    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.05e+04  |
|    n_updates            | 7049      |
|    policy_gradient_loss | -2.55e-09 |
|    value_loss           | 1.19e+05  |
---------------------------------------
Eval num_timesteps=876500, episode_reward=659.43 +/- 583.62
Episode length: 34.26 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 876500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 920      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 856      |
|    time_elapsed    | 3174     |
|    total_timesteps | 876544   |
---------------------------------
Eval num_timesteps=877000, episode_reward=804.56 +/- 638.63
Episode length: 35.66 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 877000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.62e-14 |
|    explained_variance   | 0.204     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.69e+04  |
|    n_updates            | 7059      |
|    policy_gradient_loss | 1.52e-09  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=877500, episode_reward=663.23 +/- 640.72
Episode length: 32.92 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 877500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 879      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 857      |
|    time_elapsed    | 3177     |
|    total_timesteps | 877568   |
---------------------------------
Eval num_timesteps=878000, episode_reward=969.24 +/- 704.63
Episode length: 37.52 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.5      |
|    mean_reward          | 969       |
| time/                   |           |
|    total_timesteps      | 878000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.36e-16 |
|    explained_variance   | -0.00523  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.53e+04  |
|    n_updates            | 7069      |
|    policy_gradient_loss | 4.42e-10  |
|    value_loss           | 1.23e+05  |
---------------------------------------
Eval num_timesteps=878500, episode_reward=861.45 +/- 659.14
Episode length: 36.08 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 878500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 858      |
|    time_elapsed    | 3181     |
|    total_timesteps | 878592   |
---------------------------------
Eval num_timesteps=879000, episode_reward=816.51 +/- 677.84
Episode length: 35.32 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 879000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.18e-14 |
|    explained_variance   | 0.188     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.49e+04  |
|    n_updates            | 7079      |
|    policy_gradient_loss | -4.26e-09 |
|    value_loss           | 1.36e+05  |
---------------------------------------
Eval num_timesteps=879500, episode_reward=702.20 +/- 601.33
Episode length: 34.04 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 879500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 859      |
|    time_elapsed    | 3184     |
|    total_timesteps | 879616   |
---------------------------------
Eval num_timesteps=880000, episode_reward=796.81 +/- 657.01
Episode length: 35.46 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 797       |
| time/                   |           |
|    total_timesteps      | 880000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.95e-09 |
|    explained_variance   | 0.14      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.08e+04  |
|    n_updates            | 7089      |
|    policy_gradient_loss | -3.96e-10 |
|    value_loss           | 6.97e+04  |
---------------------------------------
Eval num_timesteps=880500, episode_reward=1023.71 +/- 775.12
Episode length: 36.22 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 880500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 706      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 860      |
|    time_elapsed    | 3188     |
|    total_timesteps | 880640   |
---------------------------------
Eval num_timesteps=881000, episode_reward=956.40 +/- 738.81
Episode length: 35.32 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 956       |
| time/                   |           |
|    total_timesteps      | 881000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.54e-11 |
|    explained_variance   | 0.0899    |
|    learning_rate        | 0.0005    |
|    loss                 | 4e+04     |
|    n_updates            | 7099      |
|    policy_gradient_loss | 5.82e-12  |
|    value_loss           | 9.44e+04  |
---------------------------------------
Eval num_timesteps=881500, episode_reward=662.28 +/- 542.38
Episode length: 34.76 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 881500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 718      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 861      |
|    time_elapsed    | 3191     |
|    total_timesteps | 881664   |
---------------------------------
Eval num_timesteps=882000, episode_reward=976.26 +/- 750.20
Episode length: 36.40 +/- 6.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 976       |
| time/                   |           |
|    total_timesteps      | 882000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.89e-16 |
|    explained_variance   | 0.111     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.44e+04  |
|    n_updates            | 7109      |
|    policy_gradient_loss | 9.08e-10  |
|    value_loss           | 9.03e+04  |
---------------------------------------
Eval num_timesteps=882500, episode_reward=763.56 +/- 645.55
Episode length: 35.12 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 882500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 718      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 862      |
|    time_elapsed    | 3195     |
|    total_timesteps | 882688   |
---------------------------------
Eval num_timesteps=883000, episode_reward=817.39 +/- 606.84
Episode length: 35.90 +/- 6.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 883000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.11e-14 |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.47e+04  |
|    n_updates            | 7119      |
|    policy_gradient_loss | 3.83e-09  |
|    value_loss           | 1.19e+05  |
---------------------------------------
Eval num_timesteps=883500, episode_reward=883.09 +/- 749.40
Episode length: 35.00 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 883500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 863      |
|    time_elapsed    | 3198     |
|    total_timesteps | 883712   |
---------------------------------
Eval num_timesteps=884000, episode_reward=874.55 +/- 771.72
Episode length: 34.26 +/- 7.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 884000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.17e-16 |
|    explained_variance   | 0.174     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.57e+04  |
|    n_updates            | 7129      |
|    policy_gradient_loss | -1.26e-09 |
|    value_loss           | 9.45e+04  |
---------------------------------------
Eval num_timesteps=884500, episode_reward=811.93 +/- 726.12
Episode length: 34.14 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 884500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 864      |
|    time_elapsed    | 3201     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=885000, episode_reward=881.33 +/- 667.26
Episode length: 35.96 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 885000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-25 |
|    explained_variance   | 0.107     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.32e+04  |
|    n_updates            | 7139      |
|    policy_gradient_loss | 1.11e-09  |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=885500, episode_reward=1007.34 +/- 750.98
Episode length: 36.64 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 885500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 865      |
|    time_elapsed    | 3205     |
|    total_timesteps | 885760   |
---------------------------------
Eval num_timesteps=886000, episode_reward=826.97 +/- 661.41
Episode length: 35.72 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 886000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.44e-20 |
|    explained_variance   | 0.227     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.07e+04  |
|    n_updates            | 7149      |
|    policy_gradient_loss | -2.84e-09 |
|    value_loss           | 1.63e+05  |
---------------------------------------
Eval num_timesteps=886500, episode_reward=762.13 +/- 648.56
Episode length: 34.78 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 886500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 866      |
|    time_elapsed    | 3208     |
|    total_timesteps | 886784   |
---------------------------------
Eval num_timesteps=887000, episode_reward=881.32 +/- 707.58
Episode length: 35.72 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 887000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.72e-13 |
|    explained_variance   | 0.134     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.65e+04  |
|    n_updates            | 7159      |
|    policy_gradient_loss | 1.85e-09  |
|    value_loss           | 9.21e+04  |
---------------------------------------
Eval num_timesteps=887500, episode_reward=979.56 +/- 730.37
Episode length: 36.58 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 887500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 867      |
|    time_elapsed    | 3212     |
|    total_timesteps | 887808   |
---------------------------------
Eval num_timesteps=888000, episode_reward=973.45 +/- 728.84
Episode length: 36.50 +/- 6.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 973       |
| time/                   |           |
|    total_timesteps      | 888000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.44e-14 |
|    explained_variance   | 0.141     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.18e+04  |
|    n_updates            | 7169      |
|    policy_gradient_loss | -4.37e-10 |
|    value_loss           | 8.5e+04   |
---------------------------------------
Eval num_timesteps=888500, episode_reward=797.16 +/- 630.38
Episode length: 35.60 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 888500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 868      |
|    time_elapsed    | 3216     |
|    total_timesteps | 888832   |
---------------------------------
Eval num_timesteps=889000, episode_reward=777.43 +/- 672.14
Episode length: 35.04 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 777       |
| time/                   |           |
|    total_timesteps      | 889000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.96e-12 |
|    explained_variance   | 0.229     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.13e+04  |
|    n_updates            | 7179      |
|    policy_gradient_loss | -2.97e-09 |
|    value_loss           | 9.75e+04  |
---------------------------------------
Eval num_timesteps=889500, episode_reward=760.25 +/- 652.02
Episode length: 34.88 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 889500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 869      |
|    time_elapsed    | 3219     |
|    total_timesteps | 889856   |
---------------------------------
Eval num_timesteps=890000, episode_reward=901.83 +/- 678.93
Episode length: 36.38 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 902       |
| time/                   |           |
|    total_timesteps      | 890000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.3e-13  |
|    explained_variance   | 0.187     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.7e+04   |
|    n_updates            | 7189      |
|    policy_gradient_loss | -1.44e-09 |
|    value_loss           | 8.84e+04  |
---------------------------------------
Eval num_timesteps=890500, episode_reward=724.87 +/- 635.56
Episode length: 34.42 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 890500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 870      |
|    time_elapsed    | 3222     |
|    total_timesteps | 890880   |
---------------------------------
Eval num_timesteps=891000, episode_reward=847.36 +/- 693.90
Episode length: 35.30 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 847       |
| time/                   |           |
|    total_timesteps      | 891000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.15e-12 |
|    explained_variance   | 0.216     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.65e+04  |
|    n_updates            | 7199      |
|    policy_gradient_loss | 1.87e-09  |
|    value_loss           | 9.21e+04  |
---------------------------------------
Eval num_timesteps=891500, episode_reward=884.18 +/- 702.27
Episode length: 35.82 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 891500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 871      |
|    time_elapsed    | 3226     |
|    total_timesteps | 891904   |
---------------------------------
Eval num_timesteps=892000, episode_reward=800.73 +/- 675.20
Episode length: 34.70 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 892000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.22e-14 |
|    explained_variance   | 0.179     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.71e+04  |
|    n_updates            | 7209      |
|    policy_gradient_loss | 1.75e-09  |
|    value_loss           | 9.24e+04  |
---------------------------------------
Eval num_timesteps=892500, episode_reward=890.04 +/- 707.54
Episode length: 35.54 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 892500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 797      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 872      |
|    time_elapsed    | 3229     |
|    total_timesteps | 892928   |
---------------------------------
Eval num_timesteps=893000, episode_reward=778.95 +/- 657.90
Episode length: 34.82 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 779       |
| time/                   |           |
|    total_timesteps      | 893000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-11 |
|    explained_variance   | 0.104     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.48e+04  |
|    n_updates            | 7219      |
|    policy_gradient_loss | -1.18e-09 |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=893500, episode_reward=769.15 +/- 649.76
Episode length: 34.72 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 893500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 873      |
|    time_elapsed    | 3233     |
|    total_timesteps | 893952   |
---------------------------------
Eval num_timesteps=894000, episode_reward=1042.81 +/- 720.87
Episode length: 37.28 +/- 6.16
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 37.3           |
|    mean_reward          | 1.04e+03       |
| time/                   |                |
|    total_timesteps      | 894000         |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -3.68e-07      |
|    explained_variance   | 0.137          |
|    learning_rate        | 0.0005         |
|    loss                 | 4.17e+04       |
|    n_updates            | 7229           |
|    policy_gradient_loss | 1.2e-09        |
|    value_loss           | 6.95e+04       |
--------------------------------------------
Eval num_timesteps=894500, episode_reward=774.63 +/- 649.32
Episode length: 34.74 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 894500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 874      |
|    time_elapsed    | 3236     |
|    total_timesteps | 894976   |
---------------------------------
Eval num_timesteps=895000, episode_reward=942.31 +/- 727.34
Episode length: 36.24 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 942       |
| time/                   |           |
|    total_timesteps      | 895000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.55e-09 |
|    explained_variance   | 0.0909    |
|    learning_rate        | 0.0005    |
|    loss                 | 8.03e+04  |
|    n_updates            | 7239      |
|    policy_gradient_loss | 3.21e-08  |
|    value_loss           | 1.34e+05  |
---------------------------------------
Eval num_timesteps=895500, episode_reward=814.45 +/- 694.26
Episode length: 35.00 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 895500   |
---------------------------------
Eval num_timesteps=896000, episode_reward=889.84 +/- 706.79
Episode length: 35.70 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 875      |
|    time_elapsed    | 3241     |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=896500, episode_reward=783.05 +/- 651.45
Episode length: 35.26 +/- 6.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 783       |
| time/                   |           |
|    total_timesteps      | 896500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.48e-13 |
|    explained_variance   | 0.164     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.81e+04  |
|    n_updates            | 7249      |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 7.31e+04  |
---------------------------------------
Eval num_timesteps=897000, episode_reward=829.67 +/- 647.79
Episode length: 35.48 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 876      |
|    time_elapsed    | 3245     |
|    total_timesteps | 897024   |
---------------------------------
Eval num_timesteps=897500, episode_reward=911.10 +/- 729.25
Episode length: 35.52 +/- 7.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 911       |
| time/                   |           |
|    total_timesteps      | 897500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.11e-10 |
|    explained_variance   | 0.0566    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.27e+04  |
|    n_updates            | 7259      |
|    policy_gradient_loss | 5.01e-10  |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=898000, episode_reward=968.74 +/- 735.99
Episode length: 35.98 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 898000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 877      |
|    time_elapsed    | 3248     |
|    total_timesteps | 898048   |
---------------------------------
Eval num_timesteps=898500, episode_reward=921.70 +/- 723.49
Episode length: 35.70 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 922       |
| time/                   |           |
|    total_timesteps      | 898500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.14e-12 |
|    explained_variance   | 0.0674    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.1e+04   |
|    n_updates            | 7269      |
|    policy_gradient_loss | 1.25e-09  |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=899000, episode_reward=694.06 +/- 607.96
Episode length: 34.52 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 878      |
|    time_elapsed    | 3252     |
|    total_timesteps | 899072   |
---------------------------------
Eval num_timesteps=899500, episode_reward=804.39 +/- 672.39
Episode length: 35.20 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 804       |
| time/                   |           |
|    total_timesteps      | 899500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-22 |
|    explained_variance   | 0.0615    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.18e+04  |
|    n_updates            | 7279      |
|    policy_gradient_loss | -2.55e-09 |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=900000, episode_reward=789.20 +/- 668.53
Episode length: 34.40 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 879      |
|    time_elapsed    | 3255     |
|    total_timesteps | 900096   |
---------------------------------
Eval num_timesteps=900500, episode_reward=733.61 +/- 591.10
Episode length: 34.72 +/- 5.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 734       |
| time/                   |           |
|    total_timesteps      | 900500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.51e-19 |
|    explained_variance   | 0.178     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.96e+04  |
|    n_updates            | 7289      |
|    policy_gradient_loss | -3.49e-10 |
|    value_loss           | 1.63e+05  |
---------------------------------------
Eval num_timesteps=901000, episode_reward=872.65 +/- 674.87
Episode length: 35.70 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 880      |
|    time_elapsed    | 3258     |
|    total_timesteps | 901120   |
---------------------------------
Eval num_timesteps=901500, episode_reward=938.32 +/- 657.26
Episode length: 36.68 +/- 5.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 938       |
| time/                   |           |
|    total_timesteps      | 901500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.05e-10 |
|    explained_variance   | 0.0959    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.16e+04  |
|    n_updates            | 7299      |
|    policy_gradient_loss | 1.86e-09  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=902000, episode_reward=984.82 +/- 747.72
Episode length: 36.16 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 985      |
| time/              |          |
|    total_timesteps | 902000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 881      |
|    time_elapsed    | 3262     |
|    total_timesteps | 902144   |
---------------------------------
Eval num_timesteps=902500, episode_reward=690.90 +/- 575.45
Episode length: 34.22 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 691       |
| time/                   |           |
|    total_timesteps      | 902500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.93e-11 |
|    explained_variance   | 0.0211    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.97e+04  |
|    n_updates            | 7309      |
|    policy_gradient_loss | 2.63e-09  |
|    value_loss           | 1.06e+05  |
---------------------------------------
Eval num_timesteps=903000, episode_reward=780.15 +/- 697.35
Episode length: 34.98 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 885      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 882      |
|    time_elapsed    | 3265     |
|    total_timesteps | 903168   |
---------------------------------
Eval num_timesteps=903500, episode_reward=880.16 +/- 642.97
Episode length: 36.76 +/- 5.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 880       |
| time/                   |           |
|    total_timesteps      | 903500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-11 |
|    explained_variance   | 0.0753    |
|    learning_rate        | 0.0005    |
|    loss                 | 6.68e+04  |
|    n_updates            | 7319      |
|    policy_gradient_loss | 1.74e-09  |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=904000, episode_reward=733.21 +/- 615.48
Episode length: 34.76 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 904000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 869      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 883      |
|    time_elapsed    | 3269     |
|    total_timesteps | 904192   |
---------------------------------
Eval num_timesteps=904500, episode_reward=822.52 +/- 726.77
Episode length: 34.48 +/- 7.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 823       |
| time/                   |           |
|    total_timesteps      | 904500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-14 |
|    explained_variance   | 0.0954    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.01e+04  |
|    n_updates            | 7329      |
|    policy_gradient_loss | 3.38e-10  |
|    value_loss           | 9.07e+04  |
---------------------------------------
Eval num_timesteps=905000, episode_reward=729.08 +/- 635.53
Episode length: 35.24 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 884      |
|    time_elapsed    | 3272     |
|    total_timesteps | 905216   |
---------------------------------
Eval num_timesteps=905500, episode_reward=717.51 +/- 611.12
Episode length: 34.50 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 718       |
| time/                   |           |
|    total_timesteps      | 905500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.88e-10 |
|    explained_variance   | 0.148     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.33e+04  |
|    n_updates            | 7339      |
|    policy_gradient_loss | -2.69e-09 |
|    value_loss           | 9.68e+04  |
---------------------------------------
Eval num_timesteps=906000, episode_reward=771.48 +/- 691.32
Episode length: 34.50 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 885      |
|    time_elapsed    | 3276     |
|    total_timesteps | 906240   |
---------------------------------
Eval num_timesteps=906500, episode_reward=838.10 +/- 655.55
Episode length: 35.66 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 906500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-13 |
|    explained_variance   | 0.167     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.41e+04  |
|    n_updates            | 7349      |
|    policy_gradient_loss | -1e-09    |
|    value_loss           | 8.51e+04  |
---------------------------------------
Eval num_timesteps=907000, episode_reward=831.54 +/- 682.67
Episode length: 35.72 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 695      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 886      |
|    time_elapsed    | 3280     |
|    total_timesteps | 907264   |
---------------------------------
Eval num_timesteps=907500, episode_reward=869.44 +/- 660.85
Episode length: 36.16 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 869       |
| time/                   |           |
|    total_timesteps      | 907500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.05e-11 |
|    explained_variance   | 0.179     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.22e+04  |
|    n_updates            | 7359      |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 9.1e+04   |
---------------------------------------
Eval num_timesteps=908000, episode_reward=845.66 +/- 683.54
Episode length: 35.18 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 908000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 887      |
|    time_elapsed    | 3283     |
|    total_timesteps | 908288   |
---------------------------------
Eval num_timesteps=908500, episode_reward=763.35 +/- 649.07
Episode length: 35.00 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 763       |
| time/                   |           |
|    total_timesteps      | 908500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.01e-12 |
|    explained_variance   | 0.104     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.43e+04  |
|    n_updates            | 7369      |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 8.98e+04  |
---------------------------------------
Eval num_timesteps=909000, episode_reward=855.36 +/- 630.33
Episode length: 36.22 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 888      |
|    time_elapsed    | 3287     |
|    total_timesteps | 909312   |
---------------------------------
Eval num_timesteps=909500, episode_reward=812.13 +/- 614.96
Episode length: 35.94 +/- 5.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 909500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.66e-09 |
|    explained_variance   | 0.181     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.15e+04  |
|    n_updates            | 7379      |
|    policy_gradient_loss | 1.12e-09  |
|    value_loss           | 9.57e+04  |
---------------------------------------
Eval num_timesteps=910000, episode_reward=753.07 +/- 720.42
Episode length: 33.40 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 889      |
|    time_elapsed    | 3290     |
|    total_timesteps | 910336   |
---------------------------------
Eval num_timesteps=910500, episode_reward=876.79 +/- 694.92
Episode length: 35.62 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 910500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-10 |
|    explained_variance   | -0.00342  |
|    learning_rate        | 0.0005    |
|    loss                 | 6.02e+04  |
|    n_updates            | 7389      |
|    policy_gradient_loss | -4.2e-09  |
|    value_loss           | 1.31e+05  |
---------------------------------------
Eval num_timesteps=911000, episode_reward=788.08 +/- 648.67
Episode length: 35.60 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 969      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 890      |
|    time_elapsed    | 3294     |
|    total_timesteps | 911360   |
---------------------------------
Eval num_timesteps=911500, episode_reward=885.71 +/- 668.62
Episode length: 36.08 +/- 5.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 886       |
| time/                   |           |
|    total_timesteps      | 911500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.35e-09 |
|    explained_variance   | 0.135     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.18e+04  |
|    n_updates            | 7399      |
|    policy_gradient_loss | 2.42e-09  |
|    value_loss           | 9.03e+04  |
---------------------------------------
Eval num_timesteps=912000, episode_reward=959.65 +/- 710.79
Episode length: 36.78 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37       |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 276      |
|    iterations      | 891      |
|    time_elapsed    | 3297     |
|    total_timesteps | 912384   |
---------------------------------
Eval num_timesteps=912500, episode_reward=1037.60 +/- 759.63
Episode length: 36.66 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 1.04e+03  |
| time/                   |           |
|    total_timesteps      | 912500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.62e-12 |
|    explained_variance   | 0.162     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.63e+04  |
|    n_updates            | 7409      |
|    policy_gradient_loss | 1.83e-09  |
|    value_loss           | 9.36e+04  |
---------------------------------------
Eval num_timesteps=913000, episode_reward=1053.66 +/- 760.14
Episode length: 36.38 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 929      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 892      |
|    time_elapsed    | 3301     |
|    total_timesteps | 913408   |
---------------------------------
Eval num_timesteps=913500, episode_reward=984.77 +/- 736.01
Episode length: 36.26 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 985       |
| time/                   |           |
|    total_timesteps      | 913500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.85e-10 |
|    explained_variance   | 0.183     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.93e+04  |
|    n_updates            | 7419      |
|    policy_gradient_loss | 1.15e-09  |
|    value_loss           | 8.81e+04  |
---------------------------------------
Eval num_timesteps=914000, episode_reward=725.97 +/- 644.18
Episode length: 34.40 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 914000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 893      |
|    time_elapsed    | 3304     |
|    total_timesteps | 914432   |
---------------------------------
Eval num_timesteps=914500, episode_reward=936.10 +/- 742.94
Episode length: 35.30 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 936       |
| time/                   |           |
|    total_timesteps      | 914500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-10 |
|    explained_variance   | 0.14      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.08e+04  |
|    n_updates            | 7429      |
|    policy_gradient_loss | 1.43e-09  |
|    value_loss           | 7.69e+04  |
---------------------------------------
Eval num_timesteps=915000, episode_reward=641.29 +/- 525.76
Episode length: 34.02 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 894      |
|    time_elapsed    | 3308     |
|    total_timesteps | 915456   |
---------------------------------
Eval num_timesteps=915500, episode_reward=994.01 +/- 717.98
Episode length: 36.92 +/- 6.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 994       |
| time/                   |           |
|    total_timesteps      | 915500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.1e-15  |
|    explained_variance   | 0.0686    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.07e+04  |
|    n_updates            | 7439      |
|    policy_gradient_loss | -9.72e-10 |
|    value_loss           | 9.78e+04  |
---------------------------------------
Eval num_timesteps=916000, episode_reward=813.06 +/- 697.36
Episode length: 35.10 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 916000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 786      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 895      |
|    time_elapsed    | 3311     |
|    total_timesteps | 916480   |
---------------------------------
Eval num_timesteps=916500, episode_reward=974.00 +/- 701.59
Episode length: 37.00 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 974       |
| time/                   |           |
|    total_timesteps      | 916500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-14 |
|    explained_variance   | 0.222     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.71e+04  |
|    n_updates            | 7449      |
|    policy_gradient_loss | 8.5e-10   |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=917000, episode_reward=857.22 +/- 668.54
Episode length: 35.48 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=917500, episode_reward=897.46 +/- 737.50
Episode length: 35.44 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 917500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 896      |
|    time_elapsed    | 3316     |
|    total_timesteps | 917504   |
---------------------------------
Eval num_timesteps=918000, episode_reward=746.10 +/- 639.95
Episode length: 34.26 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 746       |
| time/                   |           |
|    total_timesteps      | 918000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1e-15    |
|    explained_variance   | 0.084     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.15e+04  |
|    n_updates            | 7459      |
|    policy_gradient_loss | -5.18e-10 |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=918500, episode_reward=741.36 +/- 661.16
Episode length: 34.14 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 918500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 897      |
|    time_elapsed    | 3320     |
|    total_timesteps | 918528   |
---------------------------------
Eval num_timesteps=919000, episode_reward=829.46 +/- 687.05
Episode length: 34.86 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 919000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.08e-13 |
|    explained_variance   | 0.203     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.05e+04  |
|    n_updates            | 7469      |
|    policy_gradient_loss | -2.33e-09 |
|    value_loss           | 1.3e+05   |
---------------------------------------
Eval num_timesteps=919500, episode_reward=788.57 +/- 680.14
Episode length: 35.06 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 919500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 898      |
|    time_elapsed    | 3323     |
|    total_timesteps | 919552   |
---------------------------------
Eval num_timesteps=920000, episode_reward=886.90 +/- 681.22
Episode length: 36.26 +/- 5.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 887       |
| time/                   |           |
|    total_timesteps      | 920000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.89e-16 |
|    explained_variance   | 0.148     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.7e+04   |
|    n_updates            | 7479      |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 1.08e+05  |
---------------------------------------
Eval num_timesteps=920500, episode_reward=825.32 +/- 687.31
Episode length: 34.62 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 920500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 855      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 899      |
|    time_elapsed    | 3327     |
|    total_timesteps | 920576   |
---------------------------------
Eval num_timesteps=921000, episode_reward=895.29 +/- 697.01
Episode length: 36.40 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 895       |
| time/                   |           |
|    total_timesteps      | 921000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.07e-13 |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.51e+04  |
|    n_updates            | 7489      |
|    policy_gradient_loss | 3.78e-10  |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=921500, episode_reward=808.11 +/- 620.15
Episode length: 36.14 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 921500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 900      |
|    time_elapsed    | 3330     |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=922000, episode_reward=756.81 +/- 658.14
Episode length: 35.16 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 757       |
| time/                   |           |
|    total_timesteps      | 922000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.92e-15 |
|    explained_variance   | 0.131     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.42e+04  |
|    n_updates            | 7499      |
|    policy_gradient_loss | 1.93e-09  |
|    value_loss           | 8.89e+04  |
---------------------------------------
Eval num_timesteps=922500, episode_reward=862.99 +/- 701.69
Episode length: 35.22 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 922500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 901      |
|    time_elapsed    | 3334     |
|    total_timesteps | 922624   |
---------------------------------
Eval num_timesteps=923000, episode_reward=837.30 +/- 706.63
Episode length: 35.12 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 837       |
| time/                   |           |
|    total_timesteps      | 923000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.74e-13 |
|    explained_variance   | 0.14      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.77e+04  |
|    n_updates            | 7509      |
|    policy_gradient_loss | 6.87e-10  |
|    value_loss           | 9.52e+04  |
---------------------------------------
Eval num_timesteps=923500, episode_reward=820.59 +/- 652.08
Episode length: 35.42 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 923500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 902      |
|    time_elapsed    | 3337     |
|    total_timesteps | 923648   |
---------------------------------
Eval num_timesteps=924000, episode_reward=874.79 +/- 691.40
Episode length: 35.68 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 924000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.06e-15 |
|    explained_variance   | 0.14      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.21e+04  |
|    n_updates            | 7519      |
|    policy_gradient_loss | 1.77e-09  |
|    value_loss           | 8.12e+04  |
---------------------------------------
Eval num_timesteps=924500, episode_reward=655.08 +/- 630.17
Episode length: 32.58 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 924500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 903      |
|    time_elapsed    | 3341     |
|    total_timesteps | 924672   |
---------------------------------
Eval num_timesteps=925000, episode_reward=755.12 +/- 613.60
Episode length: 35.72 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 925000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.79e-15 |
|    explained_variance   | 0.118     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.88e+04  |
|    n_updates            | 7529      |
|    policy_gradient_loss | -1.5e-09  |
|    value_loss           | 1.29e+05  |
---------------------------------------
Eval num_timesteps=925500, episode_reward=937.64 +/- 662.85
Episode length: 36.82 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 925500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 904      |
|    time_elapsed    | 3345     |
|    total_timesteps | 925696   |
---------------------------------
Eval num_timesteps=926000, episode_reward=901.14 +/- 724.98
Episode length: 35.50 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 901       |
| time/                   |           |
|    total_timesteps      | 926000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.97e-16 |
|    explained_variance   | 0.058     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.96e+04  |
|    n_updates            | 7539      |
|    policy_gradient_loss | -3.49e-09 |
|    value_loss           | 9.75e+04  |
---------------------------------------
Eval num_timesteps=926500, episode_reward=852.70 +/- 725.56
Episode length: 34.80 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 926500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 879      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 905      |
|    time_elapsed    | 3348     |
|    total_timesteps | 926720   |
---------------------------------
Eval num_timesteps=927000, episode_reward=847.11 +/- 666.17
Episode length: 35.52 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 847       |
| time/                   |           |
|    total_timesteps      | 927000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.86e-16 |
|    explained_variance   | 0.172     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.71e+04  |
|    n_updates            | 7549      |
|    policy_gradient_loss | -8.09e-10 |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=927500, episode_reward=803.18 +/- 588.33
Episode length: 35.88 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 927500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 927      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 906      |
|    time_elapsed    | 3352     |
|    total_timesteps | 927744   |
---------------------------------
Eval num_timesteps=928000, episode_reward=815.19 +/- 655.64
Episode length: 34.90 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 815       |
| time/                   |           |
|    total_timesteps      | 928000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.05e-16 |
|    explained_variance   | 0.146     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.38e+04  |
|    n_updates            | 7559      |
|    policy_gradient_loss | 2.64e-09  |
|    value_loss           | 1.13e+05  |
---------------------------------------
Eval num_timesteps=928500, episode_reward=986.86 +/- 730.31
Episode length: 36.54 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 928500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 914      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 907      |
|    time_elapsed    | 3355     |
|    total_timesteps | 928768   |
---------------------------------
Eval num_timesteps=929000, episode_reward=824.77 +/- 672.24
Episode length: 35.54 +/- 6.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.5     |
|    mean_reward          | 825      |
| time/                   |          |
|    total_timesteps      | 929000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.7e-10 |
|    explained_variance   | 0.24     |
|    learning_rate        | 0.0005   |
|    loss                 | 3.53e+04 |
|    n_updates            | 7569     |
|    policy_gradient_loss | 3.83e-09 |
|    value_loss           | 1.04e+05 |
--------------------------------------
Eval num_timesteps=929500, episode_reward=1090.04 +/- 743.81
Episode length: 37.38 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 929500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 908      |
|    time_elapsed    | 3359     |
|    total_timesteps | 929792   |
---------------------------------
Eval num_timesteps=930000, episode_reward=697.65 +/- 611.52
Episode length: 34.02 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 698       |
| time/                   |           |
|    total_timesteps      | 930000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.2e-09  |
|    explained_variance   | 0.0945    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.77e+04  |
|    n_updates            | 7579      |
|    policy_gradient_loss | -4.42e-10 |
|    value_loss           | 8.6e+04   |
---------------------------------------
Eval num_timesteps=930500, episode_reward=911.26 +/- 742.66
Episode length: 35.46 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 930500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 909      |
|    time_elapsed    | 3362     |
|    total_timesteps | 930816   |
---------------------------------
Eval num_timesteps=931000, episode_reward=664.01 +/- 607.67
Episode length: 34.06 +/- 5.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 664       |
| time/                   |           |
|    total_timesteps      | 931000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.13e-10 |
|    explained_variance   | 0.129     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.89e+04  |
|    n_updates            | 7589      |
|    policy_gradient_loss | -8.44e-10 |
|    value_loss           | 8.58e+04  |
---------------------------------------
Eval num_timesteps=931500, episode_reward=927.25 +/- 732.46
Episode length: 36.02 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 931500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 910      |
|    time_elapsed    | 3366     |
|    total_timesteps | 931840   |
---------------------------------
Eval num_timesteps=932000, episode_reward=993.02 +/- 766.98
Episode length: 35.80 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 993       |
| time/                   |           |
|    total_timesteps      | 932000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.13e-09 |
|    explained_variance   | 0.13      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.1e+04   |
|    n_updates            | 7599      |
|    policy_gradient_loss | 2.37e-09  |
|    value_loss           | 7.92e+04  |
---------------------------------------
Eval num_timesteps=932500, episode_reward=887.21 +/- 680.56
Episode length: 36.08 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 932500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 911      |
|    time_elapsed    | 3369     |
|    total_timesteps | 932864   |
---------------------------------
Eval num_timesteps=933000, episode_reward=819.78 +/- 697.43
Episode length: 34.54 +/- 7.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 820       |
| time/                   |           |
|    total_timesteps      | 933000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.61e-10 |
|    explained_variance   | 0.138     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.57e+04  |
|    n_updates            | 7609      |
|    policy_gradient_loss | -1.2e-09  |
|    value_loss           | 7.99e+04  |
---------------------------------------
Eval num_timesteps=933500, episode_reward=802.13 +/- 705.87
Episode length: 34.54 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 933500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 912      |
|    time_elapsed    | 3373     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=934000, episode_reward=776.90 +/- 673.93
Episode length: 35.18 +/- 7.72
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.2           |
|    mean_reward          | 777            |
| time/                   |                |
|    total_timesteps      | 934000         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -5.83e-07      |
|    explained_variance   | 0.203          |
|    learning_rate        | 0.0005         |
|    loss                 | 4.46e+04       |
|    n_updates            | 7619           |
|    policy_gradient_loss | -1.18e-08      |
|    value_loss           | 8.94e+04       |
--------------------------------------------
Eval num_timesteps=934500, episode_reward=946.54 +/- 723.15
Episode length: 35.72 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 934500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 884      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 913      |
|    time_elapsed    | 3376     |
|    total_timesteps | 934912   |
---------------------------------
Eval num_timesteps=935000, episode_reward=860.37 +/- 682.76
Episode length: 36.14 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 860       |
| time/                   |           |
|    total_timesteps      | 935000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.47e-08 |
|    explained_variance   | 0.00366   |
|    learning_rate        | 0.0005    |
|    loss                 | 7.48e+04  |
|    n_updates            | 7629      |
|    policy_gradient_loss | -2.56e-08 |
|    value_loss           | 1.61e+05  |
---------------------------------------
Eval num_timesteps=935500, episode_reward=822.34 +/- 700.43
Episode length: 35.06 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 935500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 881      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 914      |
|    time_elapsed    | 3380     |
|    total_timesteps | 935936   |
---------------------------------
Eval num_timesteps=936000, episode_reward=869.53 +/- 677.01
Episode length: 35.80 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 870           |
| time/                   |               |
|    total_timesteps      | 936000        |
| train/                  |               |
|    approx_kl            | 2.7151546e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -6.07e-05     |
|    explained_variance   | 0.156         |
|    learning_rate        | 0.0005        |
|    loss                 | 3.19e+04      |
|    n_updates            | 7639          |
|    policy_gradient_loss | 3.98e-06      |
|    value_loss           | 6.35e+04      |
-------------------------------------------
Eval num_timesteps=936500, episode_reward=837.36 +/- 700.68
Episode length: 35.20 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 936500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 878      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 915      |
|    time_elapsed    | 3383     |
|    total_timesteps | 936960   |
---------------------------------
Eval num_timesteps=937000, episode_reward=755.18 +/- 605.44
Episode length: 35.38 +/- 6.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 755           |
| time/                   |               |
|    total_timesteps      | 937000        |
| train/                  |               |
|    approx_kl            | 5.6112185e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -6.4e-06      |
|    explained_variance   | 0.115         |
|    learning_rate        | 0.0005        |
|    loss                 | 4.97e+04      |
|    n_updates            | 7649          |
|    policy_gradient_loss | 1.61e-05      |
|    value_loss           | 8.67e+04      |
-------------------------------------------
Eval num_timesteps=937500, episode_reward=876.15 +/- 687.09
Episode length: 35.80 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 937500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 916      |
|    time_elapsed    | 3387     |
|    total_timesteps | 937984   |
---------------------------------
Eval num_timesteps=938000, episode_reward=847.79 +/- 702.64
Episode length: 34.94 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 848       |
| time/                   |           |
|    total_timesteps      | 938000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.59e-13 |
|    explained_variance   | 0.0979    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.04e+04  |
|    n_updates            | 7659      |
|    policy_gradient_loss | 2.91e-11  |
|    value_loss           | 7.17e+04  |
---------------------------------------
Eval num_timesteps=938500, episode_reward=955.63 +/- 741.04
Episode length: 36.10 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 938500   |
---------------------------------
Eval num_timesteps=939000, episode_reward=843.75 +/- 651.64
Episode length: 35.88 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 917      |
|    time_elapsed    | 3392     |
|    total_timesteps | 939008   |
---------------------------------
Eval num_timesteps=939500, episode_reward=883.95 +/- 705.02
Episode length: 35.30 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 884       |
| time/                   |           |
|    total_timesteps      | 939500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.28e-20 |
|    explained_variance   | 0.148     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.33e+04  |
|    n_updates            | 7669      |
|    policy_gradient_loss | -1.06e-09 |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=940000, episode_reward=774.92 +/- 683.88
Episode length: 34.42 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 907      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 918      |
|    time_elapsed    | 3395     |
|    total_timesteps | 940032   |
---------------------------------
Eval num_timesteps=940500, episode_reward=742.02 +/- 676.18
Episode length: 33.78 +/- 7.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 742       |
| time/                   |           |
|    total_timesteps      | 940500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.47e-15 |
|    explained_variance   | 0.329     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.55e+04  |
|    n_updates            | 7679      |
|    policy_gradient_loss | -4.44e-09 |
|    value_loss           | 1.22e+05  |
---------------------------------------
Eval num_timesteps=941000, episode_reward=916.25 +/- 710.15
Episode length: 36.10 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 941000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 970      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 919      |
|    time_elapsed    | 3399     |
|    total_timesteps | 941056   |
---------------------------------
Eval num_timesteps=941500, episode_reward=779.56 +/- 604.97
Episode length: 35.78 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 941500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.71e-19 |
|    explained_variance   | 0.126     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.07e+04  |
|    n_updates            | 7689      |
|    policy_gradient_loss | 2.05e-09  |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=942000, episode_reward=782.24 +/- 593.78
Episode length: 36.06 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 936      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 920      |
|    time_elapsed    | 3402     |
|    total_timesteps | 942080   |
---------------------------------
Eval num_timesteps=942500, episode_reward=937.55 +/- 738.47
Episode length: 35.68 +/- 7.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 938       |
| time/                   |           |
|    total_timesteps      | 942500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.57e-14 |
|    explained_variance   | 0.304     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.52e+04  |
|    n_updates            | 7699      |
|    policy_gradient_loss | -9.43e-10 |
|    value_loss           | 1.21e+05  |
---------------------------------------
Eval num_timesteps=943000, episode_reward=842.04 +/- 639.00
Episode length: 36.02 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 943000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 921      |
|    time_elapsed    | 3406     |
|    total_timesteps | 943104   |
---------------------------------
Eval num_timesteps=943500, episode_reward=729.50 +/- 613.88
Episode length: 34.94 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 729       |
| time/                   |           |
|    total_timesteps      | 943500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.38e-11 |
|    explained_variance   | 0.123     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.63e+04  |
|    n_updates            | 7709      |
|    policy_gradient_loss | -3.73e-10 |
|    value_loss           | 9.55e+04  |
---------------------------------------
Eval num_timesteps=944000, episode_reward=884.42 +/- 738.71
Episode length: 35.22 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 922      |
|    time_elapsed    | 3409     |
|    total_timesteps | 944128   |
---------------------------------
Eval num_timesteps=944500, episode_reward=871.38 +/- 698.58
Episode length: 35.48 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 944500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.47e-12 |
|    explained_variance   | 0.0305    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.01e+04  |
|    n_updates            | 7719      |
|    policy_gradient_loss | 2.43e-09  |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=945000, episode_reward=816.38 +/- 639.58
Episode length: 35.94 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 945000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 923      |
|    time_elapsed    | 3413     |
|    total_timesteps | 945152   |
---------------------------------
Eval num_timesteps=945500, episode_reward=876.54 +/- 718.61
Episode length: 34.88 +/- 7.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 945500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.16e-10 |
|    explained_variance   | 0.233     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.32e+04  |
|    n_updates            | 7729      |
|    policy_gradient_loss | -7.1e-10  |
|    value_loss           | 9.29e+04  |
---------------------------------------
Eval num_timesteps=946000, episode_reward=732.17 +/- 621.26
Episode length: 34.34 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 924      |
|    time_elapsed    | 3416     |
|    total_timesteps | 946176   |
---------------------------------
Eval num_timesteps=946500, episode_reward=913.22 +/- 733.97
Episode length: 35.28 +/- 6.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 913           |
| time/                   |               |
|    total_timesteps      | 946500        |
| train/                  |               |
|    approx_kl            | -2.910383e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -1.32e-06     |
|    explained_variance   | 0.295         |
|    learning_rate        | 0.0005        |
|    loss                 | 2.57e+04      |
|    n_updates            | 7739          |
|    policy_gradient_loss | 5.48e-09      |
|    value_loss           | 5.65e+04      |
-------------------------------------------
Eval num_timesteps=947000, episode_reward=784.76 +/- 720.19
Episode length: 34.12 +/- 7.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 947000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 727      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 925      |
|    time_elapsed    | 3420     |
|    total_timesteps | 947200   |
---------------------------------
Eval num_timesteps=947500, episode_reward=839.78 +/- 703.54
Episode length: 35.08 +/- 6.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 840       |
| time/                   |           |
|    total_timesteps      | 947500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.08e-08 |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.0005    |
|    loss                 | 2.97e+04  |
|    n_updates            | 7749      |
|    policy_gradient_loss | -5.4e-09  |
|    value_loss           | 6.91e+04  |
---------------------------------------
Eval num_timesteps=948000, episode_reward=806.75 +/- 674.50
Episode length: 35.18 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 712      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 926      |
|    time_elapsed    | 3423     |
|    total_timesteps | 948224   |
---------------------------------
Eval num_timesteps=948500, episode_reward=742.92 +/- 630.21
Episode length: 34.96 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 948500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-10 |
|    explained_variance   | 0.11      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.7e+04   |
|    n_updates            | 7759      |
|    policy_gradient_loss | -1.49e-09 |
|    value_loss           | 9.04e+04  |
---------------------------------------
Eval num_timesteps=949000, episode_reward=700.86 +/- 590.29
Episode length: 34.74 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 949000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 927      |
|    time_elapsed    | 3427     |
|    total_timesteps | 949248   |
---------------------------------
Eval num_timesteps=949500, episode_reward=825.91 +/- 682.48
Episode length: 35.54 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 949500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.01e-17 |
|    explained_variance   | 0.047     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.61e+04  |
|    n_updates            | 7769      |
|    policy_gradient_loss | 1.07e-09  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=950000, episode_reward=751.37 +/- 669.40
Episode length: 34.26 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 928      |
|    time_elapsed    | 3430     |
|    total_timesteps | 950272   |
---------------------------------
Eval num_timesteps=950500, episode_reward=780.32 +/- 673.40
Episode length: 34.40 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 950500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.74e-15 |
|    explained_variance   | 0.352     |
|    learning_rate        | 0.0005    |
|    loss                 | 8.81e+04  |
|    n_updates            | 7779      |
|    policy_gradient_loss | 4.58e-09  |
|    value_loss           | 1.65e+05  |
---------------------------------------
Eval num_timesteps=951000, episode_reward=721.40 +/- 598.97
Episode length: 34.70 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 951000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 929      |
|    time_elapsed    | 3434     |
|    total_timesteps | 951296   |
---------------------------------
Eval num_timesteps=951500, episode_reward=881.28 +/- 748.32
Episode length: 34.64 +/- 7.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 951500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.29e-14 |
|    explained_variance   | 0.201     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.96e+04  |
|    n_updates            | 7789      |
|    policy_gradient_loss | -4.89e-10 |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=952000, episode_reward=861.69 +/- 720.39
Episode length: 35.36 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 930      |
|    time_elapsed    | 3437     |
|    total_timesteps | 952320   |
---------------------------------
Eval num_timesteps=952500, episode_reward=820.34 +/- 656.55
Episode length: 35.18 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 820       |
| time/                   |           |
|    total_timesteps      | 952500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.23e-13 |
|    explained_variance   | 0.289     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.52e+04  |
|    n_updates            | 7799      |
|    policy_gradient_loss | -3.6e-09  |
|    value_loss           | 1.33e+05  |
---------------------------------------
Eval num_timesteps=953000, episode_reward=854.46 +/- 736.74
Episode length: 34.64 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 953000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 931      |
|    time_elapsed    | 3441     |
|    total_timesteps | 953344   |
---------------------------------
Eval num_timesteps=953500, episode_reward=773.31 +/- 643.00
Episode length: 34.76 +/- 6.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 953500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.13e-09 |
|    explained_variance   | 0.197     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.11e+04  |
|    n_updates            | 7809      |
|    policy_gradient_loss | 1.28e-09  |
|    value_loss           | 8.79e+04  |
---------------------------------------
Eval num_timesteps=954000, episode_reward=785.15 +/- 655.79
Episode length: 34.34 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 932      |
|    time_elapsed    | 3444     |
|    total_timesteps | 954368   |
---------------------------------
Eval num_timesteps=954500, episode_reward=842.56 +/- 648.90
Episode length: 35.64 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 843       |
| time/                   |           |
|    total_timesteps      | 954500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-10 |
|    explained_variance   | 0.0965    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.25e+04  |
|    n_updates            | 7819      |
|    policy_gradient_loss | -3.03e-09 |
|    value_loss           | 9.05e+04  |
---------------------------------------
Eval num_timesteps=955000, episode_reward=836.99 +/- 715.01
Episode length: 35.20 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 955000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 933      |
|    time_elapsed    | 3448     |
|    total_timesteps | 955392   |
---------------------------------
Eval num_timesteps=955500, episode_reward=855.90 +/- 706.47
Episode length: 35.20 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 856       |
| time/                   |           |
|    total_timesteps      | 955500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.92e-09 |
|    explained_variance   | 0.0598    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.84e+04  |
|    n_updates            | 7829      |
|    policy_gradient_loss | 1.16e-11  |
|    value_loss           | 9.34e+04  |
---------------------------------------
Eval num_timesteps=956000, episode_reward=670.41 +/- 590.48
Episode length: 33.72 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 934      |
|    time_elapsed    | 3451     |
|    total_timesteps | 956416   |
---------------------------------
Eval num_timesteps=956500, episode_reward=719.44 +/- 611.80
Episode length: 34.26 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 719       |
| time/                   |           |
|    total_timesteps      | 956500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.91e-10 |
|    explained_variance   | 0.0654    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.38e+04  |
|    n_updates            | 7839      |
|    policy_gradient_loss | -3.53e-09 |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=957000, episode_reward=829.78 +/- 694.95
Episode length: 35.22 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 957000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 935      |
|    time_elapsed    | 3454     |
|    total_timesteps | 957440   |
---------------------------------
Eval num_timesteps=957500, episode_reward=742.92 +/- 637.94
Episode length: 34.78 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 957500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.12e-15 |
|    explained_variance   | 0.159     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.26e+04  |
|    n_updates            | 7849      |
|    policy_gradient_loss | 1.26e-09  |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=958000, episode_reward=826.15 +/- 634.77
Episode length: 35.96 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 860      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 936      |
|    time_elapsed    | 3458     |
|    total_timesteps | 958464   |
---------------------------------
Eval num_timesteps=958500, episode_reward=710.25 +/- 633.32
Episode length: 34.38 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 710       |
| time/                   |           |
|    total_timesteps      | 958500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.12e-15 |
|    explained_variance   | 0.237     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.72e+04  |
|    n_updates            | 7859      |
|    policy_gradient_loss | 8.27e-10  |
|    value_loss           | 1.16e+05  |
---------------------------------------
Eval num_timesteps=959000, episode_reward=736.99 +/- 645.81
Episode length: 33.96 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 959000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 913      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 937      |
|    time_elapsed    | 3461     |
|    total_timesteps | 959488   |
---------------------------------
Eval num_timesteps=959500, episode_reward=799.22 +/- 648.14
Episode length: 35.68 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 959500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.49e-16 |
|    explained_variance   | 0.0796    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.86e+04  |
|    n_updates            | 7869      |
|    policy_gradient_loss | -1.51e-09 |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=960000, episode_reward=952.19 +/- 729.56
Episode length: 36.22 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=960500, episode_reward=937.15 +/- 702.36
Episode length: 36.66 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 960500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 878      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 938      |
|    time_elapsed    | 3466     |
|    total_timesteps | 960512   |
---------------------------------
Eval num_timesteps=961000, episode_reward=770.49 +/- 714.17
Episode length: 34.06 +/- 7.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 770       |
| time/                   |           |
|    total_timesteps      | 961000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.13e-13 |
|    explained_variance   | 0.225     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.76e+04  |
|    n_updates            | 7879      |
|    policy_gradient_loss | 2.02e-09  |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=961500, episode_reward=931.70 +/- 694.58
Episode length: 36.36 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 961500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 898      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 939      |
|    time_elapsed    | 3470     |
|    total_timesteps | 961536   |
---------------------------------
Eval num_timesteps=962000, episode_reward=646.22 +/- 543.49
Episode length: 34.38 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 646       |
| time/                   |           |
|    total_timesteps      | 962000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.37e-16 |
|    explained_variance   | 0.215     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.47e+04  |
|    n_updates            | 7889      |
|    policy_gradient_loss | 4.19e-10  |
|    value_loss           | 9.19e+04  |
---------------------------------------
Eval num_timesteps=962500, episode_reward=732.00 +/- 643.89
Episode length: 34.68 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 962500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 885      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 940      |
|    time_elapsed    | 3473     |
|    total_timesteps | 962560   |
---------------------------------
Eval num_timesteps=963000, episode_reward=848.50 +/- 706.14
Episode length: 35.02 +/- 7.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 848       |
| time/                   |           |
|    total_timesteps      | 963000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.36e-18 |
|    explained_variance   | 0.113     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.78e+04  |
|    n_updates            | 7899      |
|    policy_gradient_loss | -1.68e-09 |
|    value_loss           | 1.11e+05  |
---------------------------------------
Eval num_timesteps=963500, episode_reward=956.94 +/- 718.74
Episode length: 36.32 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 963500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 941      |
|    time_elapsed    | 3477     |
|    total_timesteps | 963584   |
---------------------------------
Eval num_timesteps=964000, episode_reward=765.88 +/- 684.28
Episode length: 34.84 +/- 6.77
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.8     |
|    mean_reward          | 766      |
| time/                   |          |
|    total_timesteps      | 964000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.5e-20 |
|    explained_variance   | 0.256    |
|    learning_rate        | 0.0005   |
|    loss                 | 6.52e+04 |
|    n_updates            | 7909     |
|    policy_gradient_loss | 1.77e-09 |
|    value_loss           | 1.51e+05 |
--------------------------------------
Eval num_timesteps=964500, episode_reward=993.51 +/- 771.37
Episode length: 36.04 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 964500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 888      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 942      |
|    time_elapsed    | 3480     |
|    total_timesteps | 964608   |
---------------------------------
Eval num_timesteps=965000, episode_reward=782.79 +/- 701.61
Episode length: 34.38 +/- 7.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 783       |
| time/                   |           |
|    total_timesteps      | 965000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.11e-24 |
|    explained_variance   | 0.151     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.69e+04  |
|    n_updates            | 7919      |
|    policy_gradient_loss | 1.11e-09  |
|    value_loss           | 9.37e+04  |
---------------------------------------
Eval num_timesteps=965500, episode_reward=886.75 +/- 737.91
Episode length: 35.06 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 965500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 914      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 943      |
|    time_elapsed    | 3484     |
|    total_timesteps | 965632   |
---------------------------------
Eval num_timesteps=966000, episode_reward=837.44 +/- 662.34
Episode length: 35.80 +/- 5.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 837       |
| time/                   |           |
|    total_timesteps      | 966000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.62e-16 |
|    explained_variance   | 0.222     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.77e+04  |
|    n_updates            | 7929      |
|    policy_gradient_loss | 1.77e-09  |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=966500, episode_reward=867.32 +/- 678.20
Episode length: 35.60 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 966500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 944      |
|    time_elapsed    | 3487     |
|    total_timesteps | 966656   |
---------------------------------
Eval num_timesteps=967000, episode_reward=1082.28 +/- 751.18
Episode length: 36.88 +/- 6.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 1.08e+03  |
| time/                   |           |
|    total_timesteps      | 967000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.2e-11  |
|    explained_variance   | 0.219     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.06e+04  |
|    n_updates            | 7939      |
|    policy_gradient_loss | -2.13e-09 |
|    value_loss           | 8.98e+04  |
---------------------------------------
Eval num_timesteps=967500, episode_reward=771.48 +/- 613.52
Episode length: 35.40 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 967500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 914      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 945      |
|    time_elapsed    | 3491     |
|    total_timesteps | 967680   |
---------------------------------
Eval num_timesteps=968000, episode_reward=768.07 +/- 660.25
Episode length: 34.58 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 968000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.48e-12 |
|    explained_variance   | 0.132     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.96e+04  |
|    n_updates            | 7949      |
|    policy_gradient_loss | 2.74e-09  |
|    value_loss           | 9.53e+04  |
---------------------------------------
Eval num_timesteps=968500, episode_reward=740.65 +/- 706.75
Episode length: 33.04 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 968500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 946      |
|    time_elapsed    | 3494     |
|    total_timesteps | 968704   |
---------------------------------
Eval num_timesteps=969000, episode_reward=701.30 +/- 659.06
Episode length: 33.66 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 969000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.47e-10 |
|    explained_variance   | 0.257     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.02e+04  |
|    n_updates            | 7959      |
|    policy_gradient_loss | 1.14e-09  |
|    value_loss           | 8.58e+04  |
---------------------------------------
Eval num_timesteps=969500, episode_reward=918.41 +/- 744.92
Episode length: 35.72 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 969500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 947      |
|    time_elapsed    | 3498     |
|    total_timesteps | 969728   |
---------------------------------
Eval num_timesteps=970000, episode_reward=706.90 +/- 617.88
Episode length: 33.64 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 707       |
| time/                   |           |
|    total_timesteps      | 970000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.98e-11 |
|    explained_variance   | 0.192     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.54e+04  |
|    n_updates            | 7969      |
|    policy_gradient_loss | -7.57e-10 |
|    value_loss           | 6.9e+04   |
---------------------------------------
Eval num_timesteps=970500, episode_reward=906.28 +/- 686.20
Episode length: 36.56 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 970500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 948      |
|    time_elapsed    | 3501     |
|    total_timesteps | 970752   |
---------------------------------
Eval num_timesteps=971000, episode_reward=815.81 +/- 681.51
Episode length: 34.82 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 971000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.2e-09  |
|    explained_variance   | 0.21      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.36e+04  |
|    n_updates            | 7979      |
|    policy_gradient_loss | -1.31e-09 |
|    value_loss           | 8.64e+04  |
---------------------------------------
Eval num_timesteps=971500, episode_reward=662.05 +/- 586.75
Episode length: 33.66 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 971500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 949      |
|    time_elapsed    | 3505     |
|    total_timesteps | 971776   |
---------------------------------
Eval num_timesteps=972000, episode_reward=622.41 +/- 499.42
Episode length: 34.22 +/- 5.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 622       |
| time/                   |           |
|    total_timesteps      | 972000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.9e-10  |
|    explained_variance   | 0.169     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.75e+04  |
|    n_updates            | 7989      |
|    policy_gradient_loss | -3.93e-09 |
|    value_loss           | 8.07e+04  |
---------------------------------------
Eval num_timesteps=972500, episode_reward=864.28 +/- 677.15
Episode length: 35.56 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 972500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 950      |
|    time_elapsed    | 3508     |
|    total_timesteps | 972800   |
---------------------------------
Eval num_timesteps=973000, episode_reward=884.54 +/- 718.96
Episode length: 35.78 +/- 6.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 885       |
| time/                   |           |
|    total_timesteps      | 973000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.45e-14 |
|    explained_variance   | 0.154     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.27e+04  |
|    n_updates            | 7999      |
|    policy_gradient_loss | -1.61e-09 |
|    value_loss           | 9.83e+04  |
---------------------------------------
Eval num_timesteps=973500, episode_reward=688.84 +/- 534.13
Episode length: 35.26 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 973500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 951      |
|    time_elapsed    | 3512     |
|    total_timesteps | 973824   |
---------------------------------
Eval num_timesteps=974000, episode_reward=669.88 +/- 548.24
Episode length: 34.58 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 670       |
| time/                   |           |
|    total_timesteps      | 974000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-12 |
|    explained_variance   | 0.276     |
|    learning_rate        | 0.0005    |
|    loss                 | 7.12e+04  |
|    n_updates            | 8009      |
|    policy_gradient_loss | 1.72e-09  |
|    value_loss           | 1.15e+05  |
---------------------------------------
Eval num_timesteps=974500, episode_reward=1009.45 +/- 741.32
Episode length: 36.92 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 974500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 869      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 952      |
|    time_elapsed    | 3515     |
|    total_timesteps | 974848   |
---------------------------------
Eval num_timesteps=975000, episode_reward=707.18 +/- 672.11
Episode length: 33.58 +/- 7.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 707       |
| time/                   |           |
|    total_timesteps      | 975000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.45e-09 |
|    explained_variance   | 0.103     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.85e+04  |
|    n_updates            | 8019      |
|    policy_gradient_loss | -1.78e-09 |
|    value_loss           | 1.03e+05  |
---------------------------------------
Eval num_timesteps=975500, episode_reward=878.78 +/- 682.29
Episode length: 36.18 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 975500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 953      |
|    time_elapsed    | 3519     |
|    total_timesteps | 975872   |
---------------------------------
Eval num_timesteps=976000, episode_reward=837.94 +/- 675.28
Episode length: 34.98 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 976000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-11 |
|    explained_variance   | 0.0879    |
|    learning_rate        | 0.0005    |
|    loss                 | 3.14e+04  |
|    n_updates            | 8029      |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 8.62e+04  |
---------------------------------------
Eval num_timesteps=976500, episode_reward=716.99 +/- 629.32
Episode length: 34.62 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 976500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 954      |
|    time_elapsed    | 3522     |
|    total_timesteps | 976896   |
---------------------------------
Eval num_timesteps=977000, episode_reward=952.00 +/- 677.99
Episode length: 37.14 +/- 5.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 952       |
| time/                   |           |
|    total_timesteps      | 977000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.89e-10 |
|    explained_variance   | 0.116     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.33e+04  |
|    n_updates            | 8039      |
|    policy_gradient_loss | -3.12e-09 |
|    value_loss           | 1.01e+05  |
---------------------------------------
Eval num_timesteps=977500, episode_reward=853.17 +/- 672.21
Episode length: 35.72 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 977500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.2     |
|    ep_rew_mean     | 672      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 955      |
|    time_elapsed    | 3526     |
|    total_timesteps | 977920   |
---------------------------------
Eval num_timesteps=978000, episode_reward=624.43 +/- 563.89
Episode length: 33.58 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 624       |
| time/                   |           |
|    total_timesteps      | 978000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-11 |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.0005    |
|    loss                 | 3.42e+04  |
|    n_updates            | 8049      |
|    policy_gradient_loss | -4.54e-10 |
|    value_loss           | 6.79e+04  |
---------------------------------------
Eval num_timesteps=978500, episode_reward=780.45 +/- 664.89
Episode length: 34.10 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 978500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 956      |
|    time_elapsed    | 3529     |
|    total_timesteps | 978944   |
---------------------------------
Eval num_timesteps=979000, episode_reward=805.79 +/- 698.33
Episode length: 34.66 +/- 6.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.7     |
|    mean_reward          | 806      |
| time/                   |          |
|    total_timesteps      | 979000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.8e-17 |
|    explained_variance   | 0.179    |
|    learning_rate        | 0.0005   |
|    loss                 | 4.1e+04  |
|    n_updates            | 8059     |
|    policy_gradient_loss | 2.56e-10 |
|    value_loss           | 8.65e+04 |
--------------------------------------
Eval num_timesteps=979500, episode_reward=703.99 +/- 648.12
Episode length: 34.10 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 979500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 957      |
|    time_elapsed    | 3532     |
|    total_timesteps | 979968   |
---------------------------------
Eval num_timesteps=980000, episode_reward=924.30 +/- 704.77
Episode length: 35.40 +/- 7.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 924       |
| time/                   |           |
|    total_timesteps      | 980000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.84e-13 |
|    explained_variance   | 0.23      |
|    learning_rate        | 0.0005    |
|    loss                 | 5.84e+04  |
|    n_updates            | 8069      |
|    policy_gradient_loss | 2.7e-09   |
|    value_loss           | 1.25e+05  |
---------------------------------------
Eval num_timesteps=980500, episode_reward=919.07 +/- 718.27
Episode length: 35.94 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 980500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 843      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 958      |
|    time_elapsed    | 3536     |
|    total_timesteps | 980992   |
---------------------------------
Eval num_timesteps=981000, episode_reward=862.40 +/- 720.19
Episode length: 35.16 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 862       |
| time/                   |           |
|    total_timesteps      | 981000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.89e-14 |
|    explained_variance   | 0.103     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.18e+04  |
|    n_updates            | 8079      |
|    policy_gradient_loss | 2.15e-10  |
|    value_loss           | 1.07e+05  |
---------------------------------------
Eval num_timesteps=981500, episode_reward=693.21 +/- 564.16
Episode length: 34.88 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 981500   |
---------------------------------
Eval num_timesteps=982000, episode_reward=824.31 +/- 651.95
Episode length: 35.64 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 959      |
|    time_elapsed    | 3541     |
|    total_timesteps | 982016   |
---------------------------------
Eval num_timesteps=982500, episode_reward=992.72 +/- 758.01
Episode length: 36.24 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 993       |
| time/                   |           |
|    total_timesteps      | 982500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-12 |
|    explained_variance   | 0.294     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.41e+04  |
|    n_updates            | 8089      |
|    policy_gradient_loss | -5.97e-09 |
|    value_loss           | 1.38e+05  |
---------------------------------------
Eval num_timesteps=983000, episode_reward=829.83 +/- 693.91
Episode length: 35.26 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 960      |
|    time_elapsed    | 3544     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=983500, episode_reward=828.65 +/- 706.03
Episode length: 35.12 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 983500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.56e-12 |
|    explained_variance   | 0.0839    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.73e+04  |
|    n_updates            | 8099      |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=984000, episode_reward=755.08 +/- 631.16
Episode length: 35.48 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 731      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 961      |
|    time_elapsed    | 3548     |
|    total_timesteps | 984064   |
---------------------------------
Eval num_timesteps=984500, episode_reward=895.19 +/- 718.13
Episode length: 35.28 +/- 7.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 895       |
| time/                   |           |
|    total_timesteps      | 984500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.01e-12 |
|    explained_variance   | 0.249     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.4e+04   |
|    n_updates            | 8109      |
|    policy_gradient_loss | -4.38e-09 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=985000, episode_reward=834.38 +/- 631.22
Episode length: 36.20 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 962      |
|    time_elapsed    | 3551     |
|    total_timesteps | 985088   |
---------------------------------
Eval num_timesteps=985500, episode_reward=733.16 +/- 603.38
Episode length: 35.40 +/- 6.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 733       |
| time/                   |           |
|    total_timesteps      | 985500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.52e-17 |
|    explained_variance   | 0.201     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.89e+04  |
|    n_updates            | 8119      |
|    policy_gradient_loss | -1.16e-09 |
|    value_loss           | 9.02e+04  |
---------------------------------------
Eval num_timesteps=986000, episode_reward=864.20 +/- 669.33
Episode length: 35.44 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 986000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 963      |
|    time_elapsed    | 3555     |
|    total_timesteps | 986112   |
---------------------------------
Eval num_timesteps=986500, episode_reward=669.36 +/- 569.83
Episode length: 34.02 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 669       |
| time/                   |           |
|    total_timesteps      | 986500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.87e-11 |
|    explained_variance   | 0.174     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.16e+04  |
|    n_updates            | 8129      |
|    policy_gradient_loss | 3.67e-10  |
|    value_loss           | 1e+05     |
---------------------------------------
Eval num_timesteps=987000, episode_reward=847.96 +/- 705.65
Episode length: 34.78 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 964      |
|    time_elapsed    | 3558     |
|    total_timesteps | 987136   |
---------------------------------
Eval num_timesteps=987500, episode_reward=790.63 +/- 657.75
Episode length: 34.42 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 791       |
| time/                   |           |
|    total_timesteps      | 987500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-13 |
|    explained_variance   | 0.19      |
|    learning_rate        | 0.0005    |
|    loss                 | 4.2e+04   |
|    n_updates            | 8139      |
|    policy_gradient_loss | -3.56e-09 |
|    value_loss           | 9.67e+04  |
---------------------------------------
Eval num_timesteps=988000, episode_reward=800.11 +/- 642.92
Episode length: 35.54 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 988000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 911      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 965      |
|    time_elapsed    | 3562     |
|    total_timesteps | 988160   |
---------------------------------
Eval num_timesteps=988500, episode_reward=802.99 +/- 680.85
Episode length: 35.18 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 803       |
| time/                   |           |
|    total_timesteps      | 988500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.28e-11 |
|    explained_variance   | 0.203     |
|    learning_rate        | 0.0005    |
|    loss                 | 6.3e+04   |
|    n_updates            | 8149      |
|    policy_gradient_loss | 1.86e-10  |
|    value_loss           | 1.05e+05  |
---------------------------------------
Eval num_timesteps=989000, episode_reward=894.36 +/- 709.84
Episode length: 35.48 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 966      |
|    time_elapsed    | 3565     |
|    total_timesteps | 989184   |
---------------------------------
Eval num_timesteps=989500, episode_reward=924.60 +/- 676.63
Episode length: 36.70 +/- 5.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 925       |
| time/                   |           |
|    total_timesteps      | 989500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.63e-12 |
|    explained_variance   | 0.255     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.63e+04  |
|    n_updates            | 8159      |
|    policy_gradient_loss | -9.78e-10 |
|    value_loss           | 7.55e+04  |
---------------------------------------
Eval num_timesteps=990000, episode_reward=903.89 +/- 757.66
Episode length: 35.00 +/- 7.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 831      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 967      |
|    time_elapsed    | 3569     |
|    total_timesteps | 990208   |
---------------------------------
Eval num_timesteps=990500, episode_reward=630.91 +/- 535.97
Episode length: 34.02 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 631       |
| time/                   |           |
|    total_timesteps      | 990500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.61e-10 |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.49e+04  |
|    n_updates            | 8169      |
|    policy_gradient_loss | -4.49e-09 |
|    value_loss           | 9.8e+04   |
---------------------------------------
Eval num_timesteps=991000, episode_reward=639.68 +/- 555.74
Episode length: 33.94 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 968      |
|    time_elapsed    | 3572     |
|    total_timesteps | 991232   |
---------------------------------
Eval num_timesteps=991500, episode_reward=907.94 +/- 689.35
Episode length: 36.42 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 908       |
| time/                   |           |
|    total_timesteps      | 991500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.64e-14 |
|    explained_variance   | 0.0609    |
|    learning_rate        | 0.0005    |
|    loss                 | 4.76e+04  |
|    n_updates            | 8179      |
|    policy_gradient_loss | -6.81e-10 |
|    value_loss           | 1.02e+05  |
---------------------------------------
Eval num_timesteps=992000, episode_reward=918.78 +/- 712.04
Episode length: 36.04 +/- 7.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 969      |
|    time_elapsed    | 3576     |
|    total_timesteps | 992256   |
---------------------------------
Eval num_timesteps=992500, episode_reward=913.23 +/- 706.76
Episode length: 36.78 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 913       |
| time/                   |           |
|    total_timesteps      | 992500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.59e-19 |
|    explained_variance   | 0.0562    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.42e+04  |
|    n_updates            | 8189      |
|    policy_gradient_loss | 2.68e-10  |
|    value_loss           | 1.2e+05   |
---------------------------------------
Eval num_timesteps=993000, episode_reward=754.73 +/- 673.49
Episode length: 33.86 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 910      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 970      |
|    time_elapsed    | 3579     |
|    total_timesteps | 993280   |
---------------------------------
Eval num_timesteps=993500, episode_reward=874.38 +/- 691.81
Episode length: 36.16 +/- 5.93
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.2     |
|    mean_reward          | 874      |
| time/                   |          |
|    total_timesteps      | 993500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.7e-14 |
|    explained_variance   | 0.264    |
|    learning_rate        | 0.0005   |
|    loss                 | 5.52e+04 |
|    n_updates            | 8199     |
|    policy_gradient_loss | 1.48e-09 |
|    value_loss           | 1.17e+05 |
--------------------------------------
Eval num_timesteps=994000, episode_reward=820.70 +/- 668.21
Episode length: 34.90 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 994000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 971      |
|    time_elapsed    | 3583     |
|    total_timesteps | 994304   |
---------------------------------
Eval num_timesteps=994500, episode_reward=771.85 +/- 682.69
Episode length: 34.58 +/- 7.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 772       |
| time/                   |           |
|    total_timesteps      | 994500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.19e-10 |
|    explained_variance   | 0.118     |
|    learning_rate        | 0.0005    |
|    loss                 | 4.64e+04  |
|    n_updates            | 8209      |
|    policy_gradient_loss | -2.21e-10 |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=995000, episode_reward=1001.31 +/- 759.13
Episode length: 35.44 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 972      |
|    time_elapsed    | 3586     |
|    total_timesteps | 995328   |
---------------------------------
Eval num_timesteps=995500, episode_reward=826.93 +/- 668.57
Episode length: 35.18 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 995500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.92e-12 |
|    explained_variance   | 0.089     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.18e+04  |
|    n_updates            | 8219      |
|    policy_gradient_loss | -1.24e-09 |
|    value_loss           | 9.43e+04  |
---------------------------------------
Eval num_timesteps=996000, episode_reward=981.54 +/- 760.64
Episode length: 35.96 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 982      |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 973      |
|    time_elapsed    | 3590     |
|    total_timesteps | 996352   |
---------------------------------
Eval num_timesteps=996500, episode_reward=823.19 +/- 701.21
Episode length: 34.66 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 823       |
| time/                   |           |
|    total_timesteps      | 996500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-09 |
|    explained_variance   | 0.267     |
|    learning_rate        | 0.0005    |
|    loss                 | 5.02e+04  |
|    n_updates            | 8229      |
|    policy_gradient_loss | 1.24e-09  |
|    value_loss           | 8.68e+04  |
---------------------------------------
Eval num_timesteps=997000, episode_reward=747.51 +/- 610.79
Episode length: 35.06 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 974      |
|    time_elapsed    | 3593     |
|    total_timesteps | 997376   |
---------------------------------
Eval num_timesteps=997500, episode_reward=828.49 +/- 663.49
Episode length: 35.50 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 997500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.91e-12 |
|    explained_variance   | 0.197     |
|    learning_rate        | 0.0005    |
|    loss                 | 3.17e+04  |
|    n_updates            | 8239      |
|    policy_gradient_loss | 1.98e-09  |
|    value_loss           | 7e+04     |
---------------------------------------
Eval num_timesteps=998000, episode_reward=772.69 +/- 665.72
Episode length: 34.52 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 998000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 808      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 975      |
|    time_elapsed    | 3597     |
|    total_timesteps | 998400   |
---------------------------------
Eval num_timesteps=998500, episode_reward=789.03 +/- 672.55
Episode length: 34.10 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 998500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-19 |
|    explained_variance   | 0.0457    |
|    learning_rate        | 0.0005    |
|    loss                 | 5.89e+04  |
|    n_updates            | 8249      |
|    policy_gradient_loss | 1.47e-09  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=999000, episode_reward=767.54 +/- 632.18
Episode length: 34.96 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 898      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 976      |
|    time_elapsed    | 3600     |
|    total_timesteps | 999424   |
---------------------------------
Eval num_timesteps=999500, episode_reward=839.00 +/- 644.75
Episode length: 36.04 +/- 5.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 999500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.24e-18 |
|    explained_variance   | 0.2       |
|    learning_rate        | 0.0005    |
|    loss                 | 4.49e+04  |
|    n_updates            | 8259      |
|    policy_gradient_loss | 6.93e-10  |
|    value_loss           | 1.04e+05  |
---------------------------------------
Eval num_timesteps=1000000, episode_reward=887.18 +/- 715.31
Episode length: 35.50 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 977      |
|    time_elapsed    | 3603     |
|    total_timesteps | 1000448  |
---------------------------------
